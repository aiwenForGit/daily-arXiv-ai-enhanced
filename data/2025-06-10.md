<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 452]
- [cs.CC](#cs.CC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 8]
- [stat.ML](#stat.ML) [Total: 18]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.AR](#cs.AR) [Total: 2]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]
- [eess.SY](#eess.SY) [Total: 6]
- [cs.CR](#cs.CR) [Total: 19]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [cs.CV](#cs.CV) [Total: 65]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.AI](#cs.AI) [Total: 30]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [astro-ph.SR](#astro-ph.SR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 42]
- [astro-ph.EP](#astro-ph.EP) [Total: 2]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.RO](#cs.RO) [Total: 12]
- [cs.SE](#cs.SE) [Total: 4]
- [eess.SP](#eess.SP) [Total: 19]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.DS](#cs.DS) [Total: 4]
- [stat.ME](#stat.ME) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.NI](#cs.NI) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.HC](#cs.HC) [Total: 4]
- [math.OC](#math.OC) [Total: 8]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Mixture-of-Experts Meets In-Context Reinforcement Learning](https://arxiv.org/abs/2506.05426)
*Wenhao Wu,Fuhong Liu,Haoru Li,Zican Hu,Daoyi Dong,Chunlin Chen,Zhi Wang*

Main category: cs.LG

TL;DR: T2MIR是一个基于混合专家（MoE）的框架，通过token-wise和task-wise MoE解决ICRL中的多模态和任务多样性问题，显著提升了上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 解决ICRL中状态-动作-奖励数据的多模态性和任务多样性带来的挑战。

Method: 提出T2MIR框架，包含token-wise MoE和task-wise MoE，并引入对比学习优化任务路由。

Result: 实验表明T2MIR显著优于基线方法，提升了上下文学习能力。

Conclusion: T2MIR为ICRL提供了一种简单且可扩展的架构改进，推动了其在语言和视觉领域的应用潜力。

Abstract: In-context reinforcement learning (ICRL) has emerged as a promising paradigm
for adapting RL agents to downstream tasks through prompt conditioning.
However, two notable challenges remain in fully harnessing in-context learning
within RL domains: the intrinsic multi-modality of the state-action-reward data
and the diverse, heterogeneous nature of decision tasks. To tackle these
challenges, we propose \textbf{T2MIR} (\textbf{T}oken- and \textbf{T}ask-wise
\textbf{M}oE for \textbf{I}n-context \textbf{R}L), an innovative framework that
introduces architectural advances of mixture-of-experts (MoE) into
transformer-based decision models. T2MIR substitutes the feedforward layer with
two parallel layers: a token-wise MoE that captures distinct semantics of input
tokens across multiple modalities, and a task-wise MoE that routes diverse
tasks to specialized experts for managing a broad task distribution with
alleviated gradient conflicts. To enhance task-wise routing, we introduce a
contrastive learning method that maximizes the mutual information between the
task and its router representation, enabling more precise capture of
task-relevant information. The outputs of two MoE components are concatenated
and fed into the next layer. Comprehensive experiments show that T2MIR
significantly facilitates in-context learning capacity and outperforms various
types of baselines. We bring the potential and promise of MoE to ICRL, offering
a simple and scalable architectural enhancement to advance ICRL one step closer
toward achievements in language and vision communities. Our code is available
at https://github.com/NJU-RL/T2MIR.

</details>


### [2] [MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction](https://arxiv.org/abs/2506.05427)
*Zishan Shu,Yufan Deng,Hongyu Zhang,Zhiwei Nie,Jie Chen*

Main category: cs.LG

TL;DR: MTPNet是一种多粒度目标感知网络，用于预测活性悬崖，通过结合分子与靶蛋白的交互知识，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法仅能处理单一结合目标，限制了预测模型的适用性。

Method: MTPNet包含宏观目标语义（MTS）和微观口袋语义（MPS）指导，动态优化分子表示。

Result: 在30个代表性数据集上，MTPNet平均RMSE提升18.95%。

Conclusion: MTPNet通过条件深度学习实现活性悬崖的统一预测，加速化合物优化与设计。

Abstract: Activity cliff prediction is a critical task in drug discovery and material
design. Existing computational methods are limited to handling single binding
targets, which restricts the applicability of these prediction models. In this
paper, we present the Multi-Grained Target Perception network (MTPNet) to
incorporate the prior knowledge of interactions between the molecules and their
target proteins. Specifically, MTPNet is a unified framework for activity cliff
prediction, which consists of two components: Macro-level Target Semantic (MTS)
guidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet
dynamically optimizes molecular representations through multi-grained protein
semantic conditions. To our knowledge, it is the first time to employ the
receptor proteins as guiding information to effectively capture critical
interaction details. Extensive experiments on 30 representative activity cliff
datasets demonstrate that MTPNet significantly outperforms previous approaches,
achieving an average RMSE improvement of 18.95% on top of several mainstream
GNN architectures. Overall, MTPNet internalizes interaction patterns through
conditional deep learning to achieve unified predictions of activity cliffs,
helping to accelerate compound optimization and design. Codes are available at:
https://github.com/ZishanShu/MTPNet.

</details>


### [3] [Diffusion with a Linguistic Compass: Steering the Generation of Clinically Plausible Future sMRI Representations for Early MCI Conversion Prediction](https://arxiv.org/abs/2506.05428)
*Zhihao Tang,Chaozhuo Li,Litian Zhang,Xi Zhang*

Main category: cs.LG

TL;DR: MCI-Diff是一种基于扩散的框架，通过基线数据合成未来sMRI表示，实现实时风险评估和高预测性能。


<details>
  <summary>Details</summary>
Motivation: 早期预测轻度认知障碍（MCI）转化面临即时性（从单次基线sMRI快速预测）与准确性（利用纵向扫描捕捉疾病进展）之间的权衡。

Method: 提出多任务序列重建策略训练共享去噪网络，结合LLM驱动的“语言指南”进行临床合理性采样。

Result: 在ADNI和AIBL数据集上，MCI-Diff优于现有方法，早期转化预测准确率提高5-12%。

Conclusion: MCI-Diff在实时性和准确性上取得平衡，为MCI转化预测提供高效解决方案。

Abstract: Early prediction of Mild Cognitive Impairment (MCI) conversion is hampered by
a trade-off between immediacy--making fast predictions from a single baseline
sMRI--and accuracy--leveraging longitudinal scans to capture disease
progression. We propose MCI-Diff, a diffusion-based framework that synthesizes
clinically plausible future sMRI representations directly from baseline data,
achieving both real-time risk assessment and high predictive performance.
First, a multi-task sequence reconstruction strategy trains a shared denoising
network on interpolation and extrapolation tasks to handle irregular follow-up
sampling and learn robust latent trajectories. Second, an LLM-driven
"linguistic compass" is introduced for clinical plausibility sampling:
generated feature candidates are quantized, tokenized, and scored by a
fine-tuned language model conditioned on expected structural biomarkers,
guiding autoregressive generation toward realistic disease patterns.
Experiments on ADNI and AIBL cohorts show that MCI-Diff outperforms
state-of-the-art baselines, improving early conversion accuracy by 5-12%.

</details>


### [4] [PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling](https://arxiv.org/abs/2506.05432)
*Yuxuan Yue,Zukang Xu,Zhihang Yuan,Dawei Yang,Jianglong Wu,Liqiang Nie*

Main category: cs.LG

TL;DR: 论文提出了一种名为PCDVQ的向量量化方法，通过解耦方向与幅度的量化，显著提升了边缘部署中大型语言模型的压缩效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有向量量化方法在量化方向时误差较大，且欧氏距离作为相似性度量更关注幅度误差，与方向对量化更敏感的特性相矛盾。

Method: 提出PCDVQ框架，包括极坐标解耦（PCD）和分布对齐码本构建（DACC），分别独立量化方向与幅度，并优化码本分布。

Result: 实验表明，PCDVQ在2-bit量化水平下比基线方法至少提升1.5%的零样本准确率。

Conclusion: PCDVQ为高压缩且准确的大型语言模型提供了一种新范式。

Abstract: Large Language Models (LLMs) face significant challenges in edge deployment
due to their massive parameter scale. Vector Quantization (VQ), a
clustering-based quantization method, serves as a prevalent solution to this
issue for its extremely low-bit (even at 2-bit) and considerable accuracy.
Since a vector is a quantity in mathematics and physics that has both direction
and magnitude, existing VQ works typically quantize them in a coupled manner.
However, we find that direction exhibits significantly greater sensitivity to
quantization compared to the magnitude. For instance, when separately
clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the
accuracy drop of zero-shot tasks are 46.5\% and 2.3\%, respectively. This gap
even increases with the reduction of clustering centers. Further, Euclidean
distance, a common metric to access vector similarities in current VQ works,
places greater emphasis on reducing the magnitude error. This property is
contrary to the above finding, unavoidably leading to larger quantization
errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector
Quantization (PCDVQ), an effective and efficient VQ framework consisting of two
key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors
into their polar coordinate representations and perform independent
quantization of the direction and magnitude parameters.2) Distribution Aligned
Codebook Construction (DACC), which optimizes the direction and magnitude
codebooks in accordance with the source distribution. Experimental results show
that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\%
zero-shot accuracy, establishing a novel paradigm for accurate and highly
compressed LLMs.

</details>


### [5] [Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward](https://arxiv.org/abs/2506.05433)
*Zikang Liu,Tongtian Yue,Yepeng Tang,Longteng Guo,Junxian Cai,Qingbin Liu,Xi Chen,Jing Liu*

Main category: cs.LG

TL;DR: Prefix Grouper是一种高效的GRPO训练算法，通过共享前缀计算减少冗余，提升长上下文学习场景的效率。


<details>
  <summary>Details</summary>
Motivation: GRPO在处理长共享前缀时存在计算冗余问题，限制了其在大规模任务中的可扩展性。

Method: 提出Shared-Prefix Forward策略，将自注意力分为两部分，共享前缀仅需编码一次，同时保持端到端训练兼容性。

Result: Prefix Grouper在保持训练等效性的同时显著降低计算成本，尤其在长前缀场景中。

Conclusion: Prefix Grouper是一种即插即用的高效方法，可提升GRPO在复杂任务和大模型中的可扩展性。

Abstract: Group Relative Policy Optimization (GRPO) enhances policy learning by
computing gradients from relative comparisons among candidate outputs that
share a common input prefix. Despite its effectiveness, GRPO introduces
substantial computational overhead when processing long shared prefixes, which
must be redundantly encoded for each group member. This inefficiency becomes a
major scalability bottleneck in long-context learning scenarios. We propose
Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant
prefix computation via a Shared-Prefix Forward strategy. In particular, by
restructuring self-attention into two parts, our method enables the shared
prefix to be encoded only once, while preserving full differentiability and
compatibility with end-to-end training. We provide both theoretical and
empirical evidence that Prefix Grouper is training-equivalent to standard GRPO:
it yields identical forward outputs and backward gradients, ensuring that the
optimization dynamics and final policy performance remain unchanged.
Empirically, our experiments confirm that Prefix Grouper achieves consistent
results while significantly reducing the computational cost of training,
particularly in long-prefix scenarios. The proposed method is fully
plug-and-play: it is compatible with existing GRPO-based architectures and can
be seamlessly integrated into current training pipelines as a drop-in
replacement, requiring no structural modifications and only minimal changes to
input construction and attention computation. Prefix Grouper enables the use of
larger group sizes under the same computational budget, thereby improving the
scalability of GRPO to more complex tasks and larger models. Code is now
available at https://github.com/johncaged/PrefixGrouper

</details>


### [6] [Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks](https://arxiv.org/abs/2506.05434)
*Thomas Massena,Léo andéol,Thibaut Boissin,Franck Mamalet,Corentin Friedrich,Mathieu Serrurier,Sébastien Gerchinovitz*

Main category: cs.LG

TL;DR: 本文提出了一种名为lip-rcp的新方法，利用Lipschitz有界网络高效估计鲁棒CP集，在中大规模场景（如ImageNet）中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统CP方法在对抗攻击下失效，现有鲁棒CP方法在大规模问题中计算成本高或结果集过大。

Method: 结合1-Lipschitz鲁棒网络，提出lip-rcp方法，精确高效地估计鲁棒CP集。

Result: lip-rcp在鲁棒CP集大小和计算效率上优于现有方法，同时为普通CP集提供了新的最坏情况覆盖边界。

Conclusion: lip-rcp方法在保持高效的同时提供了鲁棒性保证，适用于大规模场景。

Abstract: Conformal Prediction (CP) has proven to be an effective post-hoc method for
improving the trustworthiness of neural networks by providing prediction sets
with finite-sample guarantees. However, under adversarial attacks, classical
conformal guarantees do not hold anymore: this problem is addressed in the
field of Robust Conformal Prediction. Several methods have been proposed to
provide robust CP sets with guarantees under adversarial perturbations, but,
for large scale problems, these sets are either too large or the methods are
too computationally demanding to be deployed in real life scenarios. In this
work, we propose a new method that leverages Lipschitz-bounded networks to
precisely and efficiently estimate robust CP sets. When combined with a
1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms
state-of-the-art results in both the size of the robust CP sets and
computational efficiency in medium and large-scale scenarios such as ImageNet.
Taking a different angle, we also study vanilla CP under attack, and derive new
worst-case coverage bounds of vanilla CP sets, which are valid simultaneously
for all adversarial attack levels. Our lip-rcp method makes this second
approach as efficient as vanilla CP while also allowing robustness guarantees.

</details>


### [7] [Event Classification of Accelerometer Data for Industrial Package Monitoring with Embedded Deep Learning](https://arxiv.org/abs/2506.05435)
*Manon Renault,Hamoud Younes,Hugo Tessier,Ronan Le Roy,Bastien Pasdeloup,Mathieu Léonardon*

Main category: cs.LG

TL;DR: 提出了一种基于嵌入式系统的包装状态监测方法，通过深度学习模型分类加速度计数据，优化设备寿命和模型大小。


<details>
  <summary>Details</summary>
Motivation: 工业应用中包装监测对效率和可持续性至关重要，需设计长寿命系统。

Method: 使用一维卷积神经网络处理不平衡多类时间序列数据，测试数据增强技术并压缩模型。

Result: 模型在两分类问题中精度达94.54%和95.83%，模型大小减少四倍，功耗316mW。

Conclusion: 方法有效实现长寿命、高效能的包装状态监测系统。

Abstract: Package monitoring is an important topic in industrial applications, with
significant implications for operational efficiency and ecological
sustainability. In this study, we propose an approach that employs an embedded
system, placed on reusable packages, to detect their state (on a Forklift, in a
Truck, or in an undetermined location). We aim to design a system with a
lifespan of several years, corresponding to the lifespan of reusable packages.
Our analysis demonstrates that maximizing device lifespan requires minimizing
wake time. We propose a pipeline that includes data processing, training, and
evaluation of the deep learning model designed for imbalanced, multiclass time
series data collected from an embedded sensor. The method uses a
one-dimensional Convolutional Neural Network architecture to classify
accelerometer data from the IoT device. Before training, two data augmentation
techniques are tested to solve the imbalance problem of the dataset: the
Synthetic Minority Oversampling TEchnique and the ADAptive SYNthetic sampling
approach. After training, compression techniques are implemented to have a
small model size. On the considered twoclass problem, the methodology yields a
precision of 94.54% for the first class and 95.83% for the second class, while
compression techniques reduce the model size by a factor of four. The trained
model is deployed on the IoT device, where it operates with a power consumption
of 316 mW during inference.

</details>


### [8] [An Unsupervised Framework for Dynamic Health Indicator Construction and Its Application in Rolling Bearing Prognostics](https://arxiv.org/abs/2506.05438)
*Tongda Sun,Chen Yin,Huailiang Zheng,Yining Dong*

Main category: cs.LG

TL;DR: 提出了一种新的动态健康指标（HI）构建方法，通过无监督框架自动提取退化特征并建模时序依赖性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有健康指标构建方法依赖专家知识且忽略动态信息，限制了退化趋势表示和预测能力。

Method: 采用基于跳跃连接的自编码器提取退化特征，并嵌入HI预测块构建动态HI，显式建模时序依赖性。

Result: 在两个轴承生命周期数据集上的实验表明，该方法优于对比方法，动态HI在预测任务中表现更优。

Conclusion: 提出的动态HI构建方法能有效捕捉退化过程的动态内容，提升退化趋势建模和预测能力。

Abstract: Health indicator (HI) plays a key role in degradation assessment and
prognostics of rolling bearings. Although various HI construction methods have
been investigated, most of them rely on expert knowledge for feature extraction
and overlook capturing dynamic information hidden in sequential degradation
processes, which limits the ability of the constructed HI for degradation trend
representation and prognostics. To address these concerns, a novel dynamic HI
that considers HI-level temporal dependence is constructed through an
unsupervised framework. Specifically, a degradation feature learning module
composed of a skip-connection-based autoencoder first maps raw signals to a
representative degradation feature space (DFS) to automatically extract
essential degradation features without the need for expert knowledge.
Subsequently, in this DFS, a new HI-generating module embedded with an inner
HI-prediction block is proposed for dynamic HI construction, where the temporal
dependence between past and current HI states is guaranteed and modeled
explicitly. On this basis, the dynamic HI captures the inherent dynamic
contents of the degradation process, ensuring its effectiveness for degradation
tendency modeling and future degradation prognostics. The experiment results on
two bearing lifecycle datasets demonstrate that the proposed HI construction
method outperforms comparison methods, and the constructed dynamic HI is
superior for prognostic tasks.

</details>


### [9] [UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss](https://arxiv.org/abs/2506.05443)
*Yiyu Lin,Yan Wang,You Zhou,Xinye Ni,Jiahui Wu,Sen Yang*

Main category: cs.LG

TL;DR: UniPTMs是一个统一的多类型蛋白质翻译后修饰（PTM）预测框架，通过创新的双路径协作架构和多种模块优化特征融合与预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在跨模态特征融合、领域泛化和架构优化方面存在局限，需要更精确的PTM预测方法。

Method: 提出UniPTMs框架，采用“主-从”双路径协作架构，结合BGCA、LDFN、MACP、BHGFN和HDWF等模块，优化特征融合与预测。

Result: UniPTMs在五种修饰类型上表现优于现有模型（MCC提高3.2%-11.4%，AP提高4.2%-14.3%），并开发了轻量版UniPTMs-mini。

Conclusion: UniPTMs通过多模态特征融合和架构创新，显著提升了PTM预测性能，同时兼顾模型复杂度与性能平衡。

Abstract: As a core mechanism of epigenetic regulation in eukaryotes, protein
post-translational modifications (PTMs) require precise prediction to decipher
dynamic life activity networks. To address the limitations of existing deep
learning models in cross-modal feature fusion, domain generalization, and
architectural optimization, this study proposes UniPTMs: the first unified
framework for multi-type PTM prediction. The framework innovatively establishes
a "Master-Slave" dual-path collaborative architecture: The master path
dynamically integrates high-dimensional representations of protein sequences,
structures, and evolutionary information through a Bidirectional Gated
Cross-Attention (BGCA) module, while the slave path optimizes feature
discrepancies and recalibration between structural and traditional features
using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale
Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and
a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level
feature integration across paths, the framework employs a Hierarchical Dynamic
Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal
features. Enhanced by a novel Hierarchical Contrastive loss function for
feature consistency optimization, UniPTMs demonstrates significant performance
improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art
models across five modification types and transcends the Single-Type Prediction
Paradigm. To strike a balance between model complexity and performance, we have
also developed a lightweight variant named UniPTMs-mini.

</details>


### [10] [Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic](https://arxiv.org/abs/2506.05445)
*Thanh Vinh Vo,Young Lee,Haozhe Ma,Chien Lu,Tze-Yun Leong*

Main category: cs.LG

TL;DR: DoSAC是一种基于因果干预的强化学习算法，通过后门调整解决隐藏混杂变量问题，提升策略的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 隐藏混杂变量会影响状态和动作，导致传统强化学习算法学习到有偏的策略，无法泛化。

Method: 提出DoSAC算法，利用后门重构器从观测数据中推断伪历史变量，实现因果干预估计。

Result: 在连续控制任务中，DoSAC在混杂环境下表现优于基线方法，具有更好的鲁棒性和泛化性。

Conclusion: DoSAC通过因果干预有效解决了隐藏混杂变量问题，提升了策略学习的可靠性。

Abstract: Hidden confounders that influence both states and actions can bias policy
learning in reinforcement learning (RL), leading to suboptimal or
non-generalizable behavior. Most RL algorithms ignore this issue, learning
policies from observational trajectories based solely on statistical
associations rather than causal effects. We propose DoSAC (Do-Calculus Soft
Actor-Critic with Backdoor Adjustment), a principled extension of the SAC
algorithm that corrects for hidden confounding via causal intervention
estimation. DoSAC estimates the interventional policy $\pi(a | \mathrm{do}(s))$
using the backdoor criterion, without requiring access to true confounders or
causal labels. To achieve this, we introduce a learnable Backdoor Reconstructor
that infers pseudo-past variables (previous state and action) from the current
state to enable backdoor adjustment from observational data. This module is
integrated into a soft actor-critic framework to compute both the
interventional policy and its entropy. Empirical results on continuous control
benchmarks show that DoSAC outperforms baselines under confounded settings,
with improved robustness, generalization, and policy reliability.

</details>


### [11] [Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning](https://arxiv.org/abs/2506.05447)
*Andrei Mircea,Supriyo Chakraborty,Nima Chitsazan,Irina Rish,Ekaterina Lobacheva*

Main category: cs.LG

TL;DR: 研究发现语言模型在训练早期会出现损失减速现象，模型规模的扩大可以缓解这一现象。


<details>
  <summary>Details</summary>
Motivation: 理解模型规模如何通过训练动态改善语言模型性能。

Method: 分析语言模型训练中的损失曲线，提出零和学习（ZSL）概念解释损失减速现象。

Result: 模型规模扩大能降低损失减速发生时的损失值，并改善减速后的损失改进速率。

Conclusion: 损失减速和ZSL现象为语言模型训练动态提供了新见解，可能直接用于改进模型性能。

Abstract: This work aims to understand how scaling improves language models,
specifically in terms of training dynamics. We find that language models
undergo loss deceleration early in training; an abrupt slowdown in the rate of
loss improvement, resulting in piecewise linear behaviour of the loss curve in
log-log space. Scaling up the model mitigates this transition by (1) decreasing
the loss at which deceleration occurs, and (2) improving the log-log rate of
loss improvement after deceleration. We attribute loss deceleration to a type
of degenerate training dynamics we term zero-sum learning (ZSL). In ZSL,
per-example gradients become systematically opposed, leading to destructive
interference in per-example changes in loss. As a result, improving loss on one
subset of examples degrades it on another, bottlenecking overall progress. Loss
deceleration and ZSL provide new insights into the training dynamics underlying
language model scaling laws, and could potentially be targeted directly to
improve language models independent of scale. We make our code and artefacts
available at: https://github.com/mirandrom/zsl

</details>


### [12] [Zeroth-Order Optimization Finds Flat Minima](https://arxiv.org/abs/2506.05454)
*Liang Zhang,Bingcong Li,Kiran Koshy Thekumparampil,Sewoong Oh,Michael Muehlebach,Niao He*

Main category: cs.LG

TL;DR: 零阶优化方法在机器学习中广泛应用，本文研究其隐式正则化特性，发现标准两点估计器倾向于选择Hessian矩阵迹较小的解（即平坦最小值），并给出了收敛速率。


<details>
  <summary>Details</summary>
Motivation: 研究零阶优化方法在梯度不可行或计算昂贵时的隐式正则化特性，填补现有理论对最终解特性的理解空白。

Method: 使用标准两点估计器进行零阶优化，分析其对Hessian矩阵迹的影响，并定义平坦最小值。

Result: 理论证明零阶优化倾向于平坦最小值，实验在二分类任务和语言模型微调中验证了结果。

Conclusion: 零阶优化方法隐式偏好平坦最小值，为相关应用提供了理论支持。

Abstract: Zeroth-order methods are extensively used in machine learning applications
where gradients are infeasible or expensive to compute, such as black-box
attacks, reinforcement learning, and language model fine-tuning. Existing
optimization theory focuses on convergence to an arbitrary stationary point,
but less is known on the implicit regularization that provides a fine-grained
characterization on which particular solutions are finally reached. We show
that zeroth-order optimization with the standard two-point estimator favors
solutions with small trace of Hessian, which is widely used in previous work to
distinguish between sharp and flat minima. We further provide convergence rates
of zeroth-order optimization to approximate flat minima for convex and
sufficiently smooth functions, where flat minima are defined as the minimizers
that achieve the smallest trace of Hessian among all optimal solutions.
Experiments on binary classification tasks with convex losses and language
model fine-tuning support our theoretical findings.

</details>


### [13] [Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors](https://arxiv.org/abs/2506.05479)
*Matei Gabriel Coşa,Marek Eliáš*

Main category: cs.LG

TL;DR: 本文研究了在Metrical Task Systems（MTS）中如何通过动态选择启发式算法来达到最佳性能，提出了一个基于遗憾的算法，并证明了其上下界。


<details>
  <summary>Details</summary>
Motivation: 在MTS中，不同启发式算法可能适用于不同类型的输入实例，但每次只能查询一个算法的动作。目标是设计一种方法，使其性能接近最佳启发式算法。

Method: 通过动态选择启发式算法，并在每次时间步仅查询一个算法的动作，同时考虑算法之间的成本关联性。

Result: 提出了一种算法，实现了$O(\text{OPT}^{2/3})$的遗憾，并基于Dekel等人的构造证明了其下界是紧的。

Conclusion: 本文为MTS中的动态启发式选择问题提供了有效的解决方案，并通过理论分析验证了其性能极限。

Abstract: We consider the following problem: We are given $\ell$ heuristics for
Metrical Task Systems (MTS), where each might be tailored to a different type
of input instances. While processing an input instance received online, we are
allowed to query the action of only one of the heuristics at each time step.
Our goal is to achieve performance comparable to the best of the given
heuristics. The main difficulty of our setting comes from the fact that the
cost paid by a heuristic at time $t$ cannot be estimated unless the same
heuristic was also queried at time $t-1$. This is related to Bandit Learning
against memory bounded adversaries (Arora et al., 2012). We show how to achieve
regret of $O(\text{OPT}^{2/3})$ and prove a tight lower bound based on the
construction of Dekel et al. (2013).

</details>


### [14] [Initial Model Incorporation for Deep Learning FWI: Pretraining or Denormalization?](https://arxiv.org/abs/2506.05484)
*Ruihua Chen,Bangyu Wu,Meng Li,Kai Yang*

Main category: cs.LG

TL;DR: 该论文研究了两种将初始模型知识嵌入神经网络的方法（预训练和去归一化）对全波形反演（FWI）的影响，发现去归一化方法更优。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在神经网络参数化FWI中更有效地利用初始模型知识，以提高反演稳定性和准确性。

Method: 比较预训练和去归一化两种初始模型嵌入方法，分析其对FWI的影响。

Result: 去归一化方法简化了工作流程，加速了收敛，并提高了反演精度。

Conclusion: 去归一化方法优于预训练，更适合神经网络参数化FWI。

Abstract: Subsurface property neural network reparameterized full waveform inversion
(FWI) has emerged as an effective unsupervised learning framework, which can
invert stably with an inaccurate starting model. It updates the trainable
neural network parameters instead of fine-tuning on the subsurface model
directly. There are primarily two ways to embed the prior knowledge of the
initial model into neural networks, that is, pretraining and denormalization.
Pretraining first regulates the neural networks' parameters by fitting the
initial velocity model; Denormalization directly adds the outputs of the
network into the initial models without pretraining. In this letter, we
systematically investigate the influence of the two ways of initial model
incorporation for the neural network reparameterized FWI. We demonstrate that
pretraining requires inverting the model perturbation based on a constant
velocity value (mean) with a two-stage implementation. It leads to a complex
workflow and inconsistency of objective functions in the two-stage process,
causing the network parameters to become inactive and lose plasticity.
Experimental results demonstrate that denormalization can simplify workflows,
accelerate convergence, and enhance inversion accuracy compared with
pretraining.

</details>


### [15] [Conformal Prediction Beyond the Seen: A Missing Mass Perspective for Uncertainty Quantification in Generative Models](https://arxiv.org/abs/2506.05497)
*Sima Noorani,Shayan Kiyani,George Pappas,Hamed Hassani*

Main category: cs.LG

TL;DR: 论文提出了Conformal Prediction with Query Oracle (CPQ)框架，用于在仅查询黑盒生成模型的情况下量化不确定性，优化覆盖率、查询预算和信息量之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型（如大语言模型）在高风险应用中需要不确定性量化（UQ），但传统方法依赖结构化输出，无法直接适用。

Method: CPQ基于两个核心原则：最优查询策略和最优映射方法，均源于统计中的缺失质量问题，并开发了新估计器。

Result: 实验表明CPQ适用于任何黑盒大语言模型，能生成比现有方法更信息丰富的预测集。

Conclusion: CPQ为生成模型的不确定性量化提供了新范式，尤其在开放任务中表现优异。

Abstract: Uncertainty quantification (UQ) is essential for safe deployment of
generative AI models such as large language models (LLMs), especially in high
stakes applications. Conformal prediction (CP) offers a principled uncertainty
quantification framework, but classical methods focus on regression and
classification, relying on geometric distances or softmax scores: tools that
presuppose structured outputs. We depart from this paradigm by studying CP in a
query only setting, where prediction sets must be constructed solely from
finite queries to a black box generative model, introducing a new trade off
between coverage, test time query budget, and informativeness. We introduce
Conformal Prediction with Query Oracle (CPQ), a framework characterizing the
optimal interplay between these objectives. Our finite sample algorithm is
built on two core principles: one governs the optimal query policy, and the
other defines the optimal mapping from queried samples to prediction sets.
Remarkably, both are rooted in the classical missing mass problem in
statistics. Specifically, the optimal query policy depends on the rate of
decay, or the derivative, of the missing mass, for which we develop a novel
estimator. Meanwhile, the optimal mapping hinges on the missing mass itself,
which we estimate using Good Turing estimators. We then turn our focus to
implementing our method for language models, where outputs are vast, variable,
and often under specified. Fine grained experiments on three real world open
ended tasks and two LLMs, show CPQ applicability to any black box LLM and
highlight: (1) individual contribution of each principle to CPQ performance,
and (2) CPQ ability to yield significantly more informative prediction sets
than existing conformal methods for language uncertainty quantification.

</details>


### [16] [The Generative Leap: Sharp Sample Complexity for Efficiently Learning Gaussian Multi-Index Models](https://arxiv.org/abs/2506.05500)
*Alex Damian,Jason D. Lee,Joan Bruna*

Main category: cs.LG

TL;DR: 论文研究了高斯多指数模型的高效无监督估计方法，提出了生成跳跃指数，并证明了其样本复杂度的必要性和充分性。


<details>
  <summary>Details</summary>
Motivation: 研究高斯多指数模型中低维子空间的高效无监督估计问题。

Method: 引入生成跳跃指数，基于低次多项式框架分析样本复杂度，并提出基于Hermite张量的谱U统计量的序贯估计方法。

Result: 证明了样本复杂度的必要性和充分性，并计算了生成跳跃指数在多种模型中的具体值。

Conclusion: 提出的方法在多指数模型中高效且无需先验知识，适用于多种神经网络结构。

Abstract: In this work we consider generic Gaussian Multi-index models, in which the
labels only depend on the (Gaussian) $d$-dimensional inputs through their
projection onto a low-dimensional $r = O_d(1)$ subspace, and we study efficient
agnostic estimation procedures for this hidden subspace. We introduce the
\emph{generative leap} exponent $k^\star$, a natural extension of the
generative exponent from [Damian et al.'24] to the multi-index setting. We
first show that a sample complexity of $n=\Theta(d^{1 \vee \k/2})$ is necessary
in the class of algorithms captured by the Low-Degree-Polynomial framework. We
then establish that this sample complexity is also sufficient, by giving an
agnostic sequential estimation procedure (that is, requiring no prior knowledge
of the multi-index model) based on a spectral U-statistic over appropriate
Hermite tensors. We further compute the generative leap exponent for several
examples including piecewise linear functions (deep ReLU networks with bias),
and general deep neural networks (with $r$-dimensional first hidden layer).

</details>


### [17] [Geometric and Physical Constraints Synergistically Enhance Neural PDE Surrogates](https://arxiv.org/abs/2506.05513)
*Yunfei Huang,David S. Greenberg*

Main category: cs.LG

TL;DR: 论文提出了一种新的输入和输出层设计，以在交错网格上满足物理定律和对称性，并系统研究了这些约束对PDE替代模型精度的影响。结果显示，对称性和物理约束显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经PDE替代模型在新初始条件和时间累积误差上泛化能力差，而物理和对称约束虽有效但无法兼容交错网格。

Method: 设计了兼容交错网格的输入和输出层，结合对称性和物理约束，研究了其对PDE替代模型的影响。

Result: 对称性和物理约束显著提升了模型性能，尤其是在泛化能力和长期预测上。

Conclusion: 结合对称性和物理约束的模型表现最佳，优于数据增强或推前训练基线，且在真实世界数据中表现更好。

Abstract: Neural PDE surrogates can improve the cost-accuracy tradeoff of classical
solvers, but often generalize poorly to new initial conditions and accumulate
errors over time. Physical and symmetry constraints have shown promise in
closing this performance gap, but existing techniques for imposing these
inductive biases are incompatible with the staggered grids commonly used in
computational fluid dynamics. Here we introduce novel input and output layers
that respect physical laws and symmetries on the staggered grids, and for the
first time systematically investigate how these constraints, individually and
in combination, affect the accuracy of PDE surrogates. We focus on two
challenging problems: shallow water equations with closed boundaries and
decaying incompressible turbulence. Compared to strong baselines, symmetries
and physical constraints consistently improve performance across tasks,
architectures, autoregressive prediction steps, accuracy measures, and network
sizes. Symmetries are more effective than physical constraints, but surrogates
with both performed best, even compared to baselines with data augmentation or
pushforward training, while themselves benefiting from the pushforward trick.
Doubly-constrained surrogates also generalize better to initial conditions and
durations beyond the range of the training data, and more accurately predict
real-world ocean currents.

</details>


### [18] [Winner-takes-all for Multivariate Probabilistic Time Series Forecasting](https://arxiv.org/abs/2506.05515)
*Adrien Cortés,Rémi Rehm,Victor Letzelter*

Main category: cs.LG

TL;DR: TimeMCL利用多选择学习（MCL）范式预测多个可能的时间序列未来，通过多头神经网络和WTA损失提升预测多样性。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列预测中的模糊性和多样性问题，MCL因其简单性和处理模糊任务的能力受到关注。

Method: 采用多头神经网络和WTA损失，适应MCL框架用于时间序列预测。

Result: 在合成数据和真实时间序列上表现良好，计算成本低。

Conclusion: TimeMCL是一种高效且多样化的时间序列预测方法。

Abstract: We introduce TimeMCL, a method leveraging the Multiple Choice Learning (MCL)
paradigm to forecast multiple plausible time series futures. Our approach
employs a neural network with multiple heads and utilizes the Winner-Takes-All
(WTA) loss to promote diversity among predictions. MCL has recently gained
attention due to its simplicity and ability to address ill-posed and ambiguous
tasks. We propose an adaptation of this framework for time-series forecasting,
presenting it as an efficient method to predict diverse futures, which we
relate to its implicit quantization objective. We provide insights into our
approach using synthetic data and evaluate it on real-world time series,
demonstrating its promising performance at a light computational cost.

</details>


### [19] [On Fitting Flow Models with Large Sinkhorn Couplings](https://arxiv.org/abs/2506.05526)
*Michal Klein,Alireza Mousavi-Hosseini,Stephen Zhang,Marco Cuturi*

Main category: cs.LG

TL;DR: 论文探讨了如何通过优化传输（OT）方法改进流模型的训练效率，特别是通过大规模Sinkhorn耦合和低熵正则化。


<details>
  <summary>Details</summary>
Motivation: 流模型在无配对数据（如从噪声生成数据）时训练困难，传统方法效率低且推理成本高。

Method: 采用大规模Sinkhorn耦合（批量大小增加3-4个数量级）和低熵正则化，利用多GPU节点扩展计算。

Result: 在合成和图像生成任务中，流模型性能显著提升。

Conclusion: 大规模Sinkhorn耦合和低熵正则化能有效提高流模型的训练效率和性能。

Abstract: Flow models transform data gradually from one modality (e.g. noise) onto
another (e.g. images). Such models are parameterized by a time-dependent
velocity field, trained to fit segments connecting pairs of source and target
points. When the pairing between source and target points is given, training
flow models boils down to a supervised regression problem. When no such pairing
exists, as is the case when generating data from noise, training flows is much
harder. A popular approach lies in picking source and target points
independently. This can, however, lead to velocity fields that are slow to
train, but also costly to integrate at inference time. In theory, one would
greatly benefit from training flow models by sampling pairs from an optimal
transport (OT) measure coupling source and target, since this would lead to a
highly efficient flow solving the Benamou and Brenier dynamical OT problem. In
practice, recent works have proposed to sample mini-batches of $n$ source and
$n$ target points and reorder them using an OT solver to form better pairs.
These works have advocated using batches of size $n\approx 256$, and considered
OT solvers that return couplings that are either sharp (using e.g. the
Hungarian algorithm) or blurred (using e.g. entropic regularization, a.k.a.
Sinkhorn). We follow in the footsteps of these works by exploring the benefits
of increasing $n$ by three to four orders of magnitude, and look more carefully
on the effect of the entropic regularization $\varepsilon$ used in the Sinkhorn
algorithm. Our analysis is facilitated by new scale invariant quantities to
report the sharpness of a coupling, while our sharded computations across
multiple GPU or GPU nodes allow scaling up $n$. We show that in both synthetic
and image generation tasks, flow models greatly benefit when fitted with large
Sinkhorn couplings, with a low entropic regularization $\varepsilon$.

</details>


### [20] [Spectral Graph Neural Networks are Incomplete on Graphs with a Simple Spectrum](https://arxiv.org/abs/2506.05530)
*Snir Hordan,Maya Bechler-Speicher,Gur Lifshitz,Nadav Dym*

Main category: cs.LG

TL;DR: 论文提出了一种基于图谱的GNN表达能力评估方法，并通过理论和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架（如k-WL和同态计数）与图谱对齐性差，难以准确评估SGNN的表达能力。

Method: 利用图的最大特征值多重性分类方法，提出SGNN的表达能力层次结构，并通过旋转等变神经网络改进SGNN的表达能力。

Result: 证明了许多SGNN在具有不同特征值的图上仍不完整，并提出了一种改进方法。

Conclusion: 提出的方法在理论和实验上均验证了其有效性，为SGNN的表达能力提供了新的评估和改进方向。

Abstract: Spectral features are widely incorporated within Graph Neural Networks (GNNs)
to improve their expressive power, or their ability to distinguish among
non-isomorphic graphs. One popular example is the usage of graph Laplacian
eigenvectors for positional encoding in MPNNs and Graph Transformers. The
expressive power of such Spectrally-enhanced GNNs (SGNNs) is usually evaluated
via the k-WL graph isomorphism test hierarchy and homomorphism counting. Yet,
these frameworks align poorly with the graph spectra, yielding limited insight
into SGNNs' expressive power. We leverage a well-studied paradigm of
classifying graphs by their largest eigenvalue multiplicity to introduce an
expressivity hierarchy for SGNNs. We then prove that many SGNNs are incomplete
even on graphs with distinct eigenvalues. To mitigate this deficiency, we adapt
rotation equivariant neural networks to the graph spectra setting to propose a
method to provably improve SGNNs' expressivity on simple spectrum graphs. We
empirically verify our theoretical claims via an image classification
experiment on the MNIST Superpixel dataset and eigenvector canonicalization on
graphs from ZINC.

</details>


### [21] [SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful Deepfake Content on Social Media Platforms](https://arxiv.org/abs/2506.05538)
*Arnesh Batra,Anushk Kumar,Jashn Khemani,Arush Gumber,Arhan Jain,Somil Gupta*

Main category: cs.LG

TL;DR: 论文提出SocialDF数据集和基于LLM的多因素检测方法，以应对社交媒体上深度伪造技术的挑战。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的快速发展带来了娱乐和可访问性的机会，但也成为传播虚假信息的工具，现有检测方法难以区分良性深度伪造和恶意操纵内容。

Method: 提出SocialDF数据集，包含多种在线生态系统的深度伪造内容，并设计基于LLM的多因素检测方法，结合面部识别、语音转录和多代理LLM管道进行交叉验证。

Result: 通过多模态验证技术（语言、行为和上下文分析），能有效区分合成媒体和真实内容。

Conclusion: SocialDF数据集和LLM多因素检测方法为深度伪造检测提供了更全面的解决方案。

Abstract: The rapid advancement of deep generative models has significantly improved
the realism of synthetic media, presenting both opportunities and security
challenges. While deepfake technology has valuable applications in
entertainment and accessibility, it has emerged as a potent vector for
misinformation campaigns, particularly on social media. Existing detection
frameworks struggle to distinguish between benign and adversarially generated
deepfakes engineered to manipulate public perception. To address this
challenge, we introduce SocialDF, a curated dataset reflecting real-world
deepfake challenges on social media platforms. This dataset encompasses
high-fidelity deepfakes sourced from various online ecosystems, ensuring broad
coverage of manipulative techniques. We propose a novel LLM-based multi-factor
detection approach that combines facial recognition, automated speech
transcription, and a multi-agent LLM pipeline to cross-verify audio-visual
cues. Our methodology emphasizes robust, multi-modal verification techniques
that incorporate linguistic, behavioral, and contextual analysis to effectively
discern synthetic media from authentic content.

</details>


### [22] [Agentomics-ML: Autonomous Machine Learning Experimentation Agent for Genomic and Transcriptomic Data](https://arxiv.org/abs/2506.05542)
*Vlastimil Martinek,Andrea Gariboldi,Dimosthenis Tzimotoudis,Aitor Alberdi Escudero,Edward Blake,David Cechak,Luke Cassar,Alessandro Balestrucci,Panagiotis Alexiou*

Main category: cs.LG

TL;DR: Agentomics-ML是一种全自动代理系统，用于生成分类模型和可重复训练与推理的文件，在计算生物学数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生物数据集的复杂性需要自动化方法生成通用预测模型，现有方法在异构数据集上泛化能力不足。

Method: 系统通过Bash与文件系统交互，完成ML实验步骤，并通过反馈调整数据表示、模型架构和超参数。

Result: 在多个基准数据集上表现优于现有代理方法，并在一个数据集上达到最先进性能。

Conclusion: Agentomics-ML缩小了全自动系统与专家构建模型的差距，代码已开源。

Abstract: The adoption of machine learning (ML) and deep learning methods has
revolutionized molecular medicine by driving breakthroughs in genomics,
transcriptomics, drug discovery, and biological systems modeling. The
increasing quantity, multimodality, and heterogeneity of biological datasets
demand automated methods that can produce generalizable predictive models.
Recent developments in large language model-based agents have shown promise for
automating end-to-end ML experimentation on structured benchmarks. However,
when applied to heterogeneous computational biology datasets, these methods
struggle with generalization and success rates. Here, we introduce
Agentomics-ML, a fully autonomous agent-based system designed to produce a
classification model and the necessary files for reproducible training and
inference. Our method follows predefined steps of an ML experimentation
process, repeatedly interacting with the file system through Bash to complete
individual steps. Once an ML model is produced, training and validation metrics
provide scalar feedback to a reflection step to identify issues such as
overfitting. This step then creates verbal feedback for future iterations,
suggesting adjustments to steps such as data representation, model
architecture, and hyperparameter choices. We have evaluated Agentomics-ML on
several established genomic and transcriptomic benchmark datasets and show that
it outperforms existing state-of-the-art agent-based methods in both
generalization and success rates. While state-of-the-art models built by domain
experts still lead in absolute performance on the majority of the computational
biology datasets used in this work, Agentomics-ML narrows the gap for fully
autonomous systems and achieves state-of-the-art performance on one of the used
benchmark datasets. The code is available at
https://github.com/BioGeMT/Agentomics-ML.

</details>


### [23] [Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning](https://arxiv.org/abs/2506.05568)
*Arian Raje,Baris Askin,Divyansh Jhunjhunwala,Gauri Joshi*

Main category: cs.LG

TL;DR: 论文提出了一种名为Ravan的自适应多头部LoRA方法，用于在联邦学习中高效微调大语言模型，解决了数据异构性和计算资源限制问题，提升了测试准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）尚未充分利用边缘设备数据，而联邦学习（FL）提供了一种在不传输私有数据的情况下协作微调LLMs的范式。然而，现有的LoRA方法在FL中因数据异构性和计算资源限制导致准确率下降。

Method: 提出Ravan方法，通过将权重更新重新参数化为多个LoRA头部的和，仅训练核心矩阵和轻量级缩放因子，优化聚焦于最有用的头部，从而在不增加通信参数的情况下恢复更高秩的更新。

Result: 在视觉和语言基准测试中，Ravan比现有参数高效基线方法提高了2-8%的测试准确率。

Conclusion: Ravan是一种鲁棒且可扩展的解决方案，适用于联邦学习中大语言模型的高效微调。

Abstract: Large language models (LLMs) have not yet effectively leveraged the vast
amounts of edge-device data, and federated learning (FL) offers a promising
paradigm to collaboratively fine-tune LLMs without transferring private edge
data to the cloud. To operate within the computation and communication
constraints of edge devices, recent literature on federated fine-tuning of LLMs
proposes the use of low-rank adaptation (LoRA) and similar parameter-efficient
methods. However, LoRA-based methods suffer from accuracy degradation in FL
settings, primarily because of data and computational heterogeneity across
clients. We propose \textsc{Ravan}, an adaptive multi-head LoRA method that
balances parameter efficiency and model expressivity by reparameterizing the
weight updates as the sum of multiple LoRA heads
$s_i\textbf{B}_i\textbf{H}_i\textbf{A}_i$ in which only the core matrices
$\textbf{H}_i$ and their lightweight scaling factors $s_i$ are trained. These
trainable scaling factors let the optimization focus on the most useful heads,
recovering a higher-rank approximation of the full update without increasing
the number of communicated parameters since clients upload $s_i\textbf{H}_i$
directly. Experiments on vision and language benchmarks show that
\textsc{Ravan} improves test accuracy by 2-8\% over prior parameter-efficient
baselines, making it a robust and scalable solution for federated fine-tuning
of LLMs.

</details>


### [24] [When can in-context learning generalize out of task distribution?](https://arxiv.org/abs/2506.05574)
*Chase Goddard,Lindsay M. Smith,Vudtiwat Ngampruetikorn,David J. Schwab*

Main category: cs.LG

TL;DR: 研究了预训练分布中任务多样性对上下文学习（ICL）能力的影响，发现任务多样性增加时，模型会从专有解过渡到泛化解。


<details>
  <summary>Details</summary>
Motivation: 探索预训练分布中任务多样性如何影响ICL能力的出现及其泛化性能。

Method: 使用线性函数训练变换器，分析任务多样性对ICL的影响，并扩展到非线性回归问题。

Result: 任务多样性增加时，模型从专有解过渡到泛化解，且模型深度和问题维度影响这一过渡。

Conclusion: 任务多样性是ICL泛化的关键因素，模型结构和问题复杂度也会影响其表现。

Abstract: In-context learning (ICL) is a remarkable capability of pretrained
transformers that allows models to generalize to unseen tasks after seeing only
a few examples. We investigate empirically the conditions necessary on the
pretraining distribution for ICL to emerge and generalize
\emph{out-of-distribution}. Previous work has focused on the number of distinct
tasks necessary in the pretraining dataset. Here, we use a different notion of
task diversity to study the emergence of ICL in transformers trained on linear
functions. We find that as task diversity increases, transformers undergo a
transition from a specialized solution, which exhibits ICL only within the
pretraining task distribution, to a solution which generalizes out of
distribution to the entire task space. We also investigate the nature of the
solutions learned by the transformer on both sides of the transition, and
observe similar transitions in nonlinear regression problems. We construct a
phase diagram to characterize how our concept of task diversity interacts with
the number of pretraining tasks. In addition, we explore how factors such as
the depth of the model and the dimensionality of the regression problem
influence the transition.

</details>


### [25] [Collaborative Learning in Agentic Systems: A Collective AI is Greater Than the Sum of Its Parts](https://arxiv.org/abs/2506.05577)
*Saptarshi Nath,Christos Peridis,Eseoghene Benjamin,Xinran Liu,Soheil Kolouri,Peter Kinnell,Zexin Li,Cong Liu,Shirin Dora,Andrea Soltoggio*

Main category: cs.LG

TL;DR: MOSAIC是一种代理AI算法，通过模块化共享和组合实现多代理的独立任务解决与知识共享，提升学习效率和任务解决能力。


<details>
  <summary>Details</summary>
Motivation: 研究如何在分散、异步、无中心控制的真实环境中，通过代理AI的协作学习实现可扩展性和开放性。

Method: 结合模块化策略组合、Wasserstein嵌入的余弦相似性估计和异步通信机制。

Result: 在RL基准测试中，MOSAIC比独立学习者更快学习，解决了一些独立学习者无法完成的任务，并观察到任务理想课程的出现。

Conclusion: 协作学习在代理系统中能实现个体和集体性能的持续提升。

Abstract: Agentic AI has gained significant interest as a research paradigm focused on
autonomy, self-directed learning, and long-term reliability of decision making.
Real-world agentic systems operate in decentralized settings on a large set of
tasks or data distributions with constraints such as limited bandwidth,
asynchronous execution, and the absence of a centralized model or even common
objectives. We posit that exploiting previously learned skills, task
similarities, and communication capabilities in a collective of agentic AI are
challenging but essential elements to enabling scalability, open-endedness, and
beneficial collaborative learning dynamics. In this paper, we introduce Modular
Sharing and Composition in Collective Learning (MOSAIC), an agentic algorithm
that allows multiple agents to independently solve different tasks while also
identifying, sharing, and reusing useful machine-learned knowledge, without
coordination, synchronization, or centralized control. MOSAIC combines three
mechanisms: (1) modular policy composition via neural network masks, (2) cosine
similarity estimation using Wasserstein embeddings for knowledge selection, and
(3) asynchronous communication and policy integration. Results on a set of RL
benchmarks show that MOSAIC has a greater sample efficiency than isolated
learners, i.e., it learns significantly faster, and in some cases, finds
solutions to tasks that cannot be solved by isolated learners. The
collaborative learning and sharing dynamics are also observed to result in the
emergence of ideal curricula of tasks, from easy to hard. These findings
support the case for collaborative learning in agentic systems to achieve
better and continuously evolving performance both at the individual and
collective levels.

</details>


### [26] [Conformal Prediction Adaptive to Unknown Subpopulation Shifts](https://arxiv.org/abs/2506.05583)
*Nien-Shao Wang,Duygu Nur Yaldiz,Yavuz Faruk Bakman,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 本文提出新方法，解决共形预测在子群体分布偏移下的失效问题，确保覆盖有效性。


<details>
  <summary>Details</summary>
Motivation: 共形预测在分布偏移时覆盖保证失效，尤其在子群体分布未知的情况下。

Method: 提出无需子群体结构先验的新算法，适应高维设置。

Result: 在视觉和语言任务中，新方法有效维持覆盖并控制风险。

Conclusion: 新方法在标准共形预测失效时仍能可靠工作。

Abstract: Conformal prediction is widely used to equip black-box machine learning
models with uncertainty quantification enjoying formal coverage guarantees.
However, these guarantees typically break down in the presence of distribution
shifts, where the data distribution at test time differs from the training (or
calibration-time) distribution. In this work, we address subpopulation shifts,
where the test environment exhibits an unknown and differing mixture of
subpopulations compared to the calibration data. We propose new methods that
provably adapt conformal prediction to such shifts, ensuring valid coverage
without requiring explicit knowledge of subpopulation structure. Our algorithms
scale to high-dimensional settings and perform effectively in realistic machine
learning tasks. Extensive experiments on vision (with vision transformers) and
language (with large language models) benchmarks demonstrate that our methods
reliably maintain coverage and controls risk in scenarios where standard
conformal prediction fails.

</details>


### [27] [TabFlex: Scaling Tabular Learning to Millions with Linear Attention](https://arxiv.org/abs/2506.05584)
*Yuchen Zeng,Tuan Dinh,Wonjun Kang,Andreas C Mueller*

Main category: cs.LG

TL;DR: TabFlex通过引入线性注意力机制提升TabPFN的效率与可扩展性，适用于大规模表格数据集，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如TabPFN）在小规模表格数据集表现优异，但难以扩展到大规模复杂数据集。

Method: 采用线性注意力机制替代复杂度为二次方的自注意力机制，提升模型效率。

Result: TabFlex在百万级样本数据集上处理速度显著提升，比TabPFN快2倍，比XGBoost快1.5倍，且计算成本大幅降低。

Conclusion: TabFlex是一种高效且可扩展的表格分类方法，尤其适合大规模数据集。

Abstract: Leveraging the in-context learning (ICL) capability of Large Language Models
(LLMs) for tabular classification has gained significant attention for its
training-free adaptability across diverse datasets. Recent advancements, like
TabPFN, excel in small-scale tabular datasets but struggle to scale for large
and complex datasets. Our work enhances the efficiency and scalability of
TabPFN for larger datasets by incorporating linear attention mechanisms as a
scalable alternative to complexity-quadratic self-attention. Our model,
TabFlex, efficiently handles tabular datasets with thousands of features and
hundreds of classes, scaling seamlessly to millions of samples. For instance,
TabFlex processes the poker-hand dataset with over a million samples in just 5
seconds. Our extensive evaluations demonstrate that TabFlex can achieve over a
2x speedup compared to TabPFN and a 1.5x speedup over XGBoost, outperforming 25
tested baselines in terms of efficiency across a diverse range of datasets.
Furthermore, TabFlex remains highly effective on large-scale datasets,
delivering strong performance with significantly reduced computational costs,
especially when combined with data-efficient techniques such as dimensionality
reduction and data sampling.

</details>


### [28] [CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions](https://arxiv.org/abs/2506.05586)
*Isha Puri,Amit Dhurandhar,Tejaswini Pedapati,Kartikeyan Shanmugam,Dennis Wei,Kush R. Varshney*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的神经网络架构CoFrNet，其灵感来自连分数的形式，具有高效训练和解释性。实验证明其在合成和真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 近年来，局部事后解释神经网络的研究较多，但构建可解释神经网络架构的工作较少。本文旨在填补这一空白。

Method: 提出基于连分数的CoFrNet架构，利用其特殊函数形式实现高效训练和解释性，并证明其通用逼近能力。

Result: 在合成和真实数据集上，CoFrNet表现优异，接近或优于其他可解释模型和多层感知机。

Conclusion: CoFrNet兼具强大的表示能力和解释性，适用于多种数据类型，为可解释神经网络架构提供了新思路。

Abstract: In recent years there has been a considerable amount of research on local
post hoc explanations for neural networks. However, work on building
interpretable neural architectures has been relatively sparse. In this paper,
we present a novel neural architecture, CoFrNet, inspired by the form of
continued fractions which are known to have many attractive properties in
number theory, such as fast convergence of approximations to real numbers. We
show that CoFrNets can be efficiently trained as well as interpreted leveraging
their particular functional form. Moreover, we prove that such architectures
are universal approximators based on a proof strategy that is different than
the typical strategy used to prove universal approximation results for neural
networks based on infinite width (or depth), which is likely to be of
independent interest. We experiment on nonlinear synthetic functions and are
able to accurately model as well as estimate feature attributions and even
higher order terms in some cases, which is a testament to the representational
power as well as interpretability of such architectures. To further showcase
the power of CoFrNets, we experiment on seven real datasets spanning tabular,
text and image modalities, and show that they are either comparable or
significantly better than other interpretable models and multilayer
perceptrons, sometimes approaching the accuracies of state-of-the-art models.

</details>


### [29] [Zero-shot protein stability prediction by inverse folding models: a free energy interpretation](https://arxiv.org/abs/2506.05596)
*Jes Frellsen,Maher M. Kassem,Tone Bengtsen,Lars Olsen,Kresten Lindorff-Larsen,Jesper Ferkinghoff-Borg,Wouter Boomsma*

Main category: cs.LG

TL;DR: 本文探讨了逆折叠模型与蛋白质稳定性之间的自由能基础关系，提出了改进零-shot稳定性预测的方法。


<details>
  <summary>Details</summary>
Motivation: 理解逆折叠模型的氨基酸偏好与热力学稳定性自由能之间的关系，以提升零-shot预测性能。

Method: 通过理论推导揭示似然比标准的简化近似，并提出改进相对稳定性估计的路径，随后进行实证评估。

Result: 实证表明，通过简单方法可以显著提升零-shot性能。

Conclusion: 研究为逆折叠模型的自由能基础提供了更清晰的理解，并展示了改进稳定性预测的潜力。

Abstract: Inverse folding models have proven to be highly effective zero-shot
predictors of protein stability. Despite this success, the link between the
amino acid preferences of an inverse folding model and the free-energy
considerations underlying thermodynamic stability remains incompletely
understood. A better understanding would be of interest not only from a
theoretical perspective, but also potentially provide the basis for stronger
zero-shot stability prediction. In this paper, we take steps to clarify the
free-energy foundations of inverse folding models. Our derivation reveals the
standard practice of likelihood ratios as a simplistic approximation and
suggests several paths towards better estimates of the relative stability. We
empirically assess these approaches and demonstrate that considerable gains in
zero-shot performance can be achieved with fairly simple means.

</details>


### [30] [FaCTR: Factorized Channel-Temporal Representation Transformers for Efficient Time Series Forecasting](https://arxiv.org/abs/2506.05597)
*Yash Vijay,Harini Subramanyan*

Main category: cs.LG

TL;DR: FaCTR是一种轻量级时空Transformer，通过动态对称跨通道交互和低秩因子分解机优化时间序列预测，性能优于现有方法且参数更少。


<details>
  <summary>Details</summary>
Motivation: Transformer在语言和视觉任务中表现优异，但在时间序列预测中因架构复杂性和数据特性（信息密度低、依赖复杂）而效果不佳。

Method: FaCTR通过低秩因子分解机建模跨通道交互，结合可学习门控机制和静态/动态协变量编码，实现轻量化和结构化设计。

Result: 在11个公开基准测试中表现最优，最大模型仅需40万参数（比基线小50倍），并支持可解释性和自监督预训练。

Conclusion: FaCTR是时间序列任务的紧凑且多功能基础模型，兼具高性能和可解释性。

Abstract: While Transformers excel in language and vision-where inputs are semantically
rich and exhibit univariate dependency structures-their architectural
complexity leads to diminishing returns in time series forecasting. Time series
data is characterized by low per-timestep information density and complex
dependencies across channels and covariates, requiring conditioning on
structured variable interactions. To address this mismatch and
overparameterization, we propose FaCTR, a lightweight spatiotemporal
Transformer with an explicitly structural design. FaCTR injects dynamic,
symmetric cross-channel interactions-modeled via a low-rank Factorization
Machine into temporally contextualized patch embeddings through a learnable
gating mechanism. It further encodes static and dynamic covariates for
multivariate conditioning. Despite its compact design, FaCTR achieves
state-of-the-art performance on eleven public forecasting benchmarks spanning
both short-term and long-term horizons, with its largest variant using close to
only 400K parameters-on average 50x smaller than competitive spatiotemporal
transformer baselines. In addition, its structured design enables
interpretability through cross-channel influence scores-an essential
requirement for real-world decision-making. Finally, FaCTR supports
self-supervised pretraining, positioning it as a compact yet versatile
foundation for downstream time series tasks.

</details>


### [31] [When Maximum Entropy Misleads Policy Optimization](https://arxiv.org/abs/2506.05615)
*Ruipeng Zhang,Ya-Chien Chang,Sicun Gao*

Main category: cs.LG

TL;DR: MaxEnt RL框架在复杂控制任务中可能因熵最大化误导策略优化，导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 研究MaxEnt RL在性能关键控制问题中的表现，分析其鲁棒性与最优性之间的权衡。

Method: 通过多种控制问题的实验，验证熵最大化对策略优化的误导效应。

Result: 实验表明，熵最大化可能阻碍精确、低熵策略的学习。

Conclusion: 研究为在复杂控制问题中平衡奖励设计与熵最大化提供了指导。

Abstract: The Maximum Entropy Reinforcement Learning (MaxEnt RL) framework is a leading
approach for achieving efficient learning and robust performance across many RL
tasks. However, MaxEnt methods have also been shown to struggle with
performance-critical control problems in practice, where non-MaxEnt algorithms
can successfully learn. In this work, we analyze how the trade-off between
robustness and optimality affects the performance of MaxEnt algorithms in
complex control tasks: while entropy maximization enhances exploration and
robustness, it can also mislead policy optimization, leading to failure in
tasks that require precise, low-entropy policies. Through experiments on a
variety of control problems, we concretely demonstrate this misleading effect.
Our analysis leads to better understanding of how to balance reward design and
entropy maximization in challenging control problems.

</details>


### [32] [LFA applied to CNNs: Efficient Singular Value Decomposition of Convolutional Mappings by Local Fourier Analysis](https://arxiv.org/abs/2506.05617)
*Antonia van Betteray,Matthias Rottmann,Karsten Kahl*

Main category: cs.LG

TL;DR: 提出了一种基于局部傅里叶分析的方法，用于高效计算卷积映射的奇异值，复杂度为O(N)，优于现有方法的O(N log N)。


<details>
  <summary>Details</summary>
Motivation: 卷积映射的奇异值具有重要的谱特性，可用于提升卷积神经网络的泛化性和鲁棒性，以及模型压缩。然而，传统方法计算奇异值资源消耗大，难以处理高维输入和多通道的情况。

Method: 利用局部傅里叶分析和卷积算子的平移不变性，提出了一种复杂度为O(N)的算法，避免了传统方法中的高资源消耗问题。

Result: 理论分析和数值实验验证了该方法的效率和可扩展性，能够计算高维卷积映射的全部奇异值及其对应的奇异向量。

Conclusion: 该方法为计算高维卷积映射的奇异值提供了一种实用且高效的解决方案，优于现有方法。

Abstract: The singular values of convolutional mappings encode interesting spectral
properties, which can be used, e.g., to improve generalization and robustness
of convolutional neural networks as well as to facilitate model compression.
However, the computation of singular values is typically very
resource-intensive. The naive approach involves unrolling the convolutional
mapping along the input and channel dimensions into a large and sparse
two-dimensional matrix, making the exact calculation of all singular values
infeasible due to hardware limitations. In particular, this is true for
matrices that represent convolutional mappings with large inputs and a high
number of channels. Existing efficient methods leverage the Fast Fourier
transformation (FFT) to transform convolutional mappings into the frequency
domain, enabling the computation of singular values for matrices representing
convolutions with larger input and channel dimensions. For a constant number of
channels in a given convolution, an FFT can compute N singular values in O(N
log N) complexity. In this work, we propose an approach of complexity O(N)
based on local Fourier analysis, which additionally exploits the shift
invariance of convolutional operators. We provide a theoretical analysis of our
algorithm's runtime and validate its efficiency through numerical experiments.
Our results demonstrate that our proposed method is scalable and offers a
practical solution to calculate the entire set of singular values - along with
the corresponding singular vectors if needed - for high-dimensional
convolutional mappings.

</details>


### [33] [Two-dimensional Taxonomy for N-ary Knowledge Representation Learning Methods](https://arxiv.org/abs/2506.05626)
*Xiaohua Lu,Liubov Tupikina,Mehwish Alam*

Main category: cs.LG

TL;DR: 该论文综述了知识超图和超关系知识图的方法，提出了一个二维分类法，并讨论了数据集、负采样策略及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱中高阶关系细节丢失和超图中实体角色忽视的问题，结合两者的优势以更好地建模现实世界知识。

Method: 提出二维分类法：第一维度按方法（如翻译模型、张量分解模型等）分类，第二维度按对实体角色和位置的感知程度分类。

Result: 综述了现有方法，并提供了分类框架，总结了数据集和负采样策略。

Conclusion: 知识超图和超关系知识图能更好地捕捉复杂结构和语义，未来研究需解决现有挑战。

Abstract: Real-world knowledge can take various forms, including structured,
semi-structured, and unstructured data. Among these, knowledge graphs are a
form of structured human knowledge that integrate heterogeneous data sources
into structured representations but typically reduce complex n-ary relations to
simple triples, thereby losing higher-order relational details. In contrast,
hypergraphs naturally represent n-ary relations with hyperedges, which directly
connect multiple entities together. Yet hypergraph representation learning
often overlooks entity roles in hyperedges, limiting the fine-grained semantic
modelling. To address these issues, knowledge hypergraphs and hyper-relational
knowledge graphs combine the advantages of knowledge graphs and hypergraphs to
better capture the complex structures and role-specific semantics of real-world
knowledge. This survey provides a comprehensive review of methods handling
n-ary relational data, covering both knowledge hypergraphs and hyper-relational
knowledge graphs literatures. We propose a two-dimensional taxonomy: the first
dimension categorises models based on their methodology, i.e.,
translation-based models, tensor factorisation-based models, deep neural
network-based models, logic rules-based models, and hyperedge expansion-based
models. The second dimension classifies models according to their awareness of
entity roles and positions in n-ary relations, dividing them into aware-less,
position-aware, and role-aware approaches. Finally, we discuss existing
datasets, negative sampling strategies, and outline open challenges to inspire
future research.

</details>


### [34] [GP-MoLFormer-Sim: Test Time Molecular Optimization through Contextual Similarity Guidance](https://arxiv.org/abs/2506.05628)
*Jiri Navratil,Jarret Ross,Payel Das,Youssef Mroueh,Samuel C Hoffman,Vijil Chenthamarakshan,Brian Belgodere*

Main category: cs.LG

TL;DR: 提出了一种无需训练的分子生成方法GP-MoLFormer-Sim，通过化学语言模型（CLM）和分子相似性引导生成，结合遗传算法（GA）在分子优化任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在药物发现和化学设计中，设计分子时保持与目标分子的相似性或特定性质至关重要，但现有方法效率不足。

Method: 利用CLM的上下文表示估计分子相似性，调整自回归采样策略，结合遗传算法优化生成过程。

Result: 在分子优化基准测试中，GP-MoLFormer-Sim+GA优于现有无需训练的基线方法。

Conclusion: 该方法为理解和引导CLM生成机制提供了新思路。

Abstract: The ability to design molecules while preserving similarity to a target
molecule and/or property is crucial for various applications in drug discovery,
chemical design, and biology. We introduce in this paper an efficient
training-free method for navigating and sampling from the molecular space with
a generative Chemical Language Model (CLM), while using the molecular
similarity to the target as a guide. Our method leverages the contextual
representations learned from the CLM itself to estimate the molecular
similarity, which is then used to adjust the autoregressive sampling strategy
of the CLM. At each step of the decoding process, the method tracks the
distance of the current generations from the target and updates the logits to
encourage the preservation of similarity in generations. We implement the
method using a recently proposed $\sim$47M parameter SMILES-based CLM,
GP-MoLFormer, and therefore refer to the method as GP-MoLFormer-Sim, which
enables a test-time update of the deep generative policy to reflect the
contextual similarity to a set of guide molecules. The method is further
integrated into a genetic algorithm (GA) and tested on a set of standard
molecular optimization benchmarks involving property optimization, molecular
rediscovery, and structure-based drug design. Results show that,
GP-MoLFormer-Sim, combined with GA (GP-MoLFormer-Sim+GA) outperforms existing
training-free baseline methods, when the oracle remains black-box. The findings
in this work are a step forward in understanding and guiding the generative
mechanisms of CLMs.

</details>


### [35] [List-Level Distribution Coupling with Applications to Speculative Decoding and Lossy Compression](https://arxiv.org/abs/2506.05632)
*Joseph Rowan,Buu Phan,Ashish Khisti*

Main category: cs.LG

TL;DR: 论文提出了一种新的概率分布耦合方法，扩展了Gumbel-max采样，并建立了接受概率的下界（列表匹配引理）。该方法应用于多草稿推测采样和分布式有损压缩，取得了竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 研究概率分布耦合问题的松弛形式，提出更高效的采样方法，并探索其在语言任务和分布式压缩中的应用。

Method: 扩展Gumbel-max采样，提出新方法生成样本，并建立接受概率下界（列表匹配引理）。应用于多草稿推测采样和分布式压缩。

Result: 在多草稿推测采样中表现与基线方法竞争，且支持草稿不变性；在分布式压缩中显著提升性能。

Conclusion: 新方法在概率分布耦合及其应用中表现出色，具有理论和实践价值。

Abstract: We study a relaxation of the problem of coupling probability distributions --
a list of samples is generated from one distribution and an accept is declared
if any one of these samples is identical to the sample generated from the other
distribution. We propose a novel method for generating samples, which extends
the Gumbel-max sampling suggested in Daliri et al. (arXiv:2408.07978) for
coupling probability distributions. We also establish a corresponding lower
bound on the acceptance probability, which we call the list matching lemma. We
next discuss two applications of our setup. First, we develop a new mechanism
for multi-draft speculative sampling that is simple to implement and achieves
performance competitive with baselines such as SpecTr and SpecInfer across a
range of language tasks. Our method also guarantees a certain degree of drafter
invariance with respect to the output tokens which is not supported by existing
schemes. We also provide a theoretical lower bound on the token level
acceptance probability. As our second application, we consider distributed
lossy compression with side information in a setting where a source sample is
compressed and available to multiple decoders, each with independent side
information. We propose a compression technique that is based on our
generalization of Gumbel-max sampling and show that it provides significant
gains in experiments involving synthetic Gaussian sources and the MNIST image
dataset.

</details>


### [36] [AutoQD: Automatic Discovery of Diverse Behaviors with Quality-Diversity Optimization](https://arxiv.org/abs/2506.05634)
*Saeed Hedayatian,Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: AutoQD提出了一种自动生成行为描述符的方法，通过嵌入马尔可夫决策过程中的策略占用度量来替代手工设计的描述符，从而在无监督强化学习和QD优化中发现多样化的策略。


<details>
  <summary>Details</summary>
Motivation: 传统QD算法依赖手工设计的行为描述符，限制了探索的多样性。AutoQD旨在通过自动生成描述符来解决这一问题。

Method: 利用随机傅里叶特征近似策略占用度量之间的最大均值差异（MMD），生成低维嵌入作为行为描述符。

Result: 实验证明AutoQD能在连续控制任务中发现多样化策略，无需预定义描述符。

Conclusion: AutoQD为无领域知识的开放学习和行为发现提供了新思路。

Abstract: Quality-Diversity (QD) algorithms have shown remarkable success in
discovering diverse, high-performing solutions, but rely heavily on
hand-crafted behavioral descriptors that constrain exploration to predefined
notions of diversity. Leveraging the equivalence between policies and occupancy
measures, we present a theoretically grounded approach to automatically
generate behavioral descriptors by embedding the occupancy measures of policies
in Markov Decision Processes. Our method, AutoQD, leverages random Fourier
features to approximate the Maximum Mean Discrepancy (MMD) between policy
occupancy measures, creating embeddings whose distances reflect meaningful
behavioral differences. A low-dimensional projection of these embeddings that
captures the most behaviorally significant dimensions is then used as
behavioral descriptors for off-the-shelf QD methods. We prove that our
embeddings converge to true MMD distances between occupancy measures as the
number of sampled trajectories and embedding dimensions increase. Through
experiments in multiple continuous control tasks we demonstrate AutoQD's
ability in discovering diverse policies without predefined behavioral
descriptors, presenting a well-motivated alternative to prior methods in
unsupervised Reinforcement Learning and QD optimization. Our approach opens new
possibilities for open-ended learning and automated behavior discovery in
sequential decision making settings without requiring domain-specific
knowledge.

</details>


### [37] [Bayesian Inference for Correlated Human Experts and Classifiers](https://arxiv.org/abs/2506.05636)
*Markelle Kelly,Alex Boyd,Sam Showalter,Mark Steyvers,Padhraic Smyth*

Main category: cs.LG

TL;DR: 论文提出了一种贝叶斯框架，通过结合预训练分类器的概率估计和人类专家的意见，以最少查询次数实现高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在机器学习应用中，结合模型输出和人类专家的意见进行预测是常见需求，但如何以最低成本获取专家意见是关键问题。

Method: 开发了一个贝叶斯框架，通过联合潜在表示建模专家相关性，支持基于模拟的推理和未观察专家标签的后验推断。

Result: 在两个真实医疗分类问题及CIFAR-10H、ImageNet-16H数据集上，显著减少了专家查询成本，同时保持高预测准确性。

Conclusion: 该方法有效降低了专家查询成本，为结合模型和专家意见的预测问题提供了实用解决方案。

Abstract: Applications of machine learning often involve making predictions based on
both model outputs and the opinions of human experts. In this context, we
investigate the problem of querying experts for class label predictions, using
as few human queries as possible, and leveraging the class probability
estimates of pre-trained classifiers. We develop a general Bayesian framework
for this problem, modeling expert correlation via a joint latent
representation, enabling simulation-based inference about the utility of
additional expert queries, as well as inference of posterior distributions over
unobserved expert labels. We apply our approach to two real-world medical
classification problems, as well as to CIFAR-10H and ImageNet-16H,
demonstrating substantial reductions relative to baselines in the cost of
querying human experts while maintaining high prediction accuracy.

</details>


### [38] [Projectable Models: One-Shot Generation of Small Specialized Transformers from Large Ones](https://arxiv.org/abs/2506.05641)
*Andrey Zhmoginov,Jihwan Lee,Mark Sandler*

Main category: cs.LG

TL;DR: 提出一种将大型Transformer参数映射到小型专用模型参数的技术，以降低计算成本并提升特定任务性能。


<details>
  <summary>Details</summary>
Motivation: 现代基础模型计算成本高且通用知识对特定任务可能冗余，需一种方法生成小型专用模型。

Method: 通过任务特定转换，将大型Transformer参数映射到小型模型参数。

Result: 在图像建模任务中，生成的小型模型性能优于通用条件模型。

Conclusion: 该方法能有效降低计算成本并提升特定任务性能。

Abstract: Modern Foundation Models (FMs) are typically trained on corpora spanning a
wide range of different data modalities, topics and downstream tasks. Utilizing
these models can be very computationally expensive and is out of reach for most
consumer devices. Furthermore, most of the broad FM knowledge may actually be
irrelevant for a specific task at hand. Here we explore a technique for mapping
parameters of a large Transformer to parameters of a smaller specialized model.
By making this transformation task-specific, we aim to capture a narrower scope
of the knowledge needed for performing a specific task by a smaller model. We
study our method on image modeling tasks, showing that performance of generated
models exceeds that of universal conditional models.

</details>


### [39] [Learning to Weight Parameters for Data Attribution](https://arxiv.org/abs/2506.05647)
*Shuangqi Li,Hieu Le,Jingyi Xu,Mathieu Salzmann*

Main category: cs.LG

TL;DR: 提出了一种针对生成模型中数据归属问题的新方法，通过学习参数重要性权重来改进归属准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在追踪训练数据对输出的影响时，通常忽略网络不同层编码信息的差异，导致归属不够精确。

Method: 通过无监督学习参数重要性权重，使归属过程适应模型结构，捕捉训练数据对输出语义（如主题、风格或背景）的贡献。

Result: 该方法在扩散模型中提高了归属准确性，并提供了对输出如何从训练数据中借鉴的细粒度见解。

Conclusion: 提出的方法通过建模参数重要性，显著提升了生成模型中数据归属的准确性和解释性。

Abstract: We study data attribution in generative models, aiming to identify which
training examples most influence a given output. Existing methods achieve this
by tracing gradients back to training data. However, they typically treat all
network parameters uniformly, ignoring the fact that different layers encode
different types of information and may thus draw information differently from
the training set. We propose a method that models this by learning parameter
importance weights tailored for attribution, without requiring labeled data.
This allows the attribution process to adapt to the structure of the model,
capturing which training examples contribute to specific semantic aspects of an
output, such as subject, style, or background. Our method improves attribution
accuracy across diffusion models and enables fine-grained insights into how
outputs borrow from training data.

</details>


### [40] [BAQ: Efficient Bit Allocation Quantization for Large Language Models](https://arxiv.org/abs/2506.05664)
*Chao Zhang,Li Wang,Samson Lasaulce,Merouane Debbah*

Main category: cs.LG

TL;DR: 提出了一种基于Hessian代理的量化位宽分配框架BAQ，通过凸优化最小化量化损失，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法未考虑权重对量化噪声的非均匀敏感性，导致性能不佳。

Method: 基于Hessian代理的敏感性度量，将位宽分配问题转化为凸优化任务，提出BAQ算法。

Result: 在125M到30B参数的LLM上，BAQ比GPTQ的困惑度降低56倍。

Conclusion: BAQ通过优化位宽分配，实现了量化损失与复杂度的良好平衡，理论分析支持其性能提升。

Abstract: Post-training model quantization is a widely adopted technique for reducing
the memory and computational costs of large language models (LLMs). However,
most existing methods rely on uniform or heuristic bitwidth assignments,
failing to account for the nonuniform sensitivity of weights to quantization
noise. In this paper, we propose a novel framework for allocating quantization
bitwidths based on sensitivity metrics derived from a Hessian proxy. We make
key assumptions, which allow the layer/component-wise loss function to be
expressed as an explicit function of the bitwidths. This enables a neat
formulation of the bit allocation problem as a convex optimization task, whose
closed-form solution adapts precision across weights to minimize the layer-wise
quantization loss. Inspecting the solution provides several insights (such as
the equal-loss structure), which are then exploited to design the proposed
\textbf{BAQ} (Bit Allocation Quantization) algorithm. The proposed algorithm
achieves a good trade-off between loss minimization and complexity and allows
BAQ to be integrated into standard quantization pipelines with minimal
overhead. Experimental results show that BAQ consistently outperforms GPTQ,
achieving up to 56$\times$ lower perplexity at the same bitwidth on large
language models ranging from 125M to 30B parameters. Leveraging our analytical
results derived from solving the optimal bit allocation problem, we also
provide a theoretical explanation for the observed gains. All codes of this
paper are available at https://github.com/CSU-ModelCompression/BAQ.

</details>


### [41] [RNE: a plug-and-play framework for diffusion density estimation and inference-time control](https://arxiv.org/abs/2506.05668)
*Jiajun He,José Miguel Hernández-Lobato,Yuanqi Du,Francisco Vargas*

Main category: cs.LG

TL;DR: Radon-Nikodym Estimator (RNE) 是一个基于路径分布密度比的灵活框架，用于扩散推断时的密度估计和控制，统一了多种现有方法。


<details>
  <summary>Details</summary>
Motivation: 提出一个统一的框架，结合理论清晰性和实践多样性，解决扩散模型中的密度估计和推断控制问题。

Method: 利用变分推断和概率原理，通过路径分布的密度比构建 RNE 框架。

Result: 实验表明，RNE 在扩散密度估计和推断控制任务（如退火、模型组合和奖励倾斜）中表现优异。

Conclusion: RNE 提供了一个理论清晰且实用的框架，能够有效统一和扩展现有的扩散模型密度估计与控制方法。

Abstract: In this paper, we introduce the Radon-Nikodym Estimator (RNE), a flexible,
plug-and-play framework for diffusion inference-time density estimation and
control, based on the concept of the density ratio between path distributions.
RNE connects and unifies a variety of existing density estimation and
inference-time control methods under a single and intuitive perspective,
stemming from basic variational inference and probabilistic principles
therefore offering both theoretical clarity and practical versatility.
Experiments demonstrate that RNE achieves promising performances in diffusion
density estimation and inference-time control tasks, including annealing,
composition of diffusion models, and reward-tilting.

</details>


### [42] [Contextually Guided Transformers via Low-Rank Adaptation](https://arxiv.org/abs/2506.05672)
*Andrey Zhmoginov,Jihwan Lee,Max Vladymyrov,Mark Sandler*

Main category: cs.LG

TL;DR: 提出了一种改进的Transformer架构（CGT），通过将上下文编码到模型权重中，减少对显式提示的依赖，从而提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的大语言模型（LLMs）依赖提示实现特定行为，导致计算开销增加。

Method: 设计了Contextually Guided Transformer（CGT），通过在每个序列位置维护上下文摘要，动态调整权重，实现模型的自适应。

Result: 在合成任务和语言建模基准测试中验证了CGT的有效性，并提升了上下文表示的可解释性。

Conclusion: 通过将上下文直接集成到模型架构中，为高效且适应性强的语言建模提供了新方向。

Abstract: Large Language Models (LLMs) based on Transformers excel at text processing,
but their reliance on prompts for specialized behavior introduces computational
overhead. We propose a modification to a Transformer architecture that
eliminates the need for explicit prompts by learning to encode context into the
model's weights. Our Contextually Guided Transformer (CGT) model maintains a
contextual summary at each sequence position, allowing it to update the weights
on the fly based on the preceding context. This approach enables the model to
self-specialize, effectively creating a tailored model for processing
information following a given prefix. We demonstrate the effectiveness of our
method on synthetic in-context learning tasks and language modeling benchmarks.
Furthermore, we introduce techniques for enhancing the interpretability of the
learned contextual representations, drawing connections to Variational
Autoencoders and promoting smoother, more consistent context encoding. This
work offers a novel direction for efficient and adaptable language modeling by
integrating context directly into the model's architecture.

</details>


### [43] [Peer-Ranked Precision: Creating a Foundational Dataset for Fine-Tuning Vision Models from DataSeeds' Annotated Imagery](https://arxiv.org/abs/2506.05673)
*Sajjad Abdoli,Freeman Lewin,Gediminas Vasiliauskas,Fabian Schonholz*

Main category: cs.LG

TL;DR: 论文探讨了AI模型开发从‘模型中心’向‘数据中心’的转变，并介绍了高质量数据集DSD及其对模型性能的提升。


<details>
  <summary>Details</summary>
Motivation: 传统AI开发过于依赖复杂模型架构，忽视了数据质量的重要性，因此需要转向以数据为中心的开发方法。

Method: 引入DataSeeds.AI的DSD数据集，包含10,610张高质量图像及多级标注，用于模型训练和评估。

Result: DSD显著提升了特定模型的性能，代码和训练模型已公开。

Conclusion: 数据中心方法是AI开发的新方向，DSD为商业和多模态AI提供了可扩展的基础。

Abstract: The development of modern Artificial Intelligence (AI) models, particularly
diffusion-based models employed in computer vision and image generation tasks,
is undergoing a paradigmatic shift in development methodologies. Traditionally
dominated by a "Model Centric" approach, in which performance gains were
primarily pursued through increasingly complex model architectures and
hyperparameter optimization, the field is now recognizing a more nuanced
"Data-Centric" approach. This emergent framework foregrounds the quality,
structure, and relevance of training data as the principal driver of model
performance. To operationalize this paradigm shift, we introduce the
DataSeeds.AI sample dataset (the "DSD"), initially comprised of approximately
10,610 high-quality human peer-ranked photography images accompanied by
extensive multi-tier annotations. The DSD is a foundational computer vision
dataset designed to usher in a new standard for commercial image datasets.
Representing a small fraction of DataSeeds.AI's 100 million-plus image catalog,
the DSD provides a scalable foundation necessary for robust commercial and
multimodal AI development. Through this in-depth exploratory analysis, we
document the quantitative improvements generated by the DSD on specific models
against known benchmarks and make the code and the trained models used in our
evaluation publicly available.

</details>


### [44] [Topology-aware Neural Flux Prediction Guided by Physics](https://arxiv.org/abs/2506.05676)
*Haoyang Jiang,Jindong Wang,Xingquan Zhu,Yi He*

Main category: cs.LG

TL;DR: 提出了一种新框架，结合显式差异矩阵和隐式物理约束，使GNN能捕捉有向图中的高频信号。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在处理有向图时难以保留高频信号，导致无法区分正向和反向拓扑结构。

Method: 结合显式差异矩阵建模方向梯度，并引入隐式物理约束确保消息传递符合自然规律。

Result: 在真实世界的水通量网络和城市交通流网络上验证了框架的有效性。

Conclusion: 新框架成功提升了GNN对有向图中高频信号的敏感性。

Abstract: Graph Neural Networks (GNNs) often struggle in preserving high-frequency
components of nodal signals when dealing with directed graphs. Such components
are crucial for modeling flow dynamics, without which a traditional GNN tends
to treat a graph with forward and reverse topologies equal.To make GNNs
sensitive to those high-frequency components thereby being capable to capture
detailed topological differences, this paper proposes a novel framework that
combines 1) explicit difference matrices that model directional gradients and
2) implicit physical constraints that enforce messages passing within GNNs to
be consistent with natural laws. Evaluations on two real-world directed graph
data, namely, water flux network and urban traffic flow network, demonstrate
the effectiveness of our proposal.

</details>


### [45] [Numerical Investigation of Sequence Modeling Theory using Controllable Memory Functions](https://arxiv.org/abs/2506.05678)
*Haotian Jiang,Zeyu Bao,Shida Wang,Qianxiao Li*

Main category: cs.LG

TL;DR: 提出了一种合成基准框架，用于评估不同序列模型捕捉不同时间结构的能力，通过生成具有明确时间依赖性的合成目标，揭示了模型的优势和局限性。


<details>
  <summary>Details</summary>
Motivation: 系统地评估序列模型对不同时间结构的捕捉能力，填补现有研究的空白。

Method: 设计了合成基准框架，生成具有不同记忆功能和时间依赖强度的目标，对多种序列模型进行实验。

Result: 实验验证了现有理论见解，并揭示了新发现，证明了方法的有效性。

Conclusion: 该方法有助于理论理解，并强调了使用具有明确结构的可控目标对评估序列模型的重要性。

Abstract: The evolution of sequence modeling architectures, from recurrent neural
networks and convolutional models to Transformers and structured state-space
models, reflects ongoing efforts to address the diverse temporal dependencies
inherent in sequential data. Despite this progress, systematically
characterizing the strengths and limitations of these architectures remains a
fundamental challenge. In this work, we propose a synthetic benchmarking
framework to evaluate how effectively different sequence models capture
distinct temporal structures. The core of this approach is to generate
synthetic targets, each characterized by a memory function and a parameter that
determines the strength of temporal dependence. This setup allows us to produce
a continuum of tasks that vary in temporal complexity, enabling fine-grained
analysis of model behavior concerning specific memory properties. We focus on
four representative memory functions, each corresponding to a distinct class of
temporal structures. Experiments on several sequence modeling architectures
confirm existing theoretical insights and reveal new findings. These results
demonstrate the effectiveness of the proposed method in advancing theoretical
understanding and highlight the importance of using controllable targets with
clearly defined structures for evaluating sequence modeling architectures.

</details>


### [46] [Learning Design-Score Manifold to Guide Diffusion Models for Offline Optimization](https://arxiv.org/abs/2506.05680)
*Tailin Zhou,Zhilin Chen,Wenlong Lyu,Zhitang Chen,Danny H. K. Tsang,Jun Zhang*

Main category: cs.LG

TL;DR: ManGO是一种基于扩散的框架，通过学习设计-评分流形，统一前向预测和后向生成，实现了在训练数据之外的泛化能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 复杂系统的优化是一个基础性挑战，传统离线优化方法在训练数据之外表现不佳，需要一种能捕捉设计-评分依赖关系的新方法。

Method: ManGO采用扩散模型学习设计-评分流形，结合无导数引导的条件生成和自适应推理时间缩放，动态优化去噪路径。

Result: ManGO在合成任务、机器人控制、材料设计等多个领域优于24种单目标和10种多目标优化方法。

Conclusion: ManGO通过统一设计-评分空间，实现了更强的泛化能力，为复杂系统优化提供了有效解决方案。

Abstract: Optimizing complex systems, from discovering therapeutic drugs to designing
high-performance materials, remains a fundamental challenge across science and
engineering, as the underlying rules are often unknown and costly to evaluate.
Offline optimization aims to optimize designs for target scores using
pre-collected datasets without system interaction. However, conventional
approaches may fail beyond training data, predicting inaccurate scores and
generating inferior designs. This paper introduces ManGO, a diffusion-based
framework that learns the design-score manifold, capturing the design-score
interdependencies holistically. Unlike existing methods that treat design and
score spaces in isolation, ManGO unifies forward prediction and backward
generation, attaining generalization beyond training data. Key to this is its
derivative-free guidance for conditional generation, coupled with adaptive
inference-time scaling that dynamically optimizes denoising paths. Extensive
evaluations demonstrate that ManGO outperforms 24 single- and 10
multi-objective optimization methods across diverse domains, including
synthetic tasks, robot control, material design, DNA sequence, and real-world
engineering optimization.

</details>


### [47] [Multi-Modal Multi-Task Federated Foundation Models for Next-Generation Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence in AR/VR/MR](https://arxiv.org/abs/2506.05683)
*Fardis Nadimi,Payam Abdisarabshali,Kasra Borazjani,Jacob Chakareski,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 论文提出了一种基于多模态多任务（M3T）联邦基础模型（FedFMs）的扩展现实（XR）系统架构，旨在通过结合基础模型的表示能力和联邦学习的隐私保护特性，解决XR系统中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 扩展现实（XR）系统需要处理多模态、隐私保护和资源限制等复杂问题，而现有的方法难以兼顾这些需求。因此，论文提出FedFMs架构，以提供一种隐私保护且高效的解决方案。

Method: 提出了一种模块化的FedFMs架构，包括模型训练和聚合的协调范式，并定义了影响FedFMs实施的SHIFT维度（传感器多样性、硬件异构性、交互性、任务可变性和时间性）。

Result: 论文展示了FedFMs在XR系统中的应用潜力，并提出了评估指标、数据集需求和设计权衡，为资源感知的FedFMs开发奠定了基础。

Conclusion: 论文为下一代XR系统中的上下文感知隐私保护智能技术奠定了技术和概念基础。

Abstract: Extended reality (XR) systems, which consist of virtual reality (VR),
augmented reality (AR), and mixed reality (XR), offer a transformative
interface for immersive, multi-modal, and embodied human-computer interaction.
In this paper, we envision that multi-modal multi-task (M3T) federated
foundation models (FedFMs) can offer transformative capabilities for XR systems
through integrating the representational strength of M3T foundation models
(FMs) with the privacy-preserving model training principles of federated
learning (FL). We present a modular architecture for FedFMs, which entails
different coordination paradigms for model training and aggregations. Central
to our vision is the codification of XR challenges that affect the
implementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality
diversity, (2) Hardware heterogeneity and system-level constraints, (3)
Interactivity and embodied personalization, (4) Functional/task variability,
and (5) Temporality and environmental variability. We illustrate the
manifestation of these dimensions across a set of emerging and anticipated
applications of XR systems. Finally, we propose evaluation metrics, dataset
requirements, and design tradeoffs necessary for the development of
resource-aware FedFMs in XR. This perspective aims to chart the technical and
conceptual foundations for context-aware privacy-preserving intelligence in the
next generation of XR systems.

</details>


### [48] [Statistically Valid Post-Deployment Monitoring Should Be Standard for AI-Based Digital Health](https://arxiv.org/abs/2506.05701)
*Pavel Dolin,Weizhi Li,Gautam Dasarathy,Visar Berisha*

Main category: cs.LG

TL;DR: 本文主张临床AI的部署后监测不足，并提出基于统计有效性和标签效率的测试框架，作为确保实际部署中可靠性和安全性的理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有临床AI工具的部署后监测计划不足（仅9%的FDA注册工具包含），且监测方法多为手动、零星和被动，无法适应动态环境。

Method: 提出基于统计假设检验的框架，将数据变化和模型性能退化视为独立问题，确保监测的科学性和可重复性。

Result: 该方法为临床AI系统的可靠性提供了统计严谨的基础，并提出了新的研究方向。

Conclusion: 统计有效的监测框架是确保临床AI系统可靠性和安全性的关键，并为技术社区提供了新的研究方向。

Abstract: This position paper argues that post-deployment monitoring in clinical AI is
underdeveloped and proposes statistically valid and label-efficient testing
frameworks as a principled foundation for ensuring reliability and safety in
real-world deployment. A recent review found that only 9% of FDA-registered
AI-based healthcare tools include a post-deployment surveillance plan. Existing
monitoring approaches are often manual, sporadic, and reactive, making them
ill-suited for the dynamic environments in which clinical models operate. We
contend that post-deployment monitoring should be grounded in label-efficient
and statistically valid testing frameworks, offering a principled alternative
to current practices. We use the term "statistically valid" to refer to methods
that provide explicit guarantees on error rates (e.g., Type I/II error), enable
formal inference under pre-defined assumptions, and support
reproducibility--features that align with regulatory requirements.
Specifically, we propose that the detection of changes in the data and model
performance degradation should be framed as distinct statistical hypothesis
testing problems. Grounding monitoring in statistical rigor ensures a
reproducible and scientifically sound basis for maintaining the reliability of
clinical AI systems. Importantly, it also opens new research directions for the
technical community--spanning theory, methods, and tools for statistically
principled detection, attribution, and mitigation of post-deployment model
failures in real-world settings.

</details>


### [49] [Action-Adaptive Continual Learning: Enabling Policy Generalization under Dynamic Action Spaces](https://arxiv.org/abs/2506.05702)
*Chaofan Pan,Jiafen Liu,Yanhua Li,Linbo Xiong,Fan Min,Wei Wei,Xin Yang*

Main category: cs.LG

TL;DR: 论文提出了一种新的持续学习问题（CL-DC），解决了动态能力变化下的策略泛化挑战，并提出了AACL框架。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法假设能力静态，但现实中能力动态变化，因此需要解决动态能力下的策略泛化问题。

Method: 提出AACL框架，通过构建动作表示空间解耦策略与动作空间，并自适应微调编码器-解码器以平衡稳定性和可塑性。

Result: 实验表明，AACL在三种环境中的基准测试中优于现有方法，实现了跨动作空间的策略泛化。

Conclusion: AACL框架有效解决了动态能力下的持续学习问题，为未来研究提供了新方向。

Abstract: Continual Learning (CL) is a powerful tool that enables agents to learn a
sequence of tasks, accumulating knowledge learned in the past and using it for
problem-solving or future task learning. However, existing CL methods often
assume that the agent's capabilities remain static within dynamic environments,
which doesn't reflect real-world scenarios where capabilities dynamically
change. This paper introduces a new and realistic problem: Continual Learning
with Dynamic Capabilities (CL-DC), posing a significant challenge for CL
agents: How can policy generalization across different action spaces be
achieved? Inspired by the cortical functions, we propose an Action-Adaptive
Continual Learning framework (AACL) to address this challenge. Our framework
decouples the agent's policy from the specific action space by building an
action representation space. For a new action space, the encoder-decoder of
action representations is adaptively fine-tuned to maintain a balance between
stability and plasticity. Furthermore, we release a benchmark based on three
environments to validate the effectiveness of methods for CL-DC. Experimental
results demonstrate that our framework outperforms popular methods by
generalizing the policy across action spaces.

</details>


### [50] [Latent Diffusion Model Based Denoising Receiver for 6G Semantic Communication: From Stochastic Differential Theory to Application](https://arxiv.org/abs/2506.05710)
*Xiucheng Wang,Honggang Jia,Nan Cheng,Dusit Niyato*

Main category: cs.LG

TL;DR: 提出了一种基于生成式人工智能（GAI）和扩散模型（DMs）的新型语义通信框架，通过理论分析和数学优化，实现了高效的语义传输和去噪。


<details>
  <summary>Details</summary>
Motivation: 解决传统语义通信在低信噪比（SNR）和分布偏移下的性能不足问题，利用扩散模型的去噪能力提升鲁棒性。

Method: 基于随机微分方程（SDEs）建立理论框架，推导SNR与去噪步长的闭式关系，设计无需微调的缩放机制，并结合变分自编码器（VAE）和预训练扩散模型构建语义收发器。

Result: 实验表明，该框架在低SNR和分布偏移下显著优于传统方法，支持零样本泛化。

Conclusion: 该研究为6G系统中GAI驱动的鲁棒语义传输提供了有前景的方向。

Abstract: In this paper, a novel semantic communication framework empowered by
generative artificial intelligence (GAI) is proposed, specifically leveraging
the capabilities of diffusion models (DMs). A rigorous theoretical foundation
is established based on stochastic differential equations (SDEs), which
elucidates the denoising properties of DMs in mitigating additive white
Gaussian noise (AWGN) in latent semantic representations. Crucially, a
closed-form analytical relationship between the signal-to-noise ratio (SNR) and
the denoising timestep is derived, enabling the optimal selection of diffusion
parameters for any given channel condition. To address the distribution
mismatch between the received signal and the DM's training data, a
mathematically principled scaling mechanism is introduced, ensuring robust
performance across a wide range of SNRs without requiring model fine-tuning.
Built upon this theoretical insight, we develop a latent diffusion model
(LDM)-based semantic transceiver, wherein a variational autoencoder (VAE) is
employed for efficient semantic compression, and a pretrained DM serves as a
universal denoiser. Notably, the proposed architecture is fully training-free
at inference time, offering high modularity and compatibility with large-scale
pretrained LDMs. This design inherently supports zero-shot generalization and
mitigates the challenges posed by out-of-distribution inputs. Extensive
experimental evaluations demonstrate that the proposed framework significantly
outperforms conventional neural-network-based semantic communication baselines,
particularly under low SNR conditions and distributional shifts, thereby
establishing a promising direction for GAI-driven robust semantic transmission
in future 6G systems.

</details>


### [51] [Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation](https://arxiv.org/abs/2506.05713)
*Zhan Zhuang,Xiequn Wang,Wei Li,Yulong Zhang,Qiushi Huang,Shuhao Chen,Xuehao Wang,Yanbin Wei,Yuhe Nie,Kede Ma,Yu Zhang,Ying Wei*

Main category: cs.LG

TL;DR: CoTo是一种渐进式训练策略，通过逐步增加适配器的激活概率，提升LoRA的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: LoRA在微调大型基础模型时容易陷入次优解，限制了模型的泛化能力和下游操作（如适配器合并和剪枝）。

Method: 提出CoTo策略，逐步增加适配器的激活概率，通过随机停用适配器促进更平衡的优化和更广泛的损失空间探索。

Result: 实验表明，CoTo提升了单任务性能、多任务合并准确率、剪枝鲁棒性，并减少了训练开销。

Conclusion: CoTo是一种兼容性强且高效的LoRA改进方法。

Abstract: Low-rank adaptation (LoRA) has emerged as a leading parameter-efficient
fine-tuning technique for adapting large foundation models, yet it often locks
adapters into suboptimal minima near their initialization. This hampers model
generalization and limits downstream operators such as adapter merging and
pruning. Here, we propose CoTo, a progressive training strategy that gradually
increases adapters' activation probability over the course of fine-tuning. By
stochastically deactivating adapters, CoTo encourages more balanced
optimization and broader exploration of the loss landscape. We provide a
theoretical analysis showing that CoTo promotes layer-wise dropout stability
and linear mode connectivity, and we adopt a cooperative-game approach to
quantify each adapter's marginal contribution. Extensive experiments
demonstrate that CoTo consistently boosts single-task performance, enhances
multi-task merging accuracy, improves pruning robustness, and reduces training
overhead, all while remaining compatible with diverse LoRA variants. Code is
available at https://github.com/zwebzone/coto.

</details>


### [52] [Ensemble Elastic DQN: A novel multi-step ensemble approach to address overestimation in deep value-based reinforcement learning](https://arxiv.org/abs/2506.05716)
*Adrian Ly,Richard Dazeley,Peter Vamplew,Francisco Cruz,Sunil Aryal*

Main category: cs.LG

TL;DR: EEDQN结合集成与弹性步长更新，解决DQN中的高估偏差和样本效率问题，在MinAtar基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究不同改进方法（如多步和集成）如何相互作用，以解决DQN中的高估偏差和样本效率问题。

Method: 提出EEDQN算法，结合集成与弹性步长更新，稳定算法性能。

Result: EEDQN在MinAtar环境中表现稳健，优于基线DQN方法，匹配或超越最先进的集成DQN。

Conclusion: 系统结合算法改进（如集成与多步方法）可显著提升性能。

Abstract: While many algorithmic extensions to Deep Q-Networks (DQN) have been
proposed, there remains limited understanding of how different improvements
interact. In particular, multi-step and ensemble style extensions have shown
promise in reducing overestimation bias, thereby improving sample efficiency
and algorithmic stability. In this paper, we introduce a novel algorithm called
Ensemble Elastic Step DQN (EEDQN), which unifies ensembles with elastic step
updates to stabilise algorithmic performance. EEDQN is designed to address two
major challenges in deep reinforcement learning: overestimation bias and sample
efficiency. We evaluated EEDQN against standard and ensemble DQN variants
across the MinAtar benchmark, a set of environments that emphasise behavioral
learning while reducing representational complexity. Our results show that
EEDQN achieves consistently robust performance across all tested environments,
outperforming baseline DQN methods and matching or exceeding state-of-the-art
ensemble DQNs in final returns on most of the MinAtar environments. These
findings highlight the potential of systematically combining algorithmic
improvements and provide evidence that ensemble and multi-step methods, when
carefully integrated, can yield substantial gains.

</details>


### [53] [Grokking Beyond the Euclidean Norm of Model Parameters](https://arxiv.org/abs/2506.05718)
*Pascal Jr Tikeng Notsawo,Guillaume Dumas,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: 论文探讨了通过正则化（显式或隐式）诱导神经网络在梯度优化中的延迟泛化现象（grokking），并分析了不同正则化方法对泛化的影响。


<details>
  <summary>Details</summary>
Motivation: 研究grokking现象的成因，尤其是正则化在其中的作用，以扩展对神经网络泛化行为的理解。

Method: 通过梯度下降结合小但非零的正则化（如ℓ₁或核范数正则化）来诱导grokking，并分析过参数化和数据选择的影响。

Result: 发现正则化可以诱导grokking，且过参数化可以在不使用显式正则化的情况下实现grokking；ℓ₂范数在某些情况下不能可靠反映泛化性能。

Conclusion: 正则化和数据选择是诱导grokking的关键因素，过参数化进一步扩展了其可能性，为理解神经网络泛化提供了新视角。

Abstract: Grokking refers to a delayed generalization following overfitting when
optimizing artificial neural networks with gradient-based methods. In this
work, we demonstrate that grokking can be induced by regularization, either
explicit or implicit. More precisely, we show that when there exists a model
with a property $P$ (e.g., sparse or low-rank weights) that generalizes on the
problem of interest, gradient descent with a small but non-zero regularization
of $P$ (e.g., $\ell_1$ or nuclear norm regularization) results in grokking.
This extends previous work showing that small non-zero weight decay induces
grokking. Moreover, our analysis shows that over-parameterization by adding
depth makes it possible to grok or ungrok without explicitly using
regularization, which is impossible in shallow cases. We further show that the
$\ell_2$ norm is not a reliable proxy for generalization when the model is
regularized toward a different property $P$, as the $\ell_2$ norm grows in many
cases where no weight decay is used, but the model generalizes anyway. We also
show that grokking can be amplified solely through data selection, with any
other hyperparameter fixed.

</details>


### [54] [Any-Class Presence Likelihood for Robust Multi-Label Classification with Abundant Negative Data](https://arxiv.org/abs/2506.05721)
*Dumindu Tissera,Omar Awadallah,Muhammad Umair Danish,Ayan Sadhu,Katarina Grolinger*

Main category: cs.LG

TL;DR: 论文提出了一种改进的多标签分类（MLC）损失函数，通过归一化加权几何平均预测类别概率，解决负数据过多的问题，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 多标签分类中，大量未标记的负数据会干扰学习过程，影响正实例的准确分类。现有方法将负数据单独分类会引入冗余，因此需要一种更有效的方法。

Method: 重新设计MLC损失函数，通过归一化加权几何平均预测类别概率，引入正则化参数控制负类概率对正实例的贡献。

Result: 在多个大规模数据集上测试，新损失函数显著提升了性能，F1、F2和平均精度分别提高了6.01、8.06和3.11个百分点。

Conclusion: 提出的损失函数有效解决了负数据问题，提升了多标签分类性能，且无需额外参数或计算复杂度。

Abstract: Multi-label Classification (MLC) assigns an instance to one or more
non-exclusive classes. A challenge arises when the dataset contains a large
proportion of instances with no assigned class, referred to as negative data,
which can overwhelm the learning process and hinder the accurate identification
and classification of positive instances. Nevertheless, it is common in MLC
applications such as industrial defect detection, agricultural disease
identification, and healthcare diagnosis to encounter large amounts of negative
data. Assigning a separate negative class to these instances further
complicates the learning objective and introduces unnecessary redundancies. To
address this challenge, we redesign standard MLC loss functions by deriving a
likelihood of any class being present, formulated by a normalized weighted
geometric mean of the predicted class probabilities. We introduce a
regularization parameter that controls the relative contribution of the absent
class probabilities to the any-class presence likelihood in positive instances.
The any-class presence likelihood complements the multi-label learning by
encouraging the network to become more aware of implicit positive instances and
improve the label classification within those positive instances. Experiments
on large-scale datasets with negative data: SewerML, modified COCO, and
ChestX-ray14, across various networks and base loss functions show that our
loss functions consistently improve MLC performance of their standard loss
counterparts, achieving gains of up to 6.01 percentage points in F1, 8.06 in
F2, and 3.11 in mean average precision, all without additional parameters or
computational complexity. Code available at:
https://github.com/ML-for-Sensor-Data-Western/gmean-mlc

</details>


### [55] [Generalized Incremental Learning under Concept Drift across Evolving Data Streams](https://arxiv.org/abs/2506.05736)
*En Yu,Jie Lu,Guangquan Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为CSFA的新框架，用于解决开放环境流数据中的概念漂移问题，通过原型校准和无源适应算法实现稳定适应。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据流具有非平稳性，现有方法忽略了标签空间和分布的联合演化问题，导致适应学习系统面临挑战。

Method: CSFA框架包含训练无关的原型校准机制和基于可靠代理间隙的锐度感知最小化算法（RSGS），用于动态适应新类别和分布对齐。

Result: 实验证明CSFA在开放世界流场景中表现优于现有方法，能够稳定适应语义和分布的演化。

Conclusion: CSFA为开放环境流数据中的概念漂移问题提供了统一的解决方案，具有高效性和鲁棒性。

Abstract: Real-world data streams exhibit inherent non-stationarity characterized by
concept drift, posing significant challenges for adaptive learning systems.
While existing methods address isolated distribution shifts, they overlook the
critical co-evolution of label spaces and distributions under limited
supervision and persistent uncertainty. To address this, we formalize
Generalized Incremental Learning under Concept Drift (GILCD), characterizing
the joint evolution of distributions and label spaces in open-environment
streaming contexts, and propose a novel framework called Calibrated Source-Free
Adaptation (CSFA). First, CSFA introduces a training-free prototype calibration
mechanism that dynamically fuses emerging prototypes with base representations,
enabling stable new-class identification without optimization overhead. Second,
we design a novel source-free adaptation algorithm, i.e., Reliable Surrogate
Gap Sharpness-aware (RSGS) minimization. It integrates sharpness-aware
perturbation loss optimization with surrogate gap minimization, while employing
entropy-based uncertainty filtering to discard unreliable samples. This
mechanism ensures robust distribution alignment and mitigates generalization
degradation caused by uncertainties. Therefore, CSFA establishes a unified
framework for stable adaptation to evolving semantics and distributions in
open-world streaming scenarios. Extensive experiments validate the superior
performance and effectiveness of CSFA compared to state-of-the-art approaches.

</details>


### [56] [Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance](https://arxiv.org/abs/2506.05748)
*Rudransh Agnihotri,Ananya Pandey*

Main category: cs.LG

TL;DR: 通过冻结的7B LLM、一行JSON规则和rank-16 LoRA适配器，替代传统重型奖励模型，实现高效、低成本的RLHF，并在RewardBench和GSM-8K上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决RLHF中奖励模型训练的高成本和复杂性，提出一种轻量级替代方案。

Method: 使用冻结的7B LLM，结合JSON规则和LoRA适配器，无需离线调优，直接作为奖励模型。

Result: 在RewardBench上达到96.2%准确率，优于27B-70B模型；7B模型在GSM-8K上以92%准确率超越70B DPO基线。

Conclusion: 该方法通过轻量级设计，实现了高效、透明且可调的奖励功能，为RLHF提供了新的解决方案。

Abstract: Reward-model training is the cost bottleneck in modern Reinforcement Learning
Human Feedback (RLHF) pipelines, often requiring tens of billions of parameters
and an offline preference-tuning phase. In the proposed method, a frozen,
instruction-tuned 7B LLM is augmented with only a one line JSON rubric and a
rank-16 LoRA adapter (affecting just 0.8% of the model's parameters), enabling
it to serve as a complete substitute for the previously used heavyweight
evaluation models. The plug-and-play judge achieves 96.2% accuracy on
RewardBench, outperforming specialized reward networks ranging from 27B to 70B
parameters. Additionally, it allows a 7B actor to outperform the top 70B DPO
baseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K
utilizing online PPO. Thorough ablations indicate that (i) six in context
demonstrations deliver the majority of the zero-to-few-shot improvements
(+2pp), and (ii) the LoRA effectively addresses the remaining disparity,
particularly in the safety and adversarial Chat-Hard segments. The proposed
model introduces HH-Rationales, a subset of 10,000 pairs from Anthropic
HH-RLHF, to examine interpretability, accompanied by human generated
justifications. GPT-4 scoring indicates that our LoRA judge attains
approximately = 9/10 in similarity to human explanations, while zero-shot
judges score around =5/10. These results indicate that the combination of
prompt engineering and tiny LoRA produces a cost effective, transparent, and
easily adjustable reward function, removing the offline phase while achieving
new state-of-the-art outcomes for both static evaluation and online RLHF.

</details>


### [57] [Integrating Spatiotemporal Features in LSTM for Spatially Informed COVID-19 Hospitalization Forecasting](https://arxiv.org/abs/2506.05752)
*Zhongying Wang,Thoai D. Ngo,Hamidreza Zoraghein,Benjamin Lucas,Morteza Karimzadeh*

Main category: cs.LG

TL;DR: 该研究提出了一种基于LSTM的新框架，结合时空特征SPH，显著提升了COVID-19住院预测的准确性，尤其在变异株流行期间表现突出。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行凸显了准确、及时预测住院需求的重要性，但现有模型在变异株流行期间表现不佳。

Method: 采用LSTM框架，引入时空特征SPH，并结合多时间范围集成策略。

Result: 模型在Delta和Omicron流行期间的预测表现优于基准模型，尤其在Omicron期间提升显著。

Conclusion: 研究不仅改进了住院预测，还证明了时空特征在传染病传播建模中的重要性。

Abstract: The COVID-19 pandemic's severe impact highlighted the need for accurate,
timely hospitalization forecasting to support effective healthcare planning.
However, most forecasting models struggled, especially during variant surges,
when they were needed most. This study introduces a novel Long Short-Term
Memory (LSTM) framework for forecasting daily state-level incident
hospitalizations in the United States. We present a spatiotemporal feature,
Social Proximity to Hospitalizations (SPH), derived from Facebook's Social
Connectedness Index to improve forecasts. SPH serves as a proxy for interstate
population interaction, capturing transmission dynamics across space and time.
Our parallel LSTM architecture captures both short- and long-term temporal
dependencies, and our multi-horizon ensembling strategy balances consistency
and forecasting error. Evaluation against COVID-19 Forecast Hub ensemble models
during the Delta and Omicron surges reveals superiority of our model. On
average, our model surpasses the ensemble by 27, 42, 54, and 69
hospitalizations per state on the $7^{th}$, $14^{th}$, $21^{st}$, and $28^{th}$
forecast days, respectively, during the Omicron surge. Data-ablation
experiments confirm SPH's predictive power, highlighting its effectiveness in
enhancing forecasting models. This research not only advances hospitalization
forecasting but also underscores the significance of spatiotemporal features,
such as SPH, in refining predictive performance in modeling the complex
dynamics of infectious disease spread.

</details>


### [58] [FlowOE: Imitation Learning with Flow Policy from Ensemble RL Experts for Optimal Execution under Heston Volatility and Concave Market Impacts](https://arxiv.org/abs/2506.05755)
*Yang Li,Zhi Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于流匹配模型的模仿学习框架flowOE，用于优化金融市场的动态执行策略，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统最优执行策略（如静态Almgren-Chriss模型）在动态市场中表现不佳，需要更灵活的方法。

Method: flowOE通过模仿学习从多种专家策略中自适应选择最佳行为，并结合改进的损失函数优化策略。

Result: 实验表明flowOE在多种市场条件下表现优于传统基准，实现了更高的利润和更低的风险。

Conclusion: flowOE展示了在动态市场中提升最优执行策略的潜力和实用性。

Abstract: Optimal execution in financial markets refers to the process of strategically
transacting a large volume of assets over a period to achieve the best possible
outcome by balancing the trade-off between market impact costs and timing or
volatility risks. Traditional optimal execution strategies, such as static
Almgren-Chriss models, often prove suboptimal in dynamic financial markets.
This paper propose flowOE, a novel imitation learning framework based on flow
matching models, to address these limitations. FlowOE learns from a diverse set
of expert traditional strategies and adaptively selects the most suitable
expert behavior for prevailing market conditions. A key innovation is the
incorporation of a refining loss function during the imitation process,
enabling flowOE not only to mimic but also to improve upon the learned expert
actions. To the best of our knowledge, this work is the first to apply flow
matching models in a stochastic optimal execution problem. Empirical
evaluations across various market conditions demonstrate that flowOE
significantly outperforms both the specifically calibrated expert models and
other traditional benchmarks, achieving higher profits with reduced risk. These
results underscore the practical applicability and potential of flowOE to
enhance adaptive optimal execution.

</details>


### [59] [BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning](https://arxiv.org/abs/2506.05762)
*Yunpeng Qing,Shuo Chen,Yixiao Chi,Shunyu Liu,Sixu Lin,Changqing Zou*

Main category: cs.LG

TL;DR: 论文提出了一种双向轨迹扩散（BiTrajDiff）框架，用于离线强化学习中的数据增强，通过同时生成未来和历史的轨迹来提升数据集多样性。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强技术仅关注从给定状态重构未来轨迹，忽略了历史过渡的探索，导致行为模式多样性不足，尤其是可能产生高回报的关键状态。

Method: BiTrajDiff通过两个独立的扩散过程生成未来和历史的轨迹，利用关键状态作为锚点扩展状态空间的未探索区域。

Result: 在D4RL基准测试中，BiTrajDiff表现优于其他先进数据增强方法。

Conclusion: BiTrajDiff通过双向轨迹生成有效提升了离线强化学习的性能，为数据多样性提供了新思路。

Abstract: Recent advances in offline Reinforcement Learning (RL) have proven that
effective policy learning can benefit from imposing conservative constraints on
pre-collected datasets. However, such static datasets often exhibit
distribution bias, resulting in limited generalizability. To address this
limitation, a straightforward solution is data augmentation (DA), which
leverages generative models to enrich data distribution. Despite the promising
results, current DA techniques focus solely on reconstructing future
trajectories from given states, while ignoring the exploration of history
transitions that reach them. This single-direction paradigm inevitably hinders
the discovery of diverse behavior patterns, especially those leading to
critical states that may have yielded high-reward outcomes. In this work, we
introduce Bidirectional Trajectory Diffusion (BiTrajDiff), a novel DA framework
for offline RL that models both future and history trajectories from any
intermediate states. Specifically, we decompose the trajectory generation task
into two independent yet complementary diffusion processes: one generating
forward trajectories to predict future dynamics, and the other generating
backward trajectories to trace essential history transitions.BiTrajDiff can
efficiently leverage critical states as anchors to expand into potentially
valuable yet underexplored regions of the state space, thereby facilitating
dataset diversity. Extensive experiments on the D4RL benchmark suite
demonstrate that BiTrajDiff achieves superior performance compared to other
advanced DA methods across various offline RL backbones.

</details>


### [60] [Exploring Microstructural Dynamics in Cryptocurrency Limit Order Books: Better Inputs Matter More Than Stacking Another Hidden Layer](https://arxiv.org/abs/2506.05764)
*Haochuan,Wang*

Main category: cs.LG

TL;DR: 研究发现，在加密货币价格预测中，数据预处理和特征工程比增加神经网络复杂度更重要，简单模型表现优于复杂模型。


<details>
  <summary>Details</summary>
Motivation: 探讨在加密货币限价订单簿（LOB）数据中，增加神经网络复杂度是否真的能提升短期价格预测性能，还是数据预处理和特征工程起主要作用。

Method: 比较了从逻辑回归、XGBoost到深度架构（DeepLOB、Conv1D+LSTM）的多种模型，使用BTC/USDT的LOB数据，并引入两种数据过滤方法（卡尔曼滤波、Savitzky-Golay滤波）和两种标签方案（二元和三元）。

Result: 经过数据预处理和超参数调优后，简单模型的性能可以匹配甚至超过复杂网络，同时提供更快的推理速度和更高的可解释性。

Conclusion: 在加密货币价格预测中，数据预处理和特征工程是关键，简单模型在性能和效率上更具优势。

Abstract: Cryptocurrency price dynamics are driven largely by microstructural supply
demand imbalances in the limit order book (LOB), yet the highly noisy nature of
LOB data complicates the signal extraction process. Prior research has
demonstrated that deep-learning architectures can yield promising predictive
performance on pre-processed equity and futures LOB data, but they often treat
model complexity as an unqualified virtue. In this paper, we aim to examine
whether adding extra hidden layers or parameters to "blackbox ish" neural
networks genuinely enhances short term price forecasting, or if gains are
primarily attributable to data preprocessing and feature engineering. We
benchmark a spectrum of models from interpretable baselines, logistic
regression, XGBoost to deep architectures (DeepLOB, Conv1D+LSTM) on BTC/USDT
LOB snapshots sampled at 100 ms to multi second intervals using publicly
available Bybit data. We introduce two data filtering pipelines (Kalman,
Savitzky Golay) and evaluate both binary (up/down) and ternary (up/flat/down)
labeling schemes. Our analysis compares models on out of sample accuracy,
latency, and robustness to noise. Results reveal that, with data preprocessing
and hyperparameter tuning, simpler models can match and even exceed the
performance of more complex networks, offering faster inference and greater
interpretability.

</details>


### [61] [AANet: Virtual Screening under Structural Uncertainty via Alignment and Aggregation](https://arxiv.org/abs/2506.05768)
*Wenyu Zhu,Jianhui Wang,Bowen Gao,Yinjun Jia,Haichuan Tan,Ya-Qin Zhang,Wei-Ying Ma,Yanyan Lan*

Main category: cs.LG

TL;DR: 提出了一种基于对齐和聚合的框架，用于在结构不确定性下实现准确的虚拟筛选，显著提升了在apo结构上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟筛选方法依赖于已知配体结合口袋的holo蛋白结构，而在apo或预测结构（如AlphaFold2生成）上表现不佳，限制了早期药物发现的应用。

Method: 方法包含两部分：(1) 三模态对比学习模块，对齐配体、holo口袋和检测到的空腔表示；(2) 基于交叉注意力的适配器，动态聚合候选结合位点。

Result: 在apo结构基准测试中，方法显著优于现有技术，EF1%从11.75提升至37.19，同时在holo结构上保持强性能。

Conclusion: 该方法为缺乏实验解析蛋白-配体复合物的场景提供了有前景的解决方案，推动了首创药物的发现。

Abstract: Virtual screening (VS) is a critical component of modern drug discovery, yet
most existing methods--whether physics-based or deep learning-based--are
developed around holo protein structures with known ligand-bound pockets.
Consequently, their performance degrades significantly on apo or predicted
structures such as those from AlphaFold2, which are more representative of
real-world early-stage drug discovery, where pocket information is often
missing. In this paper, we introduce an alignment-and-aggregation framework to
enable accurate virtual screening under structural uncertainty. Our method
comprises two core components: (1) a tri-modal contrastive learning module that
aligns representations of the ligand, the holo pocket, and cavities detected
from structures, thereby enhancing robustness to pocket localization error; and
(2) a cross-attention based adapter for dynamically aggregating candidate
binding sites, enabling the model to learn from activity data even without
precise pocket annotations. We evaluated our method on a newly curated
benchmark of apo structures, where it significantly outperforms
state-of-the-art methods in blind apo setting, improving the early enrichment
factor (EF1%) from 11.75 to 37.19. Notably, it also maintains strong
performance on holo structures. These results demonstrate the promise of our
approach in advancing first-in-class drug discovery, particularly in scenarios
lacking experimentally resolved protein-ligand complexes.

</details>


### [62] [Evaluating Neuron Explanations: A Unified Framework with Sanity Checks](https://arxiv.org/abs/2506.05774)
*Tuomas Oikarinen,Ge Yan,Tsui-Wei Weng*

Main category: cs.LG

TL;DR: 本文提出了一个统一的数学框架来评估神经网络单元的解释方法，并提出了两个简单的合理性检验，发现许多常用指标未能通过这些检验。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络中单个单元的功能是机制可解释性的重要基础，但需要确保生成的文本解释可靠且真实。

Method: 统一现有解释评估方法于一个数学框架下，比较评估指标，并应用统计方法。提出两个合理性检验。

Result: 许多常用评估指标未能通过合理性检验，且在概念标签大幅改变时得分不变。

Conclusion: 提出了未来评估应遵循的指南，并确定了一组可靠的评估指标。

Abstract: Understanding the function of individual units in a neural network is an
important building block for mechanistic interpretability. This is often done
by generating a simple text explanation of the behavior of individual neurons
or units. For these explanations to be useful, we must understand how reliable
and truthful they are. In this work we unify many existing explanation
evaluation methods under one mathematical framework. This allows us to compare
existing evaluation metrics, understand the evaluation pipeline with increased
clarity and apply existing statistical methods on the evaluation. In addition,
we propose two simple sanity checks on the evaluation metrics and show that
many commonly used metrics fail these tests and do not change their score after
massive changes to the concept labels. Based on our experimental and
theoretical results, we propose guidelines that future evaluations should
follow and identify a set of reliable evaluation metrics.

</details>


### [63] [Exploiting Similarity for Computation and Communication-Efficient Decentralized Optimization](https://arxiv.org/abs/2506.05791)
*Yuki Takezawa,Xiaowen Jiang,Anton Rodomanov,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 提出了一种新的稳定近端去中心化优化方法（SPDO），在减少通信复杂度的同时降低了计算开销，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 去中心化优化中通信复杂度是关键问题，现有方法在解决子问题时计算开销大。

Method: 提出SPDO方法，放宽子问题精度要求并利用平均功能相似性。

Result: SPDO在通信和计算复杂度上达到最优，实验表现显著优于现有方法。

Conclusion: SPDO是一种高效的去中心化优化方法，适用于功能相似性高的场景。

Abstract: Reducing communication complexity is critical for efficient decentralized
optimization. The proximal decentralized optimization (PDO) framework is
particularly appealing, as methods within this framework can exploit functional
similarity among nodes to reduce communication rounds. Specifically, when local
functions at different nodes are similar, these methods achieve faster
convergence with fewer communication steps. However, existing PDO methods often
require highly accurate solutions to subproblems associated with the proximal
operator, resulting in significant computational overhead. In this work, we
propose the Stabilized Proximal Decentralized Optimization (SPDO) method, which
achieves state-of-the-art communication and computational complexities within
the PDO framework. Additionally, we refine the analysis of existing PDO methods
by relaxing subproblem accuracy requirements and leveraging average functional
similarity. Experimental results demonstrate that SPDO significantly
outperforms existing methods.

</details>


### [64] [EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator](https://arxiv.org/abs/2506.05797)
*Qianyi Chen,Tianrun Gao,Chenbo Jiang,Tailin Wu*

Main category: cs.LG

TL;DR: EqCollide是一种端到端的等变神经场模拟器，用于模拟可变形物体及其碰撞，解决了现有方法在物理对称性、碰撞处理和可扩展性上的不足。


<details>
  <summary>Details</summary>
Motivation: 模拟可变形物体的碰撞是一个复杂且具有挑战性的任务，现有数据驱动方法在物理对称性、碰撞处理和可扩展性上存在不足。

Method: 提出了一种等变编码器将物体几何和速度映射到潜在控制点，并使用基于等变图神经网络的神经常微分方程通过碰撞感知消息传递建模控制点间的交互。通过查询神经场重建速度场，实现连续且分辨率无关的运动预测。

Result: 实验表明，EqCollide在多种物体配置下实现了准确、稳定且可扩展的模拟，其滚动均方误差比最佳基线模型低24.34%至35.82%。

Conclusion: EqCollide能够泛化到更多碰撞物体和更长的时间范围，并对群作用变换的输入保持鲁棒性。

Abstract: Simulating collisions of deformable objects is a fundamental yet challenging
task due to the complexity of modeling solid mechanics and multi-body
interactions. Existing data-driven methods often suffer from lack of
equivariance to physical symmetries, inadequate handling of collisions, and
limited scalability. Here we introduce EqCollide, the first end-to-end
equivariant neural fields simulator for deformable objects and their
collisions. We propose an equivariant encoder to map object geometry and
velocity into latent control points. A subsequent equivariant Graph Neural
Network-based Neural Ordinary Differential Equation models the interactions
among control points via collision-aware message passing. To reconstruct
velocity fields, we query a neural field conditioned on control point features,
enabling continuous and resolution-independent motion predictions. Experimental
results show that EqCollide achieves accurate, stable, and scalable simulations
across diverse object configurations, and our model achieves 24.34% to 35.82%
lower rollout MSE even compared with the best-performing baseline model.
Furthermore, our model could generalize to more colliding objects and extended
temporal horizons, and stay robust to input transformed with group action.

</details>


### [65] [Option Pricing Using Ensemble Learning](https://arxiv.org/abs/2506.05799)
*Zeyuan Li,Qingdao Huang*

Main category: cs.LG

TL;DR: 本文研究了集成学习在期权定价中的应用，并与经典机器学习模型进行了比较，提出了一种新的实验策略和评估机制。


<details>
  <summary>Details</summary>
Motivation: 期权定价需要高预测精度和低结构复杂度，集成学习的灵活性、高精度和精细结构与之契合。

Method: 采用集成学习方法，引入参数传递实验策略，开发了结合评分和加权评估的机制，并研究了滑动窗口技术与噪声的交互。

Result: 研究发现集成学习在准确性、局部特征提取和噪声鲁棒性方面表现优异，揭示了滑动窗口与噪声的潜在联系。

Conclusion: 集成学习在期权定价中具有优势，实验策略和评估机制为金融理论与计算方法的结合提供了有序框架。

Abstract: Ensemble learning is characterized by flexibility, high precision, and
refined structure. As a critical component within computational finance, option
pricing with machine learning requires both high predictive accuracy and
reduced structural complexity-features that align well with the inherent
advantages of ensemble learning. This paper investigates the application of
ensemble learning to option pricing, and conducts a comparative analysis with
classical machine learning models to assess their performance in terms of
accuracy, local feature extraction, and robustness to noise. A novel
experimental strategy is introduced, leveraging parameter transfer across
experiments to improve robustness and realism in financial simulations.Building
upon this strategy, an evaluation mechanism is developed that incorporates a
scoring strategy and a weighted evaluation strategy explicitly emphasizing the
foundational role of financial theory. This mechanism embodies an orderly
integration of theoretical finance and computational methods. In addition, the
study examines the interaction between sliding window technique and noise,
revealing nuanced patterns that suggest a potential connection relevant to
ongoing research in machine learning and data science.

</details>


### [66] [Neural Collapse in Cumulative Link Models for Ordinal Regression: An Analysis with Unconstrained Feature Model](https://arxiv.org/abs/2506.05801)
*Chuang Ma,Tomoyuki Obuchi,Toshiyuki Tanaka*

Main category: cs.LG

TL;DR: 研究发现，在深度序数回归任务中会出现一种称为“序数神经坍缩”（ONC）的现象，其特征包括类内特征坍缩、类均值与分类器对齐以及潜在变量按类顺序排列。


<details>
  <summary>Details</summary>
Motivation: 探索深度序数回归任务中是否会出现类似“神经坍缩”的现象，以深化对深度神经网络行为的理解。

Method: 结合累积链接模型和无约束特征模型（UFM），理论分析并实证验证ONC现象。

Result: ONC现象确实存在，并表现出三种特性：类内特征坍缩、类均值与分类器对齐、潜在变量按类顺序排列。

Conclusion: ONC现象为序数回归任务提供了新的理论支持，并展示了固定阈值的应用潜力。

Abstract: A phenomenon known as ''Neural Collapse (NC)'' in deep classification tasks,
in which the penultimate-layer features and the final classifiers exhibit an
extremely simple geometric structure, has recently attracted considerable
attention, with the expectation that it can deepen our understanding of how
deep neural networks behave. The Unconstrained Feature Model (UFM) has been
proposed to explain NC theoretically, and there emerges a growing body of work
that extends NC to tasks other than classification and leverages it for
practical applications. In this study, we investigate whether a similar
phenomenon arises in deep Ordinal Regression (OR) tasks, via combining the
cumulative link model for OR and UFM. We show that a phenomenon we call Ordinal
Neural Collapse (ONC) indeed emerges and is characterized by the following
three properties: (ONC1) all optimal features in the same class collapse to
their within-class mean when regularization is applied; (ONC2) these class
means align with the classifier, meaning that they collapse onto a
one-dimensional subspace; (ONC3) the optimal latent variables (corresponding to
logits or preactivations in classification tasks) are aligned according to the
class order, and in particular, in the zero-regularization limit, a highly
local and simple geometric relationship emerges between the latent variables
and the threshold values. We prove these properties analytically within the UFM
framework with fixed threshold values and corroborate them empirically across a
variety of datasets. We also discuss how these insights can be leveraged in OR,
highlighting the use of fixed thresholds.

</details>


### [67] [Positional Encoding meets Persistent Homology on Graphs](https://arxiv.org/abs/2506.05814)
*Yogesh Verma,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: 论文分析了消息传递图神经网络（GNNs）的局部归纳偏差问题，提出了一种结合位置编码（PE）和持久同调（PH）的新方法PiPE，证明其比两者更具表达力，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决GNNs因局部归纳偏差而无法充分利用结构信息（如连通性和循环）的问题，探索PE和PH方法的优缺点。

Method: 通过理论分析比较PE和PH的表达能力，设计了一种结合两者的新方法PiPE。

Result: PiPE在分子性质预测、图分类和分布外泛化等任务中表现优于PE和PH。

Conclusion: PiPE是一种更具表达力的方法，为图表示学习提供了新的方向。

Abstract: The local inductive bias of message-passing graph neural networks (GNNs)
hampers their ability to exploit key structural information (e.g., connectivity
and cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged
as two promising approaches to mitigate this issue. PE schemes endow GNNs with
location-aware features, while PH methods enhance GNNs with multiresolution
topological features. However, a rigorous theoretical characterization of the
relative merits and shortcomings of PE and PH has remained elusive. We bridge
this gap by establishing that neither paradigm is more expressive than the
other, providing novel constructions where one approach fails but the other
succeeds. Our insights inform the design of a novel learnable method, PiPE
(Persistence-informed Positional Encoding), which is provably more expressive
than both PH and PE. PiPE demonstrates strong performance across a variety of
tasks (e.g., molecule property prediction, graph classification, and
out-of-distribution generalization), thereby advancing the frontiers of graph
representation learning. Code is available at
https://github.com/Aalto-QuML/PIPE.

</details>


### [68] [Learning Along the Arrow of Time: Hyperbolic Geometry for Backward-Compatible Representation Learning](https://arxiv.org/abs/2506.05826)
*Ngoc Bui,Menglin Yang,Runjin Chen,Leonardo Neves,Mingxuan Ju,Rex Ying,Neil Shah,Tong Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于双曲几何的向后兼容表示学习方法，通过将嵌入提升到双曲空间并考虑旧模型的不确定性，实现了模型的代际一致性。


<details>
  <summary>Details</summary>
Motivation: 现有欧几里得空间中的兼容性方法忽略了旧嵌入模型的不确定性，强制新模型重建过时的表示，阻碍了新模型的学习过程。

Method: 将嵌入提升到双曲空间，约束更新后的嵌入位于旧嵌入的蕴含锥内，并引入动态调整对齐权重的对比对齐损失。

Result: 实验验证了该方法在实现兼容性方面的优越性。

Conclusion: 该方法为构建更具弹性和适应性的机器学习系统提供了新思路。

Abstract: Backward compatible representation learning enables updated models to
integrate seamlessly with existing ones, avoiding to reprocess stored data.
Despite recent advances, existing compatibility approaches in Euclidean space
neglect the uncertainty in the old embedding model and force the new model to
reconstruct outdated representations regardless of their quality, thereby
hindering the learning process of the new model. In this paper, we propose to
switch perspectives to hyperbolic geometry, where we treat time as a natural
axis for capturing a model's confidence and evolution. By lifting embeddings
into hyperbolic space and constraining updated embeddings to lie within the
entailment cone of the old ones, we maintain generational consistency across
models while accounting for uncertainties in the representations. To further
enhance compatibility, we introduce a robust contrastive alignment loss that
dynamically adjusts alignment weights based on the uncertainty of the old
embeddings. Experiments validate the superiority of the proposed method in
achieving compatibility, paving the way for more resilient and adaptable
machine learning systems.

</details>


### [69] [Heartcare Suite: Multi-dimensional Understanding of ECG with Raw Multi-lead Signal Modeling](https://arxiv.org/abs/2506.05831)
*Yihan Xie,Sijing Li,Tianwei Lin,Zhuonan Wang,Chenglin Yang,Yu Zhong,Wenqiao Zhang,Haoyuan Li,Hao Jiang,Fengda Zhang,Qishan Chen,Jun Xiao,Yueting Zhuang,Beng Chin Ooi*

Main category: cs.LG

TL;DR: Heartcare Suite是一个多模态ECG理解框架，包含数据集、基准测试和模型HeartcareGPT，通过实验验证其在ECG任务中的高效性和领先性能。


<details>
  <summary>Details</summary>
Motivation: 提升ECG的多模态理解和评估能力，为医学多模态大语言模型（Med-MLLMs）在ECG场景中的优化提供支持。

Method: 框架包含三个部分：高质量数据集Heartcare-220K、系统性基准测试Heartcare-Bench和模型HeartcareGPT（采用定制化分词器Beat）。

Result: HeartcareGPT在多个临床任务中表现出强大的泛化能力和最先进的性能。

Conclusion: Heartcare Suite有效推动了ECG多模态理解和评估的进展，项目已开源。

Abstract: We present Heartcare Suite, a multimodal comprehensive framework for
finegrained electrocardiogram (ECG) understanding. It comprises three key
components: (i) Heartcare-220K, a high-quality, structured, and comprehensive
multimodal ECG dataset covering essential tasks such as disease diagnosis,
waveform morphology analysis, and rhythm interpretation. (ii) Heartcare-Bench,
a systematic and multi-dimensional benchmark designed to evaluate diagnostic
intelligence and guide the optimization of Medical Multimodal Large Language
Models (Med-MLLMs) in ECG scenarios. and (iii) HeartcareGPT with a tailored
tokenizer Bidirectional ECG Abstract Tokenization (Beat), which compresses raw
multi-lead signals into semantically rich discrete tokens via duallevel vector
quantization and query-guided bidirectional diffusion mechanism. Built upon
Heartcare-220K, HeartcareGPT achieves strong generalization and SoTA
performance across multiple clinically meaningful tasks. Extensive experiments
demonstrate that Heartcare Suite is highly effective in advancing ECGspecific
multimodal understanding and evaluation. Our project is available at
https://github.com/DCDmllm/Heartcare-Suite .

</details>


### [70] [Wavelet-based Disentangled Adaptive Normalization for Non-stationary Times Series Forecasting](https://arxiv.org/abs/2506.05857)
*Junpeng Lin,Tian Lan,Bo Zhang,Ke Lin,Dandan Miao,Huiru He,Jiantao Ye,Chen Zhang,Yan-fu Li*

Main category: cs.LG

TL;DR: 提出了一种基于小波的解耦自适应归一化（WDAN）框架，用于处理非平稳时间序列预测中的多组分特性。


<details>
  <summary>Details</summary>
Motivation: 非平稳时间序列的统计特性随时间变化，现有方法常忽略其多组分特性，导致预测性能不佳。

Method: 使用离散小波变换将输入分解为低频趋势和高频波动，并分别应用定制归一化策略。对强非平稳趋势部分，采用一阶差分提取稳定特征。

Result: 在多个基准测试中，WDAN显著提升了不同主干模型的预测准确性。

Conclusion: WDAN是一种模型无关的框架，能有效处理时间序列的非平稳性，提升预测性能。

Abstract: Forecasting non-stationary time series is a challenging task because their
statistical properties often change over time, making it hard for deep models
to generalize well. Instance-level normalization techniques can help address
shifts in temporal distribution. However, most existing methods overlook the
multi-component nature of time series, where different components exhibit
distinct non-stationary behaviors. In this paper, we propose Wavelet-based
Disentangled Adaptive Normalization (WDAN), a model-agnostic framework designed
to address non-stationarity in time series forecasting. WDAN uses discrete
wavelet transforms to break down the input into low-frequency trends and
high-frequency fluctuations. It then applies tailored normalization strategies
to each part. For trend components that exhibit strong non-stationarity, we
apply first-order differencing to extract stable features used for predicting
normalization parameters. Extensive experiments on multiple benchmarks
demonstrate that WDAN consistently improves forecasting accuracy across various
backbone model. Code is available at this repository:
https://github.com/MonBG/WDAN.

</details>


### [71] [Loss Functions for Predictor-based Neural Architecture Search](https://arxiv.org/abs/2506.05869)
*Han Ji,Yuqi Feng,Jiahao Fan,Yanan Sun*

Main category: cs.LG

TL;DR: 本文首次全面研究了性能预测器中损失函数的作用，将其分为回归、排序和加权三类，并通过实验验证了它们的组合效果。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索（NAS）中评估成本高，性能预测器通过损失函数选择直接影响效果，但现有研究未深入探讨不同损失函数的特性与效果。

Method: 将损失函数分为回归、排序和加权三类，评估了八种损失函数在13个任务和五个搜索空间中的表现。

Result: 研究发现特定类别的损失函数组合可提升预测器性能，并为不同任务选择损失函数提供了实用指导。

Conclusion: 本文为NAS社区中性能预测器的损失函数开发提供了有价值的见解。

Abstract: Evaluation is a critical but costly procedure in neural architecture search
(NAS). Performance predictors have been widely adopted to reduce evaluation
costs by directly estimating architecture performance. The effectiveness of
predictors is heavily influenced by the choice of loss functions. While
traditional predictors employ regression loss functions to evaluate the
absolute accuracy of architectures, recent approaches have explored various
ranking-based loss functions, such as pairwise and listwise ranking losses, to
focus on the ranking of architecture performance. Despite their success in NAS,
the effectiveness and characteristics of these loss functions have not been
thoroughly investigated. In this paper, we conduct the first comprehensive
study on loss functions in performance predictors, categorizing them into three
main types: regression, ranking, and weighted loss functions. Specifically, we
assess eight loss functions using a range of NAS-relevant metrics on 13 tasks
across five search spaces. Our results reveal that specific categories of loss
functions can be effectively combined to enhance predictor-based NAS.
Furthermore, our findings could provide practical guidance for selecting
appropriate loss functions for various tasks. We hope this work provides
meaningful insights to guide the development of loss functions for
predictor-based methods in the NAS community.

</details>


### [72] [BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures](https://arxiv.org/abs/2506.05871)
*Xiannan Hu,Tianyou Zeng,Xiaoming Yuan,Liwei Song,Guangyuan Zhang,Bangzheng He*

Main category: cs.LG

TL;DR: BestServe是一个轻量级框架，通过模拟推理和动态调度快速评估LLM服务策略，避免高成本基准测试。


<details>
  <summary>Details</summary>
Motivation: 为大规模语言模型（LLM）服务设计高效资源分配和并行策略是一个耗时且试错的过程，需要自动化解决方案。

Method: BestServe基于改进的屋顶模型和CPU-GPU调度动态构建推理模拟器，支持多种架构，快速评估策略。

Result: 在单CPU上几分钟内确定最优策略，预测误差在20%以内，适用于快速部署。

Conclusion: BestServe因其轻量设计和强扩展性，成为LLM服务策略评估的实用工具。

Abstract: Serving large language models (LLMs) to millions of users requires efficient
resource allocation and parallelism strategies. It is a labor intensive
trial-and-error process to find such a strategy. We present BestServe, a novel
framework for ranking serving strategies by estimating goodput under various
operating scenarios. Supporting both collocated and disaggregated
architectures, BestServe leverages an inference simulator built on an adapted
roofline model and CPU-GPU dispatch dynamics. Our framework determines the
optimal strategy in minutes on a single standard CPU, eliminating the need for
costly benchmarking, while achieving predictions within a $20\%$ error margin.
It appeals to be practical for rapid deployment planning because of its
lightweight design and strong extensibility.

</details>


### [73] [Interpretable Clustering Ensemble](https://arxiv.org/abs/2506.05877)
*Hang Lv,Lianyu Hu,Mudi Jiang,Xinying Liu,Zengyou He*

Main category: cs.LG

TL;DR: 提出了一种可解释的聚类集成算法，填补了该领域缺乏可解释性方法的空白。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中（如医疗诊断和金融风险评估），算法不仅需要准确，还需具备可解释性以确保决策透明可信。

Method: 将基础分区视为分类变量，在原始特征空间中构建决策树，并使用统计关联测试指导树的构建过程。

Result: 实验表明，该算法性能与最先进的聚类集成方法相当，同时具备可解释性。

Conclusion: 这是首个专为聚类集成设计的可解释算法，为未来可解释聚类研究提供了新视角。

Abstract: Clustering ensemble has emerged as an important research topic in the field
of machine learning. Although numerous methods have been proposed to improve
clustering quality, most existing approaches overlook the need for
interpretability in high-stakes applications. In domains such as medical
diagnosis and financial risk assessment, algorithms must not only be accurate
but also interpretable to ensure transparent and trustworthy decision-making.
Therefore, to fill the gap of lack of interpretable algorithms in the field of
clustering ensemble, we propose the first interpretable clustering ensemble
algorithm in the literature. By treating base partitions as categorical
variables, our method constructs a decision tree in the original feature space
and use the statistical association test to guide the tree building process.
Experimental results demonstrate that our algorithm achieves comparable
performance to state-of-the-art (SOTA) clustering ensemble methods while
maintaining an additional feature of interpretability. To the best of our
knowledge, this is the first interpretable algorithm specifically designed for
clustering ensemble, offering a new perspective for future research in
interpretable clustering.

</details>


### [74] [A projection-based framework for gradient-free and parallel learning](https://arxiv.org/abs/2506.05878)
*Andreas Bergmeister,Manish Krishan Lal,Stefanie Jegelka,Suvrit Sra*

Main category: cs.LG

TL;DR: 论文提出了一种基于可行性搜索的神经网络训练方法，替代传统的梯度下降损失最小化框架，利用投影算子和迭代投影算法，将训练问题转化为大规模可行性问题。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降方法在处理非可微操作和并行化方面存在局限性，因此需要一种新的训练框架。

Method: 通过将训练问题转化为可行性问题，利用投影算子和迭代投影算法进行参数优化，并开发了基于JAX的软件框架PJAX。

Result: 实验表明，该方法在并行化和处理非可微操作方面具有优势，能够训练多种网络架构（MLPs、CNNs、RNNs）。

Conclusion: 该框架为神经网络训练提供了一种可行的替代方案，尤其在并行化和非可微操作场景下表现突出。

Abstract: We present a feasibility-seeking approach to neural network training. This
mathematical optimization framework is distinct from conventional
gradient-based loss minimization and uses projection operators and iterative
projection algorithms. We reformulate training as a large-scale feasibility
problem: finding network parameters and states that satisfy local constraints
derived from its elementary operations. Training then involves projecting onto
these constraints, a local operation that can be parallelized across the
network. We introduce PJAX, a JAX-based software framework that enables this
paradigm. PJAX composes projection operators for elementary operations,
automatically deriving the solution operators for the feasibility problems
(akin to autodiff for derivatives). It inherently supports GPU/TPU
acceleration, provides a familiar NumPy-like API, and is extensible. We train
diverse architectures (MLPs, CNNs, RNNs) on standard benchmarks using PJAX,
demonstrating its functionality and generality. Our results show that this
approach is as a compelling alternative to gradient-based training, with clear
advantages in parallelism and the ability to handle non-differentiable
operations.

</details>


### [75] [NILMFormer: Non-Intrusive Load Monitoring that Accounts for Non-Stationarity](https://arxiv.org/abs/2506.05880)
*Adrien Petralia,Philippe Charpentier,Youssef Kadhi,Themis Palpanas*

Main category: cs.LG

TL;DR: NILMFormer是一种基于Transformer的架构，通过子序列平稳化/去平稳化方案和新型位置编码，显著提升了非侵入式负载监测（NILM）的性能。


<details>
  <summary>Details</summary>
Motivation: 传统NILM方法依赖于统计数据和静态客户信息，而现有深度学习解决方案因数据分布漂移问题影响性能。

Method: 提出NILMFormer，结合子序列平稳化/去平稳化方案和基于时间戳的位置编码。

Result: 在4个真实数据集上显著优于现有方法，并已部署为EDF的能耗监测服务核心算法。

Conclusion: NILMFormer有效解决了数据分布漂移问题，为NILM提供了高效解决方案。

Abstract: Millions of smart meters have been deployed worldwide, collecting the total
power consumed by individual households. Based on these data, electricity
suppliers offer their clients energy monitoring solutions to provide feedback
on the consumption of their individual appliances. Historically, such estimates
have relied on statistical methods that use coarse-grained total monthly
consumption and static customer data, such as appliance ownership.
Non-Intrusive Load Monitoring (NILM) is the problem of disaggregating a
household's collected total power consumption to retrieve the consumed power
for individual appliances. Current state-of-the-art (SotA) solutions for NILM
are based on deep-learning (DL) and operate on subsequences of an entire
household consumption reading. However, the non-stationary nature of real-world
smart meter data leads to a drift in the data distribution within each
segmented window, which significantly affects model performance. This paper
introduces NILMFormer, a Transformer-based architecture that incorporates a new
subsequence stationarization/de-stationarization scheme to mitigate the
distribution drift and that uses a novel positional encoding that relies only
on the subsequence's timestamp information. Experiments with 4 real-world
datasets show that NILMFormer significantly outperforms the SotA approaches.
Our solution has been deployed as the backbone algorithm for EDF's
(Electricit\'e De France) consumption monitoring service, delivering detailed
insights to millions of customers about their individual appliances' power
consumption. This paper appeared in KDD 2025.

</details>


### [76] [Few Labels are all you need: A Weakly Supervised Framework for Appliance Localization in Smart-Meter Series](https://arxiv.org/abs/2506.05895)
*Adrien Petralia,Paul Boniol,Philippe Charpentier,Themis Palpanas*

Main category: cs.LG

TL;DR: CamAL提出了一种弱监督方法，用于家电模式定位，仅需家电存在信息即可训练，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能电表数据难以分解为单个家电的用电模式，现有全监督方法需要昂贵且稀缺的标签数据。

Method: CamAL结合深度学习分类器和可解释分类方法，仅需家电存在信息即可定位用电模式。

Result: 在4个真实数据集上，CamAL显著优于现有弱监督基线，且性能接近全监督方法。

Conclusion: CamAL为智能电网管理提供了一种高效且低成本的解决方案，减少了对标签数据的依赖。

Abstract: Improving smart grid system management is crucial in the fight against
climate change, and enabling consumers to play an active role in this effort is
a significant challenge for electricity suppliers. In this regard, millions of
smart meters have been deployed worldwide in the last decade, recording the
main electricity power consumed in individual households. This data produces
valuable information that can help them reduce their electricity footprint;
nevertheless, the collected signal aggregates the consumption of the different
appliances running simultaneously in the house, making it difficult to
apprehend. Non-Intrusive Load Monitoring (NILM) refers to the challenge of
estimating the power consumption, pattern, or on/off state activation of
individual appliances using the main smart meter signal. Recent methods
proposed to tackle this task are based on a fully supervised deep-learning
approach that requires both the aggregate signal and the ground truth of
individual appliance power. However, such labels are expensive to collect and
extremely scarce in practice, as they require conducting intrusive surveys in
households to monitor each appliance. In this paper, we introduce CamAL, a
weakly supervised approach for appliance pattern localization that only
requires information on the presence of an appliance in a household to be
trained. CamAL merges an ensemble of deep-learning classifiers combined with an
explainable classification method to be able to localize appliance patterns.
Our experimental evaluation, conducted on 4 real-world datasets, demonstrates
that CamAL significantly outperforms existing weakly supervised baselines and
that current SotA fully supervised NILM approaches require significantly more
labels to reach CamAL performances. The source of our experiments is available
at: https://github.com/adrienpetralia/CamAL. This paper appeared in ICDE 2025.

</details>


### [77] [A Driving Regime-Embedded Deep Learning Framework for Modeling Intra-Driver Heterogeneity in Multi-Scale Car-Following Dynamics](https://arxiv.org/abs/2506.05902)
*Shirui Zhou,Jiying Yan,Junfang Tian,Tao Wang,Yongfu Li,Shiquan Zhong*

Main category: cs.LG

TL;DR: 提出了一种新的数据驱动跟车模型框架，通过结合离散驾驶状态分类和连续运动预测，显著提高了对驾驶员动态异质性的建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以准确捕捉单一驾驶员在不同条件下的动态异质性，因此需要一种更全面的方法来表征驾驶行为的复杂性。

Method: 采用混合深度学习架构，结合GRU进行离散驾驶状态分类和LSTM进行连续运动预测，并使用DTW和分段算法识别驾驶状态。

Result: 模型显著降低了加速度、速度和间距的预测误差（最大MSE改进达58.47%），并能重现关键交通现象。

Conclusion: 该框架有效统一了离散决策和连续动态，为驾驶员异质性建模提供了新思路。

Abstract: A fundamental challenge in car-following modeling lies in accurately
representing the multi-scale complexity of driving behaviors, particularly the
intra-driver heterogeneity where a single driver's actions fluctuate
dynamically under varying conditions. While existing models, both conventional
and data-driven, address behavioral heterogeneity to some extent, they often
emphasize inter-driver heterogeneity or rely on simplified assumptions,
limiting their ability to capture the dynamic heterogeneity of a single driver
under different driving conditions. To address this gap, we propose a novel
data-driven car-following framework that systematically embeds discrete driving
regimes (e.g., steady-state following, acceleration, cruising) into vehicular
motion predictions. Leveraging high-resolution traffic trajectory datasets, the
proposed hybrid deep learning architecture combines Gated Recurrent Units for
discrete driving regime classification with Long Short-Term Memory networks for
continuous kinematic prediction, unifying discrete decision-making processes
and continuous vehicular dynamics to comprehensively represent inter- and
intra-driver heterogeneity. Driving regimes are identified using a bottom-up
segmentation algorithm and Dynamic Time Warping, ensuring robust
characterization of behavioral states across diverse traffic scenarios.
Comparative analyses demonstrate that the framework significantly reduces
prediction errors for acceleration (maximum MSE improvement reached 58.47\%),
speed, and spacing metrics while reproducing critical traffic phenomena, such
as stop-and-go wave propagation and oscillatory dynamics.

</details>


### [78] [DeviceScope: An Interactive App to Detect and Localize Appliance Patterns in Electricity Consumption Time Series](https://arxiv.org/abs/2506.05912)
*Adrien Petralia,Paul Boniol,Philippe Charpentier,Themis Palpanas*

Main category: cs.LG

TL;DR: DeviceScope是一个交互式工具，帮助非专业用户理解智能电表数据，通过检测和定位单个电器模式。


<details>
  <summary>Details</summary>
Motivation: 智能电表数据量大且复杂，非专业用户难以理解，且缺乏标注数据训练模型。

Method: 基于CamAL（类激活映射电器定位）的弱监督方法，仅需知道电器存在即可训练。

Result: DeviceScope能够有效检测和定位电器模式。

Conclusion: DeviceScope为智能电表数据分析提供了实用工具，解决了数据理解和标注稀缺问题。

Abstract: In recent years, electricity suppliers have installed millions of smart
meters worldwide to improve the management of the smart grid system. These
meters collect a large amount of electrical consumption data to produce
valuable information to help consumers reduce their electricity footprint.
However, having non-expert users (e.g., consumers or sales advisors) understand
these data and derive usage patterns for different appliances has become a
significant challenge for electricity suppliers because these data record the
aggregated behavior of all appliances. At the same time, ground-truth labels
(which could train appliance detection and localization models) are expensive
to collect and extremely scarce in practice. This paper introduces DeviceScope,
an interactive tool designed to facilitate understanding smart meter data by
detecting and localizing individual appliance patterns within a given time
period. Our system is based on CamAL (Class Activation Map-based Appliance
Localization), a novel weakly supervised approach for appliance localization
that only requires the knowledge of the existence of an appliance in a
household to be trained. This paper appeared in ICDE 2025.

</details>


### [79] [Over-PINNs: Enhancing Physics-Informed Neural Networks via Higher-Order Partial Derivative Overdetermination of PDEs](https://arxiv.org/abs/2506.05918)
*Wenxuan Huo,Qiang He,Gang Zhu,Weifeng Huang*

Main category: cs.LG

TL;DR: Over-PINNs框架通过自动微分生成高阶辅助方程，增强PINNs在复杂问题中的准确性，显著提升解精度且计算成本增加不大。


<details>
  <summary>Details</summary>
Motivation: 解决PINNs在处理复杂问题时精度不足的问题。

Method: 利用自动微分生成高阶辅助方程，作为额外损失项嵌入训练过程。

Result: 在多种PDE求解中表现出强通用性，显著提升精度且计算成本可控。

Conclusion: Over-PINNs框架通过过定方法有效提升了PINNs的物理信息捕捉能力。

Abstract: Partial differential equations (PDEs) serve as the cornerstone of
mathematical physics. In recent years, Physics-Informed Neural Networks (PINNs)
have significantly reduced the dependence on large datasets by embedding
physical laws directly into the training of neural networks. However, when
dealing with complex problems, the accuracy of PINNs still has room for
improvement. To address this issue, we introduce the Over-PINNs framework,
which leverages automatic differentiation (AD) to generate higher-order
auxiliary equations that impose additional physical constraints. These
equations are incorporated as extra loss terms in the training process,
effectively enhancing the model's ability to capture physical information
through an "overdetermined" approach. Numerical results illustrate that this
method exhibits strong versatility in solving various types of PDEs. It
achieves a significant improvement in solution accuracy without incurring
substantial additional computational costs.

</details>


### [80] [Machine Learning Predictions for Traffic Equilibria in Road Renovation Scheduling](https://arxiv.org/abs/2506.05933)
*Robbert Bosch,Wouter van Heeswijk,Patricia Rogetzer,Martijn Mes*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的替代模型，用于预测道路维护计划对交通拥堵的影响，以减少传统模拟方法的高计算负担。XGBoost模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 道路维护计划对交通拥堵的影响难以精确预测，传统模拟方法计算成本高，需寻找更高效的替代方案。

Method: 将问题建模为监督学习任务，使用独热编码、交通特征工程和启发式近似方法，评估多种回归模型。

Result: XGBoost模型在MAPE和Pinball损失上表现最优，显著优于其他模型。

Conclusion: 机器学习替代模型可有效降低大规模交通分配问题的计算负担，XGBoost是首选方法。

Abstract: Accurately estimating the impact of road maintenance schedules on traffic
conditions is important because maintenance operations can substantially worsen
congestion if not carefully planned. Reliable estimates allow planners to avoid
excessive delays during periods of roadwork. Since the exact increase in
congestion is difficult to predict analytically, traffic simulations are
commonly used to assess the redistribution of the flow of traffic. However,
when applied to long-term maintenance planning involving many overlapping
projects and scheduling alternatives, these simulations must be run thousands
of times, resulting in a significant computational burden. This paper
investigates the use of machine learning-based surrogate models to predict
network-wide congestion caused by simultaneous road renovations. We frame the
problem as a supervised learning task, using one-hot encodings, engineered
traffic features, and heuristic approximations. A range of linear,
ensemble-based, probabilistic, and neural regression models is evaluated under
an online learning framework in which data progressively becomes available. The
experimental results show that the Costliest Subset Heuristic provides a
reasonable approximation when limited training data is available, and that most
regression models fail to outperform it, with the exception of XGBoost, which
achieves substantially better accuracy. In overall performance, XGBoost
significantly outperforms alternatives in a range of metrics, most strikingly
Mean Absolute Percentage Error (MAPE) and Pinball loss, where it achieves a
MAPE of 11% and outperforms the next-best model by 20% and 38% respectively.
This modeling approach has the potential to reduce the computational burden of
large-scale traffic assignment problems in maintenance planning.

</details>


### [81] [Quantifying Adversarial Uncertainty in Evidential Deep Learning using Conflict Resolution](https://arxiv.org/abs/2506.05937)
*Charmaine Barker,Daniel Bethell,Simos Gerasimou*

Main category: cs.LG

TL;DR: Conflict-aware Evidential Deep Learning (C-EDL) 是一种轻量级后处理不确定性量化方法，通过生成多样化的任务保留变换和量化表示分歧，显著提升了对抗性和分布外输入的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在高风险应用中的可靠性至关重要，但现有方法（如 Evidential Deep Learning, EDL）对对抗性和分布外输入容易产生过度自信的错误。

Method: C-EDL 通过为每个输入生成多样化的任务保留变换，并量化表示分歧来校准不确定性估计，无需重新训练模型。

Result: 实验表明，C-EDL 在多种数据集、攻击类型和不确定性指标上显著优于现有方法，对分布外数据和对抗性数据的覆盖率分别降低了 55% 和 90%。

Conclusion: C-EDL 是一种高效且轻量级的方法，能够显著提升模型对对抗性和分布外输入的鲁棒性，同时保持高计算效率。

Abstract: Reliability of deep learning models is critical for deployment in high-stakes
applications, where out-of-distribution or adversarial inputs may lead to
detrimental outcomes. Evidential Deep Learning, an efficient paradigm for
uncertainty quantification, models predictions as Dirichlet distributions of a
single forward pass. However, EDL is particularly vulnerable to adversarially
perturbed inputs, making overconfident errors. Conflict-aware Evidential Deep
Learning (C-EDL) is a lightweight post-hoc uncertainty quantification approach
that mitigates these issues, enhancing adversarial and OOD robustness without
retraining. C-EDL generates diverse, task-preserving transformations per input
and quantifies representational disagreement to calibrate uncertainty estimates
when needed. C-EDL's conflict-aware prediction adjustment improves detection of
OOD and adversarial inputs, maintaining high in-distribution accuracy and low
computational overhead. Our experimental evaluation shows that C-EDL
significantly outperforms state-of-the-art EDL variants and competitive
baselines, achieving substantial reductions in coverage for OOD data (up to
55%) and adversarial data (up to 90%), across a range of datasets, attack
types, and uncertainty metrics.

</details>


### [82] [Exponential Family Variational Flow Matching for Tabular Data Generation](https://arxiv.org/abs/2506.05940)
*Andrés Guzmán-Cordero,Floor Eijkelboom,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: TabbyFlow是一种基于变分流匹配（VFM）的表格数据生成方法，通过指数族变分流匹配（EF-VFM）处理混合连续和离散特征，实现了高效的数据驱动目标。


<details>
  <summary>Details</summary>
Motivation: 尽管去噪扩散和流匹配在生成建模中取得了重大进展，但在表格数据上的应用仍然有限，而表格数据在现实世界中非常普遍。

Method: 提出了TabbyFlow方法，利用指数族分布表示异构数据类型，并通过矩匹配实现概率路径的学习。

Result: 在表格数据基准测试中表现出优于基线的性能。

Conclusion: TabbyFlow通过EF-VFM方法，为混合连续和离散特征的表格数据生成提供了高效且先进的解决方案。

Abstract: While denoising diffusion and flow matching have driven major advances in
generative modeling, their application to tabular data remains limited, despite
its ubiquity in real-world applications. To this end, we develop TabbyFlow, a
variational Flow Matching (VFM) method for tabular data generation. To apply
VFM to data with mixed continuous and discrete features, we introduce
Exponential Family Variational Flow Matching (EF-VFM), which represents
heterogeneous data types using a general exponential family distribution. We
hereby obtain an efficient, data-driven objective based on moment matching,
enabling principled learning of probability paths over mixed continuous and
discrete variables. We also establish a connection between variational flow
matching and generalized flow matching objectives based on Bregman divergences.
Evaluation on tabular data benchmarks demonstrates state-of-the-art performance
compared to baselines.

</details>


### [83] [Comparative Analysis of Modern Machine Learning Models for Retail Sales Forecasting](https://arxiv.org/abs/2506.05941)
*Luka Hobor,Mario Brcic,Lidija Polutnik,Ante Kapetanovic*

Main category: cs.LG

TL;DR: 论文研究了零售业中准确预测的重要性，比较了树集成模型和神经网络模型在零售数据上的表现，发现树模型在局部建模策略中表现更优。


<details>
  <summary>Details</summary>
Motivation: 准确的销售预测对零售业至关重要，过高或过低的预测都会带来成本或声誉损失。研究旨在评估不同预测模型在零售环境中的表现。

Method: 研究比较了树集成模型（如XGBoost和LightGBM）和神经网络模型（如N-BEATS、NHITS和Temporal Fusion Transformer）在高分辨率零售数据集上的表现，并考虑了数据预处理的影响。

Result: 树集成模型在局部建模策略中表现更优，尤其是在非插补数据上。神经网络模型虽受益于高级插补方法，但仍难以处理零售数据的复杂性。

Conclusion: 研究为零售环境中的模型选择提供了实用指导，并强调了数据预处理对提升预测性能的重要性。

Abstract: Accurate forecasting is key for all business planning. When estimated sales
are too high, brick-and-mortar retailers may incur higher costs due to unsold
inventories, higher labor and storage space costs, etc. On the other hand, when
forecasts underestimate the level of sales, firms experience lost sales,
shortages, and impact on the reputation of the retailer in their relevant
market. Accurate forecasting presents a competitive advantage for companies. It
facilitates the achievement of revenue and profit goals and execution of
pricing strategy and tactics. In this study, we provide an exhaustive
assessment of the forecasting models applied to a high-resolution
brick-and-mortar retail dataset. Our forecasting framework addresses the
problems found in retail environments, including intermittent demand, missing
values, and frequent product turnover. We compare tree-based ensembles (such as
XGBoost and LightGBM) and state-of-the-art neural network architectures
(including N-BEATS, NHITS, and the Temporal Fusion Transformer) across various
experimental settings. Our results show that localized modeling strategies
especially those using tree-based models on individual groups with non-imputed
data, consistently deliver superior forecasting accuracy and computational
efficiency. In contrast, neural models benefit from advanced imputation
methods, yet still fall short in handling the irregularities typical of
physical retail data. These results further practical understanding for model
selection in retail environment and highlight the significance of data
preprocessing to improve forecast performance.

</details>


### [84] [Additive decomposition of one-dimensional signals using Transformers](https://arxiv.org/abs/2506.05942)
*Samuele Salti,Andrea Pinto,Alessandro Lanza,Serena Morigi*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer架构的一维信号分解新方法，将信号分解为分段常数、平滑、纹理和噪声成分，并在合成数据上取得了高精度。


<details>
  <summary>Details</summary>
Motivation: 传统信号分解方法依赖数学模型，而深度学习模型在此领域的应用尚未充分探索，具有潜在优势。

Method: 利用Transformer架构对一维信号进行加法分解，分解为分段常数、平滑、纹理和噪声成分。

Result: 在合成数据上训练后，模型对同分布输入信号的建模和分解表现出色。

Conclusion: 该方法展示了深度学习在信号分解中的潜力，为未来研究提供了新方向。

Abstract: One-dimensional signal decomposition is a well-established and widely used
technique across various scientific fields. It serves as a highly valuable
pre-processing step for data analysis. While traditional decomposition
techniques often rely on mathematical models, recent research suggests that
applying the latest deep learning models to this problem presents an exciting,
unexplored area with promising potential. This work presents a novel method for
the additive decomposition of one-dimensional signals. We leverage the
Transformer architecture to decompose signals into their constituent
components: piece-wise constant, smooth (low-frequency oscillatory), textured
(high-frequency oscillatory), and a noise component. Our model, trained on
synthetic data, achieves excellent accuracy in modeling and decomposing input
signals from the same distribution, as demonstrated by the experimental
results.

</details>


### [85] [Learning Deterministic Policies with Policy Gradients in Constrained Markov Decision Processes](https://arxiv.org/abs/2506.05953)
*Alessandro Montenegro,Leonardo Cesani,Marco Mussi,Matteo Papini,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: C-PG是一种探索无关的算法，在梯度支配假设下具有全局最后迭代收敛保证，适用于约束强化学习（CRL）。


<details>
  <summary>Details</summary>
Motivation: 解决在满足领域特定约束的同时最大化预期回报的顺序决策问题。

Method: 引入C-PG算法，支持基于动作或参数的探索策略，并在特定噪声模型下证明其全局收敛性。

Result: C-PG在约束控制任务中表现出色，尤其在训练后部署确定性策略时优于现有基线。

Conclusion: C-PG算法在理论和实践中均有效，特别适合需要确定性策略的场景。

Abstract: Constrained Reinforcement Learning (CRL) addresses sequential decision-making
problems where agents are required to achieve goals by maximizing the expected
return while meeting domain-specific constraints. In this setting, policy-based
methods are widely used thanks to their advantages when dealing with
continuous-control problems. These methods search in the policy space with an
action-based or a parameter-based exploration strategy, depending on whether
they learn the parameters of a stochastic policy or those of a stochastic
hyperpolicy. We introduce an exploration-agnostic algorithm, called C-PG, which
enjoys global last-iterate convergence guarantees under gradient domination
assumptions. Furthermore, under specific noise models where the (hyper)policy
is expressed as a stochastic perturbation of the actions or of the parameters
of an underlying deterministic policy, we additionally establish global
last-iterate convergence guarantees of C-PG to the optimal deterministic
policy. This holds when learning a stochastic (hyper)policy and subsequently
switching off the stochasticity at the end of training, thereby deploying a
deterministic policy. Finally, we empirically validate both the action-based
(C-PGAE) and parameter-based (C-PGPE) variants of C-PG on constrained control
tasks, and compare them against state-of-the-art baselines, demonstrating their
effectiveness, in particular when deploying deterministic policies after
training.

</details>


### [86] [Pruning Spurious Subgraphs for Graph Out-of-Distribtuion Generalization](https://arxiv.org/abs/2506.05957)
*Tianjun Yao,Haoxuan Li,Yongqiang Chen,Tongliang Liu,Le Song,Eric Xing,Zhiqiang Shen*

Main category: cs.LG

TL;DR: PrunE是一种基于剪枝的图OOD方法，通过去除虚假边来提升图神经网络的分布外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在训练和测试数据分布偏移时性能下降的问题，尤其是虚假边与目标标签强相关的情况。

Method: 提出PrunE方法，使用两种正则化项剪枝虚假边：图大小约束和ε-概率对齐。

Result: 理论分析和实验表明，PrunE显著优于现有方法，提升了分布外泛化性能。

Conclusion: PrunE通过剪枝虚假边，更全面地保留了不变子图，为图OOD问题提供了有效解决方案。

Abstract: Graph Neural Networks (GNNs) often encounter significant performance
degradation under distribution shifts between training and test data, hindering
their applicability in real-world scenarios. Recent studies have proposed
various methods to address the out-of-distribution generalization challenge,
with many methods in the graph domain focusing on directly identifying an
invariant subgraph that is predictive of the target label. However, we argue
that identifying the edges from the invariant subgraph directly is challenging
and error-prone, especially when some spurious edges exhibit strong
correlations with the targets. In this paper, we propose PrunE, the first
pruning-based graph OOD method that eliminates spurious edges to improve OOD
generalizability. By pruning spurious edges, \mine{} retains the invariant
subgraph more comprehensively, which is critical for OOD generalization.
Specifically, PrunE employs two regularization terms to prune spurious edges:
1) graph size constraint to exclude uninformative spurious edges, and 2)
$\epsilon$-probability alignment to further suppress the occurrence of spurious
edges. Through theoretical analysis and extensive experiments, we show that
PrunE achieves superior OOD performance and outperforms previous
state-of-the-art methods significantly. Codes are available at:
\href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}.

</details>


### [87] [AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models](https://arxiv.org/abs/2506.05960)
*Adil Hasan,Thomas Peyrin*

Main category: cs.LG

TL;DR: 论文提出了一种基于向量量化的扩散模型压缩方法，显著降低了硬件资源需求，并在低比特量化下取得了优于全精度模型的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的大规模应用受限于高硬件资源需求，现有量化方法主要采用均匀标量量化，而向量量化在大型语言模型中表现优异。

Method: 采用基于码本的加法向量量化方法，对扩散模型进行压缩，并设计高效推理内核以实现硬件无关的FLOPs节省。

Result: 在LDM-4基准测试中，W4A8量化下sFID比全精度模型低1.92分，W2A8量化下FID、sFID和ISC均达到最佳结果。

Conclusion: 向量量化方法为扩散模型压缩提供了新的高效解决方案，显著提升了低比特量化的性能表现。

Abstract: Significant investments have been made towards the commodification of
diffusion models for generation of diverse media. Their mass-market adoption is
however still hobbled by the intense hardware resource requirements of
diffusion model inference. Model quantization strategies tailored specifically
towards diffusion models have been useful in easing this burden, yet have
generally explored the Uniform Scalar Quantization (USQ) family of quantization
methods. In contrast, Vector Quantization (VQ) methods, which operate on groups
of multiple related weights as the basic unit of compression, have seen
substantial success in Large Language Model (LLM) quantization. In this work,
we apply codebook-based additive vector quantization to the problem of
diffusion model compression. Our resulting approach achieves a new Pareto
frontier for the extremely low-bit weight quantization on the standard
class-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.
Notably, we report sFID 1.92 points lower than the full-precision model at W4A8
and the best-reported results for FID, sFID and ISC at W2A8. We are also able
to demonstrate FLOPs savings on arbitrary hardware via an efficient inference
kernel, as opposed to savings resulting from small integer operations which may
lack broad hardware support.

</details>


### [88] [Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning](https://arxiv.org/abs/2506.05968)
*Motoki Omura,Kazuki Ota,Takayuki Osa,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.LG

TL;DR: 论文提出了一种在连续动作空间的actor-critic框架中结合Bellman最优算子的方法，通过退火策略缓解过估计偏差，显著提升了学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有连续动作空间的RL算法依赖策略更新，样本效率低，而结合Bellman最优算子可能加速学习但会引入过估计偏差。

Method: 提出退火方法，逐步从Bellman最优算子过渡到Bellman算子，结合TD3和SAC算法。

Result: 在多种运动和操作任务中显著优于现有方法，提升了性能和超参数鲁棒性。

Conclusion: 结合Bellman最优算子并通过退火策略缓解偏差，是一种高效且鲁棒的连续动作空间RL方法。

Abstract: For continuous action spaces, actor-critic methods are widely used in online
reinforcement learning (RL). However, unlike RL algorithms for discrete
actions, which generally model the optimal value function using the Bellman
optimality operator, RL algorithms for continuous actions typically model
Q-values for the current policy using the Bellman operator. These algorithms
for continuous actions rely exclusively on policy updates for improvement,
which often results in low sample efficiency. This study examines the
effectiveness of incorporating the Bellman optimality operator into
actor-critic frameworks. Experiments in a simple environment show that modeling
optimal values accelerates learning but leads to overestimation bias. To
address this, we propose an annealing approach that gradually transitions from
the Bellman optimality operator to the Bellman operator, thereby accelerating
learning while mitigating bias. Our method, combined with TD3 and SAC,
significantly outperforms existing approaches across various locomotion and
manipulation tasks, demonstrating improved performance and robustness to
hyperparameters related to optimality.

</details>


### [89] [On Measuring Long-Range Interactions in Graph Neural Networks](https://arxiv.org/abs/2506.05971)
*Jacob Bamberger,Benjamin Gutteridge,Scott le Roux,Michael M. Bronstein,Xiaowen Dong*

Main category: cs.LG

TL;DR: 论文提出了一种衡量图任务中长距离交互的方法，填补了现有经验方法的不足，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络研究中，长距离图任务缺乏理论支撑和鲁棒性，需要一种更原则性的方法来定义和解决这一问题。

Method: 作者形式化了图任务中的长距离交互，引入了一种图操作符的范围度量，并通过合成实验验证了其有效性。

Result: 通过该度量方法，作者评估了常用任务和架构的长距离特性，并讨论了其实际表现。

Conclusion: 该研究为定义和解决图上的长距离问题提供了理论支持，其范围度量方法有助于评估新数据集和架构。

Abstract: Long-range graph tasks -- those dependent on interactions between distant
nodes -- are an open problem in graph neural network research. Real-world
benchmark tasks, especially the Long Range Graph Benchmark, have become popular
for validating the long-range capability of proposed architectures. However,
this is an empirical approach that lacks both robustness and theoretical
underpinning; a more principled characterization of the long-range problem is
required. To bridge this gap, we formalize long-range interactions in graph
tasks, introduce a range measure for operators on graphs, and validate it with
synthetic experiments. We then leverage our measure to examine commonly used
tasks and architectures, and discuss to what extent they are, in fact,
long-range. We believe our work advances efforts to define and address the
long-range problem on graphs, and that our range measure will aid evaluation of
new datasets and architectures.

</details>


### [90] [Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning](https://arxiv.org/abs/2506.05977)
*Yujia Huo,Jianchun Liu,Hongli Xu,Zhenguo Ma,Shilong Wang,Liusheng Huang*

Main category: cs.LG

TL;DR: FedBE是一种新的联邦微调框架，通过自适应扩展和动态分配策略解决分布式环境中的灾难性遗忘问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦微调方法在分布式环境中难以有效应对灾难性遗忘，且数据分布和设备能力差异加剧了这一问题。

Method: FedBE结合自适应Transformer块扩展机制和动态可训练块分配策略，分离新学知识与预训练表示，并根据客户端数据分布和能力动态分配块。

Result: 实验表明，FedBE在通用任务上准确率保留提高12-74%，收敛速度加快1.9-3.1倍，且不影响下游任务准确性。

Conclusion: FedBE有效解决了联邦微调中的灾难性遗忘问题，提升了模型在异构环境中的泛化能力。

Abstract: Federated fine-tuning (FedFT) of large language models (LLMs) has emerged as
a promising solution for adapting models to distributed data environments while
ensuring data privacy.
  Existing FedFT methods predominantly utilize parameter-efficient fine-tuning
(PEFT) techniques to reduce communication and computation overhead.
  However, they often fail to adequately address the catastrophic forgetting, a
critical challenge arising from continual adaptation in distributed
environments. The traditional centralized fine-tuning methods, which are not
designed for the heterogeneous and privacy-constrained nature of federated
environments, struggle to mitigate this issue effectively. Moreover, the
challenge is further exacerbated by significant variation in data distributions
and device capabilities across clients, which leads to intensified forgetting
and degraded model generalization. To tackle these issues, we propose FedBE, a
novel FedFT framework that integrates an adaptive transformer block expansion
mechanism with a dynamic trainable-block allocation strategy. Specifically,
FedBE expands trainable blocks within the model architecture, structurally
separating newly learned task-specific knowledge from the original pre-trained
representations. Additionally, FedBE dynamically assigns these trainable blocks
to clients based on their data distributions and computational capabilities.
This enables the framework to better accommodate heterogeneous federated
environments and enhances the generalization ability of the model.Extensive
experiments show that compared with existing federated fine-tuning methods,
FedBE achieves 12-74% higher accuracy retention on general tasks after
fine-tuning and a model convergence acceleration ratio of 1.9-3.1x without
degrading the accuracy of downstream tasks.

</details>


### [91] [AMPED: Adaptive Multi-objective Projection for balancing Exploration and skill Diversification](https://arxiv.org/abs/2506.05980)
*Geonwoo Cho,Jaemoon Lee,Jaegyun Im,Subi Lee,Jihwan Lee,Sundong Kim*

Main category: cs.LG

TL;DR: AMPED是一种新的技能强化学习方法，通过梯度手术技术平衡探索与技能多样性，并在微调阶段动态选择技能，性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时优化探索与技能多样性这两个冲突目标，需要一种新方法来解决这一问题。

Method: AMPED通过梯度手术技术平衡探索与技能多样性目标，并在微调阶段引入动态技能选择模块。

Result: AMPED在多个基准测试中性能优于现有技能强化学习基线。

Conclusion: AMPED通过显式协调探索与多样性，实现了鲁棒且可泛化的技能学习。

Abstract: Skill-based reinforcement learning (SBRL) enables rapid adaptation in
environments with sparse rewards by pretraining a skill-conditioned policy.
Effective skill learning requires jointly maximizing both exploration and skill
diversity. However, existing methods often face challenges in simultaneously
optimizing for these two conflicting objectives. In this work, we propose a new
method, Adaptive Multi-objective Projection for balancing Exploration and skill
Diversification (AMPED), which explicitly addresses both exploration and skill
diversification. We begin by conducting extensive ablation studies to identify
and define a set of objectives that effectively capture the aspects of
exploration and skill diversity, respectively. During the skill pretraining
phase, AMPED introduces a gradient surgery technique to balance the objectives
of exploration and skill diversity, mitigating conflicts and reducing reliance
on heuristic tuning. In the subsequent fine-tuning phase, AMPED incorporates a
skill selector module that dynamically selects suitable skills for downstream
tasks, based on task-specific performance signals. Our approach achieves
performance that surpasses SBRL baselines across various benchmarks. These
results highlight the importance of explicitly harmonizing exploration and
diversity and demonstrate the effectiveness of AMPED in enabling robust and
generalizable skill learning. Project Page: https://geonwoo.me/amped/

</details>


### [92] [Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning](https://arxiv.org/abs/2506.05985)
*Yuheng Lei,Sitong Mao,Shunbo Zhou,Hongyuan Zhang,Xuelong Li,Ping Luo*

Main category: cs.LG

TL;DR: 提出DMPEL方法，通过动态组合低秩专家库和轻量级路由器，实现终身机器人学习中的高效知识迁移和遗忘缓解。


<details>
  <summary>Details</summary>
Motivation: 解决终身学习中测试时任务标识不切实际和知识共享受限的问题。

Method: 动态混合渐进参数高效专家库（DMPEL），结合系数重放技术。

Result: 在LIBERO基准测试中表现优于现有方法，成功率高且资源占用低。

Conclusion: DMPEL在终身学习中实现了高效适应和遗忘缓解，具有实际应用潜力。

Abstract: A generalist agent must continuously learn and adapt throughout its lifetime,
achieving efficient forward transfer while minimizing catastrophic forgetting.
Previous work within the dominant pretrain-then-finetune paradigm has explored
parameter-efficient fine-tuning for single-task adaptation, effectively
steering a frozen pretrained model with a small number of parameters. However,
in the context of lifelong learning, these methods rely on the impractical
assumption of a test-time task identifier and restrict knowledge sharing among
isolated adapters. To address these limitations, we propose Dynamic Mixture of
Progressive Parameter-Efficient Expert Library (DMPEL) for lifelong robot
learning. DMPEL progressively learn a low-rank expert library and employs a
lightweight router to dynamically combine experts into an end-to-end policy,
facilitating flexible behavior during lifelong adaptation. Moreover, by
leveraging the modular structure of the fine-tuned parameters, we introduce
coefficient replay to guide the router in accurately retrieving frozen experts
for previously encountered tasks, thereby mitigating catastrophic forgetting.
This method is significantly more storage- and computationally-efficient than
applying demonstration replay to the entire policy. Extensive experiments on
the lifelong manipulation benchmark LIBERO demonstrate that our framework
outperforms state-of-the-art lifelong learning methods in success rates across
continual adaptation, while utilizing minimal trainable parameters and storage.

</details>


### [93] [RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory](https://arxiv.org/abs/2506.05994)
*Yi-Chun Liao,Chieh-Lin Tsai,Yuan-Hao Chang,Camélia Slimani,Jalil Boukhobza,Tei-Wei Kuo*

Main category: cs.LG

TL;DR: RETENTION框架通过迭代剪枝算法和树映射方案，显著降低了基于树的模型推理中CAM的容量需求，同时保持精度损失在3%以内。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在非结构化数据上表现优异，但基于树的集成模型在结构化数据上仍占优势。现有CAM加速方案存在内存消耗高和利用率低的问题。

Method: 提出RETENTION框架，包括针对袋装模型的迭代剪枝算法和两种创新的数据放置策略以减少CAM中的冗余。

Result: 实验显示，树映射方案单独实现空间效率提升1.46×至21.30×，完整框架提升4.35×至207.12×，精度损失小于3%。

Conclusion: RETENTION有效减少CAM容量需求，为基于树的模型加速提供了资源高效的方向。

Abstract: Although deep learning has demonstrated remarkable capabilities in learning
from unstructured data, modern tree-based ensemble models remain superior in
extracting relevant information and learning from structured datasets. While
several efforts have been made to accelerate tree-based models, the inherent
characteristics of the models pose significant challenges for conventional
accelerators. Recent research leveraging content-addressable memory (CAM)
offers a promising solution for accelerating tree-based models, yet existing
designs suffer from excessive memory consumption and low utilization. This work
addresses these challenges by introducing RETENTION, an end-to-end framework
that significantly reduces CAM capacity requirement for tree-based model
inference. We propose an iterative pruning algorithm with a novel pruning
criterion tailored for bagging-based models (e.g., Random Forest), which
minimizes model complexity while ensuring controlled accuracy degradation.
Additionally, we present a tree mapping scheme that incorporates two innovative
data placement strategies to alleviate the memory redundancy caused by the
widespread use of don't care states in CAM. Experimental results show that
implementing the tree mapping scheme alone achieves $1.46\times$ to $21.30
\times$ better space efficiency, while the full RETENTION framework yields
$4.35\times$ to $207.12\times$ improvement with less than 3% accuracy loss.
These results demonstrate that RETENTION is highly effective in reducing CAM
capacity requirement, providing a resource-efficient direction for tree-based
model acceleration.

</details>


### [94] [Machine learning for in-situ composition mapping in a self-driving magnetron sputtering system](https://arxiv.org/abs/2506.05999)
*Sanna Jarl,Jens Sjölund,Robert J. W. Frost,Anders Holst,Jonathan J. S. Scragg*

Main category: cs.LG

TL;DR: 本文提出了一种基于磁控共溅射的自驱动实验室（SDL）方法，用于快速生成多元素组合薄膜的成分分布图，避免了传统耗时且易出错的外部分析。


<details>
  <summary>Details</summary>
Motivation: 在薄膜科学中，现有的SDL主要局限于溶液合成方法，无法覆盖无机材料的广泛化学空间。本文旨在通过磁控溅射技术扩展SDL的应用范围。

Method: 结合组合框架和机器学习（ML），利用石英晶体微天平传感器进行原位测量，通过高斯过程（GP）和主动学习预测薄膜成分分布。

Result: 采用贝叶斯主动学习（BALM）方法，仅需10次实验即可学习单源的沉积速率，验证了共溅射成分分布的预测准确性。

Conclusion: 该方法显著提高了材料探索的效率，展示了ML引导的SDL在加速材料发现中的潜力。

Abstract: Self-driving labs (SDLs), employing automation and machine learning (ML) to
accelerate experimental procedures, have enormous potential in the discovery of
new materials. However, in thin film science, SDLs are mainly restricted to
solution-based synthetic methods which are easier to automate but cannot access
the broad chemical space of inorganic materials. This work presents an SDL
based on magnetron co-sputtering. We are using combinatorial frameworks,
obtaining accurate composition maps on multi-element, compositionally graded
thin films. This normally requires time-consuming ex-situ analysis prone to
systematic errors. We present a rapid and calibration-free in-situ, ML driven
approach to produce composition maps for arbitrary source combinations and
sputtering conditions. We develop a method to predict the composition
distribution in a multi-element combinatorial thin film, using in-situ
measurements from quartz-crystal microbalance sensors placed in a sputter
chamber. For a given source, the sensor readings are learned as a function of
the sputtering pressure and magnetron power, through active learning using
Gaussian processes (GPs). The final GPs are combined with a geometric model of
the deposition flux distribution in the chamber, which allows interpolation of
the deposition rates from each source, at any position across the sample. We
investigate several acquisition functions for the ML procedure. A fully
Bayesian GP - BALM (Bayesian active learning MacKay) - achieved the best
performance, learning the deposition rates for a single source in 10
experiments. Prediction accuracy for co-sputtering composition distributions
was verified experimentally. Our framework dramatically increases throughput by
avoiding the need for extensive characterisation or calibration, thus
demonstrating the potential of ML-guided SDLs to accelerate materials
exploration.

</details>


### [95] [LaDEEP: A Deep Learning-based Surrogate Model for Large Deformation of Elastic-Plastic Solids](https://arxiv.org/abs/2506.06001)
*Shilong Tao,Zhe Feng,Haonan Sun,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.LG

TL;DR: LaDEEP是一种基于深度学习的替代模型，用于解决弹性-塑性固体的大变形问题，通过分区编码和Transformer模块实现高效预测，速度和准确性显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器在精度和效率之间存在固有权衡，而现有深度学习模型未针对特定问题特性设计，难以处理复杂的弹性-塑性固体变形。

Method: 采用分区编码将固体区域转换为令牌序列，设计基于Transformer的两阶段模块预测变形。

Result: LaDEEP比有限元方法快五个数量级，准确性相当，并比其他深度学习基线平均提升20.47%。

Conclusion: LaDEEP在实际工业生产中表现出色，验证了其在复杂弹性-塑性固体变形问题中的高效性和准确性。

Abstract: Scientific computing for large deformation of elastic-plastic solids is
critical for numerous real-world applications. Classical numerical solvers rely
primarily on local discrete linear approximation and are constrained by an
inherent trade-off between accuracy and efficiency. Recently, deep learning
models have achieved impressive progress in solving the continuum mechanism.
While previous models have explored various architectures and constructed
coefficient-solution mappings, they are designed for general instances without
considering specific problem properties and hard to accurately handle with
complex elastic-plastic solids involving contact, loading and unloading. In
this work, we take stretch bending, a popular metal fabrication technique, as
our case study and introduce LaDEEP, a deep learning-based surrogate model for
\textbf{La}rge \textbf{De}formation of \textbf{E}lastic-\textbf{P}lastic
Solids. We encode the partitioned regions of the involved slender solids into a
token sequence to maintain their essential order property. To characterize the
physical process of the solid deformation, a two-stage Transformer-based module
is designed to predict the deformation with the sequence of tokens as input.
Empirically, LaDEEP achieves five magnitudes faster speed than finite element
methods with a comparable accuracy, and gains 20.47\% relative improvement on
average compared to other deep learning baselines. We have also deployed our
model into a real-world industrial production system, and it has shown
remarkable performance in both accuracy and efficiency.

</details>


### [96] [What Really is a Member? Discrediting Membership Inference via Poisoning](https://arxiv.org/abs/2506.06003)
*Neal Mangaokar,Ashish Hooda,Zhuohang Li,Bradley A. Malin,Kassem Fawaz,Somesh Jha,Atul Prakash,Amrita Roy Chowdhury*

Main category: cs.LG

TL;DR: 研究表明，即使放宽成员推断测试的定义以包含语义邻居，测试仍不可靠，且可能通过数据投毒攻击导致错误预测。


<details>
  <summary>Details</summary>
Motivation: 探讨成员推断测试在放宽定义后的可靠性，揭示其易受数据投毒攻击的弱点。

Method: 提出一种数据投毒攻击方法，并通过理论分析和实证验证其效果。

Result: 攻击能显著降低现有测试的性能，甚至低于随机水平。

Conclusion: 成员推断测试在放宽定义后仍不可靠，且存在准确性与抗投毒能力之间的权衡。

Abstract: Membership inference tests aim to determine whether a particular data point
was included in a language model's training set. However, recent works have
shown that such tests often fail under the strict definition of membership
based on exact matching, and have suggested relaxing this definition to include
semantic neighbors as members as well. In this work, we show that membership
inference tests are still unreliable under this relaxation - it is possible to
poison the training dataset in a way that causes the test to produce incorrect
predictions for a target point. We theoretically reveal a trade-off between a
test's accuracy and its robustness to poisoning. We also present a concrete
instantiation of this poisoning attack and empirically validate its
effectiveness. Our results show that it can degrade the performance of existing
tests to well below random.

</details>


### [97] [LightGTS: A Lightweight General Time Series Forecasting Model](https://arxiv.org/abs/2506.06005)
*Yihang Wang,Yuying Qiu,Peng Chen,Yang Shu,Zhongwen Rao,Lujia Pan,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: LightGTS是一个轻量级通用时间序列预测模型，通过周期性建模和高效解码技术，在多种数据集上实现高性能，同时显著降低计算负担。


<details>
  <summary>Details</summary>
Motivation: 现有通用时间序列预测模型参数庞大，计算成本高，限制了在资源受限场景的应用。本文旨在设计一个轻量级模型，通过周期性建模提升效率。

Method: 提出Periodical Tokenization提取跨数据集的周期性模式，并引入Periodical Parallel Decoding利用历史标记改进预测。

Result: 在9个真实世界基准测试中，LightGTS在零样本和全样本设置下均达到最优性能，且效率显著优于现有模型。

Conclusion: LightGTS通过周期性建模和轻量化设计，在保持高性能的同时显著降低了计算成本，适用于资源受限场景。

Abstract: Existing works on general time series forecasting build foundation models
with heavy model parameters through large-scale multi-source pre-training.
These models achieve superior generalization ability across various datasets at
the cost of significant computational burdens and limitations in
resource-constrained scenarios. This paper introduces LightGTS, a lightweight
general time series forecasting model designed from the perspective of
consistent periodical modeling. To handle diverse scales and intrinsic periods
in multi-source pre-training, we introduce Periodical Tokenization, which
extracts consistent periodic patterns across different datasets with varying
scales. To better utilize the periodicity in the decoding process, we further
introduce Periodical Parallel Decoding, which leverages historical tokens to
improve forecasting. Based on the two techniques above which fully leverage the
inductive bias of periods inherent in time series, LightGTS uses a lightweight
model to achieve outstanding performance on general time series forecasting. It
achieves state-of-the-art forecasting performance on 9 real-world benchmarks in
both zero-shot and full-shot settings with much better efficiency compared with
existing time series foundation models.

</details>


### [98] [Unisoma: A Unified Transformer-based Solver for Multi-Solid Systems](https://arxiv.org/abs/2506.06021)
*Shilong Tao,Zhe Feng,Haonan Sun,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.LG

TL;DR: 论文提出了一种新的显式建模方法Unisoma，用于处理多固体系统的复杂相互作用，优于现有的隐式建模方法。


<details>
  <summary>Details</summary>
Motivation: 多固体系统的复杂相互作用建模具有挑战性，现有深度学习的隐式建模方法难以准确捕捉物理交互。

Method: 提出了Unisoma，一种基于Transformer的显式建模方法，通过接触模块和自适应交互分配机制直接捕捉物理交互，并通过三元关系学习变形。

Result: 在七个数据集和两个复杂任务上，Unisoma表现优于现有方法。

Conclusion: 显式建模更适合多固体系统，Unisoma为相关应用提供了有效的解决方案。

Abstract: Multi-solid systems are foundational to a wide range of real-world
applications, yet modeling their complex interactions remains challenging.
Existing deep learning methods predominantly rely on implicit modeling, where
the factors influencing solid deformation are not explicitly represented but
are instead indirectly learned. However, as the number of solids increases,
these methods struggle to accurately capture intricate physical interactions.
In this paper, we introduce a novel explicit modeling paradigm that
incorporates factors influencing solid deformation through structured modules.
Specifically, we present Unisoma, a unified and flexible Transformer-based
model capable of handling variable numbers of solids. Unisoma directly captures
physical interactions using contact modules and adaptive interaction allocation
mechanism, and learns the deformation through a triplet relationship. Compared
to implicit modeling techniques, explicit modeling is more well-suited for
multi-solid systems with diverse coupling patterns, as it enables detailed
treatment of each solid while preventing information blending and confusion.
Experimentally, Unisoma achieves consistent state-of-the-art performance across
seven well-established datasets and two complex multi-solid tasks. Code is
avaiable at \href{this link}{https://github.com/therontau0054/Unisoma}.

</details>


### [99] [Do-PFN: In-Context Learning for Causal Effect Estimation](https://arxiv.org/abs/2506.06039)
*Jake Robertson,Arik Reuter,Siyuan Guo,Noah Hollmann,Frank Hutter,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 论文提出了一种基于预训练网络（PFNs）的方法（Do-PFN），用于在无需真实因果图的情况下准确估计因果效应。


<details>
  <summary>Details</summary>
Motivation: 现有因果效应估计方法依赖干预数据或真实因果图，限制了实际应用。本文探索预训练网络在因果效应估计中的潜力。

Method: 预训练PFNs于多种因果结构的合成数据，通过上下文学习预测干预结果。

Result: 实验表明，Do-PFN能在未知因果图的情况下准确估计因果效应，且具有可扩展性和鲁棒性。

Conclusion: Do-PFN为因果效应估计提供了一种无需真实因果图的新方法，具有广泛适用性。

Abstract: Estimation of causal effects is critical to a range of scientific
disciplines. Existing methods for this task either require interventional data,
knowledge about the ground truth causal graph, or rely on assumptions such as
unconfoundedness, restricting their applicability in real-world settings. In
the domain of tabular machine learning, Prior-data fitted networks (PFNs) have
achieved state-of-the-art predictive performance, having been pre-trained on
synthetic data to solve tabular prediction problems via in-context learning. To
assess whether this can be transferred to the harder problem of causal effect
estimation, we pre-train PFNs on synthetic data drawn from a wide variety of
causal structures, including interventions, to predict interventional outcomes
given observational data. Through extensive experiments on synthetic case
studies, we show that our approach allows for the accurate estimation of causal
effects without knowledge of the underlying causal graph. We also perform
ablation studies that elucidate Do-PFN's scalability and robustness across
datasets with a variety of causal characteristics.

</details>


### [100] [Diffusion-Based Hierarchical Graph Neural Networks for Simulating Nonlinear Solid Mechanics](https://arxiv.org/abs/2506.06045)
*Tobias Würth,Niklas Freymuth,Gerhard Neumann,Luise Kärger*

Main category: cs.LG

TL;DR: 论文提出了一种名为ROBIN的新型学习模拟器，通过结合滚动扩散和分层图神经网络，解决了现有图基学习模拟器在捕捉全局现象和长期误差积累上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图基学习模拟器在模拟物理系统时难以捕捉全局现象（如弯曲或长程相关性），且因依赖局部消息传递和直接下一步预测而容易在长期模拟中积累误差。

Method: ROBIN结合了滚动扩散（一种并行化推理方案）和基于代数多重网格的分层图神经网络，实现了多尺度消息传递。

Result: 在涉及几何、材料和接触非线性的2D和3D固体力学基准测试中，ROBIN实现了最先进的准确性，显著优于现有方法，并将推理时间缩短了一个数量级。

Conclusion: ROBIN通过其创新的架构，有效解决了现有方法的局限性，为复杂物理系统的模拟提供了高效且准确的解决方案。

Abstract: Graph-based learned simulators have emerged as a promising approach for
simulating physical systems on unstructured meshes, offering speed and
generalization across diverse geometries. However, they often struggle with
capturing global phenomena, such as bending or long-range correlations, and
suffer from error accumulation over long rollouts due to their reliance on
local message passing and direct next-step prediction. We address these
limitations by introducing the Rolling Diffusion-Batched Inference Network
(ROBIN), a novel learned simulator that integrates two key innovations: (i)
Rolling Diffusion, a parallelized inference scheme that amortizes the cost of
diffusion-based refinement across physical time steps by overlapping denoising
steps across a temporal window. (ii) A Hierarchical Graph Neural Network built
on algebraic multigrid coarsening, enabling multiscale message passing across
different mesh resolutions. This architecture, implemented via
Algebraic-hierarchical Message Passing Networks, captures both fine-scale local
dynamics and global structural effects critical for phenomena like beam bending
or multi-body contact. We validate ROBIN on challenging 2D and 3D solid
mechanics benchmarks involving geometric, material, and contact nonlinearities.
ROBIN achieves state-of-the-art accuracy on all tasks, substantially
outperforming existing next-step learned simulators while reducing inference
time by up to an order of magnitude compared to standard diffusion simulators.

</details>


### [101] [TRUST: Test-time Resource Utilization for Superior Trustworthiness](https://arxiv.org/abs/2506.06048)
*Haripriya Harikumar,Santu Rana*

Main category: cs.LG

TL;DR: 提出一种新的测试时优化方法，通过减少分类器权重噪声，提升预测置信度的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统不确定性估计方法（如dropout）难以清晰区分可靠与不可靠预测，原因是分类器权重噪声影响了细粒度统计信息。

Method: 提出一种测试时优化方法，考虑噪声影响，生成更可靠的置信度估计，并定义单调子集选择函数。

Result: 在AUSE和AURC等标准风险指标上表现优越，能有效识别训练与测试分布差异，区分分布内外样本，并揭示CNN与ViT分类器的关键差异。

Conclusion: 该方法显著提升了不确定性估计的可靠性，适用于多种视觉数据集。

Abstract: Standard uncertainty estimation techniques, such as dropout, often struggle
to clearly distinguish reliable predictions from unreliable ones. We attribute
this limitation to noisy classifier weights, which, while not impairing overall
class-level predictions, render finer-level statistics less informative. To
address this, we propose a novel test-time optimization method that accounts
for the impact of such noise to produce more reliable confidence estimates.
This score defines a monotonic subset-selection function, where population
accuracy consistently increases as samples with lower scores are removed, and
it demonstrates superior performance in standard risk-based metrics such as
AUSE and AURC. Additionally, our method effectively identifies discrepancies
between training and test distributions, reliably differentiates
in-distribution from out-of-distribution samples, and elucidates key
differences between CNN and ViT classifiers across various vision datasets.

</details>


### [102] [System-Aware Unlearning Algorithms: Use Lesser, Forget Faster](https://arxiv.org/abs/2506.06073)
*Linda Lu,Ayush Sekhari,Karthik Sridharan*

Main category: cs.LG

TL;DR: 论文提出了一种新的机器遗忘定义——系统感知遗忘，旨在针对仅能访问系统存储数据的攻击者提供遗忘保证，并通过选择性采样方法实现高效的线性分类遗忘算法。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘的严格定义（要求遗忘后模型与重新训练的模型几乎相同）对高效算法开发构成挑战，且假设的攻击者能力过于强大。因此，论文提出更实际的系统感知遗忘定义。

Method: 提出系统感知遗忘定义，并基于选择性采样方法设计了一种精确的线性分类遗忘算法，同时推广到一般函数分类。

Result: 理论分析了删除容量、准确性、内存和计算时间之间的权衡关系。

Conclusion: 系统感知遗忘定义更实际且高效，为机器遗忘算法的设计提供了新方向。

Abstract: Machine unlearning addresses the problem of updating a machine learning
model/system trained on a dataset $S$ so that the influence of a set of
deletion requests $U \subseteq S$ on the unlearned model is minimized. The gold
standard definition of unlearning demands that the updated model, after
deletion, be nearly identical to the model obtained by retraining. This
definition is designed for a worst-case attacker (one who can recover not only
the unlearned model but also the remaining data samples, i.e., $S \setminus
U$). Such a stringent definition has made developing efficient unlearning
algorithms challenging. However, such strong attackers are also unrealistic. In
this work, we propose a new definition, system-aware unlearning, which aims to
provide unlearning guarantees against an attacker that can at best only gain
access to the data stored in the system for learning/unlearning requests and
not all of $S\setminus U$. With this new definition, we use the simple
intuition that if a system can store less to make its learning/unlearning
updates, it can be more secure and update more efficiently against a
system-aware attacker. Towards that end, we present an exact system-aware
unlearning algorithm for linear classification using a selective sampling-based
approach, and we generalize the method for classification with general function
classes. We theoretically analyze the tradeoffs between deletion capacity,
accuracy, memory, and computation time.

</details>


### [103] [Flexible Operator Fusion for Fast Sparse Transformer with Diverse Masking on GPU](https://arxiv.org/abs/2506.06095)
*Wenhao Dai,Haodong Deng,Mengfei Rong,Xinyu Yang,Hongyu Liu,Fangxin Liu,Hailong Yang,Weifeng Liu,Qingxiao Sun*

Main category: cs.LG

TL;DR: STOF框架通过灵活的掩码和GPU上的算子融合优化稀疏Transformer，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏Transformer研究较少关注性能优化，且基于规则的机制无法适应不同序列长度和混合类型算子的融合机会。

Method: STOF统一了多头注意力的存储格式和内核实现，通过两阶段搜索引擎确定最佳参数设置，并映射融合方案到编译模板。

Result: 实验表明，STOF在MHA计算和端到端推理中分别实现了1.7倍和1.5倍的最大加速。

Conclusion: STOF通过优化稀疏Transformer，显著提升了性能，解决了现有方法的不足。

Abstract: Large language models are popular around the world due to their powerful
understanding capabilities. As the core component of LLMs, accelerating
Transformer through parallelization has gradually become a hot research topic.
Mask layers introduce sparsity into Transformer to reduce calculations.
However, previous works rarely focus on the performance optimization of sparse
Transformer. Moreover, rule-based mechanisms ignore the fusion opportunities of
mixed-type operators and fail to adapt to various sequence lengths. To address
the above problems, we propose STOF, a framework that incorporates
optimizations for Sparse Transformer via flexible masking and operator fusion
on GPU. We firstly unify the storage format and kernel implementation for the
multi-head attention. Then, we map fusion schemes to compilation templates and
determine the optimal parameter setting through a two-stage search engine. The
experimental results show that compared to the state-of-the-art work, STOF
achieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end
inference.

</details>


### [104] [Text-to-LoRA: Instant Transformer Adaption](https://arxiv.org/abs/2506.06105)
*Rujikorn Charakorn,Edoardo Cetin,Yujin Tang,Robert Tjarko Lange*

Main category: cs.LG

TL;DR: 论文提出Text-to-LoRA (T2L)，一种通过自然语言描述动态调整大语言模型（LLMs）的方法，避免了传统微调的高成本和复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要精心设计数据集和反复微调，成本高且对超参数敏感。T2L旨在通过自然语言描述实现快速、低成本的模型适配。

Method: T2L是一种超网络，通过单次前向传播生成LoRA适配器，训练基于9个预训练LoRA适配器（如GSM8K、Arc等）。

Result: T2L生成的LoRA实例在测试集上表现与任务专用适配器相当，并能压缩数百个LoRA实例，零样本泛化到新任务。

Conclusion: T2L为大规模模型适配提供了一种低成本、高效的方法，推动了基础模型的民主化应用。

Abstract: While Foundation Models provide a general tool for rapid content creation,
they regularly require task-specific adaptation. Traditionally, this exercise
involves careful curation of datasets and repeated fine-tuning of the
underlying model. Fine-tuning techniques enable practitioners to adapt
foundation models for many new applications but require expensive and lengthy
training while being notably sensitive to hyperparameter choices. To overcome
these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting
large language models (LLMs) on the fly solely based on a natural language
description of the target task. T2L is a hypernetwork trained to construct
LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9
pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc
reconstructed LoRA instances match the performance of task-specific adapters
across the corresponding test sets. Furthermore, T2L can compress hundreds of
LoRA instances and zero-shot generalize to entirely unseen tasks. This approach
provides a significant step towards democratizing the specialization of
foundation models and enables language-based adaptation with minimal compute
requirements.
  Our code is available at https://github.com/SakanaAI/text-to-lora

</details>


### [105] [Synthetic Tabular Data: Methods, Attacks and Defenses](https://arxiv.org/abs/2506.06108)
*Graham Cormode,Samuel Maddock,Enayat Ullah,Shripad Gade*

Main category: cs.LG

TL;DR: 本文综述了表格合成数据生成的关键进展和主要概念，包括基于概率图模型和深度学习的方法，并探讨了合成数据的局限性及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 合成数据被视为解决隐私问题的潜在方案，本文旨在总结该领域的技术进展和挑战。

Method: 综述了概率图模型和深度学习在合成数据生成中的应用，并分析了攻击原始数据的方法。

Result: 总结了合成数据生成的技术进展，同时指出其隐私保护局限性。

Conclusion: 合成数据生成领域仍有待解决的问题和扩展方向。

Abstract: Synthetic data is often positioned as a solution to replace sensitive
fixed-size datasets with a source of unlimited matching data, freed from
privacy concerns. There has been much progress in synthetic data generation
over the last decade, leveraging corresponding advances in machine learning and
data analytics. In this survey, we cover the key developments and the main
concepts in tabular synthetic data generation, including paradigms based on
probabilistic graphical models and on deep learning. We provide background and
motivation, before giving a technical deep-dive into the methodologies. We also
address the limitations of synthetic data, by studying attacks that seek to
retrieve information about the original sensitive data. Finally, we present
extensions and open problems in this area.

</details>


### [106] [Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness](https://arxiv.org/abs/2506.06112)
*Cheng-Long Wang,Qi Li,Zihang Xiang,Yinzhi Cao,Di Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为IAM的框架，用于量化机器学习模型中的数据遗忘效果，解决了现有方法在计算资源和粒度上的局限性。


<details>
  <summary>Details</summary>
Motivation: 数据隐私和安全问题日益突出，需要高效评估机器学习模型中的数据遗忘效果，现有方法如MIA在计算资源和粒度上存在不足。

Method: 提出IAM框架，通过插值模型的泛化-拟合行为间隙来量化样本级遗忘效果，适用于精确和近似遗忘。

Result: IAM在二元包含测试中表现优异，适用于大型语言模型，且计算资源需求低。理论分析表明其评分机制高效。

Conclusion: IAM为数据遗忘提供了高效评估工具，揭示了近似遗忘算法中的过度遗忘和不足遗忘风险，强调了更强的安全保障需求。

Abstract: Growing concerns over data privacy and security highlight the importance of
machine unlearning--removing specific data influences from trained models
without full retraining. Techniques like Membership Inference Attacks (MIAs)
are widely used to externally assess successful unlearning. However, existing
methods face two key limitations: (1) maximizing MIA effectiveness (e.g., via
online attacks) requires prohibitive computational resources, often exceeding
retraining costs; (2) MIAs, designed for binary inclusion tests, struggle to
capture granular changes in approximate unlearning. To address these
challenges, we propose the Interpolated Approximate Measurement (IAM), a
framework natively designed for unlearning inference. IAM quantifies
sample-level unlearning completeness by interpolating the model's
generalization-fitting behavior gap on queried samples. IAM achieves strong
performance in binary inclusion tests for exact unlearning and high correlation
for approximate unlearning--scalable to LLMs using just one pre-trained shadow
model. We theoretically analyze how IAM's scoring mechanism maintains
performance efficiently. We then apply IAM to recent approximate unlearning
algorithms, revealing general risks of both over-unlearning and
under-unlearning, underscoring the need for stronger safeguards in approximate
unlearning systems. The code is available at
https://github.com/Happy2Git/Unlearning_Inference_IAM.

</details>


### [107] [Scalable unsupervised feature selection via weight stability](https://arxiv.org/abs/2506.06114)
*Xudong Zhang,Renato Cordeiro de Amorim*

Main category: cs.LG

TL;DR: 提出了一种基于Minkowski加权k-means++的新初始化策略，并开发了两种特征选择算法FS-MWK++和SFS-MWK++，显著提升了高维数据聚类性能。


<details>
  <summary>Details</summary>
Motivation: 高维数据中无关特征会掩盖有意义的结构，无监督特征选择对提升聚类性能至关重要。

Method: 提出Minkowski加权k-means++初始化策略，基于数据特征相关性概率选择中心点，并开发FS-MWK++和SFS-MWK++两种特征选择算法。

Result: 理论保证和实验表明，新方法在多种情况下优于现有方法。

Conclusion: 新方法在无监督特征选择中表现出色，显著提升了聚类性能。

Abstract: Unsupervised feature selection is critical for improving clustering
performance in high-dimensional data, where irrelevant features can obscure
meaningful structure. In this work, we introduce the Minkowski weighted
$k$-means++, a novel initialisation strategy for the Minkowski Weighted
$k$-means. Our initialisation selects centroids probabilistically using feature
relevance estimates derived from the data itself. Building on this, we propose
two new feature selection algorithms, FS-MWK++, which aggregates feature
weights across a range of Minkowski exponents to identify stable and
informative features, and SFS-MWK++, a scalable variant based on subsampling.
We support our approach with a theoretical guarantee under mild assumptions and
extensive experiments showing that our methods consistently outperform existing
alternatives.

</details>


### [108] [Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library](https://arxiv.org/abs/2506.06122)
*Weixun Wang,Shaopan Xiong,Gengru Chen,Wei Gao,Sheng Guo,Yancheng He,Ju Huang,Jiaheng Liu,Zhendong Li,Xiaoyang Li,Zichen Liu,Haizhou Zhao,Dakai An,Lunxi Cao,Qiyang Cao,Wanxi Deng,Feilei Du,Yiliang Gu,Jiahe Li,Xiang Li,Mingjie Liu,Yijia Luo,Zihe Liu,Yadao Wang,Pei Wang,Tianyuan Wu,Yanan Wu,Yuheng Zhao,Shuaibing Zhao,Jin Yang,Siran Yang,Yingshui Tan,Huimin Yi,Yuchi Xu,Yujin Yuan,Xingyao Zhang,Lin Qu,Wenbo Su,Wei Wang,Jiamang Wang,Bo Zheng*

Main category: cs.LG

TL;DR: ROLL是一个高效、可扩展且用户友好的强化学习优化库，针对大规模学习场景设计，满足技术先驱、开发者和研究者的需求。


<details>
  <summary>Details</summary>
Motivation: 为不同用户群体（技术先驱、开发者和研究者）提供成本效益高、容错性强的大规模训练工具，同时支持灵活的实验和开发。

Method: 采用单控制器架构、并行策略和数据传输模块、细粒度生命周期管理的rollout调度器、环境与奖励工作器，以及灵活的AutoDeviceMapping资源分配。

Result: 实现了高效、可扩展且灵活的强化学习优化，支持大规模训练和快速实验。

Conclusion: ROLL通过模块化设计和灵活的资源管理，成功满足了不同用户群体的需求，为大规模强化学习提供了实用工具。

Abstract: We introduce ROLL, an efficient, scalable, and user-friendly library designed
for Reinforcement Learning Optimization for Large-scale Learning. ROLL caters
to three primary user groups: tech pioneers aiming for cost-effective,
fault-tolerant large-scale training, developers requiring flexible control over
training workflows, and researchers seeking agile experimentation. ROLL is
built upon several key modules to serve these user groups effectively. First, a
single-controller architecture combined with an abstraction of the parallel
worker simplifies the development of the training pipeline. Second, the
parallel strategy and data transfer modules enable efficient and scalable
training. Third, the rollout scheduler offers fine-grained management of each
sample's lifecycle during the rollout stage. Fourth, the environment worker and
reward worker support rapid and flexible experimentation with agentic RL
algorithms and reward designs. Finally, AutoDeviceMapping allows users to
assign resources to different models flexibly across various stages.

</details>


### [109] [Flow-Attentional Graph Neural Networks](https://arxiv.org/abs/2506.06127)
*Pascal Plettenberg,Dominik Köhler,Bernhard Sick,Josephine M. Thomas*

Main category: cs.LG

TL;DR: 提出了一种基于基尔霍夫第一定律的流注意力机制，提升了图神经网络在流图数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络未考虑流图数据中的守恒定律（如电流、交通流量），导致性能下降。

Method: 提出流注意力机制，改进现有图注意力机制以满足基尔霍夫第一定律。

Result: 实验证明流注意力在电路和电网数据集上提升了图分类和回归任务的性能。

Conclusion: 流注意力机制能有效捕捉流图数据的守恒特性，提升模型表现。

Abstract: Graph Neural Networks (GNNs) have become essential for learning from
graph-structured data. However, existing GNNs do not consider the conservation
law inherent in graphs associated with a flow of physical resources, such as
electrical current in power grids or traffic in transportation networks, which
can lead to reduced model performance. To address this, we propose flow
attention, which adapts existing graph attention mechanisms to satisfy
Kirchhoff\'s first law. Furthermore, we discuss how this modification
influences the expressivity and identify sets of non-isomorphic graphs that can
be discriminated by flow attention but not by standard attention. Through
extensive experiments on two flow graph datasets (electronic circuits and power
grids), we demonstrate that flow attention enhances the performance of
attention-based GNNs on both graph-level classification and regression tasks.

</details>


### [110] [Gradient Similarity Surgery in Multi-Task Deep Learning](https://arxiv.org/abs/2506.06130)
*Thomas Borsani,Andrea Rosani,Giuseppe Nicosia,Giuseppe Di Fatta*

Main category: cs.LG

TL;DR: 论文提出了一种新的梯度手术方法SAM-GS，通过梯度相似性度量优化多任务深度学习中的梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: 多任务深度学习（MTDL）中，任务梯度可能因方向或大小不同而产生冲突，影响收敛速度和稳定性。

Method: 提出SAM-GS方法，基于梯度相似性度量调整梯度轨迹，结合梯度均衡和一阶动量调制。

Result: 实验证明SAM-GS在合成问题和MTL基准测试中有效。

Conclusion: 梯度相似性在MTDL中对梯度聚合和优化学习过程起关键作用。

Abstract: The multi-task learning ($MTL$) paradigm aims to simultaneously learn
multiple tasks within a single model capturing higher-level, more general
hidden patterns that are shared by the tasks. In deep learning, a significant
challenge in the backpropagation training process is the design of advanced
optimisers to improve the convergence speed and stability of the gradient
descent learning rule. In particular, in multi-task deep learning ($MTDL$) the
multitude of tasks may generate potentially conflicting gradients that would
hinder the concurrent convergence of the diverse loss functions. This challenge
arises when the gradients of the task objectives have either different
magnitudes or opposite directions, causing one or a few to dominate or to
interfere with each other, thus degrading the training process. Gradient
surgery methods address the problem explicitly dealing with conflicting
gradients by adjusting the overall gradient trajectory. This work introduces a
novel gradient surgery method, the Similarity-Aware Momentum Gradient Surgery
(SAM-GS), which provides an effective and scalable approach based on a gradient
magnitude similarity measure to guide the optimisation process. The SAM-GS
surgery adopts gradient equalisation and modulation of the first-order
momentum. A series of experimental tests have shown the effectiveness of SAM-GS
on synthetic problems and $MTL$ benchmarks. Gradient magnitude similarity plays
a crucial role in regularising gradient aggregation in $MTDL$ for the
optimisation of the learning process.

</details>


### [111] [Table-r1: Self-supervised and Reinforcement Learning for Program-based Table Reasoning in Small Language Models](https://arxiv.org/abs/2506.06137)
*Rihui Jin,Zheyu Xin,Xing Xie,Zuoyi Li,Guilin Qi,Yongrui Chen,Xinbang Dai,Tongtong Wu,Gholamreza Haffari*

Main category: cs.LG

TL;DR: Table-r1是一种针对小语言模型（SLMs）的两阶段程序化表格推理方法，通过布局转换推理和混合范式优化，显著提升了表格推理的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLMs）在表格推理（TR）中表现受限，尤其是在数值推理和布局泛化方面。程序化TR（P-TR）虽能部分解决这些问题，但SLMs在代码生成和布局适应上仍存在不足。

Method: Table-r1采用两阶段方法：第一阶段通过自监督学习任务（布局转换推理）提升表格布局泛化能力；第二阶段采用混合范式优化（Group Relative Policy Optimization），增强推理一致性并支持动态回退到文本TR（T-TR）。

Result: 在四个TR基准测试中，Table-r1显著优于其他SLM方法，准确率比基础模型（LLaMA-8B）提升至少15%，且性能接近大语言模型（LLMs）。

Conclusion: Table-r1为SLMs提供了一种高效的表格推理解决方案，缩小了与LLMs的性能差距，同时解决了布局适应和推理一致性问题。

Abstract: Table reasoning (TR) requires structured reasoning over semi-structured
tabular data and remains challenging, particularly for small language models
(SLMs, e.g., LLaMA-8B) due to their limited capacity compared to large LMs
(LLMs, e.g., GPT-4o). To narrow this gap, we explore program-based TR (P-TR),
which circumvents key limitations of text-based TR (T-TR), notably in numerical
reasoning, by generating executable programs. However, applying P-TR to SLMs
introduces two challenges: (i) vulnerability to heterogeneity in table layouts,
and (ii) inconsistency in reasoning due to limited code generation capability.
We propose Table-r1, a two-stage P-TR method designed for SLMs. Stage 1
introduces an innovative self-supervised learning task, Layout Transformation
Inference, to improve tabular layout generalization from a programmatic view.
Stage 2 adopts a mix-paradigm variant of Group Relative Policy Optimization,
enhancing P-TR consistency while allowing dynamic fallback to T-TR when needed.
Experiments on four TR benchmarks demonstrate that Table-r1 outperforms all
SLM-based methods, achieving at least a 15% accuracy improvement over the base
model (LLaMA-8B) across all datasets and reaching performance competitive with
LLMs.

</details>


### [112] [carps: A Framework for Comparing N Hyperparameter Optimizers on M Benchmarks](https://arxiv.org/abs/2506.06143)
*Carolin Benjamins,Helena Graf,Sarah Segel,Difan Deng,Tim Ruhkopf,Leona Hennig,Soham Basu,Neeratyoy Mallik,Edward Bergman,Deyao Chen,François Clément,Matthias Feurer,Katharina Eggensperger,Frank Hutter,Carola Doerr,Marius Lindauer*

Main category: cs.LG

TL;DR: 提出了一个名为carps的基准框架，用于评估超参数优化（HPO）方法，支持多种任务类型，并提供代表性任务子集以提升效率。


<details>
  <summary>Details</summary>
Motivation: 为了简化和标准化HPO方法的原型设计和基准测试，开发了carps框架。

Method: carps框架整合了多种优化器和基准任务，并通过最小化子集的星差异来选取代表性任务。

Result: 提供了包含3,336个任务的库，并为每种任务类型推荐了10到30个代表性任务，同时建立了基线结果。

Conclusion: carps框架推动了HPO评估的标准化，为未来研究提供了高效的工具和基准。

Abstract: Hyperparameter Optimization (HPO) is crucial to develop well-performing
machine learning models. In order to ease prototyping and benchmarking of HPO
methods, we propose carps, a benchmark framework for Comprehensive Automated
Research Performance Studies allowing to evaluate N optimizers on M benchmark
tasks. In this first release of carps, we focus on the four most important
types of HPO task types: blackbox, multi-fidelity, multi-objective and
multi-fidelity-multi-objective. With 3 336 tasks from 5 community benchmark
collections and 28 variants of 9 optimizer families, we offer the biggest go-to
library to date to evaluate and compare HPO methods. The carps framework relies
on a purpose-built, lightweight interface, gluing together optimizers and
benchmark tasks. It also features an analysis pipeline, facilitating the
evaluation of optimizers on benchmarks. However, navigating a huge number of
tasks while developing and comparing methods can be computationally infeasible.
To address this, we obtain a subset of representative tasks by minimizing the
star discrepancy of the subset, in the space spanned by the full set. As a
result, we propose an initial subset of 10 to 30 diverse tasks for each task
type, and include functionality to re-compute subsets as more benchmarks become
available, enabling efficient evaluations. We also establish a first set of
baseline results on these tasks as a measure for future comparisons. With carps
(https://www.github.com/automl/CARP-S), we make an important step in the
standardization of HPO evaluation.

</details>


### [113] [ENMA: Tokenwise Autoregression for Generative Neural PDE Operators](https://arxiv.org/abs/2506.06158)
*Armand Kassaï Koupaï,Lise Le Boudec,Louis Serrano,Patrick Gallinari*

Main category: cs.LG

TL;DR: ENMA是一种生成神经算子，用于解决时间依赖参数偏微分方程（PDEs）的挑战，通过生成掩码自回归变换器和流匹配损失在潜在空间预测未来动态。


<details>
  <summary>Details</summary>
Motivation: 解决时间依赖参数PDEs的泛化问题，尤其是在数据不确定或不完整的情况下，需要一种生成模型来建模时空动态。

Method: ENMA使用生成掩码自回归变换器和流匹配损失在压缩潜在空间预测动态，通过注意力机制和时空卷积编码器处理不规则采样数据。

Result: ENMA能够泛化到新的PDE体系，支持时间依赖参数PDEs的一次性代理建模。

Conclusion: ENMA提供了一个鲁棒且适应性强的框架，适用于复杂PDE问题的求解。

Abstract: Solving time-dependent parametric partial differential equations (PDEs)
remains a fundamental challenge for neural solvers, particularly when
generalizing across a wide range of physical parameters and dynamics. When data
is uncertain or incomplete-as is often the case-a natural approach is to turn
to generative models. We introduce ENMA, a generative neural operator designed
to model spatio-temporal dynamics arising from physical phenomena. ENMA
predicts future dynamics in a compressed latent space using a generative masked
autoregressive transformer trained with flow matching loss, enabling tokenwise
generation. Irregularly sampled spatial observations are encoded into uniform
latent representations via attention mechanisms and further compressed through
a spatio-temporal convolutional encoder. This allows ENMA to perform in-context
learning at inference time by conditioning on either past states of the target
trajectory or auxiliary context trajectories with similar dynamics. The result
is a robust and adaptable framework that generalizes to new PDE regimes and
supports one-shot surrogate modeling of time-dependent parametric PDEs.

</details>


### [114] [The Lock-in Hypothesis: Stagnation by Algorithm](https://arxiv.org/abs/2506.06166)
*Tianyi Alex Qiu,Zhonghao He,Tejasveer Chugh,Max Kleiman-Weiner*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLMs）与人类用户的反馈循环，发现这种循环可能导致多样性的丧失和错误信念的固化。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs与人类用户的互动如何形成类似回音室的反馈循环，进而影响信念多样性和正确性。

Method: 通过基于代理的LLM模拟和真实GPT使用数据，形式化并验证假设。

Result: 分析显示，新GPT版本发布后，多样性出现突然且持续的下降，支持反馈循环假说。

Conclusion: 人类-AI反馈循环可能加剧信念固化，需警惕其对多样性和真理性的潜在负面影响。

Abstract: The training and deployment of large language models (LLMs) create a feedback
loop with human users: models learn human beliefs from data, reinforce these
beliefs with generated content, reabsorb the reinforced beliefs, and feed them
back to users again and again. This dynamic resembles an echo chamber. We
hypothesize that this feedback loop entrenches the existing values and beliefs
of users, leading to a loss of diversity and potentially the lock-in of false
beliefs. We formalize this hypothesis and test it empirically with agent-based
LLM simulations and real-world GPT usage data. Analysis reveals sudden but
sustained drops in diversity after the release of new GPT iterations,
consistent with the hypothesized human-AI feedback loop. Code and data
available at https://thelockinhypothesis.com

</details>


### [115] [Reusing Trajectories in Policy Gradients Enables Fast Convergence](https://arxiv.org/abs/2506.06178)
*Alessandro Montenegro,Federico Mansutti,Marco Mussi,Matteo Papini,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: 论文提出了一种改进的策略梯度方法（RPG），通过重用过去的离策略轨迹加速收敛，理论证明其样本复杂度达到最优。


<details>
  <summary>Details</summary>
Motivation: 传统策略梯度方法依赖新数据，样本效率低，而重用过去轨迹的理论研究不足。

Method: 引入幂均值校正的多重要性加权估计器，提出RPG算法结合新旧轨迹更新策略。

Result: 理论证明RPG的样本复杂度为$\widetilde{O}(\epsilon^{-1})$，优于现有方法。

Conclusion: RPG通过重用轨迹显著提升效率，理论和实验均验证其优越性。

Abstract: Policy gradient (PG) methods are a class of effective reinforcement learning
algorithms, particularly when dealing with continuous control problems. These
methods learn the parameters of parametric policies via stochastic gradient
ascent, typically using on-policy trajectory data to estimate the policy
gradient. However, such reliance on fresh data makes them sample-inefficient.
Indeed, vanilla PG methods require $O(\epsilon^{-2})$ trajectories to reach an
$\epsilon$-approximate stationary point. A common strategy to improve
efficiency is to reuse off-policy information from past iterations, such as
previous gradients or trajectories. While gradient reuse has received
substantial theoretical attention, leading to improved rates of
$O(\epsilon^{-3/2})$, the reuse of past trajectories remains largely unexplored
from a theoretical perspective. In this work, we provide the first rigorous
theoretical evidence that extensive reuse of past off-policy trajectories can
significantly accelerate convergence in PG methods. We introduce a power mean
correction to the multiple importance weighting estimator and propose RPG
(Retrospective Policy Gradient), a PG algorithm that combines old and new
trajectories for policy updates. Through a novel analysis, we show that, under
established assumptions, RPG achieves a sample complexity of
$\widetilde{O}(\epsilon^{-1})$, the best known rate in the literature. We
further validate empirically our approach against PG methods with
state-of-the-art rates.

</details>


### [116] [A Theoretical Study of (Hyper) Self-Attention through the Lens of Interactions: Representation, Training, Generalization](https://arxiv.org/abs/2506.06179)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: 论文研究了自注意力机制的理论基础，表明单层线性自注意力能高效表示、学习和泛化捕捉成对交互的函数，包括分布外场景。


<details>
  <summary>Details</summary>
Motivation: 探索自注意力机制的理论基础，揭示其在多领域中的通用性。

Method: 通过分析自注意力作为交互学习器的作用，提出新模块HyperFeatureAttention和HyperAttention。

Result: 理论分析和实验验证表明自注意力能学习交互函数并泛化到分布外场景。

Conclusion: 自注意力是一种通用的交互学习器，新模块能扩展其能力以捕捉多实体依赖关系。

Abstract: Self-attention has emerged as a core component of modern neural
architectures, yet its theoretical underpinnings remain elusive. In this paper,
we study self-attention through the lens of interacting entities, ranging from
agents in multi-agent reinforcement learning to alleles in genetic sequences,
and show that a single layer linear self-attention can efficiently represent,
learn, and generalize functions capturing pairwise interactions, including
out-of-distribution scenarios. Our analysis reveals that self-attention acts as
a mutual interaction learner under minimal assumptions on the diversity of
interaction patterns observed during training, thereby encompassing a wide
variety of real-world domains. In addition, we validate our theoretical
insights through experiments demonstrating that self-attention learns
interaction functions and generalizes across both population distributions and
out-of-distribution scenarios. Building on our theories, we introduce
HyperFeatureAttention, a novel neural network module designed to learn
couplings of different feature-level interactions between entities.
Furthermore, we propose HyperAttention, a new module that extends beyond
pairwise interactions to capture multi-entity dependencies, such as three-way,
four-way, or general n-way interactions.

</details>


### [117] [Antithetic Noise in Diffusion Models](https://arxiv.org/abs/2506.06185)
*Jing Jia,Sifan Liu,Bowen Song,Wei Yuan,Liyue Shen,Guanyang Wang*

Main category: cs.LG

TL;DR: 研究了扩散模型中初始噪声的对称性及其应用，发现初始噪声与其否定配对可产生强负相关样本，并基于此提出了提升图像多样性和不确定性量化的方法。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型中初始噪声的对称性及其潜在应用，以提升模型性能和多样性。

Method: 通过实验和理论分析验证初始噪声与其否定配对的负相关性，提出对称性猜想，并基于此开发了两种应用方法。

Result: 实现了图像多样性的提升和不确定性量化的优化（如置信区间缩小90%），并通过随机化准蒙特卡洛估计器进一步提高了准确性。

Conclusion: 该框架无需训练、模型无关且无运行时开销，为扩散模型的应用提供了新的可能性。

Abstract: We initiate a systematic study of antithetic initial noise in diffusion
models. Across unconditional models trained on diverse datasets,
text-conditioned latent-diffusion models, and diffusion-posterior samplers, we
find that pairing each initial noise with its negation consistently yields
strongly negatively correlated samples. To explain this phenomenon, we combine
experiments and theoretical analysis, leading to a symmetry conjecture that the
learned score function is approximately affine antisymmetric (odd symmetry up
to a constant shift), and provide evidence supporting it. Leveraging this
negative correlation, we enable two applications: (1) enhancing image diversity
in models like Stable Diffusion without quality loss, and (2) sharpening
uncertainty quantification (e.g., up to 90% narrower confidence intervals) when
estimating downstream statistics. Building on these gains, we extend the
two-point pairing to a randomized quasi-Monte Carlo estimator, which further
improves estimation accuracy. Our framework is training-free, model-agnostic,
and adds no runtime overhead.

</details>


### [118] [Physics-Informed Neural Networks for Control of Single-Phase Flow Systems Governed by Partial Differential Equations](https://arxiv.org/abs/2506.06188)
*Luis Kin Miyatake,Eduardo Camponogara,Eric Aislan Antonelo,Alexey Pavlov*

Main category: cs.LG

TL;DR: 论文扩展了PINC框架，将其从ODE控制扩展到PDE控制，通过结合神经网络与物理守恒定律，实现了无需标记数据的单相流建模与控制。


<details>
  <summary>Details</summary>
Motivation: 解决PDE控制的挑战，尤其是在瞬态条件下，为工程应用提供高效且无需标记数据的流体流监控与优化方法。

Method: 采用两阶段网络结构：稳态网络学习平衡解，瞬态网络捕捉动态响应；通过简化假设降低空间维度，结合MPC推导最优控制策略。

Result: 数值实验验证了PINC模型能准确表示流动力学并实现实时控制，无需迭代求解器。

Conclusion: PINC框架为流体流监控与优化提供了高效且无需标记数据的解决方案，具有工程应用潜力。

Abstract: The modeling and control of single-phase flow systems governed by Partial
Differential Equations (PDEs) present challenges, especially under transient
conditions. In this work, we extend the Physics-Informed Neural Nets for
Control (PINC) framework, originally proposed to modeling and control of
Ordinary Differential Equations (ODE) without the need of any labeled data, to
the PDE case, particularly to single-phase incompressible and compressible
flows, integrating neural networks with physical conservation laws. The PINC
model for PDEs is structured into two stages: a steady-state network, which
learns equilibrium solutions for a wide range of control inputs, and a
transient network, which captures dynamic responses under time-varying boundary
conditions. We propose a simplifying assumption that reduces the dimensionality
of the spatial coordinate regarding the initial condition, allowing the
efficient training of the PINC network. This simplification enables the
derivation of optimal control policies using Model Predictive Control (MPC). We
validate our approach through numerical experiments, demonstrating that the
PINC model, which is trained exclusively using physical laws, i.e., without
labeled data, accurately represents flow dynamics and enables real-time control
applications. The results highlight the PINC's capability to efficiently
approximate PDE solutions without requiring iterative solvers, making it a
promising alternative for fluid flow monitoring and optimization in engineering
applications.

</details>


### [119] [ICU-TSB: A Benchmark for Temporal Patient Representation Learning for Unsupervised Stratification into Patient Cohorts](https://arxiv.org/abs/2506.06192)
*Dimitrios Proios,Alban Bornet,Anthony Yazdani,Jose F Rodrigues Jr,Douglas Teodoro*

Main category: cs.LG

TL;DR: ICU-TSB是一个基于ICU电子健康记录（EHR）的基准测试，用于评估患者分层方法，通过时间表示学习和聚类分析，验证了其在发现临床相关患者群体方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 患者分层对个性化医疗至关重要，ICU的EHR数据提供了丰富的时间临床信息，可用于此目的。

Method: 提出了ICU-TSB基准，采用层次化评估框架，结合疾病分类法，比较了统计方法和RNN（如LSTM和GRU）在患者表示学习中的表现。

Result: 时间表示学习能够发现临床相关的患者群体，但任务仍具挑战性，v-measure在分类法不同层级上介于0.40至0.46之间。

Conclusion: ICU-TSB为患者分层提供了可复现的评估工具，并探索了为聚类分配可解释标签的策略，推动了该领域的实用化。

Abstract: Patient stratification identifying clinically meaningful subgroups is
essential for advancing personalized medicine through improved diagnostics and
treatment strategies. Electronic health records (EHRs), particularly those from
intensive care units (ICUs), contain rich temporal clinical data that can be
leveraged for this purpose. In this work, we introduce ICU-TSB (Temporal
Stratification Benchmark), the first comprehensive benchmark for evaluating
patient stratification based on temporal patient representation learning using
three publicly available ICU EHR datasets. A key contribution of our benchmark
is a novel hierarchical evaluation framework utilizing disease taxonomies to
measure the alignment of discovered clusters with clinically validated disease
groupings. In our experiments with ICU-TSB, we compared statistical methods and
several recurrent neural networks, including LSTM and GRU, for their ability to
generate effective patient representations for subsequent clustering of patient
trajectories. Our results demonstrate that temporal representation learning can
rediscover clinically meaningful patient cohorts; nevertheless, it remains a
challenging task, with v-measuring varying from up to 0.46 at the top level of
the taxonomy to up to 0.40 at the lowest level. To further enhance the
practical utility of our findings, we also evaluate multiple strategies for
assigning interpretable labels to the identified clusters. The experiments and
benchmark are fully reproducible and available at
https://github.com/ds4dh/CBMS2025stratification.

</details>


### [120] [Transformative or Conservative? Conservation laws for ResNets and Transformers](https://arxiv.org/abs/2506.06194)
*Sibylle Marcotte,Rémi Gribonval,Gabriel Peyré*

Main category: cs.LG

TL;DR: 该论文研究了现代架构（如卷积ResNets和Transformer网络）中的守恒定律，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 研究现代架构中的守恒定律，以补充目前对浅层ReLU和线性网络的理解。

Method: 通过分析基本构建块（如ReLU或线性网络、注意力层和残差块）的守恒定律，并将其扩展到现代架构。

Result: 发现现代架构中的守恒定律可以通过分析独立构建块来表征，并验证了这些定律在离散优化（如SGD）中的持续性。

Conclusion: 论文为现代架构中的守恒定律提供了系统分析，并展示了其在离散优化中的适用性。

Abstract: While conservation laws in gradient flow training dynamics are well
understood for (mostly shallow) ReLU and linear networks, their study remains
largely unexplored for more practical architectures. This paper bridges this
gap by deriving and analyzing conservation laws for modern architectures, with
a focus on convolutional ResNets and Transformer networks. For this, we first
show that basic building blocks such as ReLU (or linear) shallow networks, with
or without convolution, have easily expressed conservation laws, and no more
than the known ones. In the case of a single attention layer, we also
completely describe all conservation laws, and we show that residual blocks
have the same conservation laws as the same block without a skip connection. We
then introduce the notion of conservation laws that depend only on a subset of
parameters (corresponding e.g. to a pair of consecutive layers, to a residual
block, or to an attention layer). We demonstrate that the characterization of
such laws can be reduced to the analysis of the corresponding building block in
isolation. Finally, we examine how these newly discovered conservation
principles, initially established in the continuous gradient flow regime,
persist under discrete optimization dynamics, particularly in the context of
Stochastic Gradient Descent (SGD).

</details>


### [121] [How to craft a deep reinforcement learning policy for wind farm flow control](https://arxiv.org/abs/2506.06204)
*Elie Kadoche,Pascal Bianchi,Florence Carton,Philippe Ciblat,Damien Ernst*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习的新方法，用于设计风电场中的尾流转向控制器，以优化能量生产。


<details>
  <summary>Details</summary>
Motivation: 风电场中涡轮机之间的尾流效应会显著降低整体能量生产，现有的机器学习方法仅适用于准静态风况或小型风电场。

Method: 结合图注意力网络和多头自注意力块的新架构，以及新的奖励函数和训练策略，计算每个涡轮的偏航角。

Result: 模型在低精度稳态模拟中，训练步骤减少10倍，能量生产提升高达14%。

Conclusion: 这是首个能在时变风况下有效泛化的深度强化学习尾流转向控制器。

Abstract: Within wind farms, wake effects between turbines can significantly reduce
overall energy production. Wind farm flow control encompasses methods designed
to mitigate these effects through coordinated turbine control. Wake steering,
for example, consists in intentionally misaligning certain turbines with the
wind to optimize airflow and increase power output. However, designing a robust
wake steering controller remains challenging, and existing machine learning
approaches are limited to quasi-static wind conditions or small wind farms.
This work presents a new deep reinforcement learning methodology to develop a
wake steering policy that overcomes these limitations. Our approach introduces
a novel architecture that combines graph attention networks and multi-head
self-attention blocks, alongside a novel reward function and training strategy.
The resulting model computes the yaw angles of each turbine, optimizing energy
production in time-varying wind conditions. An empirical study conducted on
steady-state, low-fidelity simulation, shows that our model requires
approximately 10 times fewer training steps than a fully connected neural
network and achieves more robust performance compared to a strong optimization
baseline, increasing energy production by up to 14 %. To the best of our
knowledge, this is the first deep reinforcement learning-based wake steering
controller to generalize effectively across any time-varying wind conditions in
a low-fidelity, steady-state numerical simulation setting.

</details>


### [122] [Model-Driven Graph Contrastive Learning](https://arxiv.org/abs/2506.06212)
*Ali Azizpour,Nicolas Zilberstein,Santiago Segarra*

Main category: cs.LG

TL;DR: MGCL是一种基于模型驱动的图对比学习框架，利用图生成模型（graphons）指导对比学习，通过数据生成过程优化表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有GCL方法依赖手动设计的启发式增强策略，未考虑数据分布，且忽略同模型生成图间的相似性。MGCL旨在通过图生成模型实现数据自适应和原则性增强。

Method: MGCL首先估计观测数据的图生成模型，定义基于图生成模型的增强过程。对于图级任务，MGCL对数据集聚类并为每组估计图生成模型，以反映共享语义和结构。

Result: 在基准数据集上的实验表明，MGCL实现了最先进的性能。

Conclusion: MGCL通过将生成模型融入GCL，展示了数据自适应增强的优势。

Abstract: We propose $\textbf{MGCL}$, a model-driven graph contrastive learning (GCL)
framework that leverages graphons (probabilistic generative models for graphs)
to guide contrastive learning by accounting for the data's underlying
generative process. GCL has emerged as a powerful self-supervised framework for
learning expressive node or graph representations without relying on annotated
labels, which are often scarce in real-world data. By contrasting augmented
views of graph data, GCL has demonstrated strong performance across various
downstream tasks, such as node and graph classification. However, existing
methods typically rely on manually designed or heuristic augmentation
strategies that are not tailored to the underlying data distribution and
operate at the individual graph level, ignoring similarities among graphs
generated from the same model. Conversely, in our proposed approach, MGCL first
estimates the graphon associated with the observed data and then defines a
graphon-informed augmentation process, enabling data-adaptive and principled
augmentations. Additionally, for graph-level tasks, MGCL clusters the dataset
and estimates a graphon per group, enabling contrastive pairs to reflect shared
semantics and structure. Extensive experiments on benchmark datasets
demonstrate that MGCL achieves state-of-the-art performance, highlighting the
advantages of incorporating generative models into GCL.

</details>


### [123] [Corrector Sampling in Language Models](https://arxiv.org/abs/2506.06215)
*Itai Gat,Neta Shaul,Uriel Singer,Yaron Lipman*

Main category: cs.LG

TL;DR: 提出了一种名为RPT的新采样方法，通过迭代重新访问和替换先前生成的文本来减少自回归语言模型中的错误累积。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型因其固定的从左到右的标记生成方式会导致错误累积，需要一种方法来缓解这一问题。

Method: 提出RPT方法，通过迭代重新访问和替换先前生成的文本来减少错误累积，同时保持模型的预测质量和速度。

Result: 在8B参数模型上仅用100B数据进行微调后，RPT在推理和编码基准测试中实现了约10%的相对改进。

Conclusion: RPT是一种有效的采样方法，能够显著减少自回归语言模型中的错误累积，提升模型性能。

Abstract: Autoregressive language models accumulate errors due to their fixed,
irrevocable left-to-right token generation. To address this, we propose a new
sampling method called Resample-Previous-Tokens (RPT). RPT mitigates error
accumulation by iteratively revisiting and potentially replacing tokens in a
window of previously generated text. This method can be integrated into
existing autoregressive models, preserving their next-token-prediction quality
and speed. Fine-tuning a pretrained 8B parameter model with RPT for only 100B
resulted in ~10% relative improvements on reasoning and coding benchmarks
compared to the standard sampling.

</details>


### [124] [Towards an Explainable Comparison and Alignment of Feature Embeddings](https://arxiv.org/abs/2506.06231)
*Mohammad Jalali,Bahar Dibaei Nia,Farzan Farnia*

Main category: cs.LG

TL;DR: 提出了SPEC框架，用于比较和调整嵌入模型，通过核矩阵差异分析聚类差异，并实现线性计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型的比较主要关注数值性能，缺乏对聚类差异的可解释性分析。

Method: 利用核矩阵的谱分解检测嵌入间的聚类差异，并提出优化问题调整嵌入对齐。

Result: 在ImageNet和MS-COCO等大规模数据集上验证了SPEC的有效性。

Conclusion: SPEC提供了一种可扩展且可解释的方法来比较和调整嵌入模型。

Abstract: While several feature embedding models have been developed in the literature,
comparisons of these embeddings have largely focused on their numerical
performance in classification-related downstream applications. However, an
interpretable comparison of different embeddings requires identifying and
analyzing mismatches between sample groups clustered within the embedding
spaces. In this work, we propose the \emph{Spectral Pairwise Embedding
Comparison (SPEC)} framework to compare embeddings and identify their
differences in clustering a reference dataset. Our approach examines the kernel
matrices derived from two embeddings and leverages the eigendecomposition of
the difference kernel matrix to detect sample clusters that are captured
differently by the two embeddings. We present a scalable implementation of this
kernel-based approach, with computational complexity that grows linearly with
the sample size. Furthermore, we introduce an optimization problem using this
framework to align two embeddings, ensuring that clusters identified in one
embedding are also captured in the other model. We provide numerical results
demonstrating the SPEC's application to compare and align embeddings on
large-scale datasets such as ImageNet and MS-COCO. The code is available at
[https://github.com/mjalali/embedding-comparison](github.com/mjalali/embedding-comparison).

</details>


### [125] [Neural Responses to Affective Sentences Reveal Signatures of Depression](https://arxiv.org/abs/2506.06244)
*Aditya Kommineni,Woojae Jeong,Kleanthis Avramidis,Colin McDaniel,Myzelle Hughes,Thomas McGee,Elsi Kaiser,Kristina Lerman,Idan A. Blank,Dani Byrd,Assal Habibi,B. Rael Cahn,Sudarsana Kadiri,Takfarinas Medani,Richard M. Leahy,Shrikanth Narayanan*

Main category: cs.LG

TL;DR: 研究通过EEG测量健康与抑郁个体对自我参照情感句子的神经反应，发现抑郁情绪处理的时间动态变化，深度学习模型能区分健康与抑郁个体及抑郁亚组。


<details>
  <summary>Details</summary>
Motivation: 深入了解抑郁症的神经认知基础，尤其是情感和自我参照处理功能的变化。

Method: 使用表面EEG测量健康与抑郁个体对自我参照情感句子的神经反应，并训练深度学习模型分析数据。

Result: 发现抑郁个体在句子观看时的神经活动显著不同，深度学习模型区分健康与抑郁的AUC为0.707，区分抑郁亚组的AUC为0.624。

Conclusion: 抑郁具有稳定的神经特征，可能为未来诊断工具提供依据。

Abstract: Major Depressive Disorder (MDD) is a highly prevalent mental health
condition, and a deeper understanding of its neurocognitive foundations is
essential for identifying how core functions such as emotional and
self-referential processing are affected. We investigate how depression alters
the temporal dynamics of emotional processing by measuring neural responses to
self-referential affective sentences using surface electroencephalography (EEG)
in healthy and depressed individuals. Our results reveal significant
group-level differences in neural activity during sentence viewing, suggesting
disrupted integration of emotional and self-referential information in
depression. Deep learning model trained on these responses achieves an area
under the receiver operating curve (AUC) of 0.707 in distinguishing healthy
from depressed participants, and 0.624 in differentiating depressed subgroups
with and without suicidal ideation. Spatial ablations highlight anterior
electrodes associated with semantic and affective processing as key
contributors. These findings suggest stable, stimulus-driven neural signatures
of depression that may inform future diagnostic tools.

</details>


### [126] [Lagrangian-based Equilibrium Propagation: generalisation to arbitrary boundary conditions & equivalence with Hamiltonian Echo Learning](https://arxiv.org/abs/2506.06248)
*Guillaume Pourcel,Debabrota Basu,Maxence Ernoult,Aditya Gilra*

Main category: cs.LG

TL;DR: GLEP扩展了EP算法，适用于时变输入，并展示了不同边界条件下的学习算法。HEL是GLEP的特例，继承了EP的硬件友好特性。


<details>
  <summary>Details</summary>
Motivation: 将EP算法扩展到时变输入，解决变分描述和边界条件的挑战。

Method: 提出GLEP，扩展EP的变分框架，分析不同边界条件下的学习算法。

Result: GLEP生成多种算法，HEL是唯一具备EP硬件友好特性的特例。

Conclusion: GLEP为时变输入提供通用框架，HEL是其实用且高效的实现。

Abstract: Equilibrium Propagation (EP) is a learning algorithm for training
Energy-based Models (EBMs) on static inputs which leverages the variational
description of their fixed points. Extending EP to time-varying inputs is a
challenging problem, as the variational description must apply to the entire
system trajectory rather than just fixed points, and careful consideration of
boundary conditions becomes essential. In this work, we present Generalized
Lagrangian Equilibrium Propagation (GLEP), which extends the variational
formulation of EP to time-varying inputs. We demonstrate that GLEP yields
different learning algorithms depending on the boundary conditions of the
system, many of which are impractical for implementation. We then show that
Hamiltonian Echo Learning (HEL) -- which includes the recently proposed
Recurrent HEL (RHEL) and the earlier known Hamiltonian Echo Backpropagation
(HEB) algorithms -- can be derived as a special case of GLEP. Notably, HEL is
the only instance of GLEP we found that inherits the properties that make EP a
desirable alternative to backpropagation for hardware implementations: it
operates in a "forward-only" manner (i.e. using the same system for both
inference and learning), it scales efficiently (requiring only two or more
passes through the system regardless of model size), and enables local
learning.

</details>


### [127] [Distillation Robustifies Unlearning](https://arxiv.org/abs/2506.06278)
*Bruce W. Lee,Addie Foote,Alex Infanger,Leni Shor,Harish Kamath,Jacob Goldman-Wetzler,Bryce Woodworth,Alex Cloud,Alexander Matt Turner*

Main category: cs.LG

TL;DR: 当前LLM遗忘方法不够鲁棒，容易被微调恢复。提出UNDO方法，通过蒸馏增强遗忘鲁棒性，计算成本低且效果好。


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘方法易被微调恢复，输出微调不足以实现鲁棒遗忘，需探索更有效方法。

Method: 提出UNDO方法，通过蒸馏未学习模型到部分噪声版本，平衡计算成本与鲁棒性。

Result: UNDO在合成任务中接近从头训练的鲁棒性，计算成本低，仅需少量标注数据；在WMDP基准上也表现良好。

Conclusion: UNDO为实际应用提供了一种高效鲁棒的遗忘方法，结合蒸馏可方便实现能力移除。

Abstract: Current LLM unlearning methods are not robust: they can be reverted easily
with a few steps of finetuning. This is true even for the idealized unlearning
method of training to imitate an oracle model that was never exposed to
unwanted information, suggesting that output-based finetuning is insufficient
to achieve robust unlearning. In a similar vein, we find that training a
randomly initialized student to imitate an unlearned model transfers desired
behaviors while leaving undesired capabilities behind. In other words,
distillation robustifies unlearning. Building on this insight, we propose
Unlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an
unlearned model into a partially noised copy of itself. UNDO introduces a
tunable tradeoff between compute cost and robustness, establishing a new Pareto
frontier on synthetic language and arithmetic tasks. At its strongest setting,
UNDO matches the robustness of a model retrained from scratch with perfect data
filtering while using only 60-80% of the compute and requiring only 0.01% of
the pretraining data to be labeled. We also show that UNDO robustifies
unlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP)
benchmark. Since distillation is widely used in practice, incorporating an
unlearning step beforehand offers a convenient path to robust capability
removal.

</details>


### [128] [Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias](https://arxiv.org/abs/2506.06280)
*Yuanzhe Hu,Kinshuk Goel,Vlad Killiakov,Yaoqing Yang*

Main category: cs.LG

TL;DR: 论文提出FARMS方法，通过固定纵横比的子矩阵采样解决权重矩阵纵横比对重尾性度量的偏差问题，提升模型诊断和超参数分配的准确性。


<details>
  <summary>Details</summary>
Motivation: 权重矩阵的纵横比会影响重尾性度量的准确性，导致模型诊断和层间超参数分配不准确。

Method: 提出FARMS方法，通过固定纵横比的子矩阵采样，测量子矩阵的平均谱密度重尾性，消除纵横比偏差。

Result: FARMS在多种应用领域（CV、SciML、LLM）中提升了谱分析的准确性，并在LLM剪枝实验中降低了17.3%的困惑度。

Conclusion: FARMS是一种简单有效的方法，能够消除纵横比偏差，提升模型诊断和超参数分配的准确性。

Abstract: Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight
matrices has been an active area of research in recent years. At a high level,
eigenspectrum analysis of DNNs involves measuring the heavytailness of the
empirical spectral densities (ESD) of weight matrices. It provides insight into
how well a model is trained and can guide decisions on assigning better
layer-wise training hyperparameters. In this paper, we address a challenge
associated with such eigenspectrum methods: the impact of the aspect ratio of
weight matrices on estimated heavytailness metrics. We demonstrate that
matrices of varying sizes (and aspect ratios) introduce a non-negligible bias
in estimating heavytailness metrics, leading to inaccurate model diagnosis and
layer-wise hyperparameter assignment. To overcome this challenge, we propose
FARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the
weight matrices by subsampling submatrices with a fixed aspect ratio. Instead
of measuring the heavytailness of the original ESD, we measure the average ESD
of these subsampled submatrices. We show that measuring the heavytailness of
these submatrices with the fixed aspect ratio can effectively mitigate the
aspect ratio bias. We validate our approach across various optimization
techniques and application domains that involve eigenspectrum analysis of
weights, including image classification in computer vision (CV) models,
scientific machine learning (SciML) model training, and large language model
(LLM) pruning. Our results show that despite its simplicity, FARMS uniformly
improves the accuracy of eigenspectrum analysis while enabling more effective
layer-wise hyperparameter assignment in these application domains. In one of
the LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model
by 17.3% when compared with the state-of-the-art method.

</details>


### [129] [Mixture-of-Experts Meets In-Context Reinforcement Learning](https://arxiv.org/abs/2506.05426)
*Wenhao Wu,Fuhong Liu,Haoru Li,Zican Hu,Daoyi Dong,Chunlin Chen,Zhi Wang*

Main category: cs.LG

TL;DR: T2MIR是一种基于混合专家（MoE）的框架，通过令牌和任务级MoE提升上下文强化学习（ICRL）的性能，解决了多模态数据和任务多样性的挑战。


<details>
  <summary>Details</summary>
Motivation: ICRL在适应下游任务时面临多模态数据和任务异质性的挑战，需要更高效的架构来提升性能。

Method: T2MIR采用令牌级和任务级MoE，结合对比学习优化任务路由，增强模型对多模态和任务多样性的处理能力。

Result: 实验表明T2MIR显著提升了上下文学习能力，优于多种基线方法。

Conclusion: T2MIR为ICRL提供了简单可扩展的架构改进，推动了其在语言和视觉领域的应用潜力。

Abstract: In-context reinforcement learning (ICRL) has emerged as a promising paradigm
for adapting RL agents to downstream tasks through prompt conditioning.
However, two notable challenges remain in fully harnessing in-context learning
within RL domains: the intrinsic multi-modality of the state-action-reward data
and the diverse, heterogeneous nature of decision tasks. To tackle these
challenges, we propose \textbf{T2MIR} (\textbf{T}oken- and \textbf{T}ask-wise
\textbf{M}oE for \textbf{I}n-context \textbf{R}L), an innovative framework that
introduces architectural advances of mixture-of-experts (MoE) into
transformer-based decision models. T2MIR substitutes the feedforward layer with
two parallel layers: a token-wise MoE that captures distinct semantics of input
tokens across multiple modalities, and a task-wise MoE that routes diverse
tasks to specialized experts for managing a broad task distribution with
alleviated gradient conflicts. To enhance task-wise routing, we introduce a
contrastive learning method that maximizes the mutual information between the
task and its router representation, enabling more precise capture of
task-relevant information. The outputs of two MoE components are concatenated
and fed into the next layer. Comprehensive experiments show that T2MIR
significantly facilitates in-context learning capacity and outperforms various
types of baselines. We bring the potential and promise of MoE to ICRL, offering
a simple and scalable architectural enhancement to advance ICRL one step closer
toward achievements in language and vision communities. Our code is available
at https://github.com/NJU-RL/T2MIR.

</details>


### [130] [MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction](https://arxiv.org/abs/2506.05427)
*Zishan Shu,Yufan Deng,Hongyu Zhang,Zhiwei Nie,Jie Chen*

Main category: cs.LG

TL;DR: MTPNet是一种用于活性悬崖预测的多粒度目标感知网络，通过结合分子与靶蛋白的相互作用先验知识，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法仅能处理单一结合目标，限制了预测模型的适用性。MTPNet旨在通过多粒度蛋白质语义条件动态优化分子表征。

Method: MTPNet由宏观目标语义（MTS）和微观口袋语义（MPS）两部分组成，利用受体蛋白作为指导信息捕获关键相互作用细节。

Result: 在30个代表性活性悬崖数据集上的实验表明，MTPNet显著优于现有方法，平均RMSE提升了18.95%。

Conclusion: MTPNet通过条件深度学习内化相互作用模式，实现了活性悬崖的统一预测，有助于加速化合物优化与设计。

Abstract: Activity cliff prediction is a critical task in drug discovery and material
design. Existing computational methods are limited to handling single binding
targets, which restricts the applicability of these prediction models. In this
paper, we present the Multi-Grained Target Perception network (MTPNet) to
incorporate the prior knowledge of interactions between the molecules and their
target proteins. Specifically, MTPNet is a unified framework for activity cliff
prediction, which consists of two components: Macro-level Target Semantic (MTS)
guidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet
dynamically optimizes molecular representations through multi-grained protein
semantic conditions. To our knowledge, it is the first time to employ the
receptor proteins as guiding information to effectively capture critical
interaction details. Extensive experiments on 30 representative activity cliff
datasets demonstrate that MTPNet significantly outperforms previous approaches,
achieving an average RMSE improvement of 18.95% on top of several mainstream
GNN architectures. Overall, MTPNet internalizes interaction patterns through
conditional deep learning to achieve unified predictions of activity cliffs,
helping to accelerate compound optimization and design. Codes are available at:
https://github.com/ZishanShu/MTPNet.

</details>


### [131] [Diffusion with a Linguistic Compass: Steering the Generation of Clinically Plausible Future sMRI Representations for Early MCI Conversion Prediction](https://arxiv.org/abs/2506.05428)
*Zhihao Tang,Chaozhuo Li,Litian Zhang,Xi Zhang*

Main category: cs.LG

TL;DR: MCI-Diff是一种基于扩散的框架，通过从基线数据合成未来sMRI表示，实现实时风险评估和高预测性能。


<details>
  <summary>Details</summary>
Motivation: 早期预测轻度认知障碍（MCI）转化存在即时性（从单次基线sMRI快速预测）与准确性（利用纵向扫描捕捉疾病进展）之间的权衡。

Method: 1. 多任务序列重建策略训练共享去噪网络处理不规则随访采样；2. 引入LLM驱动的“语言指南”进行临床合理性采样。

Result: 在ADNI和AIBL队列中，MCI-Diff优于现有基线方法，早期转化准确性提高5-12%。

Conclusion: MCI-Diff在实时性和准确性上取得平衡，显著提升了MCI早期转化的预测性能。

Abstract: Early prediction of Mild Cognitive Impairment (MCI) conversion is hampered by
a trade-off between immediacy--making fast predictions from a single baseline
sMRI--and accuracy--leveraging longitudinal scans to capture disease
progression. We propose MCI-Diff, a diffusion-based framework that synthesizes
clinically plausible future sMRI representations directly from baseline data,
achieving both real-time risk assessment and high predictive performance.
First, a multi-task sequence reconstruction strategy trains a shared denoising
network on interpolation and extrapolation tasks to handle irregular follow-up
sampling and learn robust latent trajectories. Second, an LLM-driven
"linguistic compass" is introduced for clinical plausibility sampling:
generated feature candidates are quantized, tokenized, and scored by a
fine-tuned language model conditioned on expected structural biomarkers,
guiding autoregressive generation toward realistic disease patterns.
Experiments on ADNI and AIBL cohorts show that MCI-Diff outperforms
state-of-the-art baselines, improving early conversion accuracy by 5-12%.

</details>


### [132] [PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling](https://arxiv.org/abs/2506.05432)
*Yuxuan Yue,Zukang Xu,Zhihang Yuan,Dawei Yang,Jianglong Wu,Liqiang Nie*

Main category: cs.LG

TL;DR: 论文提出PCDVQ框架，通过解耦向量的方向和幅度进行量化，显著提升了低比特量化下LLMs的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有向量量化方法通常耦合量化向量的方向和幅度，但研究发现方向对量化更敏感，现有方法因欧氏距离的局限性导致量化误差较大。

Method: 提出Polar Coordinate Decoupled Vector Quantization (PCDVQ)，包括两个模块：1) 极坐标解耦（PCD），独立量化方向和幅度；2) 分布对齐码本构建（DACC），优化码本以匹配源分布。

Result: 实验显示PCDVQ在2比特量化下比基线方法至少提升1.5%的零样本准确率。

Conclusion: PCDVQ为高压缩LLMs提供了一种新范式，显著提升了量化精度。

Abstract: Large Language Models (LLMs) face significant challenges in edge deployment
due to their massive parameter scale. Vector Quantization (VQ), a
clustering-based quantization method, serves as a prevalent solution to this
issue for its extremely low-bit (even at 2-bit) and considerable accuracy.
Since a vector is a quantity in mathematics and physics that has both direction
and magnitude, existing VQ works typically quantize them in a coupled manner.
However, we find that direction exhibits significantly greater sensitivity to
quantization compared to the magnitude. For instance, when separately
clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the
accuracy drop of zero-shot tasks are 46.5\% and 2.3\%, respectively. This gap
even increases with the reduction of clustering centers. Further, Euclidean
distance, a common metric to access vector similarities in current VQ works,
places greater emphasis on reducing the magnitude error. This property is
contrary to the above finding, unavoidably leading to larger quantization
errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector
Quantization (PCDVQ), an effective and efficient VQ framework consisting of two
key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors
into their polar coordinate representations and perform independent
quantization of the direction and magnitude parameters.2) Distribution Aligned
Codebook Construction (DACC), which optimizes the direction and magnitude
codebooks in accordance with the source distribution. Experimental results show
that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\%
zero-shot accuracy, establishing a novel paradigm for accurate and highly
compressed LLMs.

</details>


### [133] [Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward](https://arxiv.org/abs/2506.05433)
*Zikang Liu,Tongtian Yue,Yepeng Tang,Longteng Guo,Junxian Cai,Qingbin Liu,Xi Chen,Jing Liu*

Main category: cs.LG

TL;DR: Prefix Grouper是一种高效的GRPO训练算法，通过共享前缀计算减少冗余，提升长上下文学习场景下的计算效率。


<details>
  <summary>Details</summary>
Motivation: GRPO在处理长共享前缀时存在计算冗余问题，成为扩展性的瓶颈。

Method: 采用Shared-Prefix Forward策略，将自注意力分为两部分，共享前缀仅编码一次，保持端到端训练兼容性。

Result: 理论和实验证明Prefix Grouper与标准GRPO训练等效，显著降低计算成本。

Conclusion: Prefix Grouper可无缝集成现有GRPO架构，提升任务复杂性和模型规模的扩展性。

Abstract: Group Relative Policy Optimization (GRPO) enhances policy learning by
computing gradients from relative comparisons among candidate outputs that
share a common input prefix. Despite its effectiveness, GRPO introduces
substantial computational overhead when processing long shared prefixes, which
must be redundantly encoded for each group member. This inefficiency becomes a
major scalability bottleneck in long-context learning scenarios. We propose
Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant
prefix computation via a Shared-Prefix Forward strategy. In particular, by
restructuring self-attention into two parts, our method enables the shared
prefix to be encoded only once, while preserving full differentiability and
compatibility with end-to-end training. We provide both theoretical and
empirical evidence that Prefix Grouper is training-equivalent to standard GRPO:
it yields identical forward outputs and backward gradients, ensuring that the
optimization dynamics and final policy performance remain unchanged.
Empirically, our experiments confirm that Prefix Grouper achieves consistent
results while significantly reducing the computational cost of training,
particularly in long-prefix scenarios. The proposed method is fully
plug-and-play: it is compatible with existing GRPO-based architectures and can
be seamlessly integrated into current training pipelines as a drop-in
replacement, requiring no structural modifications and only minimal changes to
input construction and attention computation. Prefix Grouper enables the use of
larger group sizes under the same computational budget, thereby improving the
scalability of GRPO to more complex tasks and larger models. Code is now
available at https://github.com/johncaged/PrefixGrouper

</details>


### [134] [Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks](https://arxiv.org/abs/2506.05434)
*Thomas Massena,Léo andéol,Thibaut Boissin,Franck Mamalet,Corentin Friedrich,Mathieu Serrurier,Sébastien Gerchinovitz*

Main category: cs.LG

TL;DR: 本文提出了一种基于Lipschitz约束网络的新方法（lip-rcp），用于高效精确地估计鲁棒共形预测集，并在中大规模场景（如ImageNet）中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测（CP）在对抗攻击下无法保证预测集的可靠性，而现有鲁棒CP方法在大规模问题中计算成本高或预测集过大，难以实际应用。

Method: 利用Lipschitz约束网络设计lip-rcp方法，结合1-Lipschitz鲁棒网络，高效生成鲁棒CP集。

Result: lip-rcp在中大规模场景（如ImageNet）中，预测集大小和计算效率均优于现有方法。同时，还分析了传统CP在攻击下的最坏覆盖边界。

Conclusion: lip-rcp方法在保证鲁棒性的同时，计算效率与传统CP相当，为大规模应用提供了可行方案。

Abstract: Conformal Prediction (CP) has proven to be an effective post-hoc method for
improving the trustworthiness of neural networks by providing prediction sets
with finite-sample guarantees. However, under adversarial attacks, classical
conformal guarantees do not hold anymore: this problem is addressed in the
field of Robust Conformal Prediction. Several methods have been proposed to
provide robust CP sets with guarantees under adversarial perturbations, but,
for large scale problems, these sets are either too large or the methods are
too computationally demanding to be deployed in real life scenarios. In this
work, we propose a new method that leverages Lipschitz-bounded networks to
precisely and efficiently estimate robust CP sets. When combined with a
1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms
state-of-the-art results in both the size of the robust CP sets and
computational efficiency in medium and large-scale scenarios such as ImageNet.
Taking a different angle, we also study vanilla CP under attack, and derive new
worst-case coverage bounds of vanilla CP sets, which are valid simultaneously
for all adversarial attack levels. Our lip-rcp method makes this second
approach as efficient as vanilla CP while also allowing robustness guarantees.

</details>


### [135] [Event Classification of Accelerometer Data for Industrial Package Monitoring with Embedded Deep Learning](https://arxiv.org/abs/2506.05435)
*Manon Renault,Hamoud Younes,Hugo Tessier,Ronan Le Roy,Bastien Pasdeloup,Mathieu Léonardon*

Main category: cs.LG

TL;DR: 提出一种基于嵌入式系统的包装状态监测方法，通过深度学习模型分类加速度计数据，优化设备寿命和模型大小。


<details>
  <summary>Details</summary>
Motivation: 工业应用中包装监测对效率和可持续性至关重要，需设计长寿命系统以匹配可重复使用包装的生命周期。

Method: 使用一维卷积神经网络处理不平衡多类时间序列数据，测试两种数据增强技术（SMOTE和ADASYN），并应用模型压缩技术。

Result: 模型在两分类问题中精度达94.54%和95.83%，模型大小减少四倍，推理功耗为316 mW。

Conclusion: 方法有效解决了包装状态监测问题，同时优化了设备寿命和模型性能。

Abstract: Package monitoring is an important topic in industrial applications, with
significant implications for operational efficiency and ecological
sustainability. In this study, we propose an approach that employs an embedded
system, placed on reusable packages, to detect their state (on a Forklift, in a
Truck, or in an undetermined location). We aim to design a system with a
lifespan of several years, corresponding to the lifespan of reusable packages.
Our analysis demonstrates that maximizing device lifespan requires minimizing
wake time. We propose a pipeline that includes data processing, training, and
evaluation of the deep learning model designed for imbalanced, multiclass time
series data collected from an embedded sensor. The method uses a
one-dimensional Convolutional Neural Network architecture to classify
accelerometer data from the IoT device. Before training, two data augmentation
techniques are tested to solve the imbalance problem of the dataset: the
Synthetic Minority Oversampling TEchnique and the ADAptive SYNthetic sampling
approach. After training, compression techniques are implemented to have a
small model size. On the considered twoclass problem, the methodology yields a
precision of 94.54% for the first class and 95.83% for the second class, while
compression techniques reduce the model size by a factor of four. The trained
model is deployed on the IoT device, where it operates with a power consumption
of 316 mW during inference.

</details>


### [136] [An Unsupervised Framework for Dynamic Health Indicator Construction and Its Application in Rolling Bearing Prognostics](https://arxiv.org/abs/2506.05438)
*Tongda Sun,Chen Yin,Huailiang Zheng,Yining Dong*

Main category: cs.LG

TL;DR: 提出了一种新的动态健康指标（HI）构建方法，通过无监督框架自动提取退化特征并建模时序依赖关系，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有健康指标构建方法依赖专家知识且忽略动态信息，限制了退化趋势表征和预测能力。

Method: 采用基于跳跃连接的自编码器提取退化特征，并嵌入HI预测块构建动态HI，建模时序依赖。

Result: 在两个轴承生命周期数据集上验证，所提方法优于对比方法，动态HI在预测任务中表现更优。

Conclusion: 动态HI能有效捕捉退化过程的动态内容，提升退化趋势建模和预测能力。

Abstract: Health indicator (HI) plays a key role in degradation assessment and
prognostics of rolling bearings. Although various HI construction methods have
been investigated, most of them rely on expert knowledge for feature extraction
and overlook capturing dynamic information hidden in sequential degradation
processes, which limits the ability of the constructed HI for degradation trend
representation and prognostics. To address these concerns, a novel dynamic HI
that considers HI-level temporal dependence is constructed through an
unsupervised framework. Specifically, a degradation feature learning module
composed of a skip-connection-based autoencoder first maps raw signals to a
representative degradation feature space (DFS) to automatically extract
essential degradation features without the need for expert knowledge.
Subsequently, in this DFS, a new HI-generating module embedded with an inner
HI-prediction block is proposed for dynamic HI construction, where the temporal
dependence between past and current HI states is guaranteed and modeled
explicitly. On this basis, the dynamic HI captures the inherent dynamic
contents of the degradation process, ensuring its effectiveness for degradation
tendency modeling and future degradation prognostics. The experiment results on
two bearing lifecycle datasets demonstrate that the proposed HI construction
method outperforms comparison methods, and the constructed dynamic HI is
superior for prognostic tasks.

</details>


### [137] [UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss](https://arxiv.org/abs/2506.05443)
*Yiyu Lin,Yan Wang,You Zhou,Xinye Ni,Jiahui Wu,Sen Yang*

Main category: cs.LG

TL;DR: UniPTMs是一个统一的多类型蛋白质翻译后修饰（PTM）预测框架，通过创新的双路径协作架构和动态特征融合，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在跨模态特征融合、领域泛化和架构优化方面存在局限性，需要一种更高效的PTM预测方法。

Method: 提出UniPTMs框架，采用主从双路径协作架构，结合BGCA模块、LDFN网络、MACP金字塔和BHGFN网络，并引入HDWF机制和对比损失函数。

Result: 在五种修饰类型上，UniPTMs的性能显著优于现有模型（MCC提升3.2%-11.4%，AP提升4.2%-14.3%）。

Conclusion: UniPTMs不仅突破了单类型预测范式，还通过轻量级变体UniPTMs-mini平衡了模型复杂性与性能。

Abstract: As a core mechanism of epigenetic regulation in eukaryotes, protein
post-translational modifications (PTMs) require precise prediction to decipher
dynamic life activity networks. To address the limitations of existing deep
learning models in cross-modal feature fusion, domain generalization, and
architectural optimization, this study proposes UniPTMs: the first unified
framework for multi-type PTM prediction. The framework innovatively establishes
a "Master-Slave" dual-path collaborative architecture: The master path
dynamically integrates high-dimensional representations of protein sequences,
structures, and evolutionary information through a Bidirectional Gated
Cross-Attention (BGCA) module, while the slave path optimizes feature
discrepancies and recalibration between structural and traditional features
using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale
Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and
a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level
feature integration across paths, the framework employs a Hierarchical Dynamic
Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal
features. Enhanced by a novel Hierarchical Contrastive loss function for
feature consistency optimization, UniPTMs demonstrates significant performance
improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art
models across five modification types and transcends the Single-Type Prediction
Paradigm. To strike a balance between model complexity and performance, we have
also developed a lightweight variant named UniPTMs-mini.

</details>


### [138] [Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic](https://arxiv.org/abs/2506.05445)
*Thanh Vinh Vo,Young Lee,Haozhe Ma,Chien Lu,Tze-Yun Leong*

Main category: cs.LG

TL;DR: DoSAC是一种基于因果干预的强化学习算法，通过后门调整解决隐藏混杂因素问题，提升策略学习的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 隐藏混杂因素会影响状态和动作，导致强化学习策略学习出现偏差，传统RL算法忽略此问题。

Method: 提出DoSAC算法，结合后门调整和软演员-评论家框架，通过可学习的后门重构器推断伪过去变量，实现因果干预。

Result: 在连续控制基准测试中，DoSAC在混杂环境下表现优于基线方法，具有更好的鲁棒性、泛化能力和策略可靠性。

Conclusion: DoSAC通过因果干预有效解决了隐藏混杂因素问题，为强化学习提供了一种更可靠的策略学习方法。

Abstract: Hidden confounders that influence both states and actions can bias policy
learning in reinforcement learning (RL), leading to suboptimal or
non-generalizable behavior. Most RL algorithms ignore this issue, learning
policies from observational trajectories based solely on statistical
associations rather than causal effects. We propose DoSAC (Do-Calculus Soft
Actor-Critic with Backdoor Adjustment), a principled extension of the SAC
algorithm that corrects for hidden confounding via causal intervention
estimation. DoSAC estimates the interventional policy $\pi(a | \mathrm{do}(s))$
using the backdoor criterion, without requiring access to true confounders or
causal labels. To achieve this, we introduce a learnable Backdoor Reconstructor
that infers pseudo-past variables (previous state and action) from the current
state to enable backdoor adjustment from observational data. This module is
integrated into a soft actor-critic framework to compute both the
interventional policy and its entropy. Empirical results on continuous control
benchmarks show that DoSAC outperforms baselines under confounded settings,
with improved robustness, generalization, and policy reliability.

</details>


### [139] [Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning](https://arxiv.org/abs/2506.05447)
*Andrei Mircea,Supriyo Chakraborty,Nima Chitsazan,Irina Rish,Ekaterina Lobacheva*

Main category: cs.LG

TL;DR: 研究发现语言模型在训练早期会出现损失减速现象，模型规模的扩大可以缓解这一现象。


<details>
  <summary>Details</summary>
Motivation: 理解模型规模如何通过训练动态影响语言模型的性能。

Method: 分析损失曲线的对数-对数空间行为，提出零和学习（ZSL）概念解释损失减速。

Result: 模型规模扩大能降低损失减速发生的损失值，并改善减速后的损失改进速率。

Conclusion: 损失减速和ZSL为语言模型规模定律提供了新见解，可能直接优化模型性能。

Abstract: This work aims to understand how scaling improves language models,
specifically in terms of training dynamics. We find that language models
undergo loss deceleration early in training; an abrupt slowdown in the rate of
loss improvement, resulting in piecewise linear behaviour of the loss curve in
log-log space. Scaling up the model mitigates this transition by (1) decreasing
the loss at which deceleration occurs, and (2) improving the log-log rate of
loss improvement after deceleration. We attribute loss deceleration to a type
of degenerate training dynamics we term zero-sum learning (ZSL). In ZSL,
per-example gradients become systematically opposed, leading to destructive
interference in per-example changes in loss. As a result, improving loss on one
subset of examples degrades it on another, bottlenecking overall progress. Loss
deceleration and ZSL provide new insights into the training dynamics underlying
language model scaling laws, and could potentially be targeted directly to
improve language models independent of scale. We make our code and artefacts
available at: https://github.com/mirandrom/zsl

</details>


### [140] [Zeroth-Order Optimization Finds Flat Minima](https://arxiv.org/abs/2506.05454)
*Liang Zhang,Bingcong Li,Kiran Koshy Thekumparampil,Sewoong Oh,Michael Muehlebach,Niao He*

Main category: cs.LG

TL;DR: 本文研究了零阶优化方法在机器学习中的隐式正则化特性，发现标准两点估计器倾向于选择Hessian矩阵迹较小的解（即平坦最小值），并提供了收敛速率的理论分析。


<details>
  <summary>Details</summary>
Motivation: 零阶方法在梯度难以计算或不可行的情况下广泛应用，但现有理论主要关注收敛性，对隐式正则化的研究较少。本文旨在填补这一空白。

Method: 使用标准两点估计器的零阶优化方法，分析其对Hessian矩阵迹的影响，并定义平坦最小值为Hessian迹最小的最优解。

Result: 理论证明零阶优化倾向于选择平坦最小值，并在凸且充分光滑的函数上提供了收敛速率。实验验证了理论结果。

Conclusion: 零阶优化方法隐式偏好平坦最小值，为实际应用提供了理论支持。

Abstract: Zeroth-order methods are extensively used in machine learning applications
where gradients are infeasible or expensive to compute, such as black-box
attacks, reinforcement learning, and language model fine-tuning. Existing
optimization theory focuses on convergence to an arbitrary stationary point,
but less is known on the implicit regularization that provides a fine-grained
characterization on which particular solutions are finally reached. We show
that zeroth-order optimization with the standard two-point estimator favors
solutions with small trace of Hessian, which is widely used in previous work to
distinguish between sharp and flat minima. We further provide convergence rates
of zeroth-order optimization to approximate flat minima for convex and
sufficiently smooth functions, where flat minima are defined as the minimizers
that achieve the smallest trace of Hessian among all optimal solutions.
Experiments on binary classification tasks with convex losses and language
model fine-tuning support our theoretical findings.

</details>


### [141] [Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors](https://arxiv.org/abs/2506.05479)
*Matei Gabriel Coşa,Marek Eliáš*

Main category: cs.LG

TL;DR: 本文研究了在Metrical Task Systems（MTS）中，给定多个启发式方法时，如何在在线处理输入实例时仅查询一个启发式方法，以实现与最佳启发式方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决在MTS中，由于启发式方法的成本无法直接估计，导致难以选择最佳启发式方法的问题。

Method: 提出了一种方法，通过限制每步仅查询一个启发式方法，并利用Bandit Learning对抗内存受限的对手。

Result: 实现了$O(\text{OPT}^{2/3})$的遗憾值，并基于Dekel等人的构造证明了紧下界。

Conclusion: 该方法在MTS中有效实现了与最佳启发式方法相当的性能，并提供了理论保证。

Abstract: We consider the following problem: We are given $\ell$ heuristics for
Metrical Task Systems (MTS), where each might be tailored to a different type
of input instances. While processing an input instance received online, we are
allowed to query the action of only one of the heuristics at each time step.
Our goal is to achieve performance comparable to the best of the given
heuristics. The main difficulty of our setting comes from the fact that the
cost paid by a heuristic at time $t$ cannot be estimated unless the same
heuristic was also queried at time $t-1$. This is related to Bandit Learning
against memory bounded adversaries (Arora et al., 2012). We show how to achieve
regret of $O(\text{OPT}^{2/3})$ and prove a tight lower bound based on the
construction of Dekel et al. (2013).

</details>


### [142] [Initial Model Incorporation for Deep Learning FWI: Pretraining or Denormalization?](https://arxiv.org/abs/2506.05484)
*Ruihua Chen,Bangyu Wu,Meng Li,Kai Yang*

Main category: cs.LG

TL;DR: 论文比较了两种将初始模型知识嵌入神经网络的方法（预训练和去归一化）对全波形反演的影响，发现去归一化方法更优。


<details>
  <summary>Details</summary>
Motivation: 研究如何更有效地将初始模型知识嵌入神经网络，以提升全波形反演的稳定性和准确性。

Method: 通过预训练和去归一化两种方式嵌入初始模型知识，并比较其对神经网络参数化的全波形反演的影响。

Result: 去归一化方法简化了工作流程，加速了收敛，并提高了反演精度，优于预训练方法。

Conclusion: 去归一化是一种更优的初始模型嵌入方法，适用于神经网络参数化的全波形反演。

Abstract: Subsurface property neural network reparameterized full waveform inversion
(FWI) has emerged as an effective unsupervised learning framework, which can
invert stably with an inaccurate starting model. It updates the trainable
neural network parameters instead of fine-tuning on the subsurface model
directly. There are primarily two ways to embed the prior knowledge of the
initial model into neural networks, that is, pretraining and denormalization.
Pretraining first regulates the neural networks' parameters by fitting the
initial velocity model; Denormalization directly adds the outputs of the
network into the initial models without pretraining. In this letter, we
systematically investigate the influence of the two ways of initial model
incorporation for the neural network reparameterized FWI. We demonstrate that
pretraining requires inverting the model perturbation based on a constant
velocity value (mean) with a two-stage implementation. It leads to a complex
workflow and inconsistency of objective functions in the two-stage process,
causing the network parameters to become inactive and lose plasticity.
Experimental results demonstrate that denormalization can simplify workflows,
accelerate convergence, and enhance inversion accuracy compared with
pretraining.

</details>


### [143] [Conformal Prediction Beyond the Seen: A Missing Mass Perspective for Uncertainty Quantification in Generative Models](https://arxiv.org/abs/2506.05497)
*Sima Noorani,Shayan Kiyani,George Pappas,Hamed Hassani*

Main category: cs.LG

TL;DR: 该论文提出了一种名为CPQ的框架，用于在仅查询黑盒生成模型的情况下进行不确定性量化，解决了覆盖率、查询预算和信息量之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型（如大语言模型）在高风险应用中需要可靠的不确定性量化方法，而传统方法依赖于结构化输出，无法直接适用于生成式模型。

Method: 基于统计中的缺失质量问题，提出了两种核心原则：最优查询策略和最优映射方法，并开发了新的估计器。

Result: 实验表明，CPQ能够显著提高预测集的信息量，适用于任何黑盒大语言模型。

Conclusion: CPQ为生成式模型的不确定性量化提供了一种有效且通用的解决方案。

Abstract: Uncertainty quantification (UQ) is essential for safe deployment of
generative AI models such as large language models (LLMs), especially in high
stakes applications. Conformal prediction (CP) offers a principled uncertainty
quantification framework, but classical methods focus on regression and
classification, relying on geometric distances or softmax scores: tools that
presuppose structured outputs. We depart from this paradigm by studying CP in a
query only setting, where prediction sets must be constructed solely from
finite queries to a black box generative model, introducing a new trade off
between coverage, test time query budget, and informativeness. We introduce
Conformal Prediction with Query Oracle (CPQ), a framework characterizing the
optimal interplay between these objectives. Our finite sample algorithm is
built on two core principles: one governs the optimal query policy, and the
other defines the optimal mapping from queried samples to prediction sets.
Remarkably, both are rooted in the classical missing mass problem in
statistics. Specifically, the optimal query policy depends on the rate of
decay, or the derivative, of the missing mass, for which we develop a novel
estimator. Meanwhile, the optimal mapping hinges on the missing mass itself,
which we estimate using Good Turing estimators. We then turn our focus to
implementing our method for language models, where outputs are vast, variable,
and often under specified. Fine grained experiments on three real world open
ended tasks and two LLMs, show CPQ applicability to any black box LLM and
highlight: (1) individual contribution of each principle to CPQ performance,
and (2) CPQ ability to yield significantly more informative prediction sets
than existing conformal methods for language uncertainty quantification.

</details>


### [144] [The Generative Leap: Sharp Sample Complexity for Efficiently Learning Gaussian Multi-Index Models](https://arxiv.org/abs/2506.05500)
*Alex Damian,Jason D. Lee,Joan Bruna*

Main category: cs.LG

TL;DR: 论文研究了高斯多指数模型的高效无偏估计方法，提出了生成跳跃指数k*，并证明了样本复杂度n=Θ(d^(1∨k/2))的必要性和充分性。


<details>
  <summary>Details</summary>
Motivation: 研究高斯多指数模型中隐藏子空间的高效无偏估计方法，扩展了生成指数的概念。

Method: 提出生成跳跃指数k*，并通过基于Hermite张量的谱U统计量设计了一种无偏序贯估计方法。

Result: 证明了样本复杂度n=Θ(d^(1∨k/2))的必要性和充分性，并计算了生成跳跃指数在多个例子中的值。

Conclusion: 该方法适用于高斯多指数模型，为深度神经网络等提供了理论支持。

Abstract: In this work we consider generic Gaussian Multi-index models, in which the
labels only depend on the (Gaussian) $d$-dimensional inputs through their
projection onto a low-dimensional $r = O_d(1)$ subspace, and we study efficient
agnostic estimation procedures for this hidden subspace. We introduce the
\emph{generative leap} exponent $k^\star$, a natural extension of the
generative exponent from [Damian et al.'24] to the multi-index setting. We
first show that a sample complexity of $n=\Theta(d^{1 \vee \k/2})$ is necessary
in the class of algorithms captured by the Low-Degree-Polynomial framework. We
then establish that this sample complexity is also sufficient, by giving an
agnostic sequential estimation procedure (that is, requiring no prior knowledge
of the multi-index model) based on a spectral U-statistic over appropriate
Hermite tensors. We further compute the generative leap exponent for several
examples including piecewise linear functions (deep ReLU networks with bias),
and general deep neural networks (with $r$-dimensional first hidden layer).

</details>


### [145] [Geometric and Physical Constraints Synergistically Enhance Neural PDE Surrogates](https://arxiv.org/abs/2506.05513)
*Yunfei Huang,David S. Greenberg*

Main category: cs.LG

TL;DR: 论文提出了一种新的输入和输出层设计，用于在交错网格上保持物理定律和对称性，显著提升了PDE代理模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在交错网格上难以施加物理和对称约束，导致PDE代理模型在新初始条件和长时间预测中表现不佳。

Method: 设计了新颖的输入和输出层，确保在交错网格上满足物理定律和对称性，并系统研究了这些约束对PDE代理模型的影响。

Result: 对称性和物理约束显著提升了模型性能，尤其在初始条件和长时间预测的泛化能力上表现突出。

Conclusion: 结合对称性和物理约束的代理模型表现最佳，优于数据增强和推前训练等基线方法，且在真实世界数据中更具预测准确性。

Abstract: Neural PDE surrogates can improve the cost-accuracy tradeoff of classical
solvers, but often generalize poorly to new initial conditions and accumulate
errors over time. Physical and symmetry constraints have shown promise in
closing this performance gap, but existing techniques for imposing these
inductive biases are incompatible with the staggered grids commonly used in
computational fluid dynamics. Here we introduce novel input and output layers
that respect physical laws and symmetries on the staggered grids, and for the
first time systematically investigate how these constraints, individually and
in combination, affect the accuracy of PDE surrogates. We focus on two
challenging problems: shallow water equations with closed boundaries and
decaying incompressible turbulence. Compared to strong baselines, symmetries
and physical constraints consistently improve performance across tasks,
architectures, autoregressive prediction steps, accuracy measures, and network
sizes. Symmetries are more effective than physical constraints, but surrogates
with both performed best, even compared to baselines with data augmentation or
pushforward training, while themselves benefiting from the pushforward trick.
Doubly-constrained surrogates also generalize better to initial conditions and
durations beyond the range of the training data, and more accurately predict
real-world ocean currents.

</details>


### [146] [Winner-takes-all for Multivariate Probabilistic Time Series Forecasting](https://arxiv.org/abs/2506.05515)
*Adrien Cortés,Rémi Rehm,Victor Letzelter*

Main category: cs.LG

TL;DR: TimeMCL是一种基于多选择学习（MCL）范式的方法，用于预测时间序列的多种可能未来。它通过多头部神经网络和Winner-Takes-All（WTA）损失函数提升预测多样性，并在计算成本较低的情况下表现出色。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测常面临模糊性和不确定性，传统方法难以捕捉多种可能的未来。MCL因其简单性和处理模糊任务的能力受到关注，因此作者将其应用于时间序列预测。

Method: 采用多头部神经网络和WTA损失函数，通过MCL框架预测多样化的时间序列未来，并隐含地实现量化目标。

Result: 在合成数据和真实时间序列上的实验表明，TimeMCL能以较低计算成本实现多样且准确的预测。

Conclusion: TimeMCL是一种高效且多样化的时间序列预测方法，适用于模糊性任务，具有实际应用潜力。

Abstract: We introduce TimeMCL, a method leveraging the Multiple Choice Learning (MCL)
paradigm to forecast multiple plausible time series futures. Our approach
employs a neural network with multiple heads and utilizes the Winner-Takes-All
(WTA) loss to promote diversity among predictions. MCL has recently gained
attention due to its simplicity and ability to address ill-posed and ambiguous
tasks. We propose an adaptation of this framework for time-series forecasting,
presenting it as an efficient method to predict diverse futures, which we
relate to its implicit quantization objective. We provide insights into our
approach using synthetic data and evaluate it on real-world time series,
demonstrating its promising performance at a light computational cost.

</details>


### [147] [On Fitting Flow Models with Large Sinkhorn Couplings](https://arxiv.org/abs/2506.05526)
*Michal Klein,Alireza Mousavi-Hosseini,Stephen Zhang,Marco Cuturi*

Main category: cs.LG

TL;DR: 论文探讨了通过增加批量大小和优化Sinkhorn算法的熵正则化参数，提升流模型在数据生成任务中的效率。


<details>
  <summary>Details</summary>
Motivation: 研究如何在没有源-目标点配对的情况下，通过优化传输（OT）方法训练流模型，以提高训练效率和推理速度。

Method: 采用大规模批量（n增加3-4个数量级）和Sinkhorn算法（低熵正则化参数ε）优化流模型的训练。

Result: 实验表明，在合成和图像生成任务中，使用大规模Sinkhorn耦合和低熵正则化显著提升了流模型的性能。

Conclusion: 通过优化批量大小和熵正则化参数，流模型在数据生成任务中表现更高效。

Abstract: Flow models transform data gradually from one modality (e.g. noise) onto
another (e.g. images). Such models are parameterized by a time-dependent
velocity field, trained to fit segments connecting pairs of source and target
points. When the pairing between source and target points is given, training
flow models boils down to a supervised regression problem. When no such pairing
exists, as is the case when generating data from noise, training flows is much
harder. A popular approach lies in picking source and target points
independently. This can, however, lead to velocity fields that are slow to
train, but also costly to integrate at inference time. In theory, one would
greatly benefit from training flow models by sampling pairs from an optimal
transport (OT) measure coupling source and target, since this would lead to a
highly efficient flow solving the Benamou and Brenier dynamical OT problem. In
practice, recent works have proposed to sample mini-batches of $n$ source and
$n$ target points and reorder them using an OT solver to form better pairs.
These works have advocated using batches of size $n\approx 256$, and considered
OT solvers that return couplings that are either sharp (using e.g. the
Hungarian algorithm) or blurred (using e.g. entropic regularization, a.k.a.
Sinkhorn). We follow in the footsteps of these works by exploring the benefits
of increasing $n$ by three to four orders of magnitude, and look more carefully
on the effect of the entropic regularization $\varepsilon$ used in the Sinkhorn
algorithm. Our analysis is facilitated by new scale invariant quantities to
report the sharpness of a coupling, while our sharded computations across
multiple GPU or GPU nodes allow scaling up $n$. We show that in both synthetic
and image generation tasks, flow models greatly benefit when fitted with large
Sinkhorn couplings, with a low entropic regularization $\varepsilon$.

</details>


### [148] [Spectral Graph Neural Networks are Incomplete on Graphs with a Simple Spectrum](https://arxiv.org/abs/2506.05530)
*Snir Hordan,Maya Bechler-Speicher,Gur Lifshitz,Nadav Dym*

Main category: cs.LG

TL;DR: 论文提出了一种基于图谱的SGNN表达能力评估方法，并证明了现有SGNN在简单谱图上的局限性，同时提出了一种改进方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估SGNN表达能力的框架（如k-WL和同态计数）与图谱关联性差，无法深入理解SGNN的表达能力。

Method: 利用图的最大特征值多重性分类范式，提出SGNN的表达能力层次结构，并基于旋转等变神经网络提出改进方法。

Result: 理论证明许多SGNN在特征值不同的图上仍不完全，改进方法在简单谱图上显著提升了表达能力。

Conclusion: 通过实验验证了理论结果，为SGNN的表达能力提供了新的评估和改进方向。

Abstract: Spectral features are widely incorporated within Graph Neural Networks (GNNs)
to improve their expressive power, or their ability to distinguish among
non-isomorphic graphs. One popular example is the usage of graph Laplacian
eigenvectors for positional encoding in MPNNs and Graph Transformers. The
expressive power of such Spectrally-enhanced GNNs (SGNNs) is usually evaluated
via the k-WL graph isomorphism test hierarchy and homomorphism counting. Yet,
these frameworks align poorly with the graph spectra, yielding limited insight
into SGNNs' expressive power. We leverage a well-studied paradigm of
classifying graphs by their largest eigenvalue multiplicity to introduce an
expressivity hierarchy for SGNNs. We then prove that many SGNNs are incomplete
even on graphs with distinct eigenvalues. To mitigate this deficiency, we adapt
rotation equivariant neural networks to the graph spectra setting to propose a
method to provably improve SGNNs' expressivity on simple spectrum graphs. We
empirically verify our theoretical claims via an image classification
experiment on the MNIST Superpixel dataset and eigenvector canonicalization on
graphs from ZINC.

</details>


### [149] [SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful Deepfake Content on Social Media Platforms](https://arxiv.org/abs/2506.05538)
*Arnesh Batra,Anushk Kumar,Jashn Khemani,Arush Gumber,Arhan Jain,Somil Gupta*

Main category: cs.LG

TL;DR: 论文提出SocialDF数据集和基于LLM的多因素检测方法，以应对社交媒体上深度伪造的挑战。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的快速发展带来了安全挑战，现有检测方法难以区分良性和恶意生成的深度伪造内容。

Method: 提出SocialDF数据集，并设计基于LLM的多因素检测方法，结合面部识别、语音转录和多代理LLM管道进行交叉验证。

Result: 方法通过多模态验证技术（语言、行为和上下文分析）有效区分合成媒体与真实内容。

Conclusion: SocialDF和LLM多因素检测方法为解决社交媒体上的深度伪造问题提供了有效工具。

Abstract: The rapid advancement of deep generative models has significantly improved
the realism of synthetic media, presenting both opportunities and security
challenges. While deepfake technology has valuable applications in
entertainment and accessibility, it has emerged as a potent vector for
misinformation campaigns, particularly on social media. Existing detection
frameworks struggle to distinguish between benign and adversarially generated
deepfakes engineered to manipulate public perception. To address this
challenge, we introduce SocialDF, a curated dataset reflecting real-world
deepfake challenges on social media platforms. This dataset encompasses
high-fidelity deepfakes sourced from various online ecosystems, ensuring broad
coverage of manipulative techniques. We propose a novel LLM-based multi-factor
detection approach that combines facial recognition, automated speech
transcription, and a multi-agent LLM pipeline to cross-verify audio-visual
cues. Our methodology emphasizes robust, multi-modal verification techniques
that incorporate linguistic, behavioral, and contextual analysis to effectively
discern synthetic media from authentic content.

</details>


### [150] [Agentomics-ML: Autonomous Machine Learning Experimentation Agent for Genomic and Transcriptomic Data](https://arxiv.org/abs/2506.05542)
*Vlastimil Martinek,Andrea Gariboldi,Dimosthenis Tzimotoudis,Aitor Alberdi Escudero,Edward Blake,David Cechak,Luke Cassar,Alessandro Balestrucci,Panagiotis Alexiou*

Main category: cs.LG

TL;DR: Agentomics-ML是一个全自主的基于代理的系统，用于生成分类模型和可重复训练与推理的文件，在计算生物学数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生物数据集的复杂性要求自动化方法生成通用预测模型，现有方法在异构数据集上泛化能力不足。

Method: 系统通过Bash与文件系统交互，遵循ML实验步骤，通过反馈调整数据表示、模型架构和超参数。

Result: 在多个基准数据集上，Agentomics-ML在泛化能力和成功率上优于现有方法，并在一个数据集上达到最优性能。

Conclusion: Agentomics-ML缩小了全自主系统与专家构建模型的差距，为计算生物学提供了高效工具。

Abstract: The adoption of machine learning (ML) and deep learning methods has
revolutionized molecular medicine by driving breakthroughs in genomics,
transcriptomics, drug discovery, and biological systems modeling. The
increasing quantity, multimodality, and heterogeneity of biological datasets
demand automated methods that can produce generalizable predictive models.
Recent developments in large language model-based agents have shown promise for
automating end-to-end ML experimentation on structured benchmarks. However,
when applied to heterogeneous computational biology datasets, these methods
struggle with generalization and success rates. Here, we introduce
Agentomics-ML, a fully autonomous agent-based system designed to produce a
classification model and the necessary files for reproducible training and
inference. Our method follows predefined steps of an ML experimentation
process, repeatedly interacting with the file system through Bash to complete
individual steps. Once an ML model is produced, training and validation metrics
provide scalar feedback to a reflection step to identify issues such as
overfitting. This step then creates verbal feedback for future iterations,
suggesting adjustments to steps such as data representation, model
architecture, and hyperparameter choices. We have evaluated Agentomics-ML on
several established genomic and transcriptomic benchmark datasets and show that
it outperforms existing state-of-the-art agent-based methods in both
generalization and success rates. While state-of-the-art models built by domain
experts still lead in absolute performance on the majority of the computational
biology datasets used in this work, Agentomics-ML narrows the gap for fully
autonomous systems and achieves state-of-the-art performance on one of the used
benchmark datasets. The code is available at
https://github.com/BioGeMT/Agentomics-ML.

</details>


### [151] [Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning](https://arxiv.org/abs/2506.05568)
*Arian Raje,Baris Askin,Divyansh Jhunjhunwala,Gauri Joshi*

Main category: cs.LG

TL;DR: 提出了Ravan方法，通过多头LoRA自适应调整参数，解决联邦学习中数据异构性问题，提升LLMs的微调效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有LoRA方法在联邦学习中因数据和计算异构性导致的精度下降问题。

Method: 提出Ravan方法，将权重更新重新参数化为多个LoRA头的加权和，仅训练核心矩阵和轻量级缩放因子。

Result: 在视觉和语言基准测试中，Ravan比现有参数高效方法提升了2-8%的测试准确率。

Conclusion: Ravan是一种鲁棒且可扩展的联邦学习LLMs微调解决方案。

Abstract: Large language models (LLMs) have not yet effectively leveraged the vast
amounts of edge-device data, and federated learning (FL) offers a promising
paradigm to collaboratively fine-tune LLMs without transferring private edge
data to the cloud. To operate within the computation and communication
constraints of edge devices, recent literature on federated fine-tuning of LLMs
proposes the use of low-rank adaptation (LoRA) and similar parameter-efficient
methods. However, LoRA-based methods suffer from accuracy degradation in FL
settings, primarily because of data and computational heterogeneity across
clients. We propose \textsc{Ravan}, an adaptive multi-head LoRA method that
balances parameter efficiency and model expressivity by reparameterizing the
weight updates as the sum of multiple LoRA heads
$s_i\textbf{B}_i\textbf{H}_i\textbf{A}_i$ in which only the core matrices
$\textbf{H}_i$ and their lightweight scaling factors $s_i$ are trained. These
trainable scaling factors let the optimization focus on the most useful heads,
recovering a higher-rank approximation of the full update without increasing
the number of communicated parameters since clients upload $s_i\textbf{H}_i$
directly. Experiments on vision and language benchmarks show that
\textsc{Ravan} improves test accuracy by 2-8\% over prior parameter-efficient
baselines, making it a robust and scalable solution for federated fine-tuning
of LLMs.

</details>


### [152] [When can in-context learning generalize out of task distribution?](https://arxiv.org/abs/2506.05574)
*Chase Goddard,Lindsay M. Smith,Vudtiwat Ngampruetikorn,David J. Schwab*

Main category: cs.LG

TL;DR: 论文研究了预训练分布对上下文学习（ICL）能力的影响，发现任务多样性增加时，模型会从仅适用于预训练任务的解决方案过渡到泛化到整个任务空间的解决方案。


<details>
  <summary>Details</summary>
Motivation: 探索预训练分布中任务多样性对ICL能力的影响，以及模型在不同任务多样性下的表现。

Method: 使用线性函数训练的变换器，研究任务多样性对ICL能力的影响，并构建相图分析模型行为。

Result: 任务多样性增加时，模型从仅适用于预训练任务的解决方案过渡到泛化到整个任务空间的解决方案。

Conclusion: 任务多样性是ICL能力泛化的关键因素，模型深度和问题维度也会影响这一过渡。

Abstract: In-context learning (ICL) is a remarkable capability of pretrained
transformers that allows models to generalize to unseen tasks after seeing only
a few examples. We investigate empirically the conditions necessary on the
pretraining distribution for ICL to emerge and generalize
\emph{out-of-distribution}. Previous work has focused on the number of distinct
tasks necessary in the pretraining dataset. Here, we use a different notion of
task diversity to study the emergence of ICL in transformers trained on linear
functions. We find that as task diversity increases, transformers undergo a
transition from a specialized solution, which exhibits ICL only within the
pretraining task distribution, to a solution which generalizes out of
distribution to the entire task space. We also investigate the nature of the
solutions learned by the transformer on both sides of the transition, and
observe similar transitions in nonlinear regression problems. We construct a
phase diagram to characterize how our concept of task diversity interacts with
the number of pretraining tasks. In addition, we explore how factors such as
the depth of the model and the dimensionality of the regression problem
influence the transition.

</details>


### [153] [Collaborative Learning in Agentic Systems: A Collective AI is Greater Than the Sum of Its Parts](https://arxiv.org/abs/2506.05577)
*Saptarshi Nath,Christos Peridis,Eseoghene Benjamin,Xinran Liu,Soheil Kolouri,Peter Kinnell,Zexin Li,Cong Liu,Shirin Dora,Andrea Soltoggio*

Main category: cs.LG

TL;DR: MOSAIC是一种多智能体算法，通过模块化共享和组合学习，实现无协调、异步的知识共享与重用，提高样本效率并解决孤立学习者无法完成的任务。


<details>
  <summary>Details</summary>
Motivation: 研究如何在去中心化环境中实现智能体的自主学习和协作，以解决带宽限制、异步执行和缺乏集中控制等问题。

Method: 结合模块化策略组合、Wasserstein嵌入的余弦相似性估计和异步通信与策略集成。

Result: MOSAIC在RL基准测试中表现出更高的样本效率，能解决孤立学习者无法完成的任务，并观察到任务理想课程的出现。

Conclusion: 协作学习在智能体系统中能实现个体和集体性能的持续提升。

Abstract: Agentic AI has gained significant interest as a research paradigm focused on
autonomy, self-directed learning, and long-term reliability of decision making.
Real-world agentic systems operate in decentralized settings on a large set of
tasks or data distributions with constraints such as limited bandwidth,
asynchronous execution, and the absence of a centralized model or even common
objectives. We posit that exploiting previously learned skills, task
similarities, and communication capabilities in a collective of agentic AI are
challenging but essential elements to enabling scalability, open-endedness, and
beneficial collaborative learning dynamics. In this paper, we introduce Modular
Sharing and Composition in Collective Learning (MOSAIC), an agentic algorithm
that allows multiple agents to independently solve different tasks while also
identifying, sharing, and reusing useful machine-learned knowledge, without
coordination, synchronization, or centralized control. MOSAIC combines three
mechanisms: (1) modular policy composition via neural network masks, (2) cosine
similarity estimation using Wasserstein embeddings for knowledge selection, and
(3) asynchronous communication and policy integration. Results on a set of RL
benchmarks show that MOSAIC has a greater sample efficiency than isolated
learners, i.e., it learns significantly faster, and in some cases, finds
solutions to tasks that cannot be solved by isolated learners. The
collaborative learning and sharing dynamics are also observed to result in the
emergence of ideal curricula of tasks, from easy to hard. These findings
support the case for collaborative learning in agentic systems to achieve
better and continuously evolving performance both at the individual and
collective levels.

</details>


### [154] [Conformal Prediction Adaptive to Unknown Subpopulation Shifts](https://arxiv.org/abs/2506.05583)
*Nien-Shao Wang,Duygu Nur Yaldiz,Yavuz Faruk Bakman,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 本文提出新方法，解决分布偏移下保形预测的覆盖问题，确保有效覆盖且无需子群结构知识。


<details>
  <summary>Details</summary>
Motivation: 保形预测在分布偏移下覆盖保证失效，尤其是子群偏移时，测试数据与校准数据分布不同。

Method: 提出新算法，适应子群偏移，确保覆盖有效性，适用于高维场景。

Result: 在视觉和语言基准测试中，新方法在标准保形预测失效时仍保持覆盖和风险控制。

Conclusion: 新方法有效解决了子群偏移下的保形预测问题，具有实际应用价值。

Abstract: Conformal prediction is widely used to equip black-box machine learning
models with uncertainty quantification enjoying formal coverage guarantees.
However, these guarantees typically break down in the presence of distribution
shifts, where the data distribution at test time differs from the training (or
calibration-time) distribution. In this work, we address subpopulation shifts,
where the test environment exhibits an unknown and differing mixture of
subpopulations compared to the calibration data. We propose new methods that
provably adapt conformal prediction to such shifts, ensuring valid coverage
without requiring explicit knowledge of subpopulation structure. Our algorithms
scale to high-dimensional settings and perform effectively in realistic machine
learning tasks. Extensive experiments on vision (with vision transformers) and
language (with large language models) benchmarks demonstrate that our methods
reliably maintain coverage and controls risk in scenarios where standard
conformal prediction fails.

</details>


### [155] [TabFlex: Scaling Tabular Learning to Millions with Linear Attention](https://arxiv.org/abs/2506.05584)
*Yuchen Zeng,Tuan Dinh,Wonjun Kang,Andreas C Mueller*

Main category: cs.LG

TL;DR: TabFlex通过引入线性注意力机制提升TabPFN的效率与扩展性，适用于大规模表格数据集，性能优于25种基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如TabPFN在小规模表格数据上表现优异，但难以扩展到大规模复杂数据集。

Method: 采用线性注意力机制替代复杂度二次的自注意力机制，提升模型效率与扩展性。

Result: TabFlex在处理百万级样本时仅需5秒，速度比TabPFN快2倍，比XGBoost快1.5倍。

Conclusion: TabFlex在大规模数据集上高效且性能优异，结合降维和采样技术可进一步降低计算成本。

Abstract: Leveraging the in-context learning (ICL) capability of Large Language Models
(LLMs) for tabular classification has gained significant attention for its
training-free adaptability across diverse datasets. Recent advancements, like
TabPFN, excel in small-scale tabular datasets but struggle to scale for large
and complex datasets. Our work enhances the efficiency and scalability of
TabPFN for larger datasets by incorporating linear attention mechanisms as a
scalable alternative to complexity-quadratic self-attention. Our model,
TabFlex, efficiently handles tabular datasets with thousands of features and
hundreds of classes, scaling seamlessly to millions of samples. For instance,
TabFlex processes the poker-hand dataset with over a million samples in just 5
seconds. Our extensive evaluations demonstrate that TabFlex can achieve over a
2x speedup compared to TabPFN and a 1.5x speedup over XGBoost, outperforming 25
tested baselines in terms of efficiency across a diverse range of datasets.
Furthermore, TabFlex remains highly effective on large-scale datasets,
delivering strong performance with significantly reduced computational costs,
especially when combined with data-efficient techniques such as dimensionality
reduction and data sampling.

</details>


### [156] [CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions](https://arxiv.org/abs/2506.05586)
*Isha Puri,Amit Dhurandhar,Tejaswini Pedapati,Kartikeyan Shanmugam,Dennis Wei,Kush R. Varshney*

Main category: cs.LG

TL;DR: 论文提出了一种新型神经网络架构CoFrNet，受连分数启发，具有高效训练和解释性，并在理论和实验中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 近年来，局部事后解释神经网络的研究较多，但构建可解释神经网络架构的工作较少。本文旨在填补这一空白。

Method: 提出基于连分数的CoFrNet架构，利用其特殊函数形式实现高效训练和解释，并证明其通用逼近能力。

Result: 实验表明，CoFrNet在合成函数和真实数据集上表现优异，接近或超越其他可解释模型及多层感知机。

Conclusion: CoFrNet兼具强大表示能力和可解释性，为构建高效且透明的神经网络提供了新思路。

Abstract: In recent years there has been a considerable amount of research on local
post hoc explanations for neural networks. However, work on building
interpretable neural architectures has been relatively sparse. In this paper,
we present a novel neural architecture, CoFrNet, inspired by the form of
continued fractions which are known to have many attractive properties in
number theory, such as fast convergence of approximations to real numbers. We
show that CoFrNets can be efficiently trained as well as interpreted leveraging
their particular functional form. Moreover, we prove that such architectures
are universal approximators based on a proof strategy that is different than
the typical strategy used to prove universal approximation results for neural
networks based on infinite width (or depth), which is likely to be of
independent interest. We experiment on nonlinear synthetic functions and are
able to accurately model as well as estimate feature attributions and even
higher order terms in some cases, which is a testament to the representational
power as well as interpretability of such architectures. To further showcase
the power of CoFrNets, we experiment on seven real datasets spanning tabular,
text and image modalities, and show that they are either comparable or
significantly better than other interpretable models and multilayer
perceptrons, sometimes approaching the accuracies of state-of-the-art models.

</details>


### [157] [Zero-shot protein stability prediction by inverse folding models: a free energy interpretation](https://arxiv.org/abs/2506.05596)
*Jes Frellsen,Maher M. Kassem,Tone Bengtsen,Lars Olsen,Kresten Lindorff-Larsen,Jesper Ferkinghoff-Borg,Wouter Boomsma*

Main category: cs.LG

TL;DR: 本文探讨了逆折叠模型与蛋白质稳定性之间的自由能基础关系，提出了改进零-shot稳定性预测的方法。


<details>
  <summary>Details</summary>
Motivation: 理解逆折叠模型的氨基酸偏好与热力学稳定性之间的自由能关系，以提升零-shot稳定性预测能力。

Method: 通过理论推导揭示了似然比标准的简化近似，并提出改进相对稳定性估计的方法，随后进行实证评估。

Result: 研究表明，通过简单方法可以显著提升零-shot性能。

Conclusion: 本文为逆折叠模型的自由能基础提供了更清晰的理解，并展示了改进零-shot稳定性预测的潜力。

Abstract: Inverse folding models have proven to be highly effective zero-shot
predictors of protein stability. Despite this success, the link between the
amino acid preferences of an inverse folding model and the free-energy
considerations underlying thermodynamic stability remains incompletely
understood. A better understanding would be of interest not only from a
theoretical perspective, but also potentially provide the basis for stronger
zero-shot stability prediction. In this paper, we take steps to clarify the
free-energy foundations of inverse folding models. Our derivation reveals the
standard practice of likelihood ratios as a simplistic approximation and
suggests several paths towards better estimates of the relative stability. We
empirically assess these approaches and demonstrate that considerable gains in
zero-shot performance can be achieved with fairly simple means.

</details>


### [158] [FaCTR: Factorized Channel-Temporal Representation Transformers for Efficient Time Series Forecasting](https://arxiv.org/abs/2506.05597)
*Yash Vijay,Harini Subramanyan*

Main category: cs.LG

TL;DR: FaCTR是一种轻量级时空Transformer，通过低秩因子分解机和可学习门控机制优化时间序列预测，性能优于现有方法且参数更少。


<details>
  <summary>Details</summary>
Motivation: Transformer在语言和视觉任务中表现优异，但在时间序列预测中因数据信息密度低和复杂依赖关系而效果不佳。

Method: FaCTR通过动态对称跨通道交互和低秩因子分解机设计，结合静态和动态协变量编码，实现高效预测。

Result: 在11个公共基准测试中表现最优，参数规模仅为竞争方法的1/50，且支持可解释性。

Conclusion: FaCTR是一种紧凑且多功能的时间序列预测基础模型，支持自监督预训练。

Abstract: While Transformers excel in language and vision-where inputs are semantically
rich and exhibit univariate dependency structures-their architectural
complexity leads to diminishing returns in time series forecasting. Time series
data is characterized by low per-timestep information density and complex
dependencies across channels and covariates, requiring conditioning on
structured variable interactions. To address this mismatch and
overparameterization, we propose FaCTR, a lightweight spatiotemporal
Transformer with an explicitly structural design. FaCTR injects dynamic,
symmetric cross-channel interactions-modeled via a low-rank Factorization
Machine into temporally contextualized patch embeddings through a learnable
gating mechanism. It further encodes static and dynamic covariates for
multivariate conditioning. Despite its compact design, FaCTR achieves
state-of-the-art performance on eleven public forecasting benchmarks spanning
both short-term and long-term horizons, with its largest variant using close to
only 400K parameters-on average 50x smaller than competitive spatiotemporal
transformer baselines. In addition, its structured design enables
interpretability through cross-channel influence scores-an essential
requirement for real-world decision-making. Finally, FaCTR supports
self-supervised pretraining, positioning it as a compact yet versatile
foundation for downstream time series tasks.

</details>


### [159] [When Maximum Entropy Misleads Policy Optimization](https://arxiv.org/abs/2506.05615)
*Ruipeng Zhang,Ya-Chien Chang,Sicun Gao*

Main category: cs.LG

TL;DR: MaxEnt RL框架在复杂控制任务中面临鲁棒性与最优性的权衡问题，熵最大化可能误导策略优化。


<details>
  <summary>Details</summary>
Motivation: 分析MaxEnt RL在性能关键控制问题中的表现，探究其鲁棒性与最优性的权衡。

Method: 通过多种控制问题的实验，展示熵最大化对策略优化的误导效应。

Result: 实验证明熵最大化可能导致任务失败，尤其是在需要精确、低熵策略的任务中。

Conclusion: 研究为平衡奖励设计与熵最大化提供了更深入的理解。

Abstract: The Maximum Entropy Reinforcement Learning (MaxEnt RL) framework is a leading
approach for achieving efficient learning and robust performance across many RL
tasks. However, MaxEnt methods have also been shown to struggle with
performance-critical control problems in practice, where non-MaxEnt algorithms
can successfully learn. In this work, we analyze how the trade-off between
robustness and optimality affects the performance of MaxEnt algorithms in
complex control tasks: while entropy maximization enhances exploration and
robustness, it can also mislead policy optimization, leading to failure in
tasks that require precise, low-entropy policies. Through experiments on a
variety of control problems, we concretely demonstrate this misleading effect.
Our analysis leads to better understanding of how to balance reward design and
entropy maximization in challenging control problems.

</details>


### [160] [LFA applied to CNNs: Efficient Singular Value Decomposition of Convolutional Mappings by Local Fourier Analysis](https://arxiv.org/abs/2506.05617)
*Antonia van Betteray,Matthias Rottmann,Karsten Kahl*

Main category: cs.LG

TL;DR: 提出了一种基于局部傅里叶分析的低复杂度方法，用于高效计算卷积映射的奇异值。


<details>
  <summary>Details</summary>
Motivation: 卷积映射的奇异值具有重要的谱特性，但传统计算方法资源消耗大，难以处理高维输入和多通道情况。

Method: 利用局部傅里叶分析和卷积算子的平移不变性，提出复杂度为O(N)的算法。

Result: 数值实验验证了方法的效率，能够计算高维卷积映射的全部奇异值及对应奇异向量。

Conclusion: 该方法为高维卷积映射的奇异值计算提供了实用且可扩展的解决方案。

Abstract: The singular values of convolutional mappings encode interesting spectral
properties, which can be used, e.g., to improve generalization and robustness
of convolutional neural networks as well as to facilitate model compression.
However, the computation of singular values is typically very
resource-intensive. The naive approach involves unrolling the convolutional
mapping along the input and channel dimensions into a large and sparse
two-dimensional matrix, making the exact calculation of all singular values
infeasible due to hardware limitations. In particular, this is true for
matrices that represent convolutional mappings with large inputs and a high
number of channels. Existing efficient methods leverage the Fast Fourier
transformation (FFT) to transform convolutional mappings into the frequency
domain, enabling the computation of singular values for matrices representing
convolutions with larger input and channel dimensions. For a constant number of
channels in a given convolution, an FFT can compute N singular values in O(N
log N) complexity. In this work, we propose an approach of complexity O(N)
based on local Fourier analysis, which additionally exploits the shift
invariance of convolutional operators. We provide a theoretical analysis of our
algorithm's runtime and validate its efficiency through numerical experiments.
Our results demonstrate that our proposed method is scalable and offers a
practical solution to calculate the entire set of singular values - along with
the corresponding singular vectors if needed - for high-dimensional
convolutional mappings.

</details>


### [161] [Two-dimensional Taxonomy for N-ary Knowledge Representation Learning Methods](https://arxiv.org/abs/2506.05626)
*Xiaohua Lu,Liubov Tupikina,Mehwish Alam*

Main category: cs.LG

TL;DR: 该综述探讨了知识超图和超关系知识图如何结合知识图和超图的优势，以更好地建模现实世界中的复杂关系和实体角色。


<details>
  <summary>Details</summary>
Motivation: 解决知识图简化高阶关系丢失细节以及超图忽视实体角色的问题。

Method: 提出二维分类法：按方法论（如翻译、张量分解、深度学习等）和实体角色意识（无意识、位置意识、角色意识）分类模型。

Result: 综述了处理n元关系数据的方法，包括现有数据集和负采样策略。

Conclusion: 总结了当前挑战，为未来研究提供方向。

Abstract: Real-world knowledge can take various forms, including structured,
semi-structured, and unstructured data. Among these, knowledge graphs are a
form of structured human knowledge that integrate heterogeneous data sources
into structured representations but typically reduce complex n-ary relations to
simple triples, thereby losing higher-order relational details. In contrast,
hypergraphs naturally represent n-ary relations with hyperedges, which directly
connect multiple entities together. Yet hypergraph representation learning
often overlooks entity roles in hyperedges, limiting the fine-grained semantic
modelling. To address these issues, knowledge hypergraphs and hyper-relational
knowledge graphs combine the advantages of knowledge graphs and hypergraphs to
better capture the complex structures and role-specific semantics of real-world
knowledge. This survey provides a comprehensive review of methods handling
n-ary relational data, covering both knowledge hypergraphs and hyper-relational
knowledge graphs literatures. We propose a two-dimensional taxonomy: the first
dimension categorises models based on their methodology, i.e.,
translation-based models, tensor factorisation-based models, deep neural
network-based models, logic rules-based models, and hyperedge expansion-based
models. The second dimension classifies models according to their awareness of
entity roles and positions in n-ary relations, dividing them into aware-less,
position-aware, and role-aware approaches. Finally, we discuss existing
datasets, negative sampling strategies, and outline open challenges to inspire
future research.

</details>


### [162] [GP-MoLFormer-Sim: Test Time Molecular Optimization through Contextual Similarity Guidance](https://arxiv.org/abs/2506.05628)
*Jiri Navratil,Jarret Ross,Payel Das,Youssef Mroueh,Samuel C Hoffman,Vijil Chenthamarakshan,Brian Belgodere*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的方法（GP-MoLFormer-Sim），利用化学语言模型（CLM）生成分子，并通过分子相似性引导采样策略，结合遗传算法（GA）在分子优化任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在药物发现和化学设计中，设计分子时需保持与目标分子的相似性或特定性质，现有方法效率不足，需一种更高效的无训练方法。

Method: 利用CLM的上下文表示估计分子相似性，调整自回归采样策略，结合遗传算法（GA）优化分子生成。

Result: GP-MoLFormer-Sim+GA在分子优化任务中优于现有无训练基线方法，尤其在黑盒条件下表现突出。

Conclusion: 该方法为理解和引导CLM生成机制提供了新思路，推动了分子设计领域的发展。

Abstract: The ability to design molecules while preserving similarity to a target
molecule and/or property is crucial for various applications in drug discovery,
chemical design, and biology. We introduce in this paper an efficient
training-free method for navigating and sampling from the molecular space with
a generative Chemical Language Model (CLM), while using the molecular
similarity to the target as a guide. Our method leverages the contextual
representations learned from the CLM itself to estimate the molecular
similarity, which is then used to adjust the autoregressive sampling strategy
of the CLM. At each step of the decoding process, the method tracks the
distance of the current generations from the target and updates the logits to
encourage the preservation of similarity in generations. We implement the
method using a recently proposed $\sim$47M parameter SMILES-based CLM,
GP-MoLFormer, and therefore refer to the method as GP-MoLFormer-Sim, which
enables a test-time update of the deep generative policy to reflect the
contextual similarity to a set of guide molecules. The method is further
integrated into a genetic algorithm (GA) and tested on a set of standard
molecular optimization benchmarks involving property optimization, molecular
rediscovery, and structure-based drug design. Results show that,
GP-MoLFormer-Sim, combined with GA (GP-MoLFormer-Sim+GA) outperforms existing
training-free baseline methods, when the oracle remains black-box. The findings
in this work are a step forward in understanding and guiding the generative
mechanisms of CLMs.

</details>


### [163] [List-Level Distribution Coupling with Applications to Speculative Decoding and Lossy Compression](https://arxiv.org/abs/2506.05632)
*Joseph Rowan,Buu Phan,Ashish Khisti*

Main category: cs.LG

TL;DR: 论文提出了一种新的概率分布耦合方法，扩展了Gumbel-max采样，并建立了接受概率的下界（列表匹配引理）。该方法应用于多草稿推测采样和分布式有损压缩，表现出竞争性能。


<details>
  <summary>Details</summary>
Motivation: 研究概率分布耦合问题的松弛形式，提出更高效的采样方法，并探索其在实际任务中的应用。

Method: 扩展Gumbel-max采样，提出新方法生成样本，并建立接受概率下界（列表匹配引理）。应用于多草稿推测采样和分布式有损压缩。

Result: 在多草稿推测采样中表现与基线方法竞争，且支持草稿不变性；在分布式有损压缩中，对合成高斯源和MNIST数据集有显著增益。

Conclusion: 提出的方法在理论和实验中均表现出色，适用于多种实际任务。

Abstract: We study a relaxation of the problem of coupling probability distributions --
a list of samples is generated from one distribution and an accept is declared
if any one of these samples is identical to the sample generated from the other
distribution. We propose a novel method for generating samples, which extends
the Gumbel-max sampling suggested in Daliri et al. (arXiv:2408.07978) for
coupling probability distributions. We also establish a corresponding lower
bound on the acceptance probability, which we call the list matching lemma. We
next discuss two applications of our setup. First, we develop a new mechanism
for multi-draft speculative sampling that is simple to implement and achieves
performance competitive with baselines such as SpecTr and SpecInfer across a
range of language tasks. Our method also guarantees a certain degree of drafter
invariance with respect to the output tokens which is not supported by existing
schemes. We also provide a theoretical lower bound on the token level
acceptance probability. As our second application, we consider distributed
lossy compression with side information in a setting where a source sample is
compressed and available to multiple decoders, each with independent side
information. We propose a compression technique that is based on our
generalization of Gumbel-max sampling and show that it provides significant
gains in experiments involving synthetic Gaussian sources and the MNIST image
dataset.

</details>


### [164] [AutoQD: Automatic Discovery of Diverse Behaviors with Quality-Diversity Optimization](https://arxiv.org/abs/2506.05634)
*Saeed Hedayatian,Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: AutoQD提出了一种自动生成行为描述符的方法，通过嵌入马尔可夫决策过程中的策略占用度量，避免了依赖预定义多样性描述符的限制。


<details>
  <summary>Details</summary>
Motivation: 传统QD算法依赖手工设计的行为描述符，限制了探索的多样性。AutoQD旨在自动生成描述符，实现更开放的探索。

Method: 利用随机傅里叶特征近似策略占用度量的最大均值差异（MMD），生成反映行为差异的低维嵌入，作为QD算法的输入。

Result: 实验证明AutoQD能在连续控制任务中发现多样策略，且嵌入距离随样本和维度增加收敛于真实MMD距离。

Conclusion: AutoQD为无监督强化学习和QD优化提供了新思路，支持无需领域知识的自动化行为发现。

Abstract: Quality-Diversity (QD) algorithms have shown remarkable success in
discovering diverse, high-performing solutions, but rely heavily on
hand-crafted behavioral descriptors that constrain exploration to predefined
notions of diversity. Leveraging the equivalence between policies and occupancy
measures, we present a theoretically grounded approach to automatically
generate behavioral descriptors by embedding the occupancy measures of policies
in Markov Decision Processes. Our method, AutoQD, leverages random Fourier
features to approximate the Maximum Mean Discrepancy (MMD) between policy
occupancy measures, creating embeddings whose distances reflect meaningful
behavioral differences. A low-dimensional projection of these embeddings that
captures the most behaviorally significant dimensions is then used as
behavioral descriptors for off-the-shelf QD methods. We prove that our
embeddings converge to true MMD distances between occupancy measures as the
number of sampled trajectories and embedding dimensions increase. Through
experiments in multiple continuous control tasks we demonstrate AutoQD's
ability in discovering diverse policies without predefined behavioral
descriptors, presenting a well-motivated alternative to prior methods in
unsupervised Reinforcement Learning and QD optimization. Our approach opens new
possibilities for open-ended learning and automated behavior discovery in
sequential decision making settings without requiring domain-specific
knowledge.

</details>


### [165] [Bayesian Inference for Correlated Human Experts and Classifiers](https://arxiv.org/abs/2506.05636)
*Markelle Kelly,Alex Boyd,Sam Showalter,Mark Steyvers,Padhraic Smyth*

Main category: cs.LG

TL;DR: 论文提出了一种贝叶斯框架，用于在机器学习中结合人类专家意见和模型预测，以减少专家查询次数并保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在机器学习中高效利用人类专家的标签预测，减少查询次数并提升预测效果。

Method: 开发了一个贝叶斯框架，通过联合潜在表示建模专家相关性，支持模拟推理和未观察专家标签的后验推断。

Result: 在真实医学分类问题和基准数据集上，显著减少了专家查询成本，同时保持了高预测准确性。

Conclusion: 该框架为结合机器学习和人类专家意见提供了一种高效且实用的方法。

Abstract: Applications of machine learning often involve making predictions based on
both model outputs and the opinions of human experts. In this context, we
investigate the problem of querying experts for class label predictions, using
as few human queries as possible, and leveraging the class probability
estimates of pre-trained classifiers. We develop a general Bayesian framework
for this problem, modeling expert correlation via a joint latent
representation, enabling simulation-based inference about the utility of
additional expert queries, as well as inference of posterior distributions over
unobserved expert labels. We apply our approach to two real-world medical
classification problems, as well as to CIFAR-10H and ImageNet-16H,
demonstrating substantial reductions relative to baselines in the cost of
querying human experts while maintaining high prediction accuracy.

</details>


### [166] [Projectable Models: One-Shot Generation of Small Specialized Transformers from Large Ones](https://arxiv.org/abs/2506.05641)
*Andrey Zhmoginov,Jihwan Lee,Mark Sandler*

Main category: cs.LG

TL;DR: 提出一种将大型Transformer参数映射到小型专用模型的技术，以降低计算成本并提高任务性能。


<details>
  <summary>Details</summary>
Motivation: 现代基础模型计算成本高且知识广泛，但特定任务可能只需部分知识，因此探索如何缩小模型规模以适应特定任务。

Method: 通过任务特定的参数映射技术，将大型Transformer参数转化为小型专用模型参数。

Result: 在图像建模任务中，生成的小型模型性能优于通用条件模型。

Conclusion: 该方法能有效缩小模型规模并提升特定任务性能。

Abstract: Modern Foundation Models (FMs) are typically trained on corpora spanning a
wide range of different data modalities, topics and downstream tasks. Utilizing
these models can be very computationally expensive and is out of reach for most
consumer devices. Furthermore, most of the broad FM knowledge may actually be
irrelevant for a specific task at hand. Here we explore a technique for mapping
parameters of a large Transformer to parameters of a smaller specialized model.
By making this transformation task-specific, we aim to capture a narrower scope
of the knowledge needed for performing a specific task by a smaller model. We
study our method on image modeling tasks, showing that performance of generated
models exceeds that of universal conditional models.

</details>


### [167] [Learning to Weight Parameters for Data Attribution](https://arxiv.org/abs/2506.05647)
*Shuangqi Li,Hieu Le,Jingyi Xu,Mathieu Salzmann*

Main category: cs.LG

TL;DR: 提出了一种针对生成模型中数据归属的方法，通过学习参数重要性权重来改进归属准确性，无需标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法在追踪梯度时通常忽略网络参数的不同层次对信息编码的差异，导致归属不准确。

Method: 通过学习参数重要性权重，适应模型结构，捕捉训练数据对输出不同语义方面（如主题、风格或背景）的贡献。

Result: 方法在扩散模型中提高了归属准确性，并提供了对输出如何从训练数据中借鉴的细粒度见解。

Conclusion: 该方法通过建模参数重要性权重，改进了生成模型中的数据归属，为理解模型行为提供了新视角。

Abstract: We study data attribution in generative models, aiming to identify which
training examples most influence a given output. Existing methods achieve this
by tracing gradients back to training data. However, they typically treat all
network parameters uniformly, ignoring the fact that different layers encode
different types of information and may thus draw information differently from
the training set. We propose a method that models this by learning parameter
importance weights tailored for attribution, without requiring labeled data.
This allows the attribution process to adapt to the structure of the model,
capturing which training examples contribute to specific semantic aspects of an
output, such as subject, style, or background. Our method improves attribution
accuracy across diffusion models and enables fine-grained insights into how
outputs borrow from training data.

</details>


### [168] [BAQ: Efficient Bit Allocation Quantization for Large Language Models](https://arxiv.org/abs/2506.05664)
*Chao Zhang,Li Wang,Samson Lasaulce,Merouane Debbah*

Main category: cs.LG

TL;DR: 本文提出了一种基于Hessian代理的量化位宽分配框架BAQ，通过凸优化任务自适应分配位宽，显著降低量化损失，并在实验中优于GPTQ。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法依赖均匀或启发式位宽分配，未能考虑权重对量化噪声的非均匀敏感性，导致性能不佳。

Method: 提出基于Hessian代理的灵敏度度量，将位宽分配问题转化为凸优化任务，设计BAQ算法实现自适应位宽分配。

Result: BAQ在125M至30B参数的LLMs上表现优于GPTQ，相同位宽下困惑度降低56倍。

Conclusion: BAQ通过理论分析和实验验证，实现了量化损失最小化与复杂度的良好权衡，为LLM量化提供了高效解决方案。

Abstract: Post-training model quantization is a widely adopted technique for reducing
the memory and computational costs of large language models (LLMs). However,
most existing methods rely on uniform or heuristic bitwidth assignments,
failing to account for the nonuniform sensitivity of weights to quantization
noise. In this paper, we propose a novel framework for allocating quantization
bitwidths based on sensitivity metrics derived from a Hessian proxy. We make
key assumptions, which allow the layer/component-wise loss function to be
expressed as an explicit function of the bitwidths. This enables a neat
formulation of the bit allocation problem as a convex optimization task, whose
closed-form solution adapts precision across weights to minimize the layer-wise
quantization loss. Inspecting the solution provides several insights (such as
the equal-loss structure), which are then exploited to design the proposed
\textbf{BAQ} (Bit Allocation Quantization) algorithm. The proposed algorithm
achieves a good trade-off between loss minimization and complexity and allows
BAQ to be integrated into standard quantization pipelines with minimal
overhead. Experimental results show that BAQ consistently outperforms GPTQ,
achieving up to 56$\times$ lower perplexity at the same bitwidth on large
language models ranging from 125M to 30B parameters. Leveraging our analytical
results derived from solving the optimal bit allocation problem, we also
provide a theoretical explanation for the observed gains. All codes of this
paper are available at https://github.com/CSU-ModelCompression/BAQ.

</details>


### [169] [RNE: a plug-and-play framework for diffusion density estimation and inference-time control](https://arxiv.org/abs/2506.05668)
*Jiajun He,José Miguel Hernández-Lobato,Yuanqi Du,Francisco Vargas*

Main category: cs.LG

TL;DR: Radon-Nikodym Estimator（RNE）是一种基于路径分布密度比的灵活框架，用于扩散推断时的密度估计和控制，统一了多种现有方法。


<details>
  <summary>Details</summary>
Motivation: 提出RNE是为了通过密度比的概念，统一和简化扩散推断时的密度估计与控制方法，同时提供理论清晰性和实践灵活性。

Method: RNE基于变分推断和概率原理，通过路径分布的密度比实现扩散模型的密度估计与控制。

Result: 实验表明，RNE在扩散密度估计和推断控制任务（如退火、模型组合和奖励倾斜）中表现优异。

Conclusion: RNE为扩散推断任务提供了一个统一且直观的框架，兼具理论深度和实际应用价值。

Abstract: In this paper, we introduce the Radon-Nikodym Estimator (RNE), a flexible,
plug-and-play framework for diffusion inference-time density estimation and
control, based on the concept of the density ratio between path distributions.
RNE connects and unifies a variety of existing density estimation and
inference-time control methods under a single and intuitive perspective,
stemming from basic variational inference and probabilistic principles
therefore offering both theoretical clarity and practical versatility.
Experiments demonstrate that RNE achieves promising performances in diffusion
density estimation and inference-time control tasks, including annealing,
composition of diffusion models, and reward-tilting.

</details>


### [170] [Contextually Guided Transformers via Low-Rank Adaptation](https://arxiv.org/abs/2506.05672)
*Andrey Zhmoginov,Jihwan Lee,Max Vladymyrov,Mark Sandler*

Main category: cs.LG

TL;DR: 提出了一种无需显式提示的Transformer改进模型CGT，通过动态权重调整实现上下文自适应。


<details>
  <summary>Details</summary>
Motivation: 解决传统LLMs依赖显式提示带来的计算开销问题。

Method: 设计CGT模型，动态编码上下文到权重中，支持自适应性。

Result: 在合成任务和语言建模基准上验证了有效性，并提升了表示可解释性。

Conclusion: 为高效、自适应的语言建模提供了新方向。

Abstract: Large Language Models (LLMs) based on Transformers excel at text processing,
but their reliance on prompts for specialized behavior introduces computational
overhead. We propose a modification to a Transformer architecture that
eliminates the need for explicit prompts by learning to encode context into the
model's weights. Our Contextually Guided Transformer (CGT) model maintains a
contextual summary at each sequence position, allowing it to update the weights
on the fly based on the preceding context. This approach enables the model to
self-specialize, effectively creating a tailored model for processing
information following a given prefix. We demonstrate the effectiveness of our
method on synthetic in-context learning tasks and language modeling benchmarks.
Furthermore, we introduce techniques for enhancing the interpretability of the
learned contextual representations, drawing connections to Variational
Autoencoders and promoting smoother, more consistent context encoding. This
work offers a novel direction for efficient and adaptable language modeling by
integrating context directly into the model's architecture.

</details>


### [171] [Peer-Ranked Precision: Creating a Foundational Dataset for Fine-Tuning Vision Models from DataSeeds' Annotated Imagery](https://arxiv.org/abs/2506.05673)
*Sajjad Abdoli,Freeman Lewin,Gediminas Vasiliauskas,Fabian Schonholz*

Main category: cs.LG

TL;DR: 论文探讨了AI模型开发从‘模型中心’向‘数据中心’的范式转变，并介绍了高质量数据集DSD及其对模型性能的提升。


<details>
  <summary>Details</summary>
Motivation: 传统AI开发过于依赖复杂模型架构，而忽视了数据质量的重要性。论文旨在推动‘数据为中心’的方法，通过高质量数据集提升模型性能。

Method: 引入DSD数据集（10,610张高质量图像及多层级标注），并评估其对模型性能的定量改进。

Result: DSD显著提升了特定模型的性能，代码和训练模型已公开。

Conclusion: 数据中心方法是AI发展的关键，DSD为商业和多模态AI开发提供了可扩展的基础。

Abstract: The development of modern Artificial Intelligence (AI) models, particularly
diffusion-based models employed in computer vision and image generation tasks,
is undergoing a paradigmatic shift in development methodologies. Traditionally
dominated by a "Model Centric" approach, in which performance gains were
primarily pursued through increasingly complex model architectures and
hyperparameter optimization, the field is now recognizing a more nuanced
"Data-Centric" approach. This emergent framework foregrounds the quality,
structure, and relevance of training data as the principal driver of model
performance. To operationalize this paradigm shift, we introduce the
DataSeeds.AI sample dataset (the "DSD"), initially comprised of approximately
10,610 high-quality human peer-ranked photography images accompanied by
extensive multi-tier annotations. The DSD is a foundational computer vision
dataset designed to usher in a new standard for commercial image datasets.
Representing a small fraction of DataSeeds.AI's 100 million-plus image catalog,
the DSD provides a scalable foundation necessary for robust commercial and
multimodal AI development. Through this in-depth exploratory analysis, we
document the quantitative improvements generated by the DSD on specific models
against known benchmarks and make the code and the trained models used in our
evaluation publicly available.

</details>


### [172] [Topology-aware Neural Flux Prediction Guided by Physics](https://arxiv.org/abs/2506.05676)
*Haoyang Jiang,Jindong Wang,Xingquan Zhu,Yi He*

Main category: cs.LG

TL;DR: 论文提出了一种新框架，结合显式差异矩阵和隐式物理约束，以提升GNN在定向图中对高频信号的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在处理定向图时难以保留节点信号的高频成分，而这些成分对建模流动态至关重要。

Method: 结合显式差异矩阵（建模方向梯度）和隐式物理约束（确保消息传递符合自然规律）。

Result: 在真实世界的水通量网络和城市交通流网络中验证了框架的有效性。

Conclusion: 新框架显著提升了GNN在定向图中捕捉高频信号和拓扑差异的能力。

Abstract: Graph Neural Networks (GNNs) often struggle in preserving high-frequency
components of nodal signals when dealing with directed graphs. Such components
are crucial for modeling flow dynamics, without which a traditional GNN tends
to treat a graph with forward and reverse topologies equal.To make GNNs
sensitive to those high-frequency components thereby being capable to capture
detailed topological differences, this paper proposes a novel framework that
combines 1) explicit difference matrices that model directional gradients and
2) implicit physical constraints that enforce messages passing within GNNs to
be consistent with natural laws. Evaluations on two real-world directed graph
data, namely, water flux network and urban traffic flow network, demonstrate
the effectiveness of our proposal.

</details>


### [173] [Numerical Investigation of Sequence Modeling Theory using Controllable Memory Functions](https://arxiv.org/abs/2506.05678)
*Haotian Jiang,Zeyu Bao,Shida Wang,Qianxiao Li*

Main category: cs.LG

TL;DR: 提出了一种合成基准框架，用于评估不同序列模型捕捉不同时间结构的能力，通过生成具有明确时间依赖性的合成目标，揭示了模型的优缺点。


<details>
  <summary>Details</summary>
Motivation: 系统地评估序列模型在不同时间依赖性任务中的表现，填补现有研究中对模型能力系统化分析的空白。

Method: 设计了合成基准框架，生成具有不同时间复杂度的任务，通过四种代表性记忆函数分析模型行为。

Result: 实验验证了现有理论见解并揭示了新发现，证明了该方法的有效性。

Conclusion: 该方法有助于理论理解，并强调了使用明确结构的可控目标对评估序列模型的重要性。

Abstract: The evolution of sequence modeling architectures, from recurrent neural
networks and convolutional models to Transformers and structured state-space
models, reflects ongoing efforts to address the diverse temporal dependencies
inherent in sequential data. Despite this progress, systematically
characterizing the strengths and limitations of these architectures remains a
fundamental challenge. In this work, we propose a synthetic benchmarking
framework to evaluate how effectively different sequence models capture
distinct temporal structures. The core of this approach is to generate
synthetic targets, each characterized by a memory function and a parameter that
determines the strength of temporal dependence. This setup allows us to produce
a continuum of tasks that vary in temporal complexity, enabling fine-grained
analysis of model behavior concerning specific memory properties. We focus on
four representative memory functions, each corresponding to a distinct class of
temporal structures. Experiments on several sequence modeling architectures
confirm existing theoretical insights and reveal new findings. These results
demonstrate the effectiveness of the proposed method in advancing theoretical
understanding and highlight the importance of using controllable targets with
clearly defined structures for evaluating sequence modeling architectures.

</details>


### [174] [Learning Design-Score Manifold to Guide Diffusion Models for Offline Optimization](https://arxiv.org/abs/2506.05680)
*Tailin Zhou,Zhilin Chen,Wenlong Lyu,Zhitang Chen,Danny H. K. Tsang,Jun Zhang*

Main category: cs.LG

TL;DR: ManGO是一种基于扩散的框架，通过学习设计-评分流形，统一前向预测和后向生成，实现超越训练数据的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 复杂系统的优化是科学和工程中的基本挑战，传统方法在训练数据外表现不佳。

Method: ManGO采用无导数引导的条件生成和自适应推理时间缩放，动态优化去噪路径。

Result: 在多个领域（如机器人控制、材料设计等）中，ManGO优于24种单目标和10种多目标优化方法。

Conclusion: ManGO通过统一设计-评分空间，显著提升了离线优化的泛化能力。

Abstract: Optimizing complex systems, from discovering therapeutic drugs to designing
high-performance materials, remains a fundamental challenge across science and
engineering, as the underlying rules are often unknown and costly to evaluate.
Offline optimization aims to optimize designs for target scores using
pre-collected datasets without system interaction. However, conventional
approaches may fail beyond training data, predicting inaccurate scores and
generating inferior designs. This paper introduces ManGO, a diffusion-based
framework that learns the design-score manifold, capturing the design-score
interdependencies holistically. Unlike existing methods that treat design and
score spaces in isolation, ManGO unifies forward prediction and backward
generation, attaining generalization beyond training data. Key to this is its
derivative-free guidance for conditional generation, coupled with adaptive
inference-time scaling that dynamically optimizes denoising paths. Extensive
evaluations demonstrate that ManGO outperforms 24 single- and 10
multi-objective optimization methods across diverse domains, including
synthetic tasks, robot control, material design, DNA sequence, and real-world
engineering optimization.

</details>


### [175] [Multi-Modal Multi-Task Federated Foundation Models for Next-Generation Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence in AR/VR/MR](https://arxiv.org/abs/2506.05683)
*Fardis Nadimi,Payam Abdisarabshali,Kasra Borazjani,Jacob Chakareski,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 论文提出了一种多模态多任务联邦基础模型（FedFMs）架构，用于扩展现实（XR）系统，结合了基础模型的表征能力和联邦学习的隐私保护训练原则。


<details>
  <summary>Details</summary>
Motivation: 通过整合多模态多任务基础模型和联邦学习，解决XR系统在隐私保护和多模态交互中的挑战。

Method: 提出模块化架构，支持不同协调范式的模型训练和聚合，并定义了影响FedFMs实现的SHIFT维度。

Result: 展示了SHIFT维度在XR应用中的具体表现，并提出了评估指标、数据集需求和设计权衡。

Conclusion: 为下一代XR系统中的上下文感知隐私保护智能奠定了技术和概念基础。

Abstract: Extended reality (XR) systems, which consist of virtual reality (VR),
augmented reality (AR), and mixed reality (XR), offer a transformative
interface for immersive, multi-modal, and embodied human-computer interaction.
In this paper, we envision that multi-modal multi-task (M3T) federated
foundation models (FedFMs) can offer transformative capabilities for XR systems
through integrating the representational strength of M3T foundation models
(FMs) with the privacy-preserving model training principles of federated
learning (FL). We present a modular architecture for FedFMs, which entails
different coordination paradigms for model training and aggregations. Central
to our vision is the codification of XR challenges that affect the
implementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality
diversity, (2) Hardware heterogeneity and system-level constraints, (3)
Interactivity and embodied personalization, (4) Functional/task variability,
and (5) Temporality and environmental variability. We illustrate the
manifestation of these dimensions across a set of emerging and anticipated
applications of XR systems. Finally, we propose evaluation metrics, dataset
requirements, and design tradeoffs necessary for the development of
resource-aware FedFMs in XR. This perspective aims to chart the technical and
conceptual foundations for context-aware privacy-preserving intelligence in the
next generation of XR systems.

</details>


### [176] [Statistically Valid Post-Deployment Monitoring Should Be Standard for AI-Based Digital Health](https://arxiv.org/abs/2506.05701)
*Pavel Dolin,Weizhi Li,Gautam Dasarathy,Visar Berisha*

Main category: cs.LG

TL;DR: 本文主张临床AI的部署后监测不足，并提出基于统计有效性和标签效率的测试框架，以确保实际部署中的可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 当前临床AI的部署后监测计划不足（仅9%的FDA注册工具包含此类计划），且现有方法多为手动、零星和反应式，无法适应动态环境。

Method: 提出基于统计假设检验的框架，将数据变化和模型性能退化作为独立的统计问题处理，确保科学性和可重复性。

Result: 该方法为临床AI系统的可靠性提供了科学基础，并提出了新的研究方向。

Conclusion: 统计严谨的监测框架是确保临床AI系统可靠性的关键，并为技术社区提供了新的研究方向。

Abstract: This position paper argues that post-deployment monitoring in clinical AI is
underdeveloped and proposes statistically valid and label-efficient testing
frameworks as a principled foundation for ensuring reliability and safety in
real-world deployment. A recent review found that only 9% of FDA-registered
AI-based healthcare tools include a post-deployment surveillance plan. Existing
monitoring approaches are often manual, sporadic, and reactive, making them
ill-suited for the dynamic environments in which clinical models operate. We
contend that post-deployment monitoring should be grounded in label-efficient
and statistically valid testing frameworks, offering a principled alternative
to current practices. We use the term "statistically valid" to refer to methods
that provide explicit guarantees on error rates (e.g., Type I/II error), enable
formal inference under pre-defined assumptions, and support
reproducibility--features that align with regulatory requirements.
Specifically, we propose that the detection of changes in the data and model
performance degradation should be framed as distinct statistical hypothesis
testing problems. Grounding monitoring in statistical rigor ensures a
reproducible and scientifically sound basis for maintaining the reliability of
clinical AI systems. Importantly, it also opens new research directions for the
technical community--spanning theory, methods, and tools for statistically
principled detection, attribution, and mitigation of post-deployment model
failures in real-world settings.

</details>


### [177] [Action-Adaptive Continual Learning: Enabling Policy Generalization under Dynamic Action Spaces](https://arxiv.org/abs/2506.05702)
*Chaofan Pan,Jiafen Liu,Yanhua Li,Linbo Xiong,Fan Min,Wei Wei,Xin Yang*

Main category: cs.LG

TL;DR: 论文提出了一种新问题CL-DC（动态能力的持续学习），并提出了AACL框架来解决策略在不同动作空间中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有CL方法假设智能体能力在动态环境中不变，不符合现实场景。

Method: 提出AACL框架，通过构建动作表示空间解耦策略与具体动作空间，并自适应调整编码器-解码器。

Result: 实验证明AACL在三个环境中优于现有方法，实现了策略的跨动作空间泛化。

Conclusion: AACL框架有效解决了CL-DC问题，平衡了稳定性和可塑性。

Abstract: Continual Learning (CL) is a powerful tool that enables agents to learn a
sequence of tasks, accumulating knowledge learned in the past and using it for
problem-solving or future task learning. However, existing CL methods often
assume that the agent's capabilities remain static within dynamic environments,
which doesn't reflect real-world scenarios where capabilities dynamically
change. This paper introduces a new and realistic problem: Continual Learning
with Dynamic Capabilities (CL-DC), posing a significant challenge for CL
agents: How can policy generalization across different action spaces be
achieved? Inspired by the cortical functions, we propose an Action-Adaptive
Continual Learning framework (AACL) to address this challenge. Our framework
decouples the agent's policy from the specific action space by building an
action representation space. For a new action space, the encoder-decoder of
action representations is adaptively fine-tuned to maintain a balance between
stability and plasticity. Furthermore, we release a benchmark based on three
environments to validate the effectiveness of methods for CL-DC. Experimental
results demonstrate that our framework outperforms popular methods by
generalizing the policy across action spaces.

</details>


### [178] [Latent Diffusion Model Based Denoising Receiver for 6G Semantic Communication: From Stochastic Differential Theory to Application](https://arxiv.org/abs/2506.05710)
*Xiucheng Wang,Honggang Jia,Nan Cheng,Dusit Niyato*

Main category: cs.LG

TL;DR: 提出了一种基于生成式人工智能（GAI）和扩散模型（DMs）的新型语义通信框架，通过理论分析和实验验证其在高噪声和分布偏移下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统语义通信方法在高噪声和分布不匹配条件下表现不佳，需要一种更鲁棒的解决方案。

Method: 基于随机微分方程（SDEs）建立理论框架，推导SNR与去噪步长的关系，提出无需微调的缩放机制，并设计基于潜在扩散模型（LDM）的语义收发器。

Result: 实验表明，该框架在低SNR和分布偏移下显著优于传统神经网络基线。

Conclusion: 该研究为未来6G系统中GAI驱动的鲁棒语义传输提供了有前景的方向。

Abstract: In this paper, a novel semantic communication framework empowered by
generative artificial intelligence (GAI) is proposed, specifically leveraging
the capabilities of diffusion models (DMs). A rigorous theoretical foundation
is established based on stochastic differential equations (SDEs), which
elucidates the denoising properties of DMs in mitigating additive white
Gaussian noise (AWGN) in latent semantic representations. Crucially, a
closed-form analytical relationship between the signal-to-noise ratio (SNR) and
the denoising timestep is derived, enabling the optimal selection of diffusion
parameters for any given channel condition. To address the distribution
mismatch between the received signal and the DM's training data, a
mathematically principled scaling mechanism is introduced, ensuring robust
performance across a wide range of SNRs without requiring model fine-tuning.
Built upon this theoretical insight, we develop a latent diffusion model
(LDM)-based semantic transceiver, wherein a variational autoencoder (VAE) is
employed for efficient semantic compression, and a pretrained DM serves as a
universal denoiser. Notably, the proposed architecture is fully training-free
at inference time, offering high modularity and compatibility with large-scale
pretrained LDMs. This design inherently supports zero-shot generalization and
mitigates the challenges posed by out-of-distribution inputs. Extensive
experimental evaluations demonstrate that the proposed framework significantly
outperforms conventional neural-network-based semantic communication baselines,
particularly under low SNR conditions and distributional shifts, thereby
establishing a promising direction for GAI-driven robust semantic transmission
in future 6G systems.

</details>


### [179] [Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation](https://arxiv.org/abs/2506.05713)
*Zhan Zhuang,Xiequn Wang,Wei Li,Yulong Zhang,Qiushi Huang,Shuhao Chen,Xuehao Wang,Yanbin Wei,Yuhe Nie,Kede Ma,Yu Zhang,Ying Wei*

Main category: cs.LG

TL;DR: CoTo是一种渐进式训练策略，通过逐步增加适配器的激活概率，优化LoRA的微调效果，提升模型泛化能力和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: LoRA在微调大型基础模型时容易陷入次优解，限制了模型泛化和下游操作（如适配器合并和剪枝）的效果。

Method: 提出CoTo策略，通过随机停用适配器，促进更平衡的优化和更广泛的损失空间探索。理论分析表明，CoTo能提升层间dropout稳定性和线性模式连通性，并通过合作博弈量化适配器的边际贡献。

Result: 实验表明，CoTo显著提升了单任务性能、多任务合并准确性、剪枝鲁棒性，并减少了训练开销。

Conclusion: CoTo是一种高效且兼容性强的LoRA改进策略，适用于多种变体。

Abstract: Low-rank adaptation (LoRA) has emerged as a leading parameter-efficient
fine-tuning technique for adapting large foundation models, yet it often locks
adapters into suboptimal minima near their initialization. This hampers model
generalization and limits downstream operators such as adapter merging and
pruning. Here, we propose CoTo, a progressive training strategy that gradually
increases adapters' activation probability over the course of fine-tuning. By
stochastically deactivating adapters, CoTo encourages more balanced
optimization and broader exploration of the loss landscape. We provide a
theoretical analysis showing that CoTo promotes layer-wise dropout stability
and linear mode connectivity, and we adopt a cooperative-game approach to
quantify each adapter's marginal contribution. Extensive experiments
demonstrate that CoTo consistently boosts single-task performance, enhances
multi-task merging accuracy, improves pruning robustness, and reduces training
overhead, all while remaining compatible with diverse LoRA variants. Code is
available at https://github.com/zwebzone/coto.

</details>


### [180] [Ensemble Elastic DQN: A novel multi-step ensemble approach to address overestimation in deep value-based reinforcement learning](https://arxiv.org/abs/2506.05716)
*Adrian Ly,Richard Dazeley,Peter Vamplew,Francisco Cruz,Sunil Aryal*

Main category: cs.LG

TL;DR: 论文提出了一种名为EEDQN的新算法，结合了集成学习和弹性步长更新，以解决深度强化学习中的高估偏差和样本效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前对DQN的不同改进方法之间的交互理解有限，尤其是多步和集成方法在减少高估偏差方面表现出潜力。

Method: EEDQN通过结合集成学习和弹性步长更新来稳定算法性能，并在MinAtar基准测试中评估其效果。

Result: EEDQN在所有测试环境中表现稳健，优于基线DQN方法，并在大多数MinAtar环境中达到或超过最先进的集成DQN。

Conclusion: 系统结合算法改进具有潜力，集成和多步方法的精心整合可以带来显著收益。

Abstract: While many algorithmic extensions to Deep Q-Networks (DQN) have been
proposed, there remains limited understanding of how different improvements
interact. In particular, multi-step and ensemble style extensions have shown
promise in reducing overestimation bias, thereby improving sample efficiency
and algorithmic stability. In this paper, we introduce a novel algorithm called
Ensemble Elastic Step DQN (EEDQN), which unifies ensembles with elastic step
updates to stabilise algorithmic performance. EEDQN is designed to address two
major challenges in deep reinforcement learning: overestimation bias and sample
efficiency. We evaluated EEDQN against standard and ensemble DQN variants
across the MinAtar benchmark, a set of environments that emphasise behavioral
learning while reducing representational complexity. Our results show that
EEDQN achieves consistently robust performance across all tested environments,
outperforming baseline DQN methods and matching or exceeding state-of-the-art
ensemble DQNs in final returns on most of the MinAtar environments. These
findings highlight the potential of systematically combining algorithmic
improvements and provide evidence that ensemble and multi-step methods, when
carefully integrated, can yield substantial gains.

</details>


### [181] [Grokking Beyond the Euclidean Norm of Model Parameters](https://arxiv.org/abs/2506.05718)
*Pascal Jr Tikeng Notsawo,Guillaume Dumas,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: 论文探讨了通过正则化（显式或隐式）诱导神经网络在梯度优化中出现的延迟泛化现象（grokking），并分析了不同正则化方法和模型参数化对grokking的影响。


<details>
  <summary>Details</summary>
Motivation: 研究grokking现象的产生机制，尤其是正则化在其中的作用，以理解神经网络优化中的延迟泛化行为。

Method: 通过实验和理论分析，研究不同正则化方法（如ℓ₁或核范数正则化）和模型深度对grokking的影响，并探讨数据选择的作用。

Result: 发现正则化可以诱导grokking，且模型深度和特定属性P的正则化是关键因素；ℓ₂范数在非权重衰减情况下不可靠。

Conclusion: 正则化和模型参数化是grokking现象的重要驱动因素，数据选择也能放大这一现象。

Abstract: Grokking refers to a delayed generalization following overfitting when
optimizing artificial neural networks with gradient-based methods. In this
work, we demonstrate that grokking can be induced by regularization, either
explicit or implicit. More precisely, we show that when there exists a model
with a property $P$ (e.g., sparse or low-rank weights) that generalizes on the
problem of interest, gradient descent with a small but non-zero regularization
of $P$ (e.g., $\ell_1$ or nuclear norm regularization) results in grokking.
This extends previous work showing that small non-zero weight decay induces
grokking. Moreover, our analysis shows that over-parameterization by adding
depth makes it possible to grok or ungrok without explicitly using
regularization, which is impossible in shallow cases. We further show that the
$\ell_2$ norm is not a reliable proxy for generalization when the model is
regularized toward a different property $P$, as the $\ell_2$ norm grows in many
cases where no weight decay is used, but the model generalizes anyway. We also
show that grokking can be amplified solely through data selection, with any
other hyperparameter fixed.

</details>


### [182] [Any-Class Presence Likelihood for Robust Multi-Label Classification with Abundant Negative Data](https://arxiv.org/abs/2506.05721)
*Dumindu Tissera,Omar Awadallah,Muhammad Umair Danish,Ayan Sadhu,Katarina Grolinger*

Main category: cs.LG

TL;DR: 论文提出了一种改进的多标签分类（MLC）损失函数，通过归一化加权几何平均预测类别概率，解决了负数据过多对学习过程的干扰问题。


<details>
  <summary>Details</summary>
Motivation: 在多标签分类任务中，负数据（未分配类别的实例）过多会干扰学习过程，影响正实例的准确分类。

Method: 重新设计标准MLC损失函数，引入归一化加权几何平均预测类别概率，并添加正则化参数控制负类概率对正实例的影响。

Result: 在多个大规模数据集上实验表明，改进的损失函数显著提升了MLC性能，F1、F2和平均精度分别提高了6.01、8.06和3.11个百分点。

Conclusion: 提出的损失函数有效解决了负数据干扰问题，提升了多标签分类性能，且无需额外参数或计算复杂度。

Abstract: Multi-label Classification (MLC) assigns an instance to one or more
non-exclusive classes. A challenge arises when the dataset contains a large
proportion of instances with no assigned class, referred to as negative data,
which can overwhelm the learning process and hinder the accurate identification
and classification of positive instances. Nevertheless, it is common in MLC
applications such as industrial defect detection, agricultural disease
identification, and healthcare diagnosis to encounter large amounts of negative
data. Assigning a separate negative class to these instances further
complicates the learning objective and introduces unnecessary redundancies. To
address this challenge, we redesign standard MLC loss functions by deriving a
likelihood of any class being present, formulated by a normalized weighted
geometric mean of the predicted class probabilities. We introduce a
regularization parameter that controls the relative contribution of the absent
class probabilities to the any-class presence likelihood in positive instances.
The any-class presence likelihood complements the multi-label learning by
encouraging the network to become more aware of implicit positive instances and
improve the label classification within those positive instances. Experiments
on large-scale datasets with negative data: SewerML, modified COCO, and
ChestX-ray14, across various networks and base loss functions show that our
loss functions consistently improve MLC performance of their standard loss
counterparts, achieving gains of up to 6.01 percentage points in F1, 8.06 in
F2, and 3.11 in mean average precision, all without additional parameters or
computational complexity. Code available at:
https://github.com/ML-for-Sensor-Data-Western/gmean-mlc

</details>


### [183] [Generalized Incremental Learning under Concept Drift across Evolving Data Streams](https://arxiv.org/abs/2506.05736)
*En Yu,Jie Lu,Guangquan Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为CSFA的框架，用于解决开放环境流数据中的概念漂移问题，通过原型校准和源无关适应算法实现稳定适应。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据流具有非平稳性，现有方法未能解决标签空间和分布的联合演化问题，尤其是在有限监督和持续不确定性下。

Method: CSFA框架包括训练无关的原型校准机制和基于可靠代理间隙锐度感知最小化的源无关适应算法。

Result: 实验证明CSFA在开放世界流场景中优于现有方法，实现了对演化的语义和分布的稳定适应。

Conclusion: CSFA为开放环境流数据中的概念漂移问题提供了一种统一的解决方案，具有优越的性能和有效性。

Abstract: Real-world data streams exhibit inherent non-stationarity characterized by
concept drift, posing significant challenges for adaptive learning systems.
While existing methods address isolated distribution shifts, they overlook the
critical co-evolution of label spaces and distributions under limited
supervision and persistent uncertainty. To address this, we formalize
Generalized Incremental Learning under Concept Drift (GILCD), characterizing
the joint evolution of distributions and label spaces in open-environment
streaming contexts, and propose a novel framework called Calibrated Source-Free
Adaptation (CSFA). First, CSFA introduces a training-free prototype calibration
mechanism that dynamically fuses emerging prototypes with base representations,
enabling stable new-class identification without optimization overhead. Second,
we design a novel source-free adaptation algorithm, i.e., Reliable Surrogate
Gap Sharpness-aware (RSGS) minimization. It integrates sharpness-aware
perturbation loss optimization with surrogate gap minimization, while employing
entropy-based uncertainty filtering to discard unreliable samples. This
mechanism ensures robust distribution alignment and mitigates generalization
degradation caused by uncertainties. Therefore, CSFA establishes a unified
framework for stable adaptation to evolving semantics and distributions in
open-world streaming scenarios. Extensive experiments validate the superior
performance and effectiveness of CSFA compared to state-of-the-art approaches.

</details>


### [184] [Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance](https://arxiv.org/abs/2506.05748)
*Rudransh Agnihotri,Ananya Pandey*

Main category: cs.LG

TL;DR: 提出了一种低成本、高效的替代方案，通过冻结的7B LLM、单行JSON规则和rank-16 LoRA适配器，替代传统重型奖励模型，在RewardBench上达到96.2%准确率，并显著提升在线RLHF性能。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型训练成本高且复杂，需要离线调整阶段，限制了RLHF的效率。

Method: 使用冻结的7B LLM，结合单行JSON规则和rank-16 LoRA适配器（仅影响0.8%参数），无需离线阶段，直接作为奖励模型。

Result: 在RewardBench上准确率达96.2%，优于27B-70B参数的专业奖励网络；7B模型在GSM-8K上以92%准确率超越70B DPO基线。

Conclusion: 该方法通过轻量级LoRA和提示工程，实现了高效、透明且可调的奖励功能，为RLHF提供了新的最优解决方案。

Abstract: Reward-model training is the cost bottleneck in modern Reinforcement Learning
Human Feedback (RLHF) pipelines, often requiring tens of billions of parameters
and an offline preference-tuning phase. In the proposed method, a frozen,
instruction-tuned 7B LLM is augmented with only a one line JSON rubric and a
rank-16 LoRA adapter (affecting just 0.8% of the model's parameters), enabling
it to serve as a complete substitute for the previously used heavyweight
evaluation models. The plug-and-play judge achieves 96.2% accuracy on
RewardBench, outperforming specialized reward networks ranging from 27B to 70B
parameters. Additionally, it allows a 7B actor to outperform the top 70B DPO
baseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K
utilizing online PPO. Thorough ablations indicate that (i) six in context
demonstrations deliver the majority of the zero-to-few-shot improvements
(+2pp), and (ii) the LoRA effectively addresses the remaining disparity,
particularly in the safety and adversarial Chat-Hard segments. The proposed
model introduces HH-Rationales, a subset of 10,000 pairs from Anthropic
HH-RLHF, to examine interpretability, accompanied by human generated
justifications. GPT-4 scoring indicates that our LoRA judge attains
approximately = 9/10 in similarity to human explanations, while zero-shot
judges score around =5/10. These results indicate that the combination of
prompt engineering and tiny LoRA produces a cost effective, transparent, and
easily adjustable reward function, removing the offline phase while achieving
new state-of-the-art outcomes for both static evaluation and online RLHF.

</details>


### [185] [Integrating Spatiotemporal Features in LSTM for Spatially Informed COVID-19 Hospitalization Forecasting](https://arxiv.org/abs/2506.05752)
*Zhongying Wang,Thoai D. Ngo,Hamidreza Zoraghein,Benjamin Lucas,Morteza Karimzadeh*

Main category: cs.LG

TL;DR: 该研究提出了一种基于LSTM的新框架，结合时空特征SPH，显著提升了COVID-19住院预测的准确性，尤其在变异株流行期间表现优异。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行凸显了准确、及时预测住院需求的重要性，但现有模型在变异株激增时表现不佳。

Method: 采用LSTM框架，引入时空特征SPH（基于Facebook社交连接指数），并设计多时间范围集成策略。

Result: 在Delta和Omicron流行期间，模型预测优于基准，尤其在Omicron期间，预测误差显著降低。

Conclusion: SPH时空特征显著提升预测性能，为传染病传播建模提供了新思路。

Abstract: The COVID-19 pandemic's severe impact highlighted the need for accurate,
timely hospitalization forecasting to support effective healthcare planning.
However, most forecasting models struggled, especially during variant surges,
when they were needed most. This study introduces a novel Long Short-Term
Memory (LSTM) framework for forecasting daily state-level incident
hospitalizations in the United States. We present a spatiotemporal feature,
Social Proximity to Hospitalizations (SPH), derived from Facebook's Social
Connectedness Index to improve forecasts. SPH serves as a proxy for interstate
population interaction, capturing transmission dynamics across space and time.
Our parallel LSTM architecture captures both short- and long-term temporal
dependencies, and our multi-horizon ensembling strategy balances consistency
and forecasting error. Evaluation against COVID-19 Forecast Hub ensemble models
during the Delta and Omicron surges reveals superiority of our model. On
average, our model surpasses the ensemble by 27, 42, 54, and 69
hospitalizations per state on the $7^{th}$, $14^{th}$, $21^{st}$, and $28^{th}$
forecast days, respectively, during the Omicron surge. Data-ablation
experiments confirm SPH's predictive power, highlighting its effectiveness in
enhancing forecasting models. This research not only advances hospitalization
forecasting but also underscores the significance of spatiotemporal features,
such as SPH, in refining predictive performance in modeling the complex
dynamics of infectious disease spread.

</details>


### [186] [FlowOE: Imitation Learning with Flow Policy from Ensemble RL Experts for Optimal Execution under Heston Volatility and Concave Market Impacts](https://arxiv.org/abs/2506.05755)
*Yang Li,Zhi Chen*

Main category: cs.LG

TL;DR: 论文提出了一种基于流匹配模型的模仿学习框架flowOE，用于优化金融市场的执行策略，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统最优执行策略（如静态Almgren-Chriss模型）在动态市场中表现不佳，需要更灵活的方法。

Method: flowOE通过模仿多种专家策略并自适应选择最佳行为，结合精炼损失函数改进专家行为。

Result: 实验表明flowOE在多种市场条件下显著优于传统基准，实现更高利润和更低风险。

Conclusion: flowOE展示了在自适应最优执行中的实际应用潜力。

Abstract: Optimal execution in financial markets refers to the process of strategically
transacting a large volume of assets over a period to achieve the best possible
outcome by balancing the trade-off between market impact costs and timing or
volatility risks. Traditional optimal execution strategies, such as static
Almgren-Chriss models, often prove suboptimal in dynamic financial markets.
This paper propose flowOE, a novel imitation learning framework based on flow
matching models, to address these limitations. FlowOE learns from a diverse set
of expert traditional strategies and adaptively selects the most suitable
expert behavior for prevailing market conditions. A key innovation is the
incorporation of a refining loss function during the imitation process,
enabling flowOE not only to mimic but also to improve upon the learned expert
actions. To the best of our knowledge, this work is the first to apply flow
matching models in a stochastic optimal execution problem. Empirical
evaluations across various market conditions demonstrate that flowOE
significantly outperforms both the specifically calibrated expert models and
other traditional benchmarks, achieving higher profits with reduced risk. These
results underscore the practical applicability and potential of flowOE to
enhance adaptive optimal execution.

</details>


### [187] [BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning](https://arxiv.org/abs/2506.05762)
*Yunpeng Qing,Shuo Chen,Yixiao Chi,Shunyu Liu,Sixu Lin,Changqing Zou*

Main category: cs.LG

TL;DR: BiTrajDiff是一种双向轨迹扩散框架，通过同时生成未来和历史轨迹增强离线强化学习的数据多样性，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中静态数据集存在分布偏差，限制了泛化能力，而现有数据增强方法仅关注未来轨迹重建，忽略了历史过渡的探索。

Method: 提出BiTrajDiff框架，将轨迹生成任务分解为两个互补的扩散过程：前向生成未来轨迹和后向生成历史轨迹，以关键状态为锚点扩展状态空间。

Result: 在D4RL基准测试中，BiTrajDiff优于其他先进数据增强方法，显著提升了离线强化学习的性能。

Conclusion: BiTrajDiff通过双向轨迹扩散有效增强数据多样性，为离线强化学习提供了更优的数据增强解决方案。

Abstract: Recent advances in offline Reinforcement Learning (RL) have proven that
effective policy learning can benefit from imposing conservative constraints on
pre-collected datasets. However, such static datasets often exhibit
distribution bias, resulting in limited generalizability. To address this
limitation, a straightforward solution is data augmentation (DA), which
leverages generative models to enrich data distribution. Despite the promising
results, current DA techniques focus solely on reconstructing future
trajectories from given states, while ignoring the exploration of history
transitions that reach them. This single-direction paradigm inevitably hinders
the discovery of diverse behavior patterns, especially those leading to
critical states that may have yielded high-reward outcomes. In this work, we
introduce Bidirectional Trajectory Diffusion (BiTrajDiff), a novel DA framework
for offline RL that models both future and history trajectories from any
intermediate states. Specifically, we decompose the trajectory generation task
into two independent yet complementary diffusion processes: one generating
forward trajectories to predict future dynamics, and the other generating
backward trajectories to trace essential history transitions.BiTrajDiff can
efficiently leverage critical states as anchors to expand into potentially
valuable yet underexplored regions of the state space, thereby facilitating
dataset diversity. Extensive experiments on the D4RL benchmark suite
demonstrate that BiTrajDiff achieves superior performance compared to other
advanced DA methods across various offline RL backbones.

</details>


### [188] [Exploring Microstructural Dynamics in Cryptocurrency Limit Order Books: Better Inputs Matter More Than Stacking Another Hidden Layer](https://arxiv.org/abs/2506.05764)
*Haochuan,Wang*

Main category: cs.LG

TL;DR: 论文探讨了加密货币价格预测中，复杂深度学习模型是否比简单模型更有效，发现经过数据预处理和调优后，简单模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证在加密货币限价订单簿（LOB）数据中，复杂神经网络是否真的比简单模型更能提升短期价格预测能力，还是性能提升主要来自数据预处理和特征工程。

Method: 通过对比一系列模型（从逻辑回归、XGBoost到深度架构如DeepLOB和Conv1D+LSTM），并引入两种数据过滤管道（卡尔曼滤波和Savitzky-Golay），评估了二元和三元标签方案的性能。

Result: 结果显示，经过数据预处理和超参数调优后，简单模型的表现可以匹配甚至超越复杂网络，同时提供更快的推理速度和更高的可解释性。

Conclusion: 论文得出结论，在加密货币价格预测中，简单模型经过优化后可能比复杂模型更具优势，尤其是在速度和可解释性方面。

Abstract: Cryptocurrency price dynamics are driven largely by microstructural supply
demand imbalances in the limit order book (LOB), yet the highly noisy nature of
LOB data complicates the signal extraction process. Prior research has
demonstrated that deep-learning architectures can yield promising predictive
performance on pre-processed equity and futures LOB data, but they often treat
model complexity as an unqualified virtue. In this paper, we aim to examine
whether adding extra hidden layers or parameters to "blackbox ish" neural
networks genuinely enhances short term price forecasting, or if gains are
primarily attributable to data preprocessing and feature engineering. We
benchmark a spectrum of models from interpretable baselines, logistic
regression, XGBoost to deep architectures (DeepLOB, Conv1D+LSTM) on BTC/USDT
LOB snapshots sampled at 100 ms to multi second intervals using publicly
available Bybit data. We introduce two data filtering pipelines (Kalman,
Savitzky Golay) and evaluate both binary (up/down) and ternary (up/flat/down)
labeling schemes. Our analysis compares models on out of sample accuracy,
latency, and robustness to noise. Results reveal that, with data preprocessing
and hyperparameter tuning, simpler models can match and even exceed the
performance of more complex networks, offering faster inference and greater
interpretability.

</details>


### [189] [AANet: Virtual Screening under Structural Uncertainty via Alignment and Aggregation](https://arxiv.org/abs/2506.05768)
*Wenyu Zhu,Jianhui Wang,Bowen Gao,Yinjun Jia,Haichuan Tan,Ya-Qin Zhang,Wei-Ying Ma,Yanyan Lan*

Main category: cs.LG

TL;DR: 提出了一种基于对齐和聚合的框架，用于在结构不确定性下进行准确的虚拟筛选，显著提升了在apo结构上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟筛选方法依赖已知配体结合口袋的holo结构，而在早期药物发现中更常见的apo或预测结构（如AlphaFold2生成）性能较差。

Method: 采用三模态对比学习模块对齐配体、holo口袋和检测到的空腔表示，并通过跨注意力适配器动态聚合候选结合位点。

Result: 在apo结构基准测试中，EF1%从11.75提升至37.19，同时在holo结构上保持高性能。

Conclusion: 该方法在缺乏实验解析蛋白-配体复合物的情况下，有望推动首创药物发现。

Abstract: Virtual screening (VS) is a critical component of modern drug discovery, yet
most existing methods--whether physics-based or deep learning-based--are
developed around holo protein structures with known ligand-bound pockets.
Consequently, their performance degrades significantly on apo or predicted
structures such as those from AlphaFold2, which are more representative of
real-world early-stage drug discovery, where pocket information is often
missing. In this paper, we introduce an alignment-and-aggregation framework to
enable accurate virtual screening under structural uncertainty. Our method
comprises two core components: (1) a tri-modal contrastive learning module that
aligns representations of the ligand, the holo pocket, and cavities detected
from structures, thereby enhancing robustness to pocket localization error; and
(2) a cross-attention based adapter for dynamically aggregating candidate
binding sites, enabling the model to learn from activity data even without
precise pocket annotations. We evaluated our method on a newly curated
benchmark of apo structures, where it significantly outperforms
state-of-the-art methods in blind apo setting, improving the early enrichment
factor (EF1%) from 11.75 to 37.19. Notably, it also maintains strong
performance on holo structures. These results demonstrate the promise of our
approach in advancing first-in-class drug discovery, particularly in scenarios
lacking experimentally resolved protein-ligand complexes.

</details>


### [190] [Evaluating Neuron Explanations: A Unified Framework with Sanity Checks](https://arxiv.org/abs/2506.05774)
*Tuomas Oikarinen,Ge Yan,Tsui-Wei Weng*

Main category: cs.LG

TL;DR: 论文提出了一个统一的数学框架来评估神经网络单元解释的可靠性，并提出了两个简单的检验方法，发现许多常用指标不可靠，最后提出了未来评估的指南和可靠指标。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络中单个单元的功能是机制可解释性的重要基础，但现有解释的可靠性和真实性需要评估。

Method: 统一现有解释评估方法为一个数学框架，比较现有指标，并提出两个检验方法。

Result: 许多常用评估指标在概念标签发生巨大变化时得分不变，不可靠。

Conclusion: 提出了未来评估应遵循的指南，并确定了一组可靠的评估指标。

Abstract: Understanding the function of individual units in a neural network is an
important building block for mechanistic interpretability. This is often done
by generating a simple text explanation of the behavior of individual neurons
or units. For these explanations to be useful, we must understand how reliable
and truthful they are. In this work we unify many existing explanation
evaluation methods under one mathematical framework. This allows us to compare
existing evaluation metrics, understand the evaluation pipeline with increased
clarity and apply existing statistical methods on the evaluation. In addition,
we propose two simple sanity checks on the evaluation metrics and show that
many commonly used metrics fail these tests and do not change their score after
massive changes to the concept labels. Based on our experimental and
theoretical results, we propose guidelines that future evaluations should
follow and identify a set of reliable evaluation metrics.

</details>


### [191] [Exploiting Similarity for Computation and Communication-Efficient Decentralized Optimization](https://arxiv.org/abs/2506.05791)
*Yuki Takezawa,Xiaowen Jiang,Anton Rodomanov,Sebastian U. Stich*

Main category: cs.LG

TL;DR: SPDO方法通过降低子问题精度要求和利用平均功能相似性，显著减少了通信和计算复杂度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 减少通信复杂度对分布式优化至关重要，现有PDO方法因需高精度子问题解而计算开销大。

Method: 提出SPDO方法，放宽子问题精度要求并利用平均功能相似性。

Result: SPDO在通信和计算复杂度上优于现有方法。

Conclusion: SPDO在PDO框架内实现了最优性能，实验验证其优越性。

Abstract: Reducing communication complexity is critical for efficient decentralized
optimization. The proximal decentralized optimization (PDO) framework is
particularly appealing, as methods within this framework can exploit functional
similarity among nodes to reduce communication rounds. Specifically, when local
functions at different nodes are similar, these methods achieve faster
convergence with fewer communication steps. However, existing PDO methods often
require highly accurate solutions to subproblems associated with the proximal
operator, resulting in significant computational overhead. In this work, we
propose the Stabilized Proximal Decentralized Optimization (SPDO) method, which
achieves state-of-the-art communication and computational complexities within
the PDO framework. Additionally, we refine the analysis of existing PDO methods
by relaxing subproblem accuracy requirements and leveraging average functional
similarity. Experimental results demonstrate that SPDO significantly
outperforms existing methods.

</details>


### [192] [EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator](https://arxiv.org/abs/2506.05797)
*Qianyi Chen,Tianrun Gao,Chenbo Jiang,Tailin Wu*

Main category: cs.LG

TL;DR: EqCollide是一种端到端的等变神经场模拟器，用于模拟可变形物体的碰撞，解决了现有方法在物理对称性、碰撞处理和可扩展性上的不足。


<details>
  <summary>Details</summary>
Motivation: 模拟可变形物体的碰撞是一项基础但复杂的任务，现有数据驱动方法在物理对称性、碰撞处理和可扩展性上存在不足。

Method: 提出了一种等变编码器将物体几何和速度映射到潜在控制点，并通过等变图神经网络和神经ODE建模控制点间的碰撞感知交互。速度场通过神经场重建。

Result: 实验表明，EqCollide在多样物体配置下实现了准确、稳定和可扩展的模拟，其滚动MSE比最佳基线模型低24.34%至35.82%。

Conclusion: EqCollide能够泛化到更多碰撞物体和更长时间范围，并对群变换输入保持鲁棒性。

Abstract: Simulating collisions of deformable objects is a fundamental yet challenging
task due to the complexity of modeling solid mechanics and multi-body
interactions. Existing data-driven methods often suffer from lack of
equivariance to physical symmetries, inadequate handling of collisions, and
limited scalability. Here we introduce EqCollide, the first end-to-end
equivariant neural fields simulator for deformable objects and their
collisions. We propose an equivariant encoder to map object geometry and
velocity into latent control points. A subsequent equivariant Graph Neural
Network-based Neural Ordinary Differential Equation models the interactions
among control points via collision-aware message passing. To reconstruct
velocity fields, we query a neural field conditioned on control point features,
enabling continuous and resolution-independent motion predictions. Experimental
results show that EqCollide achieves accurate, stable, and scalable simulations
across diverse object configurations, and our model achieves 24.34% to 35.82%
lower rollout MSE even compared with the best-performing baseline model.
Furthermore, our model could generalize to more colliding objects and extended
temporal horizons, and stay robust to input transformed with group action.

</details>


### [193] [Option Pricing Using Ensemble Learning](https://arxiv.org/abs/2506.05799)
*Zeyuan Li,Qingdao Huang*

Main category: cs.LG

TL;DR: 本文研究了集成学习在期权定价中的应用，并与经典机器学习模型进行了比较，提出了一种新的实验策略和评估机制，强调了金融理论与计算方法的结合。


<details>
  <summary>Details</summary>
Motivation: 期权定价在计算金融中需要高预测精度和低结构复杂性，而集成学习恰好具备这些优势，因此研究其应用具有重要意义。

Method: 通过比较集成学习与经典机器学习模型的性能，引入参数传递实验策略，并开发了结合评分和加权评估的机制。

Result: 研究发现集成学习在准确性、局部特征提取和抗噪性方面表现优异，同时揭示了滑动窗口技术与噪声之间的微妙关系。

Conclusion: 集成学习在期权定价中具有显著优势，且金融理论与计算方法的结合为研究提供了新的方向。

Abstract: Ensemble learning is characterized by flexibility, high precision, and
refined structure. As a critical component within computational finance, option
pricing with machine learning requires both high predictive accuracy and
reduced structural complexity-features that align well with the inherent
advantages of ensemble learning. This paper investigates the application of
ensemble learning to option pricing, and conducts a comparative analysis with
classical machine learning models to assess their performance in terms of
accuracy, local feature extraction, and robustness to noise. A novel
experimental strategy is introduced, leveraging parameter transfer across
experiments to improve robustness and realism in financial simulations.Building
upon this strategy, an evaluation mechanism is developed that incorporates a
scoring strategy and a weighted evaluation strategy explicitly emphasizing the
foundational role of financial theory. This mechanism embodies an orderly
integration of theoretical finance and computational methods. In addition, the
study examines the interaction between sliding window technique and noise,
revealing nuanced patterns that suggest a potential connection relevant to
ongoing research in machine learning and data science.

</details>


### [194] [Neural Collapse in Cumulative Link Models for Ordinal Regression: An Analysis with Unconstrained Feature Model](https://arxiv.org/abs/2506.05801)
*Chuang Ma,Tomoyuki Obuchi,Toshiyuki Tanaka*

Main category: cs.LG

TL;DR: 论文研究了深度序数回归（OR）任务中是否会出现类似“神经崩溃（NC）”的现象，提出了“序数神经崩溃（ONC）”概念，并验证了其三个特性。


<details>
  <summary>Details</summary>
Motivation: 探索深度神经网络在序数回归任务中的行为模式，以加深对神经网络的理解，并扩展NC现象的应用范围。

Method: 结合累积链接模型和Unconstrained Feature Model（UFM），理论分析ONC现象，并通过实验验证。

Result: 发现ONC现象的三个特性：类内特征崩溃、类均值与分类器对齐、潜在变量按类别顺序排列。

Conclusion: ONC现象在序数回归任务中存在，其特性可用于改进OR任务，特别是固定阈值的使用。

Abstract: A phenomenon known as ''Neural Collapse (NC)'' in deep classification tasks,
in which the penultimate-layer features and the final classifiers exhibit an
extremely simple geometric structure, has recently attracted considerable
attention, with the expectation that it can deepen our understanding of how
deep neural networks behave. The Unconstrained Feature Model (UFM) has been
proposed to explain NC theoretically, and there emerges a growing body of work
that extends NC to tasks other than classification and leverages it for
practical applications. In this study, we investigate whether a similar
phenomenon arises in deep Ordinal Regression (OR) tasks, via combining the
cumulative link model for OR and UFM. We show that a phenomenon we call Ordinal
Neural Collapse (ONC) indeed emerges and is characterized by the following
three properties: (ONC1) all optimal features in the same class collapse to
their within-class mean when regularization is applied; (ONC2) these class
means align with the classifier, meaning that they collapse onto a
one-dimensional subspace; (ONC3) the optimal latent variables (corresponding to
logits or preactivations in classification tasks) are aligned according to the
class order, and in particular, in the zero-regularization limit, a highly
local and simple geometric relationship emerges between the latent variables
and the threshold values. We prove these properties analytically within the UFM
framework with fixed threshold values and corroborate them empirically across a
variety of datasets. We also discuss how these insights can be leveraged in OR,
highlighting the use of fixed thresholds.

</details>


### [195] [Positional Encoding meets Persistent Homology on Graphs](https://arxiv.org/abs/2506.05814)
*Yogesh Verma,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: 论文分析了图神经网络（GNNs）中消息传递的局部归纳偏置问题，提出了两种解决方法：位置编码（PE）和持久同调（PH）。研究发现两者在表达能力上无优劣之分，并设计了一种新的方法PiPE，结合了两者的优势，实验证明其性能优于PE和PH。


<details>
  <summary>Details</summary>
Motivation: 解决GNNs因局部归纳偏置而无法有效利用结构信息（如连通性和循环）的问题。

Method: 通过理论分析比较PE和PH的表达能力，并提出结合两者的新方法PiPE。

Result: PiPE在分子属性预测、图分类和分布外泛化等任务中表现优异。

Conclusion: PiPE结合了PE和PH的优势，显著提升了图表示学习的性能。

Abstract: The local inductive bias of message-passing graph neural networks (GNNs)
hampers their ability to exploit key structural information (e.g., connectivity
and cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged
as two promising approaches to mitigate this issue. PE schemes endow GNNs with
location-aware features, while PH methods enhance GNNs with multiresolution
topological features. However, a rigorous theoretical characterization of the
relative merits and shortcomings of PE and PH has remained elusive. We bridge
this gap by establishing that neither paradigm is more expressive than the
other, providing novel constructions where one approach fails but the other
succeeds. Our insights inform the design of a novel learnable method, PiPE
(Persistence-informed Positional Encoding), which is provably more expressive
than both PH and PE. PiPE demonstrates strong performance across a variety of
tasks (e.g., molecule property prediction, graph classification, and
out-of-distribution generalization), thereby advancing the frontiers of graph
representation learning. Code is available at
https://github.com/Aalto-QuML/PIPE.

</details>


### [196] [Learning Along the Arrow of Time: Hyperbolic Geometry for Backward-Compatible Representation Learning](https://arxiv.org/abs/2506.05826)
*Ngoc Bui,Menglin Yang,Runjin Chen,Leonardo Neves,Mingxuan Ju,Rex Ying,Neil Shah,Tong Zhao*

Main category: cs.LG

TL;DR: 该论文提出了一种在双曲几何中实现向后兼容表示学习的方法，通过考虑旧模型的不确定性，提升新模型的学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有欧几里得空间中的兼容性方法忽略了旧嵌入模型的不确定性，强制新模型重建过时的表示，阻碍了新模型的学习。

Method: 将嵌入提升到双曲空间，约束新嵌入位于旧嵌入的蕴含锥内，并引入基于不确定性的对比对齐损失。

Result: 实验验证了该方法在实现兼容性方面的优越性。

Conclusion: 该方法为构建更具弹性和适应性的机器学习系统提供了新思路。

Abstract: Backward compatible representation learning enables updated models to
integrate seamlessly with existing ones, avoiding to reprocess stored data.
Despite recent advances, existing compatibility approaches in Euclidean space
neglect the uncertainty in the old embedding model and force the new model to
reconstruct outdated representations regardless of their quality, thereby
hindering the learning process of the new model. In this paper, we propose to
switch perspectives to hyperbolic geometry, where we treat time as a natural
axis for capturing a model's confidence and evolution. By lifting embeddings
into hyperbolic space and constraining updated embeddings to lie within the
entailment cone of the old ones, we maintain generational consistency across
models while accounting for uncertainties in the representations. To further
enhance compatibility, we introduce a robust contrastive alignment loss that
dynamically adjusts alignment weights based on the uncertainty of the old
embeddings. Experiments validate the superiority of the proposed method in
achieving compatibility, paving the way for more resilient and adaptable
machine learning systems.

</details>


### [197] [Heartcare Suite: Multi-dimensional Understanding of ECG with Raw Multi-lead Signal Modeling](https://arxiv.org/abs/2506.05831)
*Yihan Xie,Sijing Li,Tianwei Lin,Zhuonan Wang,Chenglin Yang,Yu Zhong,Wenqiao Zhang,Haoyuan Li,Hao Jiang,Fengda Zhang,Qishan Chen,Jun Xiao,Yueting Zhuang,Beng Chin Ooi*

Main category: cs.LG

TL;DR: Heartcare Suite是一个多模态ECG理解框架，包含数据集、评估基准和模型HeartcareGPT，在多个临床任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升心电图（ECG）的多模态理解和评估能力，支持疾病诊断、波形分析和节律解释等任务。

Method: 框架包括高质量数据集Heartcare-220K、评估基准Heartcare-Bench和模型HeartcareGPT，后者采用定制分词器和双向扩散机制。

Result: HeartcareGPT在多个临床任务中表现出强泛化能力和最优性能。

Conclusion: Heartcare Suite有效推动了ECG多模态理解和评估的发展。

Abstract: We present Heartcare Suite, a multimodal comprehensive framework for
finegrained electrocardiogram (ECG) understanding. It comprises three key
components: (i) Heartcare-220K, a high-quality, structured, and comprehensive
multimodal ECG dataset covering essential tasks such as disease diagnosis,
waveform morphology analysis, and rhythm interpretation. (ii) Heartcare-Bench,
a systematic and multi-dimensional benchmark designed to evaluate diagnostic
intelligence and guide the optimization of Medical Multimodal Large Language
Models (Med-MLLMs) in ECG scenarios. and (iii) HeartcareGPT with a tailored
tokenizer Bidirectional ECG Abstract Tokenization (Beat), which compresses raw
multi-lead signals into semantically rich discrete tokens via duallevel vector
quantization and query-guided bidirectional diffusion mechanism. Built upon
Heartcare-220K, HeartcareGPT achieves strong generalization and SoTA
performance across multiple clinically meaningful tasks. Extensive experiments
demonstrate that Heartcare Suite is highly effective in advancing ECGspecific
multimodal understanding and evaluation. Our project is available at
https://github.com/DCDmllm/Heartcare-Suite .

</details>


### [198] [Wavelet-based Disentangled Adaptive Normalization for Non-stationary Times Series Forecasting](https://arxiv.org/abs/2506.05857)
*Junpeng Lin,Tian Lan,Bo Zhang,Ke Lin,Dandan Miao,Huiru He,Jiantao Ye,Chen Zhang,Yan-fu Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于小波的解耦自适应归一化（WDAN）框架，用于处理非平稳时间序列预测问题，通过分解和定制归一化策略提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 非平稳时间序列的统计特性随时间变化，导致深度学习模型泛化能力差，现有方法未充分考虑时间序列的多组分特性。

Method: WDAN利用离散小波变换将输入分解为低频趋势和高频波动，并针对不同部分应用定制归一化策略，对强非平稳趋势进行一阶差分提取稳定特征。

Result: 在多个基准测试中，WDAN显著提高了不同主干模型的预测准确性。

Conclusion: WDAN是一种模型无关的框架，能有效处理非平稳时间序列预测问题，代码已开源。

Abstract: Forecasting non-stationary time series is a challenging task because their
statistical properties often change over time, making it hard for deep models
to generalize well. Instance-level normalization techniques can help address
shifts in temporal distribution. However, most existing methods overlook the
multi-component nature of time series, where different components exhibit
distinct non-stationary behaviors. In this paper, we propose Wavelet-based
Disentangled Adaptive Normalization (WDAN), a model-agnostic framework designed
to address non-stationarity in time series forecasting. WDAN uses discrete
wavelet transforms to break down the input into low-frequency trends and
high-frequency fluctuations. It then applies tailored normalization strategies
to each part. For trend components that exhibit strong non-stationarity, we
apply first-order differencing to extract stable features used for predicting
normalization parameters. Extensive experiments on multiple benchmarks
demonstrate that WDAN consistently improves forecasting accuracy across various
backbone model. Code is available at this repository:
https://github.com/MonBG/WDAN.

</details>


### [199] [Loss Functions for Predictor-based Neural Architecture Search](https://arxiv.org/abs/2506.05869)
*Han Ji,Yuqi Feng,Jiahao Fan,Yanan Sun*

Main category: cs.LG

TL;DR: 论文研究了神经架构搜索（NAS）中性能预测器的损失函数选择，比较了回归、排序和加权损失函数，发现组合使用特定类别损失函数可提升预测效果。


<details>
  <summary>Details</summary>
Motivation: 评估NAS架构性能成本高，现有性能预测器的效果受损失函数选择影响，但缺乏对损失函数特性的深入研究。

Method: 将损失函数分为回归、排序和加权三类，评估八种损失函数在13个任务和五个搜索空间中的表现。

Result: 研究发现特定类别的损失函数组合可提升预测效果，并为不同任务选择损失函数提供实用指导。

Conclusion: 研究为NAS社区提供了损失函数选择的指导，并推动预测器方法的进一步发展。

Abstract: Evaluation is a critical but costly procedure in neural architecture search
(NAS). Performance predictors have been widely adopted to reduce evaluation
costs by directly estimating architecture performance. The effectiveness of
predictors is heavily influenced by the choice of loss functions. While
traditional predictors employ regression loss functions to evaluate the
absolute accuracy of architectures, recent approaches have explored various
ranking-based loss functions, such as pairwise and listwise ranking losses, to
focus on the ranking of architecture performance. Despite their success in NAS,
the effectiveness and characteristics of these loss functions have not been
thoroughly investigated. In this paper, we conduct the first comprehensive
study on loss functions in performance predictors, categorizing them into three
main types: regression, ranking, and weighted loss functions. Specifically, we
assess eight loss functions using a range of NAS-relevant metrics on 13 tasks
across five search spaces. Our results reveal that specific categories of loss
functions can be effectively combined to enhance predictor-based NAS.
Furthermore, our findings could provide practical guidance for selecting
appropriate loss functions for various tasks. We hope this work provides
meaningful insights to guide the development of loss functions for
predictor-based methods in the NAS community.

</details>


### [200] [BestServe: Serving Strategies with Optimal Goodput in Collocation and Disaggregation Architectures](https://arxiv.org/abs/2506.05871)
*Xiannan Hu,Tianyou Zeng,Xiaoming Yuan,Liwei Song,Guangyuan Zhang,Bangzheng He*

Main category: cs.LG

TL;DR: BestServe是一个用于高效分配和并行化大型语言模型（LLM）服务资源的框架，通过模拟和预测性能快速确定最优策略。


<details>
  <summary>Details</summary>
Motivation: 为大规模LLM服务提供高效资源分配和并行策略，避免传统试错方法的高成本和时间消耗。

Method: 结合改进的roofline模型和CPU-GPU调度动态，构建推理模拟器，支持多种架构（集中式与分布式）。

Result: 在单CPU上几分钟内确定最优策略，预测误差在20%以内，无需昂贵基准测试。

Conclusion: BestServe因其轻量级设计和强扩展性，适用于快速部署规划。

Abstract: Serving large language models (LLMs) to millions of users requires efficient
resource allocation and parallelism strategies. It is a labor intensive
trial-and-error process to find such a strategy. We present BestServe, a novel
framework for ranking serving strategies by estimating goodput under various
operating scenarios. Supporting both collocated and disaggregated
architectures, BestServe leverages an inference simulator built on an adapted
roofline model and CPU-GPU dispatch dynamics. Our framework determines the
optimal strategy in minutes on a single standard CPU, eliminating the need for
costly benchmarking, while achieving predictions within a $20\%$ error margin.
It appeals to be practical for rapid deployment planning because of its
lightweight design and strong extensibility.

</details>


### [201] [Interpretable Clustering Ensemble](https://arxiv.org/abs/2506.05877)
*Hang Lv,Lianyu Hu,Mudi Jiang,Xinying Liu,Zengyou He*

Main category: cs.LG

TL;DR: 提出了一种可解释的聚类集成算法，填补了该领域缺乏可解释性方法的空白。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中（如医疗诊断和金融风险评估），算法不仅需要准确，还需具备可解释性以确保透明和可信的决策。

Method: 将基础分区视为分类变量，在原始特征空间中构建决策树，并通过统计关联测试指导树的构建过程。

Result: 实验结果表明，该算法在性能上与最先进的聚类集成方法相当，同时保持了可解释性。

Conclusion: 这是首个专为聚类集成设计的可解释算法，为未来可解释聚类研究提供了新视角。

Abstract: Clustering ensemble has emerged as an important research topic in the field
of machine learning. Although numerous methods have been proposed to improve
clustering quality, most existing approaches overlook the need for
interpretability in high-stakes applications. In domains such as medical
diagnosis and financial risk assessment, algorithms must not only be accurate
but also interpretable to ensure transparent and trustworthy decision-making.
Therefore, to fill the gap of lack of interpretable algorithms in the field of
clustering ensemble, we propose the first interpretable clustering ensemble
algorithm in the literature. By treating base partitions as categorical
variables, our method constructs a decision tree in the original feature space
and use the statistical association test to guide the tree building process.
Experimental results demonstrate that our algorithm achieves comparable
performance to state-of-the-art (SOTA) clustering ensemble methods while
maintaining an additional feature of interpretability. To the best of our
knowledge, this is the first interpretable algorithm specifically designed for
clustering ensemble, offering a new perspective for future research in
interpretable clustering.

</details>


### [202] [A projection-based framework for gradient-free and parallel learning](https://arxiv.org/abs/2506.05878)
*Andreas Bergmeister,Manish Krishan Lal,Stefanie Jegelka,Suvrit Sra*

Main category: cs.LG

TL;DR: 论文提出了一种基于可行性搜索的神经网络训练方法，使用投影算子和迭代投影算法，与传统梯度下降方法不同。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降方法在处理不可微操作时存在局限性，且并行化能力有限。本文旨在提供一种替代方案，通过可行性问题重新定义训练过程。

Method: 将训练问题转化为大规模可行性问题，利用投影算子逐层满足局部约束，并通过PJAX框架实现自动化和并行化。

Result: 实验表明，该方法在多种网络架构（MLP、CNN、RNN）上表现良好，具有并行化优势和不可微操作处理能力。

Conclusion: 该方法为神经网络训练提供了新的思路，尤其在并行化和非可微操作场景下具有潜力。

Abstract: We present a feasibility-seeking approach to neural network training. This
mathematical optimization framework is distinct from conventional
gradient-based loss minimization and uses projection operators and iterative
projection algorithms. We reformulate training as a large-scale feasibility
problem: finding network parameters and states that satisfy local constraints
derived from its elementary operations. Training then involves projecting onto
these constraints, a local operation that can be parallelized across the
network. We introduce PJAX, a JAX-based software framework that enables this
paradigm. PJAX composes projection operators for elementary operations,
automatically deriving the solution operators for the feasibility problems
(akin to autodiff for derivatives). It inherently supports GPU/TPU
acceleration, provides a familiar NumPy-like API, and is extensible. We train
diverse architectures (MLPs, CNNs, RNNs) on standard benchmarks using PJAX,
demonstrating its functionality and generality. Our results show that this
approach is as a compelling alternative to gradient-based training, with clear
advantages in parallelism and the ability to handle non-differentiable
operations.

</details>


### [203] [NILMFormer: Non-Intrusive Load Monitoring that Accounts for Non-Stationarity](https://arxiv.org/abs/2506.05880)
*Adrien Petralia,Philippe Charpentier,Youssef Kadhi,Themis Palpanas*

Main category: cs.LG

TL;DR: NILMFormer是一种基于Transformer的架构，通过子序列平稳化/去平稳化方案和数据分布漂移缓解，显著提升了非侵入式负载监测（NILM）的性能。


<details>
  <summary>Details</summary>
Motivation: 传统NILM方法依赖粗粒度数据和静态客户信息，而现有深度学习方法因数据分布漂移导致性能下降。

Method: 提出NILMFormer，结合子序列平稳化/去平稳化方案和基于时间戳的位置编码。

Result: 在4个真实数据集上显著优于现有方法，并已部署为EDF的能耗监测服务核心算法。

Conclusion: NILMFormer有效解决了数据分布漂移问题，为智能电表数据提供了更精确的负载分解。

Abstract: Millions of smart meters have been deployed worldwide, collecting the total
power consumed by individual households. Based on these data, electricity
suppliers offer their clients energy monitoring solutions to provide feedback
on the consumption of their individual appliances. Historically, such estimates
have relied on statistical methods that use coarse-grained total monthly
consumption and static customer data, such as appliance ownership.
Non-Intrusive Load Monitoring (NILM) is the problem of disaggregating a
household's collected total power consumption to retrieve the consumed power
for individual appliances. Current state-of-the-art (SotA) solutions for NILM
are based on deep-learning (DL) and operate on subsequences of an entire
household consumption reading. However, the non-stationary nature of real-world
smart meter data leads to a drift in the data distribution within each
segmented window, which significantly affects model performance. This paper
introduces NILMFormer, a Transformer-based architecture that incorporates a new
subsequence stationarization/de-stationarization scheme to mitigate the
distribution drift and that uses a novel positional encoding that relies only
on the subsequence's timestamp information. Experiments with 4 real-world
datasets show that NILMFormer significantly outperforms the SotA approaches.
Our solution has been deployed as the backbone algorithm for EDF's
(Electricit\'e De France) consumption monitoring service, delivering detailed
insights to millions of customers about their individual appliances' power
consumption. This paper appeared in KDD 2025.

</details>


### [204] [Few Labels are all you need: A Weakly Supervised Framework for Appliance Localization in Smart-Meter Series](https://arxiv.org/abs/2506.05895)
*Adrien Petralia,Paul Boniol,Philippe Charpentier,Themis Palpanas*

Main category: cs.LG

TL;DR: CamAL提出了一种弱监督方法，用于家电模式定位，仅需家电存在信息即可训练，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能电表数据难以分解为单个家电的用电信息，现有监督方法需要昂贵且稀缺的标签数据。

Method: CamAL结合深度学习分类器与可解释分类方法，仅需家电存在信息进行训练。

Result: 在4个真实数据集上，CamAL显著优于现有弱监督基线，且性能接近全监督方法。

Conclusion: CamAL为智能电网管理提供了一种高效且低成本的解决方案。

Abstract: Improving smart grid system management is crucial in the fight against
climate change, and enabling consumers to play an active role in this effort is
a significant challenge for electricity suppliers. In this regard, millions of
smart meters have been deployed worldwide in the last decade, recording the
main electricity power consumed in individual households. This data produces
valuable information that can help them reduce their electricity footprint;
nevertheless, the collected signal aggregates the consumption of the different
appliances running simultaneously in the house, making it difficult to
apprehend. Non-Intrusive Load Monitoring (NILM) refers to the challenge of
estimating the power consumption, pattern, or on/off state activation of
individual appliances using the main smart meter signal. Recent methods
proposed to tackle this task are based on a fully supervised deep-learning
approach that requires both the aggregate signal and the ground truth of
individual appliance power. However, such labels are expensive to collect and
extremely scarce in practice, as they require conducting intrusive surveys in
households to monitor each appliance. In this paper, we introduce CamAL, a
weakly supervised approach for appliance pattern localization that only
requires information on the presence of an appliance in a household to be
trained. CamAL merges an ensemble of deep-learning classifiers combined with an
explainable classification method to be able to localize appliance patterns.
Our experimental evaluation, conducted on 4 real-world datasets, demonstrates
that CamAL significantly outperforms existing weakly supervised baselines and
that current SotA fully supervised NILM approaches require significantly more
labels to reach CamAL performances. The source of our experiments is available
at: https://github.com/adrienpetralia/CamAL. This paper appeared in ICDE 2025.

</details>


### [205] [A Driving Regime-Embedded Deep Learning Framework for Modeling Intra-Driver Heterogeneity in Multi-Scale Car-Following Dynamics](https://arxiv.org/abs/2506.05902)
*Shirui Zhou,Jiying Yan,Junfang Tian,Tao Wang,Yongfu Li,Shiquan Zhong*

Main category: cs.LG

TL;DR: 论文提出了一种新的数据驱动跟车模型框架，通过结合离散驾驶状态和连续运动预测，解决了单驾驶员行为动态异质性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以准确捕捉单驾驶员在不同驾驶条件下的动态异质性，因此需要一种更全面的方法来表征驾驶行为的复杂性。

Method: 采用混合深度学习架构，结合GRU进行离散驾驶状态分类和LSTM进行连续运动预测，并使用动态时间规整算法识别驾驶状态。

Result: 模型显著降低了加速度、速度和间距的预测误差（最大MSE改进达58.47%），并能重现关键交通现象。

Conclusion: 该框架有效统一了离散决策和连续动态，为跟车建模提供了更全面的解决方案。

Abstract: A fundamental challenge in car-following modeling lies in accurately
representing the multi-scale complexity of driving behaviors, particularly the
intra-driver heterogeneity where a single driver's actions fluctuate
dynamically under varying conditions. While existing models, both conventional
and data-driven, address behavioral heterogeneity to some extent, they often
emphasize inter-driver heterogeneity or rely on simplified assumptions,
limiting their ability to capture the dynamic heterogeneity of a single driver
under different driving conditions. To address this gap, we propose a novel
data-driven car-following framework that systematically embeds discrete driving
regimes (e.g., steady-state following, acceleration, cruising) into vehicular
motion predictions. Leveraging high-resolution traffic trajectory datasets, the
proposed hybrid deep learning architecture combines Gated Recurrent Units for
discrete driving regime classification with Long Short-Term Memory networks for
continuous kinematic prediction, unifying discrete decision-making processes
and continuous vehicular dynamics to comprehensively represent inter- and
intra-driver heterogeneity. Driving regimes are identified using a bottom-up
segmentation algorithm and Dynamic Time Warping, ensuring robust
characterization of behavioral states across diverse traffic scenarios.
Comparative analyses demonstrate that the framework significantly reduces
prediction errors for acceleration (maximum MSE improvement reached 58.47\%),
speed, and spacing metrics while reproducing critical traffic phenomena, such
as stop-and-go wave propagation and oscillatory dynamics.

</details>


### [206] [DeviceScope: An Interactive App to Detect and Localize Appliance Patterns in Electricity Consumption Time Series](https://arxiv.org/abs/2506.05912)
*Adrien Petralia,Paul Boniol,Philippe Charpentier,Themis Palpanas*

Main category: cs.LG

TL;DR: DeviceScope是一个交互式工具，旨在通过检测和定位特定时间段内单个电器的用电模式，帮助非专业用户理解智能电表数据。


<details>
  <summary>Details</summary>
Motivation: 智能电表数据记录了所有电器的聚合行为，但非专业用户难以理解这些数据并提取用电模式，同时真实标签数据昂贵且稀缺。

Method: 基于CamAL（基于类激活图的电器定位），一种新型弱监督方法，仅需知道家庭中是否存在某电器即可训练。

Result: DeviceScope能够有效检测和定位单个电器的用电模式。

Conclusion: DeviceScope为解决智能电表数据理解问题提供了一种实用且高效的解决方案。

Abstract: In recent years, electricity suppliers have installed millions of smart
meters worldwide to improve the management of the smart grid system. These
meters collect a large amount of electrical consumption data to produce
valuable information to help consumers reduce their electricity footprint.
However, having non-expert users (e.g., consumers or sales advisors) understand
these data and derive usage patterns for different appliances has become a
significant challenge for electricity suppliers because these data record the
aggregated behavior of all appliances. At the same time, ground-truth labels
(which could train appliance detection and localization models) are expensive
to collect and extremely scarce in practice. This paper introduces DeviceScope,
an interactive tool designed to facilitate understanding smart meter data by
detecting and localizing individual appliance patterns within a given time
period. Our system is based on CamAL (Class Activation Map-based Appliance
Localization), a novel weakly supervised approach for appliance localization
that only requires the knowledge of the existence of an appliance in a
household to be trained. This paper appeared in ICDE 2025.

</details>


### [207] [Over-PINNs: Enhancing Physics-Informed Neural Networks via Higher-Order Partial Derivative Overdetermination of PDEs](https://arxiv.org/abs/2506.05918)
*Wenxuan Huo,Qiang He,Gang Zhu,Weifeng Huang*

Main category: cs.LG

TL;DR: Over-PINNs框架通过自动微分生成高阶辅助方程，增强物理约束，提升PINNs在复杂问题中的精度。


<details>
  <summary>Details</summary>
Motivation: 尽管PINNs通过嵌入物理定律减少了对大数据集的依赖，但在复杂问题中的精度仍有提升空间。

Method: 利用自动微分生成高阶辅助方程，作为额外损失项加入训练过程，采用“超定”方法增强物理信息捕捉能力。

Result: 该方法在多种PDE求解中表现出强通用性，显著提升精度且未显著增加计算成本。

Conclusion: Over-PINNs框架有效提升了PINNs的精度，适用于复杂PDE问题。

Abstract: Partial differential equations (PDEs) serve as the cornerstone of
mathematical physics. In recent years, Physics-Informed Neural Networks (PINNs)
have significantly reduced the dependence on large datasets by embedding
physical laws directly into the training of neural networks. However, when
dealing with complex problems, the accuracy of PINNs still has room for
improvement. To address this issue, we introduce the Over-PINNs framework,
which leverages automatic differentiation (AD) to generate higher-order
auxiliary equations that impose additional physical constraints. These
equations are incorporated as extra loss terms in the training process,
effectively enhancing the model's ability to capture physical information
through an "overdetermined" approach. Numerical results illustrate that this
method exhibits strong versatility in solving various types of PDEs. It
achieves a significant improvement in solution accuracy without incurring
substantial additional computational costs.

</details>


### [208] [Machine Learning Predictions for Traffic Equilibria in Road Renovation Scheduling](https://arxiv.org/abs/2506.05933)
*Robbert Bosch,Wouter van Heeswijk,Patricia Rogetzer,Martijn Mes*

Main category: cs.LG

TL;DR: 论文探讨了利用机器学习替代模型预测道路维护对交通拥堵的影响，XGBoost表现最佳。


<details>
  <summary>Details</summary>
Motivation: 道路维护计划对交通拥堵的影响难以预测，传统模拟方法计算成本高。

Method: 采用监督学习框架，结合多种特征和回归模型，评估XGBoost等模型性能。

Result: XGBoost在MAPE和Pinball损失上表现最优，显著优于其他模型。

Conclusion: 机器学习方法可显著降低大规模交通分配问题的计算负担。

Abstract: Accurately estimating the impact of road maintenance schedules on traffic
conditions is important because maintenance operations can substantially worsen
congestion if not carefully planned. Reliable estimates allow planners to avoid
excessive delays during periods of roadwork. Since the exact increase in
congestion is difficult to predict analytically, traffic simulations are
commonly used to assess the redistribution of the flow of traffic. However,
when applied to long-term maintenance planning involving many overlapping
projects and scheduling alternatives, these simulations must be run thousands
of times, resulting in a significant computational burden. This paper
investigates the use of machine learning-based surrogate models to predict
network-wide congestion caused by simultaneous road renovations. We frame the
problem as a supervised learning task, using one-hot encodings, engineered
traffic features, and heuristic approximations. A range of linear,
ensemble-based, probabilistic, and neural regression models is evaluated under
an online learning framework in which data progressively becomes available. The
experimental results show that the Costliest Subset Heuristic provides a
reasonable approximation when limited training data is available, and that most
regression models fail to outperform it, with the exception of XGBoost, which
achieves substantially better accuracy. In overall performance, XGBoost
significantly outperforms alternatives in a range of metrics, most strikingly
Mean Absolute Percentage Error (MAPE) and Pinball loss, where it achieves a
MAPE of 11% and outperforms the next-best model by 20% and 38% respectively.
This modeling approach has the potential to reduce the computational burden of
large-scale traffic assignment problems in maintenance planning.

</details>


### [209] [Quantifying Adversarial Uncertainty in Evidential Deep Learning using Conflict Resolution](https://arxiv.org/abs/2506.05937)
*Charmaine Barker,Daniel Bethell,Simos Gerasimou*

Main category: cs.LG

TL;DR: Conflict-aware Evidential Deep Learning (C-EDL) 是一种轻量级后处理不确定性量化方法，通过生成多样化的任务保留变换并量化表征分歧，显著提升了对抗性和分布外输入的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在高风险应用中的可靠性至关重要，但现有方法（如Evidential Deep Learning, EDL）容易受到对抗性扰动的影响，导致过度自信的错误预测。

Method: C-EDL通过为每个输入生成多样化的任务保留变换，并量化表征分歧来校准不确定性估计，无需重新训练模型。

Result: 实验表明，C-EDL在多种数据集、攻击类型和不确定性指标上显著优于现有EDL变体和基线方法，对分布外数据和对抗性数据的覆盖率分别降低了55%和90%。

Conclusion: C-EDL是一种高效且轻量的方法，能够显著提升深度学习模型在对抗性和分布外输入下的鲁棒性，同时保持高精度和低计算开销。

Abstract: Reliability of deep learning models is critical for deployment in high-stakes
applications, where out-of-distribution or adversarial inputs may lead to
detrimental outcomes. Evidential Deep Learning, an efficient paradigm for
uncertainty quantification, models predictions as Dirichlet distributions of a
single forward pass. However, EDL is particularly vulnerable to adversarially
perturbed inputs, making overconfident errors. Conflict-aware Evidential Deep
Learning (C-EDL) is a lightweight post-hoc uncertainty quantification approach
that mitigates these issues, enhancing adversarial and OOD robustness without
retraining. C-EDL generates diverse, task-preserving transformations per input
and quantifies representational disagreement to calibrate uncertainty estimates
when needed. C-EDL's conflict-aware prediction adjustment improves detection of
OOD and adversarial inputs, maintaining high in-distribution accuracy and low
computational overhead. Our experimental evaluation shows that C-EDL
significantly outperforms state-of-the-art EDL variants and competitive
baselines, achieving substantial reductions in coverage for OOD data (up to
55%) and adversarial data (up to 90%), across a range of datasets, attack
types, and uncertainty metrics.

</details>


### [210] [Exponential Family Variational Flow Matching for Tabular Data Generation](https://arxiv.org/abs/2506.05940)
*Andrés Guzmán-Cordero,Floor Eijkelboom,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: TabbyFlow是一种用于表格数据生成的变分流匹配方法，通过指数族变分流匹配（EF-VFM）处理混合连续和离散特征，实现了高效的数据驱动目标。


<details>
  <summary>Details</summary>
Motivation: 尽管去噪扩散和流匹配在生成建模中取得了重大进展，但在表格数据中的应用仍然有限，而表格数据在现实应用中非常普遍。

Method: 开发了TabbyFlow，引入EF-VFM方法，利用指数族分布表示异构数据类型，基于矩匹配实现高效学习。

Result: 在表格数据基准测试中表现出优于基线的性能。

Conclusion: TabbyFlow通过EF-VFM为混合数据类型提供了高效的生成建模方法，性能达到最先进水平。

Abstract: While denoising diffusion and flow matching have driven major advances in
generative modeling, their application to tabular data remains limited, despite
its ubiquity in real-world applications. To this end, we develop TabbyFlow, a
variational Flow Matching (VFM) method for tabular data generation. To apply
VFM to data with mixed continuous and discrete features, we introduce
Exponential Family Variational Flow Matching (EF-VFM), which represents
heterogeneous data types using a general exponential family distribution. We
hereby obtain an efficient, data-driven objective based on moment matching,
enabling principled learning of probability paths over mixed continuous and
discrete variables. We also establish a connection between variational flow
matching and generalized flow matching objectives based on Bregman divergences.
Evaluation on tabular data benchmarks demonstrates state-of-the-art performance
compared to baselines.

</details>


### [211] [Comparative Analysis of Modern Machine Learning Models for Retail Sales Forecasting](https://arxiv.org/abs/2506.05941)
*Luka Hobor,Mario Brcic,Lidija Polutnik,Ante Kapetanovic*

Main category: cs.LG

TL;DR: 论文研究了零售业中高精度销售预测的重要性，比较了基于树的集成方法和神经网络架构，发现基于树的模型在非插补数据上表现更优。


<details>
  <summary>Details</summary>
Motivation: 准确的销售预测对零售业至关重要，过高或过低的预测都会带来成本或声誉损失。本研究旨在评估不同预测模型在零售环境中的表现。

Method: 使用基于树的集成方法（如XGBoost和LightGBM）和神经网络架构（如N-BEATS和Temporal Fusion Transformer），在包含间歇性需求、缺失值和高产品周转率的零售数据集上进行实验。

Result: 基于树的模型在非插补数据上表现更优，而神经网络模型需要高级插补方法但仍难以处理零售数据的复杂性。

Conclusion: 研究强调了数据预处理的重要性，并为零售环境中的模型选择提供了实践指导。

Abstract: Accurate forecasting is key for all business planning. When estimated sales
are too high, brick-and-mortar retailers may incur higher costs due to unsold
inventories, higher labor and storage space costs, etc. On the other hand, when
forecasts underestimate the level of sales, firms experience lost sales,
shortages, and impact on the reputation of the retailer in their relevant
market. Accurate forecasting presents a competitive advantage for companies. It
facilitates the achievement of revenue and profit goals and execution of
pricing strategy and tactics. In this study, we provide an exhaustive
assessment of the forecasting models applied to a high-resolution
brick-and-mortar retail dataset. Our forecasting framework addresses the
problems found in retail environments, including intermittent demand, missing
values, and frequent product turnover. We compare tree-based ensembles (such as
XGBoost and LightGBM) and state-of-the-art neural network architectures
(including N-BEATS, NHITS, and the Temporal Fusion Transformer) across various
experimental settings. Our results show that localized modeling strategies
especially those using tree-based models on individual groups with non-imputed
data, consistently deliver superior forecasting accuracy and computational
efficiency. In contrast, neural models benefit from advanced imputation
methods, yet still fall short in handling the irregularities typical of
physical retail data. These results further practical understanding for model
selection in retail environment and highlight the significance of data
preprocessing to improve forecast performance.

</details>


### [212] [Additive decomposition of one-dimensional signals using Transformers](https://arxiv.org/abs/2506.05942)
*Samuele Salti,Andrea Pinto,Alessandro Lanza,Serena Morigi*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer架构的一维信号分解新方法，能够将信号分解为分段常数、平滑、纹理和噪声成分。


<details>
  <summary>Details</summary>
Motivation: 传统信号分解方法依赖数学模型，而深度学习模型在这一领域的应用尚未充分探索，具有潜力。

Method: 利用Transformer架构，通过合成数据训练模型，实现信号分解。

Result: 实验结果表明，该方法在相同分布信号上具有高精度。

Conclusion: 该方法为一维信号分解提供了新的深度学习解决方案，展示了Transformer架构的潜力。

Abstract: One-dimensional signal decomposition is a well-established and widely used
technique across various scientific fields. It serves as a highly valuable
pre-processing step for data analysis. While traditional decomposition
techniques often rely on mathematical models, recent research suggests that
applying the latest deep learning models to this problem presents an exciting,
unexplored area with promising potential. This work presents a novel method for
the additive decomposition of one-dimensional signals. We leverage the
Transformer architecture to decompose signals into their constituent
components: piece-wise constant, smooth (low-frequency oscillatory), textured
(high-frequency oscillatory), and a noise component. Our model, trained on
synthetic data, achieves excellent accuracy in modeling and decomposing input
signals from the same distribution, as demonstrated by the experimental
results.

</details>


### [213] [Learning Deterministic Policies with Policy Gradients in Constrained Markov Decision Processes](https://arxiv.org/abs/2506.05953)
*Alessandro Montenegro,Leonardo Cesani,Marco Mussi,Matteo Papini,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: C-PG是一种探索无关的算法，在梯度支配假设下具有全局最后迭代收敛保证，适用于约束强化学习问题。


<details>
  <summary>Details</summary>
Motivation: 解决约束强化学习问题，即在最大化预期回报的同时满足领域特定约束。

Method: 提出C-PG算法，支持基于动作或参数的探索策略，并在特定噪声模型下证明其收敛性。

Result: C-PG在约束控制任务中表现优于现有基线，尤其在训练后部署确定性策略时效果显著。

Conclusion: C-PG算法在理论和实践中均表现出色，为约束强化学习提供了有效的解决方案。

Abstract: Constrained Reinforcement Learning (CRL) addresses sequential decision-making
problems where agents are required to achieve goals by maximizing the expected
return while meeting domain-specific constraints. In this setting, policy-based
methods are widely used thanks to their advantages when dealing with
continuous-control problems. These methods search in the policy space with an
action-based or a parameter-based exploration strategy, depending on whether
they learn the parameters of a stochastic policy or those of a stochastic
hyperpolicy. We introduce an exploration-agnostic algorithm, called C-PG, which
enjoys global last-iterate convergence guarantees under gradient domination
assumptions. Furthermore, under specific noise models where the (hyper)policy
is expressed as a stochastic perturbation of the actions or of the parameters
of an underlying deterministic policy, we additionally establish global
last-iterate convergence guarantees of C-PG to the optimal deterministic
policy. This holds when learning a stochastic (hyper)policy and subsequently
switching off the stochasticity at the end of training, thereby deploying a
deterministic policy. Finally, we empirically validate both the action-based
(C-PGAE) and parameter-based (C-PGPE) variants of C-PG on constrained control
tasks, and compare them against state-of-the-art baselines, demonstrating their
effectiveness, in particular when deploying deterministic policies after
training.

</details>


### [214] [Pruning Spurious Subgraphs for Graph Out-of-Distribtuion Generalization](https://arxiv.org/abs/2506.05957)
*Tianjun Yao,Haoxuan Li,Yongqiang Chen,Tongliang Liu,Le Song,Eric Xing,Zhiqiang Shen*

Main category: cs.LG

TL;DR: PrunE是一种基于剪枝的图神经网络方法，通过消除虚假边以提高分布外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在训练和测试数据分布偏移时性能下降的问题，尤其是虚假边与目标标签强相关的情况。

Method: 提出PrunE方法，通过两种正则化项剪枝虚假边：图大小约束和ε-概率对齐。

Result: 理论分析和实验表明，PrunE在分布外泛化性能上显著优于现有方法。

Conclusion: PrunE通过剪枝虚假边有效保留了不变子图，提升了图神经网络的分布外泛化能力。

Abstract: Graph Neural Networks (GNNs) often encounter significant performance
degradation under distribution shifts between training and test data, hindering
their applicability in real-world scenarios. Recent studies have proposed
various methods to address the out-of-distribution generalization challenge,
with many methods in the graph domain focusing on directly identifying an
invariant subgraph that is predictive of the target label. However, we argue
that identifying the edges from the invariant subgraph directly is challenging
and error-prone, especially when some spurious edges exhibit strong
correlations with the targets. In this paper, we propose PrunE, the first
pruning-based graph OOD method that eliminates spurious edges to improve OOD
generalizability. By pruning spurious edges, \mine{} retains the invariant
subgraph more comprehensively, which is critical for OOD generalization.
Specifically, PrunE employs two regularization terms to prune spurious edges:
1) graph size constraint to exclude uninformative spurious edges, and 2)
$\epsilon$-probability alignment to further suppress the occurrence of spurious
edges. Through theoretical analysis and extensive experiments, we show that
PrunE achieves superior OOD performance and outperforms previous
state-of-the-art methods significantly. Codes are available at:
\href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}.

</details>


### [215] [AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models](https://arxiv.org/abs/2506.05960)
*Adil Hasan,Thomas Peyrin*

Main category: cs.LG

TL;DR: 论文提出了一种基于向量量化的扩散模型压缩方法，显著降低了硬件资源需求，并在低比特量化下取得了优于全精度模型的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的大规模应用受限于其高硬件资源需求，现有量化方法主要集中于均匀标量量化，而向量量化在大型语言模型中表现优异，因此探索其在扩散模型中的应用。

Method: 采用基于码书的加性向量量化方法对扩散模型进行压缩，并在LDM-4模型上验证其效果。

Result: 在极低比特量化（W4A8和W2A8）下，该方法在ImageNet基准测试中优于全精度模型，并实现了FLOPs的节省。

Conclusion: 向量量化在扩散模型压缩中具有显著优势，为低比特量化提供了新的解决方案。

Abstract: Significant investments have been made towards the commodification of
diffusion models for generation of diverse media. Their mass-market adoption is
however still hobbled by the intense hardware resource requirements of
diffusion model inference. Model quantization strategies tailored specifically
towards diffusion models have been useful in easing this burden, yet have
generally explored the Uniform Scalar Quantization (USQ) family of quantization
methods. In contrast, Vector Quantization (VQ) methods, which operate on groups
of multiple related weights as the basic unit of compression, have seen
substantial success in Large Language Model (LLM) quantization. In this work,
we apply codebook-based additive vector quantization to the problem of
diffusion model compression. Our resulting approach achieves a new Pareto
frontier for the extremely low-bit weight quantization on the standard
class-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps.
Notably, we report sFID 1.92 points lower than the full-precision model at W4A8
and the best-reported results for FID, sFID and ISC at W2A8. We are also able
to demonstrate FLOPs savings on arbitrary hardware via an efficient inference
kernel, as opposed to savings resulting from small integer operations which may
lack broad hardware support.

</details>


### [216] [Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning](https://arxiv.org/abs/2506.05968)
*Motoki Omura,Kazuki Ota,Takayuki Osa,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.LG

TL;DR: 论文探讨了在连续动作空间中，将贝尔曼最优算子引入actor-critic框架的效果，提出了一种退火方法以平衡学习速度和偏差，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 连续动作空间的RL算法通常依赖策略更新，导致样本效率低。研究旨在通过引入贝尔曼最优算子加速学习，同时解决由此带来的高估偏差问题。

Method: 提出了一种退火方法，逐步从贝尔曼最优算子过渡到贝尔曼算子，结合TD3和SAC算法进行实验验证。

Result: 实验表明，该方法在多种运动和操作任务中显著优于现有方法，提升了性能和超参数鲁棒性。

Conclusion: 通过退火方法平衡贝尔曼最优算子的引入，能够有效加速学习并减少偏差，为连续动作空间的RL提供了新思路。

Abstract: For continuous action spaces, actor-critic methods are widely used in online
reinforcement learning (RL). However, unlike RL algorithms for discrete
actions, which generally model the optimal value function using the Bellman
optimality operator, RL algorithms for continuous actions typically model
Q-values for the current policy using the Bellman operator. These algorithms
for continuous actions rely exclusively on policy updates for improvement,
which often results in low sample efficiency. This study examines the
effectiveness of incorporating the Bellman optimality operator into
actor-critic frameworks. Experiments in a simple environment show that modeling
optimal values accelerates learning but leads to overestimation bias. To
address this, we propose an annealing approach that gradually transitions from
the Bellman optimality operator to the Bellman operator, thereby accelerating
learning while mitigating bias. Our method, combined with TD3 and SAC,
significantly outperforms existing approaches across various locomotion and
manipulation tasks, demonstrating improved performance and robustness to
hyperparameters related to optimality.

</details>


### [217] [On Measuring Long-Range Interactions in Graph Neural Networks](https://arxiv.org/abs/2506.05971)
*Jacob Bamberger,Benjamin Gutteridge,Scott le Roux,Michael M. Bronstein,Xiaowen Dong*

Main category: cs.LG

TL;DR: 论文提出了一种形式化的长程图任务交互方法，并引入了一种图操作的范围度量，通过合成实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的长程图任务研究缺乏理论基础和鲁棒性，需要一种更原则性的方法来定义和解决问题。

Method: 形式化长程图任务交互，引入图操作的范围度量，并通过合成实验验证。

Result: 验证了范围度量的有效性，并分析了常用任务和架构的长程特性。

Conclusion: 该工作为长程图问题的定义和解决提供了理论基础，范围度量有助于新数据集和架构的评估。

Abstract: Long-range graph tasks -- those dependent on interactions between distant
nodes -- are an open problem in graph neural network research. Real-world
benchmark tasks, especially the Long Range Graph Benchmark, have become popular
for validating the long-range capability of proposed architectures. However,
this is an empirical approach that lacks both robustness and theoretical
underpinning; a more principled characterization of the long-range problem is
required. To bridge this gap, we formalize long-range interactions in graph
tasks, introduce a range measure for operators on graphs, and validate it with
synthetic experiments. We then leverage our measure to examine commonly used
tasks and architectures, and discuss to what extent they are, in fact,
long-range. We believe our work advances efforts to define and address the
long-range problem on graphs, and that our range measure will aid evaluation of
new datasets and architectures.

</details>


### [218] [Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning](https://arxiv.org/abs/2506.05977)
*Yujia Huo,Jianchun Liu,Hongli Xu,Zhenguo Ma,Shilong Wang,Liusheng Huang*

Main category: cs.LG

TL;DR: FedBE是一种新的联邦微调框架，通过自适应扩展和动态分配可训练块，解决了分布式环境中灾难性遗忘的问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦微调方法在分布式环境中难以有效解决灾难性遗忘问题，且数据分布和设备能力差异加剧了这一问题。

Method: FedBE结合自适应Transformer块扩展机制和动态可训练块分配策略，分离新学知识与预训练表示，并根据客户端数据分布和能力动态分配块。

Result: 实验表明，FedBE在通用任务上的准确率保留提高了12-74%，模型收敛速度加快1.9-3.1倍，且不影响下游任务准确率。

Conclusion: FedBE有效解决了联邦微调中的灾难性遗忘问题，提升了模型在异构环境中的泛化能力和性能。

Abstract: Federated fine-tuning (FedFT) of large language models (LLMs) has emerged as
a promising solution for adapting models to distributed data environments while
ensuring data privacy.
  Existing FedFT methods predominantly utilize parameter-efficient fine-tuning
(PEFT) techniques to reduce communication and computation overhead.
  However, they often fail to adequately address the catastrophic forgetting, a
critical challenge arising from continual adaptation in distributed
environments. The traditional centralized fine-tuning methods, which are not
designed for the heterogeneous and privacy-constrained nature of federated
environments, struggle to mitigate this issue effectively. Moreover, the
challenge is further exacerbated by significant variation in data distributions
and device capabilities across clients, which leads to intensified forgetting
and degraded model generalization. To tackle these issues, we propose FedBE, a
novel FedFT framework that integrates an adaptive transformer block expansion
mechanism with a dynamic trainable-block allocation strategy. Specifically,
FedBE expands trainable blocks within the model architecture, structurally
separating newly learned task-specific knowledge from the original pre-trained
representations. Additionally, FedBE dynamically assigns these trainable blocks
to clients based on their data distributions and computational capabilities.
This enables the framework to better accommodate heterogeneous federated
environments and enhances the generalization ability of the model.Extensive
experiments show that compared with existing federated fine-tuning methods,
FedBE achieves 12-74% higher accuracy retention on general tasks after
fine-tuning and a model convergence acceleration ratio of 1.9-3.1x without
degrading the accuracy of downstream tasks.

</details>


### [219] [AMPED: Adaptive Multi-objective Projection for balancing Exploration and skill Diversification](https://arxiv.org/abs/2506.05980)
*Geonwoo Cho,Jaemoon Lee,Jaegyun Im,Subi Lee,Jihwan Lee,Sundong Kim*

Main category: cs.LG

TL;DR: AMPED是一种新的技能强化学习方法，通过梯度手术技术平衡探索与技能多样性，并在下游任务中动态选择技能，性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时优化探索与技能多样性这两个冲突目标，AMPED旨在解决这一问题。

Method: AMPED通过梯度手术技术平衡探索与多样性目标，并引入技能选择模块动态适配下游任务。

Result: AMPED在多个基准测试中表现优于现有技能强化学习方法。

Conclusion: AMPED证明了显式协调探索与多样性的重要性，为鲁棒且通用的技能学习提供了有效方法。

Abstract: Skill-based reinforcement learning (SBRL) enables rapid adaptation in
environments with sparse rewards by pretraining a skill-conditioned policy.
Effective skill learning requires jointly maximizing both exploration and skill
diversity. However, existing methods often face challenges in simultaneously
optimizing for these two conflicting objectives. In this work, we propose a new
method, Adaptive Multi-objective Projection for balancing Exploration and skill
Diversification (AMPED), which explicitly addresses both exploration and skill
diversification. We begin by conducting extensive ablation studies to identify
and define a set of objectives that effectively capture the aspects of
exploration and skill diversity, respectively. During the skill pretraining
phase, AMPED introduces a gradient surgery technique to balance the objectives
of exploration and skill diversity, mitigating conflicts and reducing reliance
on heuristic tuning. In the subsequent fine-tuning phase, AMPED incorporates a
skill selector module that dynamically selects suitable skills for downstream
tasks, based on task-specific performance signals. Our approach achieves
performance that surpasses SBRL baselines across various benchmarks. These
results highlight the importance of explicitly harmonizing exploration and
diversity and demonstrate the effectiveness of AMPED in enabling robust and
generalizable skill learning. Project Page: https://geonwoo.me/amped/

</details>


### [220] [Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning](https://arxiv.org/abs/2506.05985)
*Yuheng Lei,Sitong Mao,Shunbo Zhou,Hongyuan Zhang,Xuelong Li,Ping Luo*

Main category: cs.LG

TL;DR: 论文提出了一种动态混合渐进参数高效专家库（DMPEL）方法，用于终身机器人学习，解决了现有方法在任务标识依赖和知识共享限制上的不足。


<details>
  <summary>Details</summary>
Motivation: 终身学习中的现有方法依赖于不切实际的任务标识假设，并限制了孤立适配器之间的知识共享，需要一种更高效且灵活的方法。

Method: DMPEL通过渐进学习低秩专家库，并利用轻量级路由器动态组合专家，形成端到端策略，同时引入系数重放以减少灾难性遗忘。

Result: 在LIBERO终身操作基准测试中，DMPEL在持续适应任务中的成功率优于现有方法，且参数和存储需求极低。

Conclusion: DMPEL为终身学习提供了一种高效且灵活的解决方案，显著提升了性能并降低了资源消耗。

Abstract: A generalist agent must continuously learn and adapt throughout its lifetime,
achieving efficient forward transfer while minimizing catastrophic forgetting.
Previous work within the dominant pretrain-then-finetune paradigm has explored
parameter-efficient fine-tuning for single-task adaptation, effectively
steering a frozen pretrained model with a small number of parameters. However,
in the context of lifelong learning, these methods rely on the impractical
assumption of a test-time task identifier and restrict knowledge sharing among
isolated adapters. To address these limitations, we propose Dynamic Mixture of
Progressive Parameter-Efficient Expert Library (DMPEL) for lifelong robot
learning. DMPEL progressively learn a low-rank expert library and employs a
lightweight router to dynamically combine experts into an end-to-end policy,
facilitating flexible behavior during lifelong adaptation. Moreover, by
leveraging the modular structure of the fine-tuned parameters, we introduce
coefficient replay to guide the router in accurately retrieving frozen experts
for previously encountered tasks, thereby mitigating catastrophic forgetting.
This method is significantly more storage- and computationally-efficient than
applying demonstration replay to the entire policy. Extensive experiments on
the lifelong manipulation benchmark LIBERO demonstrate that our framework
outperforms state-of-the-art lifelong learning methods in success rates across
continual adaptation, while utilizing minimal trainable parameters and storage.

</details>


### [221] [RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory](https://arxiv.org/abs/2506.05994)
*Yi-Chun Liao,Chieh-Lin Tsai,Yuan-Hao Chang,Camélia Slimani,Jalil Boukhobza,Tei-Wei Kuo*

Main category: cs.LG

TL;DR: RETENTION框架通过迭代剪枝和树映射方案，显著降低了树模型推理中CAM的容量需求，同时保持精度损失在3%以内。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在非结构化数据上表现优异，但树模型在结构化数据上仍占优势。现有CAM加速方案存在内存消耗高和利用率低的问题。

Method: 提出迭代剪枝算法和树映射方案，优化CAM使用，减少冗余。

Result: RETENTION框架实现空间效率提升1.46×至207.12×，精度损失小于3%。

Conclusion: RETENTION为树模型加速提供了高效资源利用的解决方案。

Abstract: Although deep learning has demonstrated remarkable capabilities in learning
from unstructured data, modern tree-based ensemble models remain superior in
extracting relevant information and learning from structured datasets. While
several efforts have been made to accelerate tree-based models, the inherent
characteristics of the models pose significant challenges for conventional
accelerators. Recent research leveraging content-addressable memory (CAM)
offers a promising solution for accelerating tree-based models, yet existing
designs suffer from excessive memory consumption and low utilization. This work
addresses these challenges by introducing RETENTION, an end-to-end framework
that significantly reduces CAM capacity requirement for tree-based model
inference. We propose an iterative pruning algorithm with a novel pruning
criterion tailored for bagging-based models (e.g., Random Forest), which
minimizes model complexity while ensuring controlled accuracy degradation.
Additionally, we present a tree mapping scheme that incorporates two innovative
data placement strategies to alleviate the memory redundancy caused by the
widespread use of don't care states in CAM. Experimental results show that
implementing the tree mapping scheme alone achieves $1.46\times$ to $21.30
\times$ better space efficiency, while the full RETENTION framework yields
$4.35\times$ to $207.12\times$ improvement with less than 3% accuracy loss.
These results demonstrate that RETENTION is highly effective in reducing CAM
capacity requirement, providing a resource-efficient direction for tree-based
model acceleration.

</details>


### [222] [Machine learning for in-situ composition mapping in a self-driving magnetron sputtering system](https://arxiv.org/abs/2506.05999)
*Sanna Jarl,Jens Sjölund,Robert J. W. Frost,Anders Holst,Jonathan J. S. Scragg*

Main category: cs.LG

TL;DR: 本文提出了一种基于磁控共溅射的自驱动实验室（SDL）方法，利用机器学习和主动学习快速生成多元素组合薄膜的成分分布图，避免了传统耗时且易出错的离线分析。


<details>
  <summary>Details</summary>
Motivation: 在薄膜科学中，现有的SDL主要局限于溶液合成方法，难以覆盖无机材料的广泛化学空间。因此，需要一种能够快速、准确且无需校准的方法来扩展SDL的应用范围。

Method: 通过磁控共溅射技术，结合石英晶体微天平传感器进行原位测量，利用高斯过程（GPs）和主动学习（BALM）预测多元素薄膜的成分分布。

Result: 实验验证表明，该方法在10次实验内即可学习单个源的沉积速率，且预测精度高，显著提高了材料探索的效率。

Conclusion: 该框架通过避免繁琐的校准和表征，展示了机器学习引导的SDL在加速材料探索中的巨大潜力。

Abstract: Self-driving labs (SDLs), employing automation and machine learning (ML) to
accelerate experimental procedures, have enormous potential in the discovery of
new materials. However, in thin film science, SDLs are mainly restricted to
solution-based synthetic methods which are easier to automate but cannot access
the broad chemical space of inorganic materials. This work presents an SDL
based on magnetron co-sputtering. We are using combinatorial frameworks,
obtaining accurate composition maps on multi-element, compositionally graded
thin films. This normally requires time-consuming ex-situ analysis prone to
systematic errors. We present a rapid and calibration-free in-situ, ML driven
approach to produce composition maps for arbitrary source combinations and
sputtering conditions. We develop a method to predict the composition
distribution in a multi-element combinatorial thin film, using in-situ
measurements from quartz-crystal microbalance sensors placed in a sputter
chamber. For a given source, the sensor readings are learned as a function of
the sputtering pressure and magnetron power, through active learning using
Gaussian processes (GPs). The final GPs are combined with a geometric model of
the deposition flux distribution in the chamber, which allows interpolation of
the deposition rates from each source, at any position across the sample. We
investigate several acquisition functions for the ML procedure. A fully
Bayesian GP - BALM (Bayesian active learning MacKay) - achieved the best
performance, learning the deposition rates for a single source in 10
experiments. Prediction accuracy for co-sputtering composition distributions
was verified experimentally. Our framework dramatically increases throughput by
avoiding the need for extensive characterisation or calibration, thus
demonstrating the potential of ML-guided SDLs to accelerate materials
exploration.

</details>


### [223] [LaDEEP: A Deep Learning-based Surrogate Model for Large Deformation of Elastic-Plastic Solids](https://arxiv.org/abs/2506.06001)
*Shilong Tao,Zhe Feng,Haonan Sun,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.LG

TL;DR: LaDEEP是一个基于深度学习的代理模型，用于解决弹性-塑性固体的大变形问题，通过分区编码和Transformer模块实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在精度和效率之间存在固有折衷，而现有深度学习模型未针对特定问题特性设计，难以处理复杂弹性-塑性固体变形。

Method: 将分区区域编码为令牌序列，设计两阶段Transformer模块预测变形。

Result: LaDEEP比有限元方法快五个数量级，精度相当，且比其他深度学习基线平均提升20.47%。

Conclusion: LaDEEP在工业系统中表现出色，证明了其在精度和效率上的优越性。

Abstract: Scientific computing for large deformation of elastic-plastic solids is
critical for numerous real-world applications. Classical numerical solvers rely
primarily on local discrete linear approximation and are constrained by an
inherent trade-off between accuracy and efficiency. Recently, deep learning
models have achieved impressive progress in solving the continuum mechanism.
While previous models have explored various architectures and constructed
coefficient-solution mappings, they are designed for general instances without
considering specific problem properties and hard to accurately handle with
complex elastic-plastic solids involving contact, loading and unloading. In
this work, we take stretch bending, a popular metal fabrication technique, as
our case study and introduce LaDEEP, a deep learning-based surrogate model for
\textbf{La}rge \textbf{De}formation of \textbf{E}lastic-\textbf{P}lastic
Solids. We encode the partitioned regions of the involved slender solids into a
token sequence to maintain their essential order property. To characterize the
physical process of the solid deformation, a two-stage Transformer-based module
is designed to predict the deformation with the sequence of tokens as input.
Empirically, LaDEEP achieves five magnitudes faster speed than finite element
methods with a comparable accuracy, and gains 20.47\% relative improvement on
average compared to other deep learning baselines. We have also deployed our
model into a real-world industrial production system, and it has shown
remarkable performance in both accuracy and efficiency.

</details>


### [224] [What Really is a Member? Discrediting Membership Inference via Poisoning](https://arxiv.org/abs/2506.06003)
*Neal Mangaokar,Ashish Hooda,Zhuohang Li,Bradley A. Malin,Kassem Fawaz,Somesh Jha,Atul Prakash,Amrita Roy Chowdhury*

Main category: cs.LG

TL;DR: 论文探讨了成员推断测试在语义邻居定义下的不可靠性，并提出了一种训练数据投毒攻击，揭示了测试准确性与抗投毒能力之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证成员推断测试在放宽定义（包括语义邻居）后是否仍然可靠，并探索其对抗投毒攻击的脆弱性。

Method: 方法包括理论分析测试准确性与抗投毒能力的权衡，并提出一种具体的训练数据投毒攻击实例。

Result: 结果表明，投毒攻击能显著降低现有测试的性能，甚至使其低于随机猜测水平。

Conclusion: 结论指出成员推断测试在放宽定义后仍不可靠，且存在准确性与抗投毒能力的根本权衡。

Abstract: Membership inference tests aim to determine whether a particular data point
was included in a language model's training set. However, recent works have
shown that such tests often fail under the strict definition of membership
based on exact matching, and have suggested relaxing this definition to include
semantic neighbors as members as well. In this work, we show that membership
inference tests are still unreliable under this relaxation - it is possible to
poison the training dataset in a way that causes the test to produce incorrect
predictions for a target point. We theoretically reveal a trade-off between a
test's accuracy and its robustness to poisoning. We also present a concrete
instantiation of this poisoning attack and empirically validate its
effectiveness. Our results show that it can degrade the performance of existing
tests to well below random.

</details>


### [225] [LightGTS: A Lightweight General Time Series Forecasting Model](https://arxiv.org/abs/2506.06005)
*Yihang Wang,Yuying Qiu,Peng Chen,Yang Shu,Zhongwen Rao,Lujia Pan,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: LightGTS是一种轻量级通用时间序列预测模型，通过周期性建模和并行解码技术，在高效的同时实现卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型参数庞大，计算负担重，资源受限场景下表现不佳。

Method: 提出Periodical Tokenization提取多源数据中的周期性模式，以及Periodical Parallel Decoding利用历史令牌优化预测。

Result: 在9个真实基准测试中，零样本和全样本设置下均达到最优性能，且效率显著提升。

Conclusion: LightGTS通过轻量级设计和高效率，为通用时间序列预测提供了新解决方案。

Abstract: Existing works on general time series forecasting build foundation models
with heavy model parameters through large-scale multi-source pre-training.
These models achieve superior generalization ability across various datasets at
the cost of significant computational burdens and limitations in
resource-constrained scenarios. This paper introduces LightGTS, a lightweight
general time series forecasting model designed from the perspective of
consistent periodical modeling. To handle diverse scales and intrinsic periods
in multi-source pre-training, we introduce Periodical Tokenization, which
extracts consistent periodic patterns across different datasets with varying
scales. To better utilize the periodicity in the decoding process, we further
introduce Periodical Parallel Decoding, which leverages historical tokens to
improve forecasting. Based on the two techniques above which fully leverage the
inductive bias of periods inherent in time series, LightGTS uses a lightweight
model to achieve outstanding performance on general time series forecasting. It
achieves state-of-the-art forecasting performance on 9 real-world benchmarks in
both zero-shot and full-shot settings with much better efficiency compared with
existing time series foundation models.

</details>


### [226] [Unisoma: A Unified Transformer-based Solver for Multi-Solid Systems](https://arxiv.org/abs/2506.06021)
*Shilong Tao,Zhe Feng,Haonan Sun,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.LG

TL;DR: 论文提出了一种新的显式建模方法Unisoma，用于处理多固体系统的复杂相互作用，通过结构化模块和Transformer架构实现，性能优于现有隐式建模方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法主要依赖隐式建模，难以准确捕捉多固体系统的复杂物理相互作用，因此需要一种更有效的显式建模方法。

Method: 提出Unisoma模型，采用结构化模块（如接触模块和自适应交互分配机制）和三重关系学习变形，直接捕捉物理相互作用。

Result: Unisoma在七个数据集和两个复杂任务上均达到最先进性能。

Conclusion: 显式建模更适合多固体系统，Unisoma通过结构化模块和Transformer架构实现了高效建模和性能提升。

Abstract: Multi-solid systems are foundational to a wide range of real-world
applications, yet modeling their complex interactions remains challenging.
Existing deep learning methods predominantly rely on implicit modeling, where
the factors influencing solid deformation are not explicitly represented but
are instead indirectly learned. However, as the number of solids increases,
these methods struggle to accurately capture intricate physical interactions.
In this paper, we introduce a novel explicit modeling paradigm that
incorporates factors influencing solid deformation through structured modules.
Specifically, we present Unisoma, a unified and flexible Transformer-based
model capable of handling variable numbers of solids. Unisoma directly captures
physical interactions using contact modules and adaptive interaction allocation
mechanism, and learns the deformation through a triplet relationship. Compared
to implicit modeling techniques, explicit modeling is more well-suited for
multi-solid systems with diverse coupling patterns, as it enables detailed
treatment of each solid while preventing information blending and confusion.
Experimentally, Unisoma achieves consistent state-of-the-art performance across
seven well-established datasets and two complex multi-solid tasks. Code is
avaiable at \href{this link}{https://github.com/therontau0054/Unisoma}.

</details>


### [227] [Do-PFN: In-Context Learning for Causal Effect Estimation](https://arxiv.org/abs/2506.06039)
*Jake Robertson,Arik Reuter,Siyuan Guo,Noah Hollmann,Frank Hutter,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 论文提出了一种基于预训练网络（PFNs）的方法，用于在不知道因果图的情况下估计因果效应，通过合成数据训练并在实验中验证了其准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有因果效应估计方法需要干预数据或因果图知识，限制了实际应用。本文旨在探索PFNs在因果效应估计中的潜力。

Method: 预训练PFNs于多种因果结构的合成数据，包括干预数据，以通过上下文学习预测干预结果。

Result: 实验表明，该方法无需因果图知识即可准确估计因果效应，且在不同因果特性的数据集中表现出可扩展性和鲁棒性。

Conclusion: PFNs在因果效应估计中具有潜力，为无需因果图知识的实际应用提供了新思路。

Abstract: Estimation of causal effects is critical to a range of scientific
disciplines. Existing methods for this task either require interventional data,
knowledge about the ground truth causal graph, or rely on assumptions such as
unconfoundedness, restricting their applicability in real-world settings. In
the domain of tabular machine learning, Prior-data fitted networks (PFNs) have
achieved state-of-the-art predictive performance, having been pre-trained on
synthetic data to solve tabular prediction problems via in-context learning. To
assess whether this can be transferred to the harder problem of causal effect
estimation, we pre-train PFNs on synthetic data drawn from a wide variety of
causal structures, including interventions, to predict interventional outcomes
given observational data. Through extensive experiments on synthetic case
studies, we show that our approach allows for the accurate estimation of causal
effects without knowledge of the underlying causal graph. We also perform
ablation studies that elucidate Do-PFN's scalability and robustness across
datasets with a variety of causal characteristics.

</details>


### [228] [Diffusion-Based Hierarchical Graph Neural Networks for Simulating Nonlinear Solid Mechanics](https://arxiv.org/abs/2506.06045)
*Tobias Würth,Niklas Freymuth,Gerhard Neumann,Luise Kärger*

Main category: cs.LG

TL;DR: 论文提出了一种名为ROBIN的新型学习模拟器，通过Rolling Diffusion和分层图神经网络解决了现有方法在捕捉全局现象和长期误差累积上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的学习模拟器在捕捉全局现象（如弯曲或长程相关性）和长期误差累积方面表现不佳，主要依赖局部消息传递和直接下一步预测。

Method: ROBIN结合了两种创新技术：(i) Rolling Diffusion，一种并行化推理方案，通过时间窗口重叠去噪步骤来分摊扩散细化成本；(ii) 基于代数多重网格粗化的分层图神经网络，实现多尺度消息传递。

Result: 在涉及几何、材料和接触非线性的2D和3D固体力学基准测试中，ROBIN在所有任务上均达到最先进精度，显著优于现有方法，并将推理时间减少一个数量级。

Conclusion: ROBIN通过创新设计有效解决了现有模拟器的局限性，在精度和效率上均有显著提升。

Abstract: Graph-based learned simulators have emerged as a promising approach for
simulating physical systems on unstructured meshes, offering speed and
generalization across diverse geometries. However, they often struggle with
capturing global phenomena, such as bending or long-range correlations, and
suffer from error accumulation over long rollouts due to their reliance on
local message passing and direct next-step prediction. We address these
limitations by introducing the Rolling Diffusion-Batched Inference Network
(ROBIN), a novel learned simulator that integrates two key innovations: (i)
Rolling Diffusion, a parallelized inference scheme that amortizes the cost of
diffusion-based refinement across physical time steps by overlapping denoising
steps across a temporal window. (ii) A Hierarchical Graph Neural Network built
on algebraic multigrid coarsening, enabling multiscale message passing across
different mesh resolutions. This architecture, implemented via
Algebraic-hierarchical Message Passing Networks, captures both fine-scale local
dynamics and global structural effects critical for phenomena like beam bending
or multi-body contact. We validate ROBIN on challenging 2D and 3D solid
mechanics benchmarks involving geometric, material, and contact nonlinearities.
ROBIN achieves state-of-the-art accuracy on all tasks, substantially
outperforming existing next-step learned simulators while reducing inference
time by up to an order of magnitude compared to standard diffusion simulators.

</details>


### [229] [TRUST: Test-time Resource Utilization for Superior Trustworthiness](https://arxiv.org/abs/2506.06048)
*Haripriya Harikumar,Santu Rana*

Main category: cs.LG

TL;DR: 提出一种新的测试时优化方法，通过减少分类器权重噪声，提升置信度估计的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统不确定性估计方法（如dropout）难以区分可靠与不可靠预测，主要因分类器权重噪声影响细粒度统计信息。

Method: 提出一种测试时优化方法，考虑噪声影响以生成更可靠的置信度估计，定义单调子集选择函数。

Result: 在AUSE和AURC等风险指标上表现优异，能有效识别训练与测试分布差异及区分分布内外样本。

Conclusion: 该方法显著提升置信度估计质量，并揭示了CNN与ViT分类器在视觉数据集上的关键差异。

Abstract: Standard uncertainty estimation techniques, such as dropout, often struggle
to clearly distinguish reliable predictions from unreliable ones. We attribute
this limitation to noisy classifier weights, which, while not impairing overall
class-level predictions, render finer-level statistics less informative. To
address this, we propose a novel test-time optimization method that accounts
for the impact of such noise to produce more reliable confidence estimates.
This score defines a monotonic subset-selection function, where population
accuracy consistently increases as samples with lower scores are removed, and
it demonstrates superior performance in standard risk-based metrics such as
AUSE and AURC. Additionally, our method effectively identifies discrepancies
between training and test distributions, reliably differentiates
in-distribution from out-of-distribution samples, and elucidates key
differences between CNN and ViT classifiers across various vision datasets.

</details>


### [230] [System-Aware Unlearning Algorithms: Use Lesser, Forget Faster](https://arxiv.org/abs/2506.06073)
*Linda Lu,Ayush Sekhari,Karthik Sridharan*

Main category: cs.LG

TL;DR: 论文提出了一种新的机器学习遗忘定义——系统感知遗忘，针对现实攻击者设计，通过减少存储需求提高安全性和效率，并提出了线性分类的精确算法。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘定义的严格性导致算法开发困难，且假设的攻击者过于强大，不切实际。因此，需要一种更现实的遗忘定义。

Method: 提出系统感知遗忘定义，基于选择性采样的方法设计精确遗忘算法，适用于线性分类和一般函数分类。

Result: 理论分析了删除容量、准确性、内存和计算时间之间的权衡。

Conclusion: 系统感知遗忘定义更实用，提出的算法在安全性和效率上表现良好。

Abstract: Machine unlearning addresses the problem of updating a machine learning
model/system trained on a dataset $S$ so that the influence of a set of
deletion requests $U \subseteq S$ on the unlearned model is minimized. The gold
standard definition of unlearning demands that the updated model, after
deletion, be nearly identical to the model obtained by retraining. This
definition is designed for a worst-case attacker (one who can recover not only
the unlearned model but also the remaining data samples, i.e., $S \setminus
U$). Such a stringent definition has made developing efficient unlearning
algorithms challenging. However, such strong attackers are also unrealistic. In
this work, we propose a new definition, system-aware unlearning, which aims to
provide unlearning guarantees against an attacker that can at best only gain
access to the data stored in the system for learning/unlearning requests and
not all of $S\setminus U$. With this new definition, we use the simple
intuition that if a system can store less to make its learning/unlearning
updates, it can be more secure and update more efficiently against a
system-aware attacker. Towards that end, we present an exact system-aware
unlearning algorithm for linear classification using a selective sampling-based
approach, and we generalize the method for classification with general function
classes. We theoretically analyze the tradeoffs between deletion capacity,
accuracy, memory, and computation time.

</details>


### [231] [Flexible Operator Fusion for Fast Sparse Transformer with Diverse Masking on GPU](https://arxiv.org/abs/2506.06095)
*Wenhao Dai,Haodong Deng,Mengfei Rong,Xinyu Yang,Hongyu Liu,Fangxin Liu,Hailong Yang,Weifeng Liu,Qingxiao Sun*

Main category: cs.LG

TL;DR: STOF框架通过灵活的掩码和GPU上的算子融合优化稀疏Transformer，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 加速Transformer并行化是热门研究，但稀疏Transformer的性能优化和混合算子融合机会常被忽视。

Method: 统一多头注意力的存储格式和内核实现，通过两阶段搜索引擎确定最佳参数设置。

Result: STOF在MHA计算和端到端推理中分别实现1.7倍和1.5倍的最大加速。

Conclusion: STOF有效解决了稀疏Transformer的性能优化问题，显著提升计算效率。

Abstract: Large language models are popular around the world due to their powerful
understanding capabilities. As the core component of LLMs, accelerating
Transformer through parallelization has gradually become a hot research topic.
Mask layers introduce sparsity into Transformer to reduce calculations.
However, previous works rarely focus on the performance optimization of sparse
Transformer. Moreover, rule-based mechanisms ignore the fusion opportunities of
mixed-type operators and fail to adapt to various sequence lengths. To address
the above problems, we propose STOF, a framework that incorporates
optimizations for Sparse Transformer via flexible masking and operator fusion
on GPU. We firstly unify the storage format and kernel implementation for the
multi-head attention. Then, we map fusion schemes to compilation templates and
determine the optimal parameter setting through a two-stage search engine. The
experimental results show that compared to the state-of-the-art work, STOF
achieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end
inference.

</details>


### [232] [Text-to-LoRA: Instant Transformer Adaption](https://arxiv.org/abs/2506.06105)
*Rujikorn Charakorn,Edoardo Cetin,Yujin Tang,Robert Tjarko Lange*

Main category: cs.LG

TL;DR: T2L是一种基于自然语言描述的即时适配大语言模型的方法，通过单次前向传播生成LoRA适配器，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法需要大量数据和计算资源，且对超参数敏感，T2L旨在解决这些问题。

Method: T2L是一种超网络，通过自然语言描述生成LoRA适配器，无需重复微调。

Result: T2L生成的适配器性能与任务专用适配器相当，并能零样本泛化到新任务。

Conclusion: T2L为大规模模型的适配提供了高效、低成本的新方法。

Abstract: While Foundation Models provide a general tool for rapid content creation,
they regularly require task-specific adaptation. Traditionally, this exercise
involves careful curation of datasets and repeated fine-tuning of the
underlying model. Fine-tuning techniques enable practitioners to adapt
foundation models for many new applications but require expensive and lengthy
training while being notably sensitive to hyperparameter choices. To overcome
these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting
large language models (LLMs) on the fly solely based on a natural language
description of the target task. T2L is a hypernetwork trained to construct
LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9
pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc
reconstructed LoRA instances match the performance of task-specific adapters
across the corresponding test sets. Furthermore, T2L can compress hundreds of
LoRA instances and zero-shot generalize to entirely unseen tasks. This approach
provides a significant step towards democratizing the specialization of
foundation models and enables language-based adaptation with minimal compute
requirements.
  Our code is available at https://github.com/SakanaAI/text-to-lora

</details>


### [233] [Synthetic Tabular Data: Methods, Attacks and Defenses](https://arxiv.org/abs/2506.06108)
*Graham Cormode,Samuel Maddock,Enayat Ullah,Shripad Gade*

Main category: cs.LG

TL;DR: 综述了表格合成数据生成的关键进展和主要概念，包括基于概率图模型和深度学习的方法，并探讨了合成数据的局限性及攻击手段。


<details>
  <summary>Details</summary>
Motivation: 解决敏感数据集的隐私问题，利用合成数据提供无限匹配数据源。

Method: 基于概率图模型和深度学习的合成数据生成方法。

Result: 总结了合成数据生成的技术进展，同时揭示了其潜在的信息泄露风险。

Conclusion: 合成数据在隐私保护方面有潜力，但仍需解决信息泄露等开放性问题。

Abstract: Synthetic data is often positioned as a solution to replace sensitive
fixed-size datasets with a source of unlimited matching data, freed from
privacy concerns. There has been much progress in synthetic data generation
over the last decade, leveraging corresponding advances in machine learning and
data analytics. In this survey, we cover the key developments and the main
concepts in tabular synthetic data generation, including paradigms based on
probabilistic graphical models and on deep learning. We provide background and
motivation, before giving a technical deep-dive into the methodologies. We also
address the limitations of synthetic data, by studying attacks that seek to
retrieve information about the original sensitive data. Finally, we present
extensions and open problems in this area.

</details>


### [234] [Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness](https://arxiv.org/abs/2506.06112)
*Cheng-Long Wang,Qi Li,Zihang Xiang,Yinzhi Cao,Di Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为IAM的框架，用于量化机器遗忘的完整性，解决了现有方法在计算资源和粒度上的局限性。


<details>
  <summary>Details</summary>
Motivation: 数据隐私和安全问题日益突出，机器遗忘（移除特定数据对模型的影响）变得重要，但现有方法（如MIA）存在计算资源消耗大和粒度不足的问题。

Method: 提出了Interpolated Approximate Measurement (IAM)框架，通过插值模型的泛化-拟合行为差距来量化样本级遗忘完整性。

Result: IAM在二进制包含测试中表现优异，适用于近似遗忘，且计算高效，仅需一个预训练的shadow模型。

Conclusion: IAM揭示了近似遗忘算法中过度遗忘和不足遗忘的风险，强调了加强近似遗忘系统安全性的必要性。

Abstract: Growing concerns over data privacy and security highlight the importance of
machine unlearning--removing specific data influences from trained models
without full retraining. Techniques like Membership Inference Attacks (MIAs)
are widely used to externally assess successful unlearning. However, existing
methods face two key limitations: (1) maximizing MIA effectiveness (e.g., via
online attacks) requires prohibitive computational resources, often exceeding
retraining costs; (2) MIAs, designed for binary inclusion tests, struggle to
capture granular changes in approximate unlearning. To address these
challenges, we propose the Interpolated Approximate Measurement (IAM), a
framework natively designed for unlearning inference. IAM quantifies
sample-level unlearning completeness by interpolating the model's
generalization-fitting behavior gap on queried samples. IAM achieves strong
performance in binary inclusion tests for exact unlearning and high correlation
for approximate unlearning--scalable to LLMs using just one pre-trained shadow
model. We theoretically analyze how IAM's scoring mechanism maintains
performance efficiently. We then apply IAM to recent approximate unlearning
algorithms, revealing general risks of both over-unlearning and
under-unlearning, underscoring the need for stronger safeguards in approximate
unlearning systems. The code is available at
https://github.com/Happy2Git/Unlearning_Inference_IAM.

</details>


### [235] [Scalable unsupervised feature selection via weight stability](https://arxiv.org/abs/2506.06114)
*Xudong Zhang,Renato Cordeiro de Amorim*

Main category: cs.LG

TL;DR: 提出了一种基于Minkowski加权k-means++的初始化策略，并开发了两种新特征选择算法FS-MWK++和SFS-MWK++，显著提升了高维数据聚类性能。


<details>
  <summary>Details</summary>
Motivation: 高维数据中无关特征会掩盖有意义的结构，无监督特征选择对提升聚类性能至关重要。

Method: 提出Minkowski加权k-means++初始化策略，并基于此开发FS-MWK++和SFS-MWK++两种特征选择算法。

Result: 理论保证和实验表明，新方法在多种情况下优于现有方法。

Conclusion: 新方法通过特征权重聚合和子采样，显著提升了高维数据聚类的效果。

Abstract: Unsupervised feature selection is critical for improving clustering
performance in high-dimensional data, where irrelevant features can obscure
meaningful structure. In this work, we introduce the Minkowski weighted
$k$-means++, a novel initialisation strategy for the Minkowski Weighted
$k$-means. Our initialisation selects centroids probabilistically using feature
relevance estimates derived from the data itself. Building on this, we propose
two new feature selection algorithms, FS-MWK++, which aggregates feature
weights across a range of Minkowski exponents to identify stable and
informative features, and SFS-MWK++, a scalable variant based on subsampling.
We support our approach with a theoretical guarantee under mild assumptions and
extensive experiments showing that our methods consistently outperform existing
alternatives.

</details>


### [236] [Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library](https://arxiv.org/abs/2506.06122)
*Weixun Wang,Shaopan Xiong,Gengru Chen,Wei Gao,Sheng Guo,Yancheng He,Ju Huang,Jiaheng Liu,Zhendong Li,Xiaoyang Li,Zichen Liu,Haizhou Zhao,Dakai An,Lunxi Cao,Qiyang Cao,Wanxi Deng,Feilei Du,Yiliang Gu,Jiahe Li,Xiang Li,Mingjie Liu,Yijia Luo,Zihe Liu,Yadao Wang,Pei Wang,Tianyuan Wu,Yanan Wu,Yuheng Zhao,Shuaibing Zhao,Jin Yang,Siran Yang,Yingshui Tan,Huimin Yi,Yuchi Xu,Yujin Yuan,Xingyao Zhang,Lin Qu,Wenbo Su,Wei Wang,Jiamang Wang,Bo Zheng*

Main category: cs.LG

TL;DR: ROLL是一个高效、可扩展且用户友好的强化学习优化库，针对大规模学习需求设计，服务于技术先驱、开发者和研究人员。


<details>
  <summary>Details</summary>
Motivation: 满足不同用户群体在大规模强化学习训练中的需求，包括成本效益、灵活控制和快速实验。

Method: 采用单控制器架构、并行策略模块、数据传递模块、rollout调度器、环境与奖励工作器，以及AutoDeviceMapping技术。

Result: 实现了高效、可扩展且灵活的强化学习训练流程。

Conclusion: ROLL为不同用户群体提供了强大的工具，支持大规模强化学习的优化与实验。

Abstract: We introduce ROLL, an efficient, scalable, and user-friendly library designed
for Reinforcement Learning Optimization for Large-scale Learning. ROLL caters
to three primary user groups: tech pioneers aiming for cost-effective,
fault-tolerant large-scale training, developers requiring flexible control over
training workflows, and researchers seeking agile experimentation. ROLL is
built upon several key modules to serve these user groups effectively. First, a
single-controller architecture combined with an abstraction of the parallel
worker simplifies the development of the training pipeline. Second, the
parallel strategy and data transfer modules enable efficient and scalable
training. Third, the rollout scheduler offers fine-grained management of each
sample's lifecycle during the rollout stage. Fourth, the environment worker and
reward worker support rapid and flexible experimentation with agentic RL
algorithms and reward designs. Finally, AutoDeviceMapping allows users to
assign resources to different models flexibly across various stages.

</details>


### [237] [Flow-Attentional Graph Neural Networks](https://arxiv.org/abs/2506.06127)
*Pascal Plettenberg,Dominik Köhler,Bernhard Sick,Josephine M. Thomas*

Main category: cs.LG

TL;DR: 提出了一种基于基尔霍夫第一定律的流注意力机制，提升了图神经网络在流图数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络未考虑流图数据中的守恒定律（如电流或交通流量），导致性能下降。

Method: 提出流注意力机制，改进现有图注意力机制以满足基尔霍夫第一定律。

Result: 在电路和电网数据集上，流注意力显著提升了图分类和回归任务的性能。

Conclusion: 流注意力机制能有效捕捉流图数据的守恒特性，提升模型表现。

Abstract: Graph Neural Networks (GNNs) have become essential for learning from
graph-structured data. However, existing GNNs do not consider the conservation
law inherent in graphs associated with a flow of physical resources, such as
electrical current in power grids or traffic in transportation networks, which
can lead to reduced model performance. To address this, we propose flow
attention, which adapts existing graph attention mechanisms to satisfy
Kirchhoff\'s first law. Furthermore, we discuss how this modification
influences the expressivity and identify sets of non-isomorphic graphs that can
be discriminated by flow attention but not by standard attention. Through
extensive experiments on two flow graph datasets (electronic circuits and power
grids), we demonstrate that flow attention enhances the performance of
attention-based GNNs on both graph-level classification and regression tasks.

</details>


### [238] [Gradient Similarity Surgery in Multi-Task Deep Learning](https://arxiv.org/abs/2506.06130)
*Thomas Borsani,Andrea Rosani,Giuseppe Nicosia,Giuseppe Di Fatta*

Main category: cs.LG

TL;DR: 论文提出了一种新的梯度手术方法SAM-GS，通过梯度相似性度量优化多任务深度学习中的梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: 多任务深度学习中的梯度冲突会阻碍任务的并发收敛，需要一种有效的方法来调整梯度轨迹。

Method: 提出了基于梯度幅度相似性的SAM-GS方法，结合梯度均衡和一阶动量的调制。

Result: 实验证明SAM-GS在合成问题和多任务基准测试中有效。

Conclusion: 梯度幅度相似性对多任务深度学习中的梯度聚合和优化过程具有重要作用。

Abstract: The multi-task learning ($MTL$) paradigm aims to simultaneously learn
multiple tasks within a single model capturing higher-level, more general
hidden patterns that are shared by the tasks. In deep learning, a significant
challenge in the backpropagation training process is the design of advanced
optimisers to improve the convergence speed and stability of the gradient
descent learning rule. In particular, in multi-task deep learning ($MTDL$) the
multitude of tasks may generate potentially conflicting gradients that would
hinder the concurrent convergence of the diverse loss functions. This challenge
arises when the gradients of the task objectives have either different
magnitudes or opposite directions, causing one or a few to dominate or to
interfere with each other, thus degrading the training process. Gradient
surgery methods address the problem explicitly dealing with conflicting
gradients by adjusting the overall gradient trajectory. This work introduces a
novel gradient surgery method, the Similarity-Aware Momentum Gradient Surgery
(SAM-GS), which provides an effective and scalable approach based on a gradient
magnitude similarity measure to guide the optimisation process. The SAM-GS
surgery adopts gradient equalisation and modulation of the first-order
momentum. A series of experimental tests have shown the effectiveness of SAM-GS
on synthetic problems and $MTL$ benchmarks. Gradient magnitude similarity plays
a crucial role in regularising gradient aggregation in $MTDL$ for the
optimisation of the learning process.

</details>


### [239] [Table-r1: Self-supervised and Reinforcement Learning for Program-based Table Reasoning in Small Language Models](https://arxiv.org/abs/2506.06137)
*Rihui Jin,Zheyu Xin,Xing Xie,Zuoyi Li,Guilin Qi,Yongrui Chen,Xinbang Dai,Tongtong Wu,Gholamreza Haffari*

Main category: cs.LG

TL;DR: 论文提出了一种针对小语言模型（SLMs）的表格推理方法Table-r1，通过程序化推理（P-TR）解决文本推理（T-TR）的局限性，并在布局泛化和推理一致性上进行了优化。


<details>
  <summary>Details</summary>
Motivation: 表格推理（TR）对小语言模型（SLMs）具有挑战性，程序化推理（P-TR）虽能解决文本推理（T-TR）的数值推理问题，但SLMs在布局泛化和代码生成能力上存在不足。

Method: 提出Table-r1方法，分为两阶段：1）通过自监督学习任务Layout Transformation Inference提升布局泛化；2）采用混合范式Group Relative Policy Optimization增强推理一致性，必要时动态回退到T-TR。

Result: 在四个TR基准测试中，Table-r1优于所有基于SLM的方法，准确率比基础模型（LLaMA-8B）提升至少15%，性能接近大语言模型（LLMs）。

Conclusion: Table-r1通过程序化推理和优化方法，显著提升了SLMs在表格推理任务中的表现，缩小了与LLMs的差距。

Abstract: Table reasoning (TR) requires structured reasoning over semi-structured
tabular data and remains challenging, particularly for small language models
(SLMs, e.g., LLaMA-8B) due to their limited capacity compared to large LMs
(LLMs, e.g., GPT-4o). To narrow this gap, we explore program-based TR (P-TR),
which circumvents key limitations of text-based TR (T-TR), notably in numerical
reasoning, by generating executable programs. However, applying P-TR to SLMs
introduces two challenges: (i) vulnerability to heterogeneity in table layouts,
and (ii) inconsistency in reasoning due to limited code generation capability.
We propose Table-r1, a two-stage P-TR method designed for SLMs. Stage 1
introduces an innovative self-supervised learning task, Layout Transformation
Inference, to improve tabular layout generalization from a programmatic view.
Stage 2 adopts a mix-paradigm variant of Group Relative Policy Optimization,
enhancing P-TR consistency while allowing dynamic fallback to T-TR when needed.
Experiments on four TR benchmarks demonstrate that Table-r1 outperforms all
SLM-based methods, achieving at least a 15% accuracy improvement over the base
model (LLaMA-8B) across all datasets and reaching performance competitive with
LLMs.

</details>


### [240] [carps: A Framework for Comparing N Hyperparameter Optimizers on M Benchmarks](https://arxiv.org/abs/2506.06143)
*Carolin Benjamins,Helena Graf,Sarah Segel,Difan Deng,Tim Ruhkopf,Leona Hennig,Soham Basu,Neeratyoy Mallik,Edward Bergman,Deyao Chen,François Clément,Matthias Feurer,Katharina Eggensperger,Frank Hutter,Carola Doerr,Marius Lindauer*

Main category: cs.LG

TL;DR: carps是一个用于超参数优化（HPO）方法评估的基准框架，提供多样化的任务和优化器，支持高效比较和分析。


<details>
  <summary>Details</summary>
Motivation: 简化HPO方法的原型设计和基准测试，推动标准化评估。

Method: 构建轻量级接口连接优化器和基准任务，并通过最小化子集星差异选择代表性任务。

Result: 提供3366个任务和28种优化器变体，提出10-30个代表性任务子集，并建立基线结果。

Conclusion: carps框架为HPO评估标准化迈出重要一步。

Abstract: Hyperparameter Optimization (HPO) is crucial to develop well-performing
machine learning models. In order to ease prototyping and benchmarking of HPO
methods, we propose carps, a benchmark framework for Comprehensive Automated
Research Performance Studies allowing to evaluate N optimizers on M benchmark
tasks. In this first release of carps, we focus on the four most important
types of HPO task types: blackbox, multi-fidelity, multi-objective and
multi-fidelity-multi-objective. With 3 336 tasks from 5 community benchmark
collections and 28 variants of 9 optimizer families, we offer the biggest go-to
library to date to evaluate and compare HPO methods. The carps framework relies
on a purpose-built, lightweight interface, gluing together optimizers and
benchmark tasks. It also features an analysis pipeline, facilitating the
evaluation of optimizers on benchmarks. However, navigating a huge number of
tasks while developing and comparing methods can be computationally infeasible.
To address this, we obtain a subset of representative tasks by minimizing the
star discrepancy of the subset, in the space spanned by the full set. As a
result, we propose an initial subset of 10 to 30 diverse tasks for each task
type, and include functionality to re-compute subsets as more benchmarks become
available, enabling efficient evaluations. We also establish a first set of
baseline results on these tasks as a measure for future comparisons. With carps
(https://www.github.com/automl/CARP-S), we make an important step in the
standardization of HPO evaluation.

</details>


### [241] [ENMA: Tokenwise Autoregression for Generative Neural PDE Operators](https://arxiv.org/abs/2506.06158)
*Armand Kassaï Koupaï,Lise Le Boudec,Louis Serrano,Patrick Gallinari*

Main category: cs.LG

TL;DR: ENMA是一种生成神经算子，用于建模物理现象中的时空动态，通过生成掩码自回归变换器和流匹配损失预测未来动态，支持对新PDE体系的一次性代理建模。


<details>
  <summary>Details</summary>
Motivation: 解决时间依赖参数偏微分方程（PDEs）的挑战，特别是在数据不确定或不完整时，需要一种能够泛化到广泛物理参数和动态的方法。

Method: ENMA使用生成掩码自回归变换器和流匹配损失在压缩潜在空间中预测动态，通过注意力机制和时空卷积编码器处理不规则采样数据。

Result: ENMA能够泛化到新的PDE体系，支持一次性代理建模，并具备上下文学习能力。

Conclusion: ENMA提供了一个鲁棒且适应性强的框架，适用于时间依赖参数PDEs的建模和预测。

Abstract: Solving time-dependent parametric partial differential equations (PDEs)
remains a fundamental challenge for neural solvers, particularly when
generalizing across a wide range of physical parameters and dynamics. When data
is uncertain or incomplete-as is often the case-a natural approach is to turn
to generative models. We introduce ENMA, a generative neural operator designed
to model spatio-temporal dynamics arising from physical phenomena. ENMA
predicts future dynamics in a compressed latent space using a generative masked
autoregressive transformer trained with flow matching loss, enabling tokenwise
generation. Irregularly sampled spatial observations are encoded into uniform
latent representations via attention mechanisms and further compressed through
a spatio-temporal convolutional encoder. This allows ENMA to perform in-context
learning at inference time by conditioning on either past states of the target
trajectory or auxiliary context trajectories with similar dynamics. The result
is a robust and adaptable framework that generalizes to new PDE regimes and
supports one-shot surrogate modeling of time-dependent parametric PDEs.

</details>


### [242] [The Lock-in Hypothesis: Stagnation by Algorithm](https://arxiv.org/abs/2506.06166)
*Tianyi Alex Qiu,Zhonghao He,Tejasveer Chugh,Max Kleiman-Weiner*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLM）与人类用户之间的反馈循环，发现这种循环可能导致信念固化、多样性丧失和虚假信念锁定。通过模拟和真实数据验证了这一假设。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM与人类用户之间的反馈循环如何影响信念多样性和真实性，揭示潜在的负面影响。

Method: 使用基于代理的LLM模拟和真实GPT使用数据，形式化并验证反馈循环假设。

Result: 分析显示，新GPT版本发布后，多样性突然且持续下降，验证了反馈循环的存在。

Conclusion: LLM与人类的反馈循环可能导致信念固化和多样性丧失，需引起关注。

Abstract: The training and deployment of large language models (LLMs) create a feedback
loop with human users: models learn human beliefs from data, reinforce these
beliefs with generated content, reabsorb the reinforced beliefs, and feed them
back to users again and again. This dynamic resembles an echo chamber. We
hypothesize that this feedback loop entrenches the existing values and beliefs
of users, leading to a loss of diversity and potentially the lock-in of false
beliefs. We formalize this hypothesis and test it empirically with agent-based
LLM simulations and real-world GPT usage data. Analysis reveals sudden but
sustained drops in diversity after the release of new GPT iterations,
consistent with the hypothesized human-AI feedback loop. Code and data
available at https://thelockinhypothesis.com

</details>


### [243] [Reusing Trajectories in Policy Gradients Enables Fast Convergence](https://arxiv.org/abs/2506.06178)
*Alessandro Montenegro,Federico Mansutti,Marco Mussi,Matteo Papini,Alberto Maria Metelli*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Policy gradient (PG) methods are a class of effective reinforcement learning
algorithms, particularly when dealing with continuous control problems. These
methods learn the parameters of parametric policies via stochastic gradient
ascent, typically using on-policy trajectory data to estimate the policy
gradient. However, such reliance on fresh data makes them sample-inefficient.
Indeed, vanilla PG methods require $O(\epsilon^{-2})$ trajectories to reach an
$\epsilon$-approximate stationary point. A common strategy to improve
efficiency is to reuse off-policy information from past iterations, such as
previous gradients or trajectories. While gradient reuse has received
substantial theoretical attention, leading to improved rates of
$O(\epsilon^{-3/2})$, the reuse of past trajectories remains largely unexplored
from a theoretical perspective. In this work, we provide the first rigorous
theoretical evidence that extensive reuse of past off-policy trajectories can
significantly accelerate convergence in PG methods. We introduce a power mean
correction to the multiple importance weighting estimator and propose RPG
(Retrospective Policy Gradient), a PG algorithm that combines old and new
trajectories for policy updates. Through a novel analysis, we show that, under
established assumptions, RPG achieves a sample complexity of
$\widetilde{O}(\epsilon^{-1})$, the best known rate in the literature. We
further validate empirically our approach against PG methods with
state-of-the-art rates.

</details>


### [244] [A Theoretical Study of (Hyper) Self-Attention through the Lens of Interactions: Representation, Training, Generalization](https://arxiv.org/abs/2506.06179)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: 论文研究了自注意力机制的理论基础，表明单层线性自注意力能高效表示和学习成对交互函数，并推广到分布外场景。


<details>
  <summary>Details</summary>
Motivation: 自注意力是现代神经架构的核心组件，但其理论基础尚不明确，本文旨在填补这一空白。

Method: 通过分析多智能体强化学习和遗传序列中的交互实体，提出单层线性自注意力模型，并验证其学习交互函数的能力。

Result: 自注意力在训练中能学习交互函数，并在分布外场景中泛化良好。此外，提出了HyperFeatureAttention和HyperAttention模块，扩展了交互建模能力。

Conclusion: 自注意力是一种通用的交互学习机制，适用于多种实际场景，新模块进一步提升了其建模复杂交互的能力。

Abstract: Self-attention has emerged as a core component of modern neural
architectures, yet its theoretical underpinnings remain elusive. In this paper,
we study self-attention through the lens of interacting entities, ranging from
agents in multi-agent reinforcement learning to alleles in genetic sequences,
and show that a single layer linear self-attention can efficiently represent,
learn, and generalize functions capturing pairwise interactions, including
out-of-distribution scenarios. Our analysis reveals that self-attention acts as
a mutual interaction learner under minimal assumptions on the diversity of
interaction patterns observed during training, thereby encompassing a wide
variety of real-world domains. In addition, we validate our theoretical
insights through experiments demonstrating that self-attention learns
interaction functions and generalizes across both population distributions and
out-of-distribution scenarios. Building on our theories, we introduce
HyperFeatureAttention, a novel neural network module designed to learn
couplings of different feature-level interactions between entities.
Furthermore, we propose HyperAttention, a new module that extends beyond
pairwise interactions to capture multi-entity dependencies, such as three-way,
four-way, or general n-way interactions.

</details>


### [245] [Antithetic Noise in Diffusion Models](https://arxiv.org/abs/2506.06185)
*Jing Jia,Sifan Liu,Bowen Song,Wei Yuan,Liyue Shen,Guanyang Wang*

Main category: cs.LG

TL;DR: 研究了扩散模型中初始噪声的对称性及其应用，发现初始噪声与其否定配对可生成负相关样本，并提出了对称性猜想。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型中初始噪声的对称性及其对样本生成和不确定性量化的影响。

Method: 结合实验和理论分析，提出初始噪声与其否定配对的对称性猜想，并验证其有效性。

Result: 发现初始噪声的对称性可提升图像多样性和不确定性量化精度。

Conclusion: 提出了一种无需训练、模型无关且无运行时开销的框架，可应用于图像生成和统计估计。

Abstract: We initiate a systematic study of antithetic initial noise in diffusion
models. Across unconditional models trained on diverse datasets,
text-conditioned latent-diffusion models, and diffusion-posterior samplers, we
find that pairing each initial noise with its negation consistently yields
strongly negatively correlated samples. To explain this phenomenon, we combine
experiments and theoretical analysis, leading to a symmetry conjecture that the
learned score function is approximately affine antisymmetric (odd symmetry up
to a constant shift), and provide evidence supporting it. Leveraging this
negative correlation, we enable two applications: (1) enhancing image diversity
in models like Stable Diffusion without quality loss, and (2) sharpening
uncertainty quantification (e.g., up to 90% narrower confidence intervals) when
estimating downstream statistics. Building on these gains, we extend the
two-point pairing to a randomized quasi-Monte Carlo estimator, which further
improves estimation accuracy. Our framework is training-free, model-agnostic,
and adds no runtime overhead.

</details>


### [246] [Physics-Informed Neural Networks for Control of Single-Phase Flow Systems Governed by Partial Differential Equations](https://arxiv.org/abs/2506.06188)
*Luis Kin Miyatake,Eduardo Camponogara,Eric Aislan Antonelo,Alexey Pavlov*

Main category: cs.LG

TL;DR: 论文扩展了PINC框架，将其从ODE控制扩展到PDE控制，用于单相流系统，结合神经网络与物理守恒定律，无需标注数据即可实现建模与控制。


<details>
  <summary>Details</summary>
Motivation: 单相流系统的PDE建模与控制存在挑战，尤其是在瞬态条件下。研究旨在通过PINC框架解决这一问题。

Method: PINC模型分为稳态网络和瞬态网络，通过简化空间坐标维度假设，结合MPC实现高效训练与控制。

Result: 数值实验表明，PINC模型能准确表示流动动力学并支持实时控制，无需迭代求解器。

Conclusion: PINC为流体流动监测与优化提供了高效替代方案，适用于工程应用。

Abstract: The modeling and control of single-phase flow systems governed by Partial
Differential Equations (PDEs) present challenges, especially under transient
conditions. In this work, we extend the Physics-Informed Neural Nets for
Control (PINC) framework, originally proposed to modeling and control of
Ordinary Differential Equations (ODE) without the need of any labeled data, to
the PDE case, particularly to single-phase incompressible and compressible
flows, integrating neural networks with physical conservation laws. The PINC
model for PDEs is structured into two stages: a steady-state network, which
learns equilibrium solutions for a wide range of control inputs, and a
transient network, which captures dynamic responses under time-varying boundary
conditions. We propose a simplifying assumption that reduces the dimensionality
of the spatial coordinate regarding the initial condition, allowing the
efficient training of the PINC network. This simplification enables the
derivation of optimal control policies using Model Predictive Control (MPC). We
validate our approach through numerical experiments, demonstrating that the
PINC model, which is trained exclusively using physical laws, i.e., without
labeled data, accurately represents flow dynamics and enables real-time control
applications. The results highlight the PINC's capability to efficiently
approximate PDE solutions without requiring iterative solvers, making it a
promising alternative for fluid flow monitoring and optimization in engineering
applications.

</details>


### [247] [ICU-TSB: A Benchmark for Temporal Patient Representation Learning for Unsupervised Stratification into Patient Cohorts](https://arxiv.org/abs/2506.06192)
*Dimitrios Proios,Alban Bornet,Anthony Yazdani,Jose F Rodrigues Jr,Douglas Teodoro*

Main category: cs.LG

TL;DR: ICU-TSB是一个用于评估基于时间患者表示学习的患者分层的基准，利用ICU电子健康记录数据，通过分层评估框架验证临床意义。


<details>
  <summary>Details</summary>
Motivation: 推动个性化医疗，通过改进诊断和治疗策略，利用ICU电子健康记录中的丰富时间数据。

Method: 引入ICU-TSB基准，比较统计方法和循环神经网络（如LSTM和GRU）生成患者表示的能力，并评估聚类效果。

Result: 时间表示学习能重新发现临床意义的患者群体，但任务仍具挑战性（v-measure最高0.46）。

Conclusion: ICU-TSB为患者分层提供了可复现的基准，未来可进一步优化聚类标签的临床解释性。

Abstract: Patient stratification identifying clinically meaningful subgroups is
essential for advancing personalized medicine through improved diagnostics and
treatment strategies. Electronic health records (EHRs), particularly those from
intensive care units (ICUs), contain rich temporal clinical data that can be
leveraged for this purpose. In this work, we introduce ICU-TSB (Temporal
Stratification Benchmark), the first comprehensive benchmark for evaluating
patient stratification based on temporal patient representation learning using
three publicly available ICU EHR datasets. A key contribution of our benchmark
is a novel hierarchical evaluation framework utilizing disease taxonomies to
measure the alignment of discovered clusters with clinically validated disease
groupings. In our experiments with ICU-TSB, we compared statistical methods and
several recurrent neural networks, including LSTM and GRU, for their ability to
generate effective patient representations for subsequent clustering of patient
trajectories. Our results demonstrate that temporal representation learning can
rediscover clinically meaningful patient cohorts; nevertheless, it remains a
challenging task, with v-measuring varying from up to 0.46 at the top level of
the taxonomy to up to 0.40 at the lowest level. To further enhance the
practical utility of our findings, we also evaluate multiple strategies for
assigning interpretable labels to the identified clusters. The experiments and
benchmark are fully reproducible and available at
https://github.com/ds4dh/CBMS2025stratification.

</details>


### [248] [Transformative or Conservative? Conservation laws for ResNets and Transformers](https://arxiv.org/abs/2506.06194)
*Sibylle Marcotte,Rémi Gribonval,Gabriel Peyré*

Main category: cs.LG

TL;DR: 论文研究了现代架构（如卷积ResNets和Transformer网络）中的守恒定律，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 研究梯度流训练动态中的守恒定律在实用架构中的应用，尤其是卷积ResNets和Transformer网络。

Method: 通过分析基本构建块（如ReLU或线性浅层网络）的守恒定律，并扩展到单注意力层和残差块。引入依赖于参数子集的守恒定律概念。

Result: 证明了这些守恒定律在连续梯度流和离散优化动态（如SGD）中的持久性。

Conclusion: 论文为现代架构中的守恒定律提供了系统分析，并展示了其在离散优化中的适用性。

Abstract: While conservation laws in gradient flow training dynamics are well
understood for (mostly shallow) ReLU and linear networks, their study remains
largely unexplored for more practical architectures. This paper bridges this
gap by deriving and analyzing conservation laws for modern architectures, with
a focus on convolutional ResNets and Transformer networks. For this, we first
show that basic building blocks such as ReLU (or linear) shallow networks, with
or without convolution, have easily expressed conservation laws, and no more
than the known ones. In the case of a single attention layer, we also
completely describe all conservation laws, and we show that residual blocks
have the same conservation laws as the same block without a skip connection. We
then introduce the notion of conservation laws that depend only on a subset of
parameters (corresponding e.g. to a pair of consecutive layers, to a residual
block, or to an attention layer). We demonstrate that the characterization of
such laws can be reduced to the analysis of the corresponding building block in
isolation. Finally, we examine how these newly discovered conservation
principles, initially established in the continuous gradient flow regime,
persist under discrete optimization dynamics, particularly in the context of
Stochastic Gradient Descent (SGD).

</details>


### [249] [How to craft a deep reinforcement learning policy for wind farm flow control](https://arxiv.org/abs/2506.06204)
*Elie Kadoche,Pascal Bianchi,Florence Carton,Philippe Ciblat,Damien Ernst*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习的新方法，用于设计风电场中的尾流导向控制器，以优化能量生产。


<details>
  <summary>Details</summary>
Motivation: 风电场中涡轮机之间的尾流效应会显著降低整体能量生产，而现有的机器学习方法仅适用于静态风况或小型风电场。

Method: 采用结合图注意力网络和多头自注意力块的新架构，以及新的奖励函数和训练策略，开发尾流导向策略。

Result: 模型在低保真稳态模拟中，比全连接神经网络少用10倍训练步骤，能量生产提升高达14%。

Conclusion: 这是首个能在时变风况下有效泛化的深度强化学习尾流导向控制器。

Abstract: Within wind farms, wake effects between turbines can significantly reduce
overall energy production. Wind farm flow control encompasses methods designed
to mitigate these effects through coordinated turbine control. Wake steering,
for example, consists in intentionally misaligning certain turbines with the
wind to optimize airflow and increase power output. However, designing a robust
wake steering controller remains challenging, and existing machine learning
approaches are limited to quasi-static wind conditions or small wind farms.
This work presents a new deep reinforcement learning methodology to develop a
wake steering policy that overcomes these limitations. Our approach introduces
a novel architecture that combines graph attention networks and multi-head
self-attention blocks, alongside a novel reward function and training strategy.
The resulting model computes the yaw angles of each turbine, optimizing energy
production in time-varying wind conditions. An empirical study conducted on
steady-state, low-fidelity simulation, shows that our model requires
approximately 10 times fewer training steps than a fully connected neural
network and achieves more robust performance compared to a strong optimization
baseline, increasing energy production by up to 14 %. To the best of our
knowledge, this is the first deep reinforcement learning-based wake steering
controller to generalize effectively across any time-varying wind conditions in
a low-fidelity, steady-state numerical simulation setting.

</details>


### [250] [Model-Driven Graph Contrastive Learning](https://arxiv.org/abs/2506.06212)
*Ali Azizpour,Nicolas Zilberstein,Santiago Segarra*

Main category: cs.LG

TL;DR: MGCL是一种基于模型驱动的图对比学习框架，利用图生成模型（graphons）指导对比学习，通过数据生成过程提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有GCL方法依赖手动设计的增强策略，未考虑数据生成分布，且忽略同模型生成的图间相似性。MGCL旨在解决这些问题。

Method: MGCL首先估计数据的graphon，定义基于graphon的增强过程，实现数据自适应增强；对图级任务，按组聚类并估计graphon。

Result: 在基准数据集上，MGCL表现优于现有方法，验证了将生成模型融入GCL的优势。

Conclusion: MGCL通过结合生成模型，提供了一种更优的图对比学习方法，显著提升了性能。

Abstract: We propose $\textbf{MGCL}$, a model-driven graph contrastive learning (GCL)
framework that leverages graphons (probabilistic generative models for graphs)
to guide contrastive learning by accounting for the data's underlying
generative process. GCL has emerged as a powerful self-supervised framework for
learning expressive node or graph representations without relying on annotated
labels, which are often scarce in real-world data. By contrasting augmented
views of graph data, GCL has demonstrated strong performance across various
downstream tasks, such as node and graph classification. However, existing
methods typically rely on manually designed or heuristic augmentation
strategies that are not tailored to the underlying data distribution and
operate at the individual graph level, ignoring similarities among graphs
generated from the same model. Conversely, in our proposed approach, MGCL first
estimates the graphon associated with the observed data and then defines a
graphon-informed augmentation process, enabling data-adaptive and principled
augmentations. Additionally, for graph-level tasks, MGCL clusters the dataset
and estimates a graphon per group, enabling contrastive pairs to reflect shared
semantics and structure. Extensive experiments on benchmark datasets
demonstrate that MGCL achieves state-of-the-art performance, highlighting the
advantages of incorporating generative models into GCL.

</details>


### [251] [Corrector Sampling in Language Models](https://arxiv.org/abs/2506.06215)
*Itai Gat,Neta Shaul,Uriel Singer,Yaron Lipman*

Main category: cs.LG

TL;DR: 提出了一种名为RPT的新采样方法，通过迭代重新访问和替换先前生成的文本来减少自回归语言模型中的错误累积。


<details>
  <summary>Details</summary>
Motivation: 解决自回归语言模型因固定的从左到右标记生成而导致的错误累积问题。

Method: 提出Resample-Previous-Tokens (RPT)方法，迭代重新访问并可能替换先前生成的文本窗口中的标记。

Result: 在8B参数模型上仅用100B数据进行微调后，推理和编码基准测试相对标准采样提升了约10%。

Conclusion: RPT方法有效减少了错误累积，且能保持现有自回归模型的预测质量和速度。

Abstract: Autoregressive language models accumulate errors due to their fixed,
irrevocable left-to-right token generation. To address this, we propose a new
sampling method called Resample-Previous-Tokens (RPT). RPT mitigates error
accumulation by iteratively revisiting and potentially replacing tokens in a
window of previously generated text. This method can be integrated into
existing autoregressive models, preserving their next-token-prediction quality
and speed. Fine-tuning a pretrained 8B parameter model with RPT for only 100B
resulted in ~10% relative improvements on reasoning and coding benchmarks
compared to the standard sampling.

</details>


### [252] [Towards an Explainable Comparison and Alignment of Feature Embeddings](https://arxiv.org/abs/2506.06231)
*Mohammad Jalali,Bahar Dibaei Nia,Farzan Farnia*

Main category: cs.LG

TL;DR: 提出了一种名为SPEC的框架，用于比较不同特征嵌入模型在聚类任务中的差异，并通过核矩阵分析识别样本群的不匹配。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注嵌入模型的数值性能，而缺乏对嵌入空间聚类差异的可解释性比较。

Method: 利用核矩阵的谱分解分析嵌入差异，提出可扩展的线性复杂度实现，并通过优化问题对齐嵌入。

Result: 在ImageNet和MS-COCO等大规模数据集上验证了SPEC的有效性。

Conclusion: SPEC为嵌入模型的比较和对齐提供了高效且可解释的工具。

Abstract: While several feature embedding models have been developed in the literature,
comparisons of these embeddings have largely focused on their numerical
performance in classification-related downstream applications. However, an
interpretable comparison of different embeddings requires identifying and
analyzing mismatches between sample groups clustered within the embedding
spaces. In this work, we propose the \emph{Spectral Pairwise Embedding
Comparison (SPEC)} framework to compare embeddings and identify their
differences in clustering a reference dataset. Our approach examines the kernel
matrices derived from two embeddings and leverages the eigendecomposition of
the difference kernel matrix to detect sample clusters that are captured
differently by the two embeddings. We present a scalable implementation of this
kernel-based approach, with computational complexity that grows linearly with
the sample size. Furthermore, we introduce an optimization problem using this
framework to align two embeddings, ensuring that clusters identified in one
embedding are also captured in the other model. We provide numerical results
demonstrating the SPEC's application to compare and align embeddings on
large-scale datasets such as ImageNet and MS-COCO. The code is available at
[https://github.com/mjalali/embedding-comparison](github.com/mjalali/embedding-comparison).

</details>


### [253] [Neural Responses to Affective Sentences Reveal Signatures of Depression](https://arxiv.org/abs/2506.06244)
*Aditya Kommineni,Woojae Jeong,Kleanthis Avramidis,Colin McDaniel,Myzelle Hughes,Thomas McGee,Elsi Kaiser,Kristina Lerman,Idan A. Blank,Dani Byrd,Assal Habibi,B. Rael Cahn,Sudarsana Kadiri,Takfarinas Medani,Richard M. Leahy,Shrikanth Narayanan*

Main category: cs.LG

TL;DR: 研究通过EEG测量抑郁与健康个体对自我参照情感句子的神经反应，发现抑郁情绪处理异常，深度学习模型可区分抑郁与健康人群及抑郁亚组。


<details>
  <summary>Details</summary>
Motivation: 深入理解抑郁症的神经认知基础，尤其是情感和自我参照处理的异常。

Method: 使用表面EEG测量健康与抑郁个体对自我参照情感句子的神经反应，并训练深度学习模型分析数据。

Result: 发现抑郁组在句子观看时神经活动显著不同，深度学习模型区分健康与抑郁的AUC为0.707，区分抑郁亚组的AUC为0.624。

Conclusion: 抑郁存在稳定的神经特征，可能为未来诊断工具提供依据。

Abstract: Major Depressive Disorder (MDD) is a highly prevalent mental health
condition, and a deeper understanding of its neurocognitive foundations is
essential for identifying how core functions such as emotional and
self-referential processing are affected. We investigate how depression alters
the temporal dynamics of emotional processing by measuring neural responses to
self-referential affective sentences using surface electroencephalography (EEG)
in healthy and depressed individuals. Our results reveal significant
group-level differences in neural activity during sentence viewing, suggesting
disrupted integration of emotional and self-referential information in
depression. Deep learning model trained on these responses achieves an area
under the receiver operating curve (AUC) of 0.707 in distinguishing healthy
from depressed participants, and 0.624 in differentiating depressed subgroups
with and without suicidal ideation. Spatial ablations highlight anterior
electrodes associated with semantic and affective processing as key
contributors. These findings suggest stable, stimulus-driven neural signatures
of depression that may inform future diagnostic tools.

</details>


### [254] [Lagrangian-based Equilibrium Propagation: generalisation to arbitrary boundary conditions & equivalence with Hamiltonian Echo Learning](https://arxiv.org/abs/2506.06248)
*Guillaume Pourcel,Debabrota Basu,Maxence Ernoult,Aditya Gilra*

Main category: cs.LG

TL;DR: GLEP扩展了EP算法，适用于时变输入，并展示了不同边界条件对学习算法的影响。HEL是GLEP的特例，继承了EP的硬件友好特性。


<details>
  <summary>Details</summary>
Motivation: 解决EP算法在时变输入中的扩展问题，并探索其实际应用的可行性。

Method: 提出GLEP，通过变分描述扩展EP至时变输入，分析不同边界条件的影响，并推导HEL作为特例。

Result: GLEP生成多种学习算法，但仅HEL具备硬件友好特性（前向操作、高效扩展、局部学习）。

Conclusion: HEL是GLEP中唯一实用的算法，继承了EP的硬件优势，适合实际应用。

Abstract: Equilibrium Propagation (EP) is a learning algorithm for training
Energy-based Models (EBMs) on static inputs which leverages the variational
description of their fixed points. Extending EP to time-varying inputs is a
challenging problem, as the variational description must apply to the entire
system trajectory rather than just fixed points, and careful consideration of
boundary conditions becomes essential. In this work, we present Generalized
Lagrangian Equilibrium Propagation (GLEP), which extends the variational
formulation of EP to time-varying inputs. We demonstrate that GLEP yields
different learning algorithms depending on the boundary conditions of the
system, many of which are impractical for implementation. We then show that
Hamiltonian Echo Learning (HEL) -- which includes the recently proposed
Recurrent HEL (RHEL) and the earlier known Hamiltonian Echo Backpropagation
(HEB) algorithms -- can be derived as a special case of GLEP. Notably, HEL is
the only instance of GLEP we found that inherits the properties that make EP a
desirable alternative to backpropagation for hardware implementations: it
operates in a "forward-only" manner (i.e. using the same system for both
inference and learning), it scales efficiently (requiring only two or more
passes through the system regardless of model size), and enables local
learning.

</details>


### [255] [Distillation Robustifies Unlearning](https://arxiv.org/abs/2506.06278)
*Bruce W. Lee,Addie Foote,Alex Infanger,Leni Shor,Harish Kamath,Jacob Goldman-Wetzler,Bryce Woodworth,Alex Cloud,Alexander Matt Turner*

Main category: cs.LG

TL;DR: 当前LLM遗忘方法不鲁棒，易被微调恢复。提出UNDO方法，通过蒸馏增强遗忘鲁棒性，计算成本低且效果好。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法易被微调恢复，输出微调不足以实现鲁棒遗忘。

Method: 提出UNDO方法，通过蒸馏未学习模型到部分噪声副本中，平衡计算成本与鲁棒性。

Result: UNDO在合成任务中匹配从头训练的鲁棒性，计算成本仅60-80%，且仅需0.01%标记数据。在WMDP基准上也表现良好。

Conclusion: 蒸馏结合遗忘步骤为鲁棒能力移除提供了实用路径。

Abstract: Current LLM unlearning methods are not robust: they can be reverted easily
with a few steps of finetuning. This is true even for the idealized unlearning
method of training to imitate an oracle model that was never exposed to
unwanted information, suggesting that output-based finetuning is insufficient
to achieve robust unlearning. In a similar vein, we find that training a
randomly initialized student to imitate an unlearned model transfers desired
behaviors while leaving undesired capabilities behind. In other words,
distillation robustifies unlearning. Building on this insight, we propose
Unlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an
unlearned model into a partially noised copy of itself. UNDO introduces a
tunable tradeoff between compute cost and robustness, establishing a new Pareto
frontier on synthetic language and arithmetic tasks. At its strongest setting,
UNDO matches the robustness of a model retrained from scratch with perfect data
filtering while using only 60-80% of the compute and requiring only 0.01% of
the pretraining data to be labeled. We also show that UNDO robustifies
unlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP)
benchmark. Since distillation is widely used in practice, incorporating an
unlearning step beforehand offers a convenient path to robust capability
removal.

</details>


### [256] [Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias](https://arxiv.org/abs/2506.06280)
*Yuanzhe Hu,Kinshuk Goel,Vlad Killiakov,Yaoqing Yang*

Main category: cs.LG

TL;DR: 论文提出FARMS方法，通过固定长宽比子矩阵采样解决权重矩阵长宽比对特征谱分析的影响，提升模型诊断和超参数分配的准确性。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络权重矩阵特征谱分析中，矩阵长宽比对重尾性估计的偏差问题，影响模型诊断和超参数分配。

Method: 提出FARMS方法，通过固定长宽比子矩阵采样，测量其平均特征谱密度，以消除长宽比偏差。

Result: FARMS在多种应用领域（如图像分类、科学机器学习和大型语言模型修剪）中显著提升特征谱分析准确性，并在LLaMA-7B模型修剪中降低困惑度17.3%。

Conclusion: FARMS是一种简单有效的方法，能够消除特征谱分析中的长宽比偏差，提升模型诊断和超参数分配的准确性。

Abstract: Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight
matrices has been an active area of research in recent years. At a high level,
eigenspectrum analysis of DNNs involves measuring the heavytailness of the
empirical spectral densities (ESD) of weight matrices. It provides insight into
how well a model is trained and can guide decisions on assigning better
layer-wise training hyperparameters. In this paper, we address a challenge
associated with such eigenspectrum methods: the impact of the aspect ratio of
weight matrices on estimated heavytailness metrics. We demonstrate that
matrices of varying sizes (and aspect ratios) introduce a non-negligible bias
in estimating heavytailness metrics, leading to inaccurate model diagnosis and
layer-wise hyperparameter assignment. To overcome this challenge, we propose
FARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the
weight matrices by subsampling submatrices with a fixed aspect ratio. Instead
of measuring the heavytailness of the original ESD, we measure the average ESD
of these subsampled submatrices. We show that measuring the heavytailness of
these submatrices with the fixed aspect ratio can effectively mitigate the
aspect ratio bias. We validate our approach across various optimization
techniques and application domains that involve eigenspectrum analysis of
weights, including image classification in computer vision (CV) models,
scientific machine learning (SciML) model training, and large language model
(LLM) pruning. Our results show that despite its simplicity, FARMS uniformly
improves the accuracy of eigenspectrum analysis while enabling more effective
layer-wise hyperparameter assignment in these application domains. In one of
the LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model
by 17.3% when compared with the state-of-the-art method.

</details>


### [257] [CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning](https://arxiv.org/abs/2506.06290)
*Mingyu Lu,Ethan Weinberger,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: CellCLIP是一种用于高内涵筛选数据的跨模态对比学习框架，通过预训练图像编码器和新型通道编码方案，显著提升了跨模态检索和生物学任务的性能。


<details>
  <summary>Details</summary>
Motivation: 高内涵筛选技术（如Cell Painting）能大规模研究细胞形态对扰动的响应，但现有方法难以处理其与自然图像的语义差异及扰动类型的多样性。

Method: 结合预训练图像编码器、新型通道编码方案和自然语言编码器，CellCLIP构建了统一的潜在空间，对齐扰动与其形态效应。

Result: CellCLIP在跨模态检索和下游生物学任务中表现最佳，同时显著减少了计算时间。

Conclusion: CellCLIP为高内涵筛选数据提供了一种高效的跨模态学习方法，解决了现有挑战。

Abstract: High-content screening (HCS) assays based on high-throughput microscopy
techniques such as Cell Painting have enabled the interrogation of cells'
morphological responses to perturbations at an unprecedented scale. The
collection of such data promises to facilitate a better understanding of the
relationships between different perturbations and their effects on cellular
state. Towards achieving this goal, recent advances in cross-modal contrastive
learning could, in theory, be leveraged to learn a unified latent space that
aligns perturbations with their corresponding morphological effects. However,
the application of such methods to HCS data is not straightforward due to
substantial differences in the semantics of Cell Painting images compared to
natural images, and the difficulty of representing different classes of
perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent
space. In response to these challenges, here we introduce CellCLIP, a
cross-modal contrastive learning framework for HCS data. CellCLIP leverages
pre-trained image encoders coupled with a novel channel encoding scheme to
better capture relationships between different microscopy channels in image
embeddings, along with natural language encoders for representing
perturbations. Our framework outperforms current open-source models,
demonstrating the best performance in both cross-modal retrieval and
biologically meaningful downstream tasks while also achieving significant
reductions in computation time.

</details>


### [258] [Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks](https://arxiv.org/abs/2506.06291)
*Xiaoke Wang,Batuhan Altundas,Zhaoxin Li,Aaron Zhao,Matthew Gombolay*

Main category: cs.LG

TL;DR: 提出了一种基于学习和GNN的框架，用于为MILP求解器生成高质量初始解，以减少计算时间和方差。


<details>
  <summary>Details</summary>
Motivation: MILP在规划和调度问题中应用广泛，但计算时间长限制了其在大规模实时场景中的使用。

Method: 结合行为克隆（BC）和强化学习（RL）训练图神经网络（GNN），为多智能体任务分配和调度问题生成初始解。

Result: 实验表明，该方法在保持解的质量和可行性的同时，减少了优化时间和方差。

Conclusion: 该框架有效提升了MILP求解器的效率，适用于实时和大规模场景。

Abstract: Mixed Integer Linear Programs (MILPs) are essential tools for solving
planning and scheduling problems across critical industries such as
construction, manufacturing, and logistics. However, their widespread adoption
is limited by long computational times, especially in large-scale, real-time
scenarios. To address this, we present a learning-based framework that
leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph
Neural Networks (GNNs), producing high-quality initial solutions for
warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling
Problems. Experimental results demonstrate that our method reduces optimization
time and variance compared to traditional techniques while maintaining solution
quality and feasibility.

</details>


### [259] [Mutual-Taught for Co-adapting Policy and Reward Models](https://arxiv.org/abs/2506.06292)
*Tianyuan Shi,Canbin Huang,Fanqi Wan,Longguang Zhong,Ziyi Yang,Weizhou Shen,Xiaojun Quan,Ming Yan*

Main category: cs.LG

TL;DR: Mutual-Taught是一种自训练方法，通过迭代优化策略模型（PM）和奖励模型（RM）来解决分布偏移问题，无需额外人工标注。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLMs）的偏好优化中，新生成的模型样本与奖励模型训练数据之间的分布偏移会降低RM的效果，进而影响PM的性能。

Method: 采用类似期望最大化（EM）算法的自训练方法：E步用当前RM反馈更新PM，M步用PM输出更新RM。

Result: 实验表明，该方法持续提升模型性能，8B策略模型在AlpacaEval-2上胜率为54.1%，8B奖励模型在RewardBench上与GPT-4o-2024-08-06表现相当。

Conclusion: Mutual-Taught通过迭代优化PM和RM，有效解决了分布偏移问题，提升了模型性能。

Abstract: During the preference optimization of large language models (LLMs),
distribution shifts may arise between newly generated model samples and the
data used to train the reward model (RM). This shift reduces the efficacy of
the RM, which in turn negatively impacts the performance of the policy model
(PM). To address this challenge, we propose Mutual-Taught, a self-training
method that iteratively improves both the PM and RM without requiring
additional human annotation. Our approach mirrors the expectation-maximization
(EM) algorithm. In the E-step, the PM is updated using feedback from the
current RM, guiding the PM toward a better approximation of the latent optimal
preference distribution. In the M-step, we update the RM by constructing
training data from the outputs of the PM before and after the E-step update.
This process ensures that the RM adapts to the evolving policy distribution.
Experimental results demonstrate that this iterative approach leads to
consistent improvements in both models. Specifically, our 8B policy model,
LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\% on
AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par
with GPT-4o-2024-08-06 on RewardBench.

</details>


### [260] [Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks](https://arxiv.org/abs/2506.06293)
*Junyi Liu,Stanley Kok*

Main category: cs.LG

TL;DR: 研究利用持久同调构建银行关系网络，结合传统借贷网络形成异构网络，提升信用评级预测效果。


<details>
  <summary>Details</summary>
Motivation: 银行信用评级对经济稳定和决策至关重要，但完整的银行间连接图常因隐私问题缺失，限制了图神经网络的应用。

Method: 使用持久同调构建银行关系网络，结合传统借贷网络形成异构网络，提出HTGNN模型。

Result: 在真实全球数据集上验证了HTGNN的有效性，预测效果显著提升。

Conclusion: 研究为投资者和监管机构提供了改进风险缓解和市场干预的工具，代码已开源。

Abstract: Agencies such as Standard & Poor's and Moody's provide bank credit ratings
that influence economic stability and decision-making by stakeholders. Accurate
and timely predictions support informed decision-making, regulatory actions,
and investor protection. However, a complete interbank connection graph is
often unavailable due to privacy concerns, complicating the direct application
of Graph Neural Networks (GNNs) for rating prediction. our research utilizes
persistent homology to construct a network that captures relationships among
banks and combines this with a traditional lending network to create a
heterogeneous network that integrates information from both sources, leading to
improved predictions. Experiments on a global, real-world dataset validate the
effectiveness of HTGNN. This research has implications for investors and
regulatory bodies in enhancing proactive risk mitigation and the implementation
of effective market interventions.The code can be find at
https://github.com/Liu-Jun-Yi/HTGNN.

</details>


### [261] [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
*Yunqing Liu,Wenqi Fan,Xiaoyong Wei,Qing Li*

Main category: cs.LG

TL;DR: GLProtein是一个结合全局结构相似性和局部氨基酸细节的蛋白质预训练框架，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 蛋白质结构信息不仅限于3D信息，还包括从氨基酸分子到蛋白质-蛋白质结构相似性的多层次信息，现有方法尚未充分整合这些信息。

Method: GLProtein创新性地结合了蛋白质掩码建模、三重结构相似性评分、蛋白质3D距离编码和基于子结构的氨基酸分子编码。

Result: 实验表明，GLProtein在蛋白质-蛋白质相互作用预测、接触预测等生物信息学任务中优于现有方法。

Conclusion: GLProtein通过整合全局和局部结构信息，为蛋白质功能预测提供了更全面的视角和更高的准确性。

Abstract: Proteins are central to biological systems, participating as building blocks
across all forms of life. Despite advancements in understanding protein
functions through protein sequence analysis, there remains potential for
further exploration in integrating protein structural information. We argue
that the structural information of proteins is not only limited to their 3D
information but also encompasses information from amino acid molecules (local
information) to protein-protein structure similarity (global information). To
address this, we propose \textbf{GLProtein}, the first framework in protein
pre-training that incorporates both global structural similarity and local
amino acid details to enhance prediction accuracy and functional insights.
GLProtein innovatively combines protein-masked modelling with triplet structure
similarity scoring, protein 3D distance encoding and substructure-based amino
acid molecule encoding. Experimental results demonstrate that GLProtein
outperforms previous methods in several bioinformatics tasks, including
predicting protein-protein interaction, contact prediction, and so on.

</details>


### [262] [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)
*Zhiyuan Liu,Yicun Yang,Yaojie Zhang,Junjie Chen,Chang Zou,Qingyuan Wei,Shaobo Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为dLLM-Cache的缓存框架，用于解决扩散式大语言模型（dLLMs）的高推理延迟问题，通过静态提示缓存和部分响应更新实现了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 扩散式大语言模型（dLLMs）在文本生成方面表现出潜力，但其高推理延迟限制了实际应用。传统加速技术不适用于dLLMs，因此需要一种新的解决方案。

Method: 提出dLLM-Cache框架，结合长间隔提示缓存和基于特征相似性的部分响应更新，实现中间计算的高效复用。

Result: 在LLaDA 8B和Dream 7B等模型上的实验表明，dLLM-Cache实现了高达9.1倍的加速，且不影响输出质量。

Conclusion: dLLM-Cache显著降低了dLLMs的推理延迟，使其接近自回归模型（ARMs）的性能，为实际应用提供了可行性。

Abstract: Autoregressive Models (ARMs) have long dominated the landscape of Large
Language Models. Recently, a new paradigm has emerged in the form of
diffusion-based Large Language Models (dLLMs), which generate text by
iteratively denoising masked segments. This approach has shown significant
advantages and potential. However, dLLMs suffer from high inference latency.
Traditional ARM acceleration techniques, such as Key-Value caching, are
incompatible with dLLMs due to their bidirectional attention mechanism. To
address this specific challenge, our work begins with a key observation that
dLLM inference involves a static prompt and a partially dynamic response, where
most tokens remain stable across adjacent denoising steps. Based on this, we
propose dLLM-Cache, a training-free adaptive caching framework that combines
long-interval prompt caching with partial response updates guided by feature
similarity. This design enables efficient reuse of intermediate computations
without compromising model performance. Extensive experiments on representative
dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1
x speedup over standard inference without compromising output quality. Notably,
our method brings dLLM inference latency close to that of ARMs under many
settings. Codes are provided in the supplementary material and will be released
publicly on GitHub.

</details>


### [263] [Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets](https://arxiv.org/abs/2506.06296)
*Hanaa El Afia,Said Ohamouddou,Raddouane Chiheb,Abdellatif El Afia*

Main category: cs.LG

TL;DR: Jacobi-KAN-DGCNN结合动态图卷积神经网络与Jacobi多项式KAN，用于三维点云分类，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索多项式扩展在动态图卷积网络中的应用，以提升分类精度和收敛速度。

Method: 用可调单变量多项式扩展替换MLP层，结合DGCNN架构，避免深层网络。

Result: 在ModelNet40数据集上，Jacobi多项式KAN层在精度和收敛速度上优于传统线性层，且参数高效。

Conclusion: 高多项式阶数不一定提升性能，需进一步研究多项式基与图学习机制的交互。

Abstract: We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph
Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks
(KAN) for the classification of three-dimensional point clouds. This method
replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate
polynomial expansions within a streamlined DGCNN architecture, circumventing
deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In
comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi
polynomials outperform the traditional linear layer-based DGCNN baseline in
terms of accuracy and convergence speed, while maintaining parameter
efficiency. Our results demonstrate that higher polynomial degrees do not
automatically improve performance, highlighting the need for further
theoretical and empirical investigation to fully understand the interactions
between polynomial bases, degrees, and the mechanisms of graph-based learning.

</details>


### [264] [Optimal patient allocation for echocardiographic assessments](https://arxiv.org/abs/2506.06297)
*Bozhi Sun,Seda Tierney,Jeffrey A. Feinstein,Frederick Damen,Alison L. Marsden,Daniele E. Schiavazzi*

Main category: cs.LG

TL;DR: 论文通过离散事件随机模拟和强化学习优化医院超声检查调度，动态分配策略优于静态预留策略。


<details>
  <summary>Details</summary>
Motivation: 医院超声检查调度面临非确定性因素（如患者爽约、到达时间不确定等）和资源不对称约束，需优化资源分配以提高效率。

Method: 基于一周运营数据预处理，构建SimPy离散事件随机模拟模型，结合Gymnasium库，比较动态分配与预留策略，并应用强化学习优化策略。

Result: 动态分配策略在资源约束下表现更优，强化学习策略进一步提升了调度效率。

Conclusion: 数据驱动的动态资源管理策略可显著提升超声检查调度效率，强化学习为优化提供了有效工具。

Abstract: Scheduling echocardiographic exams in a hospital presents significant
challenges due to non-deterministic factors (e.g., patient no-shows, patient
arrival times, diverse exam durations, etc.) and asymmetric resource
constraints between fetal and non-fetal patient streams. To address these
challenges, we first conducted extensive pre-processing on one week of
operational data from the Echo Laboratory at Stanford University's Lucile
Packard Children's Hospital, to estimate patient no-show probabilities and
derive empirical distributions of arrival times and exam durations. Based on
these inputs, we developed a discrete-event stochastic simulation model using
SimPy, and integrate it with the open source Gymnasium Python library. As a
baseline for policy optimization, we developed a comparative framework to
evaluate on-the-fly versus reservation-based allocation strategies, in which
different proportions of resources are reserved in advance. Considering a
hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2
ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation
generally yields better performance, more effectively adapting to patient
variability and resource constraints. Building on this foundation, we apply
reinforcement learning (RL) to derive an approximated optimal dynamic
allocation policy. This RL-based policy is benchmarked against the
best-performing rule-based strategies, allowing us to quantify their
differences and provide actionable insights for improving echo lab efficiency
through intelligent, data-driven resource management.

</details>


### [265] [Pairwise Calibrated Rewards for Pluralistic Alignment](https://arxiv.org/abs/2506.06298)
*Daniel Halpern,Evi Micha,Ariel D. Procaccia,Itai Shapira*

Main category: cs.LG

TL;DR: 论文提出了一种通过多奖励函数分布反映多样化人类偏好的方法，解决了现有对齐流程中少数观点被忽视的问题。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法假设存在单一理想行为，但人类偏好因用户、背景和文化而异，导致少数观点被多数信号掩盖。

Method: 通过直接从成对偏好中学习多奖励函数分布，无需标注者标识或预定义组，将标注者分歧视为信息性软标签。核心标准是成对校准。

Result: 证明即使小规模无异常集合也能准确表示多样化偏好分布，并通过实验验证了训练启发式方法的有效性。

Conclusion: 提出的方法能更忠实反映多元化价值观，校准效果提升。

Abstract: Current alignment pipelines presume a single, universal notion of desirable
behavior. However, human preferences often diverge across users, contexts, and
cultures. As a result, disagreement collapses into the majority signal and
minority perspectives are discounted. To address this, we propose reflecting
diverse human preferences through a distribution over multiple reward
functions, each inducing a distinct aligned policy. The distribution is learned
directly from pairwise preference without annotator identifiers or predefined
groups. Instead, annotator disagreements are treated as informative soft
labels. Our central criterion is pairwise calibration: for every pair of
candidate responses, the proportion of reward functions preferring one response
matches the fraction of annotators with that preference. We prove that even a
small outlier-free ensemble can accurately represent diverse preference
distributions. Empirically, we introduce and validate a practical training
heuristic to learn such ensembles, and demonstrate its effectiveness through
improved calibration, implying a more faithful representation of pluralistic
values.

</details>


### [266] [LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization](https://arxiv.org/abs/2506.06300)
*Yuanye Zhou,Zhaokun Wang,Kai Zhou,Hui Tang,Xiaofan Li*

Main category: cs.LG

TL;DR: LT-PINNs是一种新型物理信息神经网络框架，通过参数化拓扑边界曲线，避免了手动插值，提升了复杂几何优化的精度和适用性。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs依赖基于密度的拓扑描述，需要手动插值且难以处理复杂几何形状，限制了其应用。

Method: 提出LT-PINNs，将拓扑边界曲线的控制变量参数化为可学习参数，并引入边界条件损失函数和拓扑损失函数。

Result: LT-PINNs在多种PDE问题中表现优异，显著降低了相对L2误差，并能处理任意边界条件。

Conclusion: LT-PINNs为工程优化提供了更高效、精确的边界聚焦方法，适用于复杂拓扑问题。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless
tool for topology optimization, capable of simultaneously determining optimal
topologies and physical solutions. However, conventional PINNs rely on
density-based topology descriptions, which necessitate manual interpolation and
limit their applicability to complex geometries. To address this, we propose
Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for
boundary-focused engineering optimization. By parameterizing the control
variables of topology boundary curves as learnable parameters, LT-PINNs
eliminate the need for manual interpolation and enable precise boundary
determination. We further introduce specialized boundary condition loss
function and topology loss function to ensure sharp and accurate boundary
representations, even for intricate topologies. The accuracy and robustness of
LT-PINNs are validated via two types of partial differential equations (PDEs),
including elastic equation with Dirichlet boundary conditions and Laplace's
equation with Neumann boundary conditions. Furthermore, we demonstrate
effectiveness of LT-PINNs on more complex time-dependent and time-independent
flow problems without relying on measurement data, and showcase their
engineering application potential in flow velocity rearrangement, transforming
a uniform upstream velocity into a sine-shaped downstream profile. The results
demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors
compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)
LT-PINNs can handle arbitrary boundary conditions, making them suitable for a
wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries
without manual interpolation, especially for complex topologies.

</details>


### [267] [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)
*Kefan Song,Amir Moeini,Peng Wang,Lei Gong,Rohan Chandra,Yanjun Qi,Shangtong Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为ICRL prompting的新方法，通过多轮提示和奖励反馈，使LLM在推理时表现出类似强化学习的行为，显著提升了任务完成质量。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在推理时是否能够通过多轮提示和奖励反馈表现出类似强化学习的行为，从而提升任务完成效果。

Method: 提出ICRL prompting框架，通过多轮提示和奖励反馈，逐步优化LLM的响应质量。

Result: 在Game of 24、创意写作和ScienceWorld三个基准测试中，ICRL prompting显著优于基线方法，甚至在某些实验中仅依赖LLM自身生成的奖励信号也能提升性能。

Conclusion: ICRL prompting为LLM在推理时实现类似强化学习的行为提供了新范式，展示了扩展测试时计算能力的潜力。

Abstract: Reinforcement learning (RL) is a human-designed framework for solving
sequential decision making problems. In this work, we demonstrate that,
surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a
phenomenon known as in-context RL (ICRL). Specifically, we propose a novel
multi-round prompting framework called ICRL prompting. The goal is to prompt
the LLM to complete a task. After the LLM generates a response at the current
round, we give numerical scalar feedbacks for the response, called the rewards.
At the next round, we prompt the LLM again with the same task and a context
consisting of all previous responses and rewards. We observe that the quality
of the LLM's response increases as the context grows. In other words, the LLM
is able to maximize the scalar reward signal in the inference time, just like
an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,
creative writing, and ScienceWorld) and demonstrate significant performance
improvements over baseline methods such as Self-Refine and Reflexion.
Surprisingly, in some experiments the reward signals are generated by the LLM
itself, yet performance improvements are still observed from ICRL prompting,
offering a promising paradigm for scaling test-time compute.

</details>


### [268] [Wine Quality Prediction with Ensemble Trees: A Unified, Leak-Free Comparative Study](https://arxiv.org/abs/2506.06327)
*Zilang Chen*

Main category: cs.LG

TL;DR: 论文提出了一个基于五种集成学习方法的葡萄酒质量评估基准，通过泄漏避免的工作流程和多种优化技术，比较了各方法的性能，最终推荐了不同场景下的最佳模型。


<details>
  <summary>Details</summary>
Motivation: 葡萄酒质量评估通常依赖主观的人工品尝，缺乏客观性和可重复性。本文旨在通过机器学习方法提供一种准确且可重复的评估方案。

Method: 使用五种集成学习方法（随机森林、梯度提升、XGBoost、LightGBM、CatBoost），结合分层训练-测试分割、SMOTE-Tomek重采样、超参数搜索等技术，对红葡萄酒和白葡萄酒数据集进行评估。

Result: 梯度提升方法在加权F1得分上表现最佳（红葡萄酒0.693，白葡萄酒0.664），但随机森林在效率上更具优势。特征选择后，仅使用五个关键变量即可保留大部分预测信号。

Conclusion: 随机森林是性价比最高的生产模型，XGBoost和LightGBM适合GPU高效场景，梯度提升适用于离线基准测试。论文提供了完整的流程和指标集，为未来研究提供了可复现的基线。

Abstract: Accurate and reproducible wine-quality assessment is critical for production
control yet remains dominated by subjective, labour-intensive tasting panels.
We present the first unified benchmark of five ensemble learners (Random
Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the canonical Vinho
Verde red- and white-wine datasets (1,599 and 4,898 instances, 11
physicochemical attributes). Our leakage-free workflow employs an 80:20
stratified train-test split, five-fold StratifiedGroupKFold within the training
set, per-fold standardisation, SMOTE-Tomek resampling, inverse-frequency cost
weighting, Optuna hyper-parameter search (120-200 trials per model) and a
two-stage feature-selection refit. Final scores on untouched test sets are
reported with weighted F1 as the headline metric. Gradient Boosting achieves
the highest accuracy (weighted F1 0.693 +/- 0.028 for red and 0.664 +/- 0.016
for white), followed within three percentage points by Random Forest and
XGBoost. Limiting each model to its five top-ranked variables lowers
dimensionality by 55 percent while reducing weighted F1 by only 2.6 percentage
points for red and 3.0 percentage points for white, indicating that alcohol,
volatile acidity, sulphates, free SO2 and chlorides capture most predictive
signal. Runtime profiling on an EPYC 9K84/H20 node reveals a steep efficiency
gradient: Gradient Boosting averages 12 h per five-fold study, XGBoost and
LightGBM require 2-3 h, CatBoost 1 h, and Random Forest under 50 min. We
therefore recommend Random Forest as the most cost-effective production model,
XGBoost and LightGBM as GPU-efficient alternatives, and Gradient Boosting as
the accuracy ceiling for offline benchmarking. The fully documented pipeline
and metric set provide a reproducible baseline for future work on imbalanced
multi-class wine-quality prediction.

</details>


### [269] [ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications](https://arxiv.org/abs/2506.06330)
*James Afful*

Main category: cs.LG

TL;DR: ExplainBench是一个开源基准测试套件，用于系统评估局部模型解释方法，特别是在公平敏感场景下。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在高风险领域的广泛应用，对可解释和可信赖模型的需求增加，但缺乏标准化、可复现的评估框架。

Method: ExplainBench提供统一的解释算法封装、端到端模型训练和解释生成流程，并通过保真度、稀疏性和鲁棒性指标进行评估。

Result: 在COMPAS、UCI Adult Income和LendingClub等数据集上展示了不同解释方法的行为。

Conclusion: ExplainBench通过支持可复现的局部解释比较分析，推动了可解释机器学习的方法学基础，并提升了实际AI系统的问责性。

Abstract: As machine learning systems are increasingly deployed in high-stakes domains
such as criminal justice, finance, and healthcare, the demand for interpretable
and trustworthy models has intensified. Despite the proliferation of local
explanation techniques, including SHAP, LIME, and counterfactual methods, there
exists no standardized, reproducible framework for their comparative
evaluation, particularly in fairness-sensitive settings.
  We introduce ExplainBench, an open-source benchmarking suite for systematic
evaluation of local model explanations across ethically consequential datasets.
ExplainBench provides unified wrappers for popular explanation algorithms,
integrates end-to-end pipelines for model training and explanation generation,
and supports evaluation via fidelity, sparsity, and robustness metrics. The
framework includes a Streamlit-based graphical interface for interactive
exploration and is packaged as a Python module for seamless integration into
research workflows.
  We demonstrate ExplainBench on datasets commonly used in fairness research,
such as COMPAS, UCI Adult Income, and LendingClub, and showcase how different
explanation methods behave under a shared experimental protocol. By enabling
reproducible, comparative analysis of local explanations, ExplainBench advances
the methodological foundations of interpretable machine learning and
facilitates accountability in real-world AI systems.

</details>


### [270] [Extending AALpy with Passive Learning: A Generalized State-Merging Approach](https://arxiv.org/abs/2506.06333)
*Benjamin von Berg,Bernhard K. Aichernig*

Main category: cs.LG

TL;DR: AALpy是一个Python开源自动机学习库，专注于IO行为系统的主动学习，新增了被动学习中的状态合并方法实现。


<details>
  <summary>Details</summary>
Motivation: 提供一种通用的状态合并方法实现，减少算法实现的复杂性，支持现有和新算法的快速开发。

Method: 使用红蓝框架和通用内部表示，通过定义兼容性标准和评分实现状态合并算法。

Result: AALpy简化了状态合并算法的实现，现有算法仅需几行代码即可定义。

Conclusion: AALpy的通用设计为自动机学习提供了高效且灵活的解决方案。

Abstract: AALpy is a well-established open-source automata learning library written in
Python with a focus on active learning of systems with IO behavior. It provides
a wide range of state-of-the-art algorithms for different automaton types
ranging from fully deterministic to probabilistic automata. In this work, we
present the recent addition of a generalized implementation of an important
method from the domain of passive automata learning: state-merging in the
red-blue framework. Using a common internal representation for different
automaton types allows for a general and highly configurable implementation of
the red-blue framework. We describe how to define and execute state-merging
algorithms using AALpy, which reduces the implementation effort for
state-merging algorithms mainly to the definition of compatibility criteria and
scoring. This aids the implementation of both existing and novel algorithms. In
particular, defining some existing state-merging algorithms from the literature
with AALpy only takes a few lines of code.

</details>


### [271] [Optimized Local Updates in Federated Learning via Reinforcement Learning](https://arxiv.org/abs/2506.06337)
*Ali Murad,Bo Hui,Wei-Shinn Ku*

Main category: cs.LG

TL;DR: 提出了一种基于深度强化学习（DRL）的联邦学习（FL）框架，优化客户端训练数据量以提升性能并解决非独立同分布（non-IID）数据问题。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中因非独立同分布数据导致的性能下降问题，同时避免客户端过度共享数据。

Method: 利用DRL代理动态选择客户端训练数据量，以训练损失变化为奖励信号，优化每轮训练的数据分配。

Result: 实验表明，该方法在多个基准数据集和FL框架中显著提升了性能。

Conclusion: 提出的DRL框架有效优化了客户端训练数据分配，缓解了非独立同分布数据的影响，提升了联邦学习性能。

Abstract: Federated Learning (FL) is a distributed framework for collaborative model
training over large-scale distributed data, enabling higher performance while
maintaining client data privacy. However, the nature of model aggregation at
the centralized server can result in a performance drop in the presence of
non-IID data across different clients. We remark that training a client locally
on more data than necessary does not benefit the overall performance of all
clients. In this paper, we devise a novel framework that leverages a Deep
Reinforcement Learning (DRL) agent to select an optimized amount of data
necessary to train a client model without oversharing information with the
server. Starting without awareness of the client's performance, the DRL agent
utilizes the change in training loss as a reward signal and learns to optimize
the amount of training data necessary for improving the client's performance.
Specifically, after each aggregation round, the DRL algorithm considers the
local performance as the current state and outputs the optimized weights for
each class, in the training data, to be used during the next round of local
training. In doing so, the agent learns a policy that creates an optimized
partition of the local training dataset during the FL rounds. After FL, the
client utilizes the entire local training dataset to further enhance its
performance on its own data distribution, mitigating the non-IID effects of
aggregation. Through extensive experiments, we demonstrate that training FL
clients through our algorithm results in superior performance on multiple
benchmark datasets and FL frameworks. Our code is available at
https://github.com/amuraddd/optimized_client_training.git.

</details>


### [272] [From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins](https://arxiv.org/abs/2506.06359)
*Gabriel Antonesi,Tudor Cioara,Ionut Anghel,Vasilis Michalakopoulos,Elissaios Sarmas,Liana Toderean*

Main category: cs.LG

TL;DR: 综述探讨了AI在能源领域的最新进展，特别是Transformer和LLMs在智能电网中的应用，提出了Agentic Digital Twin的概念。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习在能源管理中面临泛化、情境感知和多源数据整合的挑战，而Transformer和LLMs展示了更强的能力。

Method: 综述了Transformer和LLMs的架构基础、领域适配及实际应用，并探讨了LLMs在能源领域的潜力与挑战。

Result: GenAI正逐步在能源管理的高层规划和日常操作中发挥作用，LLMs为数字孪生系统引入自主性和社交交互。

Conclusion: Agentic Digital Twin是下一代能源管理模型，整合LLMs以实现更智能的决策和操作。

Abstract: Artificial intelligence (AI) has long promised to improve energy management
in smart grids by enhancing situational awareness and supporting more effective
decision-making. While traditional machine learning has demonstrated notable
results in forecasting and optimization, it often struggles with
generalization, situational awareness, and heterogeneous data integration.
Recent advances in foundation models such as Transformer architecture and Large
Language Models (LLMs) have demonstrated improved capabilities in modelling
complex temporal and contextual relationships, as well as in multi-modal data
fusion which is essential for most AI applications in the energy sector. In
this review we synthesize the rapid expanding field of AI applications in the
energy domain focusing on Transformers and LLMs. We examine the architectural
foundations, domain-specific adaptations and practical implementations of
transformer models across various forecasting and grid management tasks. We
then explore the emerging role of LLMs in the field: adaptation and fine tuning
for the energy sector, the type of tasks they are suited for, and the new
challenges they introduce. Along the way, we highlight practical
implementations, innovations, and areas where the research frontier is rapidly
expanding. These recent developments reviewed underscore a broader trend:
Generative AI (GenAI) is beginning to augment decision-making not only in
high-level planning but also in day-to-day operations, from forecasting and
grid balancing to workforce training and asset onboarding. Building on these
developments, we introduce the concept of the Agentic Digital Twin, a
next-generation model that integrates LLMs to bring autonomy, proactivity, and
social interaction into digital twin-based energy management systems.

</details>


### [273] [Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events](https://arxiv.org/abs/2506.06380)
*Jingyi Gu,Xuan Zhang,Guiling Wang*

Main category: cs.LG

TL;DR: 本文综述了极端事件合成数据生成的首次概述，包括生成模型、评估框架和应用领域，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 极端事件（如市场崩溃、自然灾害）罕见但破坏性强，数据稀缺导致建模困难，合成数据生成成为解决方案。

Method: 系统回顾生成模型和大语言模型，结合统计理论和专门训练机制，提出评估框架和领域应用分类。

Result: 总结了生成技术和评估指标，分析了各指标在极端事件中的适用性，并识别了未充分探索的应用领域。

Conclusion: 为极端事件合成数据研究提供了结构化基础，并指出了未来挑战和方向。

Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are
rare but catastrophic, often triggering cascading failures across
interconnected systems. Accurate prediction and early warning can help minimize
losses and improve preparedness. While data-driven methods offer powerful
capabilities for extreme event modeling, they require abundant training data,
yet extreme event data is inherently scarce, creating a fundamental challenge.
Synthetic data generation has emerged as a powerful solution. However, existing
surveys focus on general data with privacy preservation emphasis, rather than
extreme events' unique performance requirements. This survey provides the first
overview of synthetic data generation for extreme events. We systematically
review generative modeling techniques and large language models, particularly
those enhanced by statistical theory as well as specialized training and
sampling mechanisms to capture heavy-tailed distributions. We summarize
benchmark datasets and introduce a tailored evaluation framework covering
statistical, dependence, visual, and task-oriented metrics. A central
contribution is our in-depth analysis of each metric's applicability in
extremeness and domain-specific adaptations, providing actionable guidance for
model evaluation in extreme settings. We categorize key application domains and
identify underexplored areas like behavioral finance, wildfires, earthquakes,
windstorms, and infectious outbreaks. Finally, we outline open challenges,
providing a structured foundation for advancing synthetic rare-event research.

</details>


### [274] [Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization](https://arxiv.org/abs/2506.06398)
*Yin Li*

Main category: cs.LG

TL;DR: 论文提出了一个理论框架，分析不同位置编码方法对Transformer表达能力、泛化能力和长序列外推能力的影响，并提出了基于正交函数的新编码方法。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer中位置编码方法的理论分析不足问题，为自然语言处理、计算机视觉等领域的设计选择提供理论支持。

Method: 通过函数逼近定义表达能力，使用Rademacher复杂度建立泛化界限，提出基于正交函数（如小波和Legendre多项式）的新编码方法。

Result: 实验表明，基于正交变换的编码方法在泛化和外推能力上优于传统的正弦编码方法。

Conclusion: 论文填补了Transformer理论的空白，为位置编码的设计提供了新的理论依据和实用方法。

Abstract: Positional encodings are a core part of transformer-based models, enabling
processing of sequential data without recurrence. This paper presents a
theoretical framework to analyze how various positional encoding methods,
including sinusoidal, learned, relative, and bias-based methods like Attention
with Linear Biases (ALiBi), impact a transformer's expressiveness,
generalization ability, and extrapolation to longer sequences. Expressiveness
is defined via function approximation, generalization bounds are established
using Rademacher complexity, and new encoding methods based on orthogonal
functions, such as wavelets and Legendre polynomials, are proposed. The
extrapolation capacity of existing and proposed encodings is analyzed,
extending ALiBi's biasing approach to a unified theoretical context.
Experimental evaluation on synthetic sequence-to-sequence tasks shows that
orthogonal transform-based encodings outperform traditional sinusoidal
encodings in generalization and extrapolation. This work addresses a critical
gap in transformer theory, providing insights for design choices in natural
language processing, computer vision, and other transformer applications.

</details>


### [275] [CoxNTF: A New Approach for Joint Clustering and Prediction in Survival Analysis](https://arxiv.org/abs/2506.06411)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: 提出了一种结合非负张量分解（NTF）和生存分析的CoxNTF方法，用于生成与生存结果相关的潜在特征表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如NMF）未结合生存信息，限制了预测能力。

Method: 使用NTF构建加权协变量张量，并通过Coxnet模型的生存概率指导张量化过程。

Result: CoxNTF在生存预测性能上与原始协变量的Coxnet相当，同时提供结构化且可解释的聚类框架。

Conclusion: CoxNTF是一种强大的工具，能有效处理特征冗余，适用于生存分析的联合聚类和预测。

Abstract: The interpretation of the results of survival analysis often benefits from
latent factor representations of baseline covariates. However, existing
methods, such as Nonnegative Matrix Factorization (NMF), do not incorporate
survival information, limiting their predictive power. We present CoxNTF, a
novel approach that uses non-negative tensor factorization (NTF) to derive
meaningful latent representations that are closely associated with survival
outcomes. CoxNTF constructs a weighted covariate tensor in which survival
probabilities derived from the Coxnet model are used to guide the tensorization
process. Our results show that CoxNTF achieves survival prediction performance
comparable to using Coxnet with the original covariates, while providing a
structured and interpretable clustering framework. In addition, the new
approach effectively handles feature redundancy, making it a powerful tool for
joint clustering and prediction in survival analysis.

</details>


### [276] [NeurNCD: Novel Class Discovery via Implicit Neural Representation](https://arxiv.org/abs/2506.06412)
*Junming Wang,Yi Shi*

Main category: cs.LG

TL;DR: NeurNCD是一种用于开放世界新类发现的数据高效框架，通过结合Embedding-NeRF模型和KL散度，替代传统显式3D分割图，提升语义嵌入和视觉嵌入空间的信息聚合能力。


<details>
  <summary>Details</summary>
Motivation: 传统显式表示（如对象描述符或3D分割图）存在离散、易产生漏洞和噪声的问题，限制了新类发现的准确性。

Method: NeurNCD采用Embedding-NeRF模型结合KL散度，集成特征查询、调制和聚类等关键组件，实现语义分割网络与隐式神经表示之间的高效信息交换。

Result: 在NYUv2和Replica数据集上，NeurNCD显著优于现有方法，无需密集标注数据或人工交互生成稀疏标签监督。

Conclusion: NeurNCD在开放和封闭世界设置中均表现出卓越的分割性能，为新类发现提供了一种高效且通用的解决方案。

Abstract: Discovering novel classes in open-world settings is crucial for real-world
applications. Traditional explicit representations, such as object descriptors
or 3D segmentation maps, are constrained by their discrete, hole-prone, and
noisy nature, which hinders accurate novel class discovery. To address these
challenges, we introduce NeurNCD, the first versatile and data-efficient
framework for novel class discovery that employs the meticulously designed
Embedding-NeRF model combined with KL divergence as a substitute for
traditional explicit 3D segmentation maps to aggregate semantic embedding and
entropy in visual embedding space. NeurNCD also integrates several key
components, including feature query, feature modulation and clustering,
facilitating efficient feature augmentation and information exchange between
the pre-trained semantic segmentation network and implicit neural
representations. As a result, our framework achieves superior segmentation
performance in both open and closed-world settings without relying on densely
labelled datasets for supervised training or human interaction to generate
sparse label supervision. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches on the NYUv2 and Replica
datasets.

</details>


### [277] [Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers](https://arxiv.org/abs/2506.06443)
*Luis Pinto*

Main category: cs.LG

TL;DR: 研究发现，分子编码器的中间层嵌入比最终层表现更好，固定嵌入平均提升5.4%，微调后平均提升8.5%，并提出了高效评估再微调的方法。


<details>
  <summary>Details</summary>
Motivation: 挑战仅依赖最终层嵌入的常规做法，探索分子编码器全层表示的价值。

Method: 对五种分子编码器进行分层分析，测试22种ADMET性质预测任务，比较固定嵌入和微调效果。

Result: 中间层嵌入表现更优，固定嵌入平均提升5.4%，微调后平均提升8.5%，部分任务提升高达40.8%。

Conclusion: 充分利用分子编码器的全层表示可显著提升性能与计算效率，代码已开源。

Abstract: Pretrained molecular encoders have become indispensable in computational
chemistry for tasks such as property prediction and molecular generation.
However, the standard practice of relying solely on final-layer embeddings for
downstream tasks may discard valuable information. In this work, we challenge
this convention by conducting a comprehensive layer-wise analysis of five
diverse molecular encoders across 22 ADMET property prediction tasks. Our
results demonstrate that embeddings from intermediate layers consistently
outperform final-layer representations. Specifically, using fixed embeddings
from the optimal intermediate layers improved downstream performance by an
average of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to
these intermediate layers yielded even greater average improvements of 8.5%,
with performance increases as high as 40.8%, achieving new state-of-the-art
results on several benchmarks. Additionally, a strong positive correlation
between fixed embedding performance and finetuning outcomes supports an
efficient evaluate-then-finetune approach, enabling identification of optimal
layers with reduced computational cost. These findings highlight the importance
of exploring the full representational depth of molecular encoders to achieve
substantial performance improvements and computational efficiency. The code is
made publicly available at
https://github.com/luispintoc/Unlocking-Chemical-Insights.

</details>


### [278] [Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance](https://arxiv.org/abs/2506.06444)
*Ruizhong Qiu,Gaotang Li,Tianxin Wei,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: 论文提出SAFFRON，一种针对LLM安全性的新型推理扩展范式，解决了传统方法在安全性上下文中的低效问题。


<details>
  <summary>Details</summary>
Motivation: 现有安全保证研究主要关注训练阶段的对齐，但易受越狱攻击影响，而推理扩展在安全性领域的潜力尚未探索。

Method: 提出SAFFRON范式，引入多叉奖励模型（MRM）减少奖励模型评估次数，并设计部分监督训练目标、保守探索约束和Trie缓存策略。

Result: 实验验证了SAFFRON的有效性，并公开了模型和数据集以推动未来研究。

Conclusion: SAFFRON为LLM安全性提供了高效解决方案，填补了推理扩展在安全领域的空白。

Abstract: Existing safety assurance research has primarily focused on training-phase
alignment to instill safe behaviors into LLMs. However, recent studies have
exposed these methods' susceptibility to diverse jailbreak attacks.
Concurrently, inference scaling has significantly advanced LLM reasoning
capabilities but remains unexplored in the context of safety assurance.
Addressing this gap, our work pioneers inference scaling for robust and
effective LLM safety against emerging threats. We reveal that conventional
inference scaling techniques, despite their success in reasoning tasks, perform
poorly in safety contexts, even falling short of basic approaches like
Best-of-N Sampling. We attribute this inefficiency to a newly identified
challenge, the exploration--efficiency dilemma, arising from the high
computational overhead associated with frequent process reward model (PRM)
evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference
scaling paradigm tailored explicitly for safety assurance. Central to our
approach is the introduction of a multifurcation reward model (MRM) that
significantly reduces the required number of reward model evaluations. To
operationalize this paradigm, we further propose: (i) a partial supervision
training objective for MRM, (ii) a conservative exploration constraint to
prevent out-of-distribution explorations, and (iii) a Trie-based key--value
caching strategy that facilitates cache sharing across sequences during tree
search. Extensive experiments validate the effectiveness of our method.
Additionally, we publicly release our trained multifurcation reward model
(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)
to accelerate future research in LLM safety. Our code, model, and data are
publicly available at https://github.com/q-rz/saffron , and our project
homepage is at https://q-rz.github.io/p/saffron .

</details>


### [279] [LETS Forecast: Learning Embedology for Time Series Forecasting](https://arxiv.org/abs/2506.06454)
*Abrar Majeedi,Viswanatha Reddy Gajjala,Satya Sai Srinath Namburi GNVV,Nada Magdi Elkordi,Yin Li*

Main category: cs.LG

TL;DR: DeepEDM是一个结合非线性动力学建模与深度学习的框架，通过学习延迟嵌入的潜在空间和核回归来逼近动态，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列通常具有复杂的非线性动态，现有深度学习方法未明确建模这些动态。

Method: DeepEDM结合了经验动态建模（EDM）和深度神经网络，利用延迟嵌入学习潜在空间，并通过核回归逼近动态。

Result: 实验表明，DeepEDM对输入噪声具有鲁棒性，并在预测精度上优于现有方法。

Conclusion: DeepEDM成功地将非线性动力学建模与深度学习结合，提升了时间序列预测的准确性。

Abstract: Real-world time series are often governed by complex nonlinear dynamics.
Understanding these underlying dynamics is crucial for precise future
prediction. While deep learning has achieved major success in time series
forecasting, many existing approaches do not explicitly model the dynamics. To
bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear
dynamical systems modeling with deep neural networks. Inspired by empirical
dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel
deep model that learns a latent space from time-delayed embeddings, and employs
kernel regression to approximate the underlying dynamics, while leveraging
efficient implementation of softmax attention and allowing for accurate
prediction of future time steps. To evaluate our method, we conduct
comprehensive experiments on synthetic data of nonlinear dynamical systems as
well as real-world time series across domains. Our results show that DeepEDM is
robust to input noise, and outperforms state-of-the-art methods in forecasting
accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.

</details>


### [280] [WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets](https://arxiv.org/abs/2506.06455)
*Antonio Jesús Banegas-Luna,Horacio Pérez-Sánchez,Carlos Martínez-Cortés*

Main category: cs.LG

TL;DR: 论文提出了一种新的共识方法WISCA，用于整合机器学习模型的解释性结果，以提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 在科学和高风险领域，模型的可解释性至关重要，但现有解释方法常产生冲突结果，需要共识策略来统一。

Method: 使用六种模型和合成数据集，结合多种模型无关的解释技术，提出WISCA方法整合概率和归一化属性。

Result: WISCA与最可靠的个体方法一致，验证了共识策略在提升解释可靠性中的价值。

Conclusion: WISCA是一种有效的共识方法，能显著提高机器学习模型解释的可靠性。

Abstract: While predictive accuracy is often prioritized in machine learning (ML)
models, interpretability remains essential in scientific and high-stakes
domains. However, diverse interpretability algorithms frequently yield
conflicting explanations, highlighting the need for consensus to harmonize
results. In this study, six ML models were trained on six synthetic datasets
with known ground truths, utilizing various model-agnostic interpretability
techniques. Consensus explanations were generated using established methods and
a novel approach: WISCA (Weighted Scaled Consensus Attributions), which
integrates class probability and normalized attributions. WISCA consistently
aligned with the most reliable individual method, underscoring the value of
robust consensus strategies in improving explanation reliability.

</details>


### [281] [Towards Infant Sleep-Optimized Driving: Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control](https://arxiv.org/abs/2506.06459)
*Ruitao Chen,Mozhang Guo,Jinge Li*

Main category: cs.LG

TL;DR: 论文提出了一种结合强化学习的智能巡航控制框架，通过优化驾驶行为提升婴儿睡眠质量，同时保持出行效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶虽提升了安全性和舒适性，但对婴儿睡眠的影响研究不足。急加速、急刹车等行为可能干扰婴儿睡眠，影响乘客舒适度和家长便利性。

Method: 整合LSTM和Transformer神经网络与强化学习，利用穿戴设备数据和车辆数据动态优化驾驶行为。

Result: 仿真结果显示，该方法显著提升了婴儿睡眠质量，同时保持了出行效率。

Conclusion: 研究为自动驾驶中乘客舒适度优化提供了新思路，尤其适用于有婴儿的家庭。

Abstract: Automated driving (AD) has substantially improved vehicle safety and driving
comfort, but their impact on passenger well-being, particularly infant sleep,
is not sufficiently studied. Sudden acceleration, abrupt braking, and sharp
maneuvers can disrupt infant sleep, compromising both passenger comfort and
parental convenience. To solve this problem, this paper explores the
integration of reinforcement learning (RL) within AD to personalize driving
behavior and optimally balance occupant comfort and travel efficiency. In
particular, we propose an intelligent cruise control framework that adapts to
varying driving conditions to enhance infant sleep quality by effectively
synergizing wearable sensing and vehicle data. Long short-term memory (LSTM)
and transformer-based neural networks are integrated with RL to model the
relationship between driving behavior and infant sleep quality under diverse
traffic and road conditions. Based on the sleep quality indicators from the
wearable sensors, driving action data from vehicle controllers, and map data
from map applications, the model dynamically computes the optimal driving
aggressiveness level, which is subsequently translated into specific AD control
strategies, e.g., the magnitude and frequency of acceleration, lane change, and
overtaking. Simulation results demonstrate that the proposed solution
significantly improves infant sleep quality compared to baseline methods, while
preserving desirable travel efficiency.

</details>


### [282] [TimeRecipe: A Time-Series Forecasting Recipe via Benchmarking Module Level Effectiveness](https://arxiv.org/abs/2506.06482)
*Zhiyuan Zhao,Juntong Ni,Shangqing Xu,Haoxin Liu,Wei Jin,B. Aditya Prakash*

Main category: cs.LG

TL;DR: TimeRecipe是一个统一的时间序列预测基准框架，通过模块级评估揭示设计选择的有效性，并推荐最佳模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有基准对模型评估较为笼统，缺乏对设计组件有效性的深入理解。

Method: TimeRecipe通过10,000多次实验，系统评估不同设计组件在多样化数据集和任务设置中的表现。

Result: 实验表明，全面探索设计空间可以超越现有最优方法，并揭示设计选择与预测场景的关联。

Conclusion: TimeRecipe提供了实用的工具包，基于实证结果推荐模型架构，填补了现有基准的不足。

Abstract: Time-series forecasting is an essential task with wide real-world
applications across domains. While recent advances in deep learning have
enabled time-series forecasting models with accurate predictions, there remains
considerable debate over which architectures and design components, such as
series decomposition or normalization, are most effective under varying
conditions. Existing benchmarks primarily evaluate models at a high level,
offering limited insight into why certain designs work better. To mitigate this
gap, we propose TimeRecipe, a unified benchmarking framework that
systematically evaluates time-series forecasting methods at the module level.
TimeRecipe conducts over 10,000 experiments to assess the effectiveness of
individual components across a diverse range of datasets, forecasting horizons,
and task settings. Our results reveal that exhaustive exploration of the design
space can yield models that outperform existing state-of-the-art methods and
uncover meaningful intuitions linking specific design choices to forecasting
scenarios. Furthermore, we release a practical toolkit within TimeRecipe that
recommends suitable model architectures based on these empirical insights. The
benchmark is available at: https://github.com/AdityaLab/TimeRecipe.

</details>


### [283] [A Certified Unlearning Approach without Access to Source Data](https://arxiv.org/abs/2506.06486)
*Umit Yigit Basaran,Sk Miraj Ahmed,Amit Roy-Chowdhury,Basak Guler*

Main category: cs.LG

TL;DR: 论文提出了一种无需原始训练数据的认证遗忘框架，通过替代数据集近似源数据统计特性，实现有效数据删除。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私法规的普及，从训练模型中删除私有或受版权保护信息的需求日益重要。传统方法依赖完整训练数据，但现实中源数据可能不可用。

Method: 利用替代数据集近似源数据统计特性，通过控制噪声缩放实现数据删除。理论保证基于统计距离，实际实现中近似该距离。

Result: 理论边界和实验验证表明，该方法在隐私敏感场景中有效可靠，保持模型实用性。

Conclusion: 提出的认证遗忘框架在无需原始数据的情况下，实现了强隐私保证和模型实用性。

Abstract: With the growing adoption of data privacy regulations, the ability to erase
private or copyrighted information from trained models has become a crucial
requirement. Traditional unlearning methods often assume access to the complete
training dataset, which is unrealistic in scenarios where the source data is no
longer available. To address this challenge, we propose a certified unlearning
framework that enables effective data removal \final{without access to the
original training data samples}. Our approach utilizes a surrogate dataset that
approximates the statistical properties of the source data, allowing for
controlled noise scaling based on the statistical distance between the two.
\updated{While our theoretical guarantees assume knowledge of the exact
statistical distance, practical implementations typically approximate this
distance, resulting in potentially weaker but still meaningful privacy
guarantees.} This ensures strong guarantees on the model's behavior
post-unlearning while maintaining its overall utility. We establish theoretical
bounds, introduce practical noise calibration techniques, and validate our
method through extensive experiments on both synthetic and real-world datasets.
The results demonstrate the effectiveness and reliability of our approach in
privacy-sensitive settings.

</details>


### [284] [Membership Inference Attacks for Unseen Classes](https://arxiv.org/abs/2506.06488)
*Pratiksha Thaker,Neil Kale,Zhiwei Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: 论文研究了在无法访问完整子类数据的情况下，成员推理攻击的性能下降问题，并提出了一种基于分位数回归的新方法，其性能优于传统的影子模型攻击。


<details>
  <summary>Details</summary>
Motivation: 现有影子模型攻击假设攻击者能访问与目标模型训练数据分布匹配的背景数据，但在实际中，攻击者可能无法访问完整子类数据。论文探讨了这种极端分布偏移下的成员推理攻击问题。

Method: 论文比较了影子模型攻击和分位数回归攻击在类丢失设置下的性能，并通过实验验证了分位数回归方法的优势。

Result: 分位数回归攻击在类丢失设置下表现优异，例如在CIFAR-100上对未见类的TPR是影子模型的11倍，在ImageNet上即使移除90%的训练类仍能取得显著效果。

Conclusion: 分位数回归攻击在极端分布偏移下优于影子模型攻击，论文还提供了理论模型说明其潜力与局限性。

Abstract: Shadow model attacks are the state-of-the-art approach for membership
inference attacks on machine learning models. However, these attacks typically
assume an adversary has access to a background (nonmember) data distribution
that matches the distribution the target model was trained on. We initiate a
study of membership inference attacks where the adversary or auditor cannot
access an entire subclass from the distribution -- a more extreme but realistic
version of distribution shift than has been studied previously. In this
setting, we first show that the performance of shadow model attacks degrades
catastrophically, and then demonstrate the promise of another approach,
quantile regression, that does not have the same limitations. We show that
quantile regression attacks consistently outperform shadow model attacks in the
class dropout setting -- for example, quantile regression attacks achieve up to
11$\times$ the TPR of shadow models on the unseen class on CIFAR-100, and
achieve nontrivial TPR on ImageNet even with 90% of training classes removed.
We also provide a theoretical model that illustrates the potential and
limitations of this approach.

</details>


### [285] [Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks](https://arxiv.org/abs/2506.06489)
*Daniel Kunin,Giovanni Luca Marchetti,Feng Chen,Dhruva Karkada,James B. Simon,Michael R. DeWeese,Surya Ganguli,Nina Miolane*

Main category: cs.LG

TL;DR: 论文提出交替梯度流（AGF）框架，用于描述小初始化下两层神经网络的特征学习动态，统一并扩展了现有分析。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络学习的特征及其动态过程，填补现有理论空白。

Method: 提出AGF框架，将梯度流行为近似为交替两步过程：休眠神经元最大化效用函数，活跃神经元最小化成本函数。

Result: AGF量化了特征获取的顺序、时间和幅度，与实验一致，并在多种架构中验证其有效性。

Conclusion: AGF为理解神经网络的特征学习提供了新视角，是理论分析的重要进展。

Abstract: What features neural networks learn, and how, remains an open question. In
this paper, we introduce Alternating Gradient Flows (AGF), an algorithmic
framework that describes the dynamics of feature learning in two-layer networks
trained from small initialization. Prior works have shown that gradient flow in
this regime exhibits a staircase-like loss curve, alternating between plateaus
where neurons slowly align to useful directions and sharp drops where neurons
rapidly grow in norm. AGF approximates this behavior as an alternating two-step
process: maximizing a utility function over dormant neurons and minimizing a
cost function over active ones. AGF begins with all neurons dormant. At each
round, a dormant neuron activates, triggering the acquisition of a feature and
a drop in the loss. AGF quantifies the order, timing, and magnitude of these
drops, matching experiments across architectures. We show that AGF unifies and
extends existing saddle-to-saddle analyses in fully connected linear networks
and attention-only linear transformers, where the learned features are singular
modes and principal components, respectively. In diagonal linear networks, we
prove AGF converges to gradient flow in the limit of vanishing initialization.
Applying AGF to quadratic networks trained to perform modular addition, we give
the first complete characterization of the training dynamics, revealing that
networks learn Fourier features in decreasing order of coefficient magnitude.
Altogether, AGF offers a promising step towards understanding feature learning
in neural networks.

</details>


### [286] [Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms](https://arxiv.org/abs/2506.06499)
*Alex Havrilla,Edward Hughes,Mikayel Samvelyan,Jacob Abernethy*

Main category: cs.LG

TL;DR: SPARQ利用质量-多样性算法生成高质量、多样化的数学问题-解决方案对，通过单一模型测量问题解决率（难度代理），显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大模型蒸馏或自然真实问题，限制了复杂和多样化问题领域的扩展性。

Method: SPARQ通过质量-多样性算法生成问题-解决方案对，基于解决率筛选数据，并用其微调模型。

Result: 生成2000万对数据，微调后模型性能提升24%；多样性和质量分别影响分布内和分布外泛化。

Conclusion: SPARQ展示了合成数据生成的可扩展性，质量与多样性对模型泛化各有贡献。

Abstract: Large language model (LLM) driven synthetic data generation has emerged as a
powerful method for improving model reasoning capabilities. However, most
methods either distill large state-of-the-art models into small students or use
natural ground-truth problem statements to guarantee problem statement quality.
This limits the scalability of these approaches to more complex and diverse
problem domains. To address this, we present SPARQ: Synthetic Problem
Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for
generating high-quality and diverse synthetic math problem and solution pairs
using only a single model by measuring a problem's solve-rate: a proxy for
problem difficulty. Starting from a seed dataset of 7.5K samples, we generate
over 20 million new problem-solution pairs. We show that filtering the
generated data by difficulty and then fine-tuning the same model on the
resulting data improves relative model performance by up to 24\%. Additionally,
we conduct ablations studying the impact of synthetic data quantity, quality
and diversity on model generalization. We find that higher quality, as measured
by problem difficulty, facilitates better in-distribution performance. Further,
while generating diverse synthetic data does not as strongly benefit
in-distribution performance, filtering for more diverse data facilitates more
robust OOD generalization. We also confirm the existence of model and data
scaling laws for synthetically generated problems, which positively benefit
downstream model generalization.

</details>


### [287] [Optimal Rates in Continual Linear Regression via Increasing Regularization](https://arxiv.org/abs/2506.06501)
*Ran Levinstein,Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: 论文研究了随机任务排序下的可实现连续线性回归，通过两种正则化方法缩小或消除理论上的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有未正则化方法的上界与理论下界存在显著差距，研究旨在通过正则化方法改进性能。

Method: 采用显式各向同性ℓ2正则化和隐式正则化（有限步预算），并将其转化为对特定替代损失的SGD优化。

Result: 固定正则化强度可实现接近最优的O(log k / k)速率，而递增正则化强度调度可达到最优O(1/k)速率。

Conclusion: 增加正则化系数或减少每任务步数的调度在理论上是有益的。

Abstract: We study realizable continual linear regression under random task orderings,
a common setting for developing continual learning theory. In this setup, the
worst-case expected loss after $k$ learning iterations admits a lower bound of
$\Omega(1/k)$. However, prior work using an unregularized scheme has only
established an upper bound of $O(1/k^{1/4})$, leaving a significant gap. Our
paper proves that this gap can be narrowed, or even closed, using two
frequently used regularization schemes: (1) explicit isotropic $\ell_2$
regularization, and (2) implicit regularization via finite step budgets. We
show that these approaches, which are used in practice to mitigate forgetting,
reduce to stochastic gradient descent (SGD) on carefully defined surrogate
losses. Through this lens, we identify a fixed regularization strength that
yields a near-optimal rate of $O(\log k / k)$. Moreover, formalizing and
analyzing a generalized variant of SGD for time-varying functions, we derive an
increasing regularization strength schedule that provably achieves an optimal
rate of $O(1/k)$. This suggests that schedules that increase the regularization
coefficient or decrease the number of steps per task are beneficial, at least
in the worst case.

</details>


### [288] [InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models](https://arxiv.org/abs/2506.06505)
*Keisuke Sugiura,Hiroki Matsutani*

Main category: cs.LG

TL;DR: InstantFT是一种基于FPGA的超快速CNN微调方法，适用于资源受限的IoT设备，显著提升了计算速度和能效。


<details>
  <summary>Details</summary>
Motivation: 由于深度神经网络（DNNs）的训练需要大量计算和内存，资源受限的IoT平台难以实现运行时适应。

Method: 通过优化参数高效微调（PEFT）中的前向和反向计算，InstantFT在FPGA上实现了快速CNN微调。

Result: 实验表明，InstantFT比现有的LoRA方法快17.4倍，微调时间仅为0.36秒，能效提升16.3倍。

Conclusion: InstantFT能够高效地在IoT设备上实现CNN的动态适应，适用于非稳态数据分布。

Abstract: Training deep neural networks (DNNs) requires significantly more computation
and memory than inference, making runtime adaptation of DNNs challenging on
resource-limited IoT platforms. We propose InstantFT, an FPGA-based method for
ultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and
backward computations in parameter-efficient fine-tuning (PEFT). Experiments on
datasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained
CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches,
while achieving comparable accuracy. Our FPGA-based InstantFT reduces the
fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x,
enabling on-the-fly adaptation of CNNs to non-stationary data distributions.

</details>


### [289] [Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs](https://arxiv.org/abs/2506.06521)
*Shulun Chen,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 论文研究了基于差距的遗憾边界问题，提出了一种方差感知的算法MVP，并给出了上界和下界分析。


<details>
  <summary>Details</summary>
Motivation: 探讨在马尔可夫决策过程（MDP）中，如何通过方差感知的方法优化遗憾边界，尤其是在子最优差距存在的情况下。

Method: 使用Monotonic Value Propagation (MVP)算法，结合方差感知的加权子最优差距分析。

Result: MVP算法实现了方差感知的遗憾上界，并证明了方差依赖的必要性。

Conclusion: 方差感知的遗憾边界分析是必要的，MVP算法在此类问题中表现优越。

Abstract: We consider the gap-dependent regret bounds for episodic MDPs. We show that
the Monotonic Value Propagation (MVP) algorithm achieves a variance-aware
gap-dependent regret bound of $$\tilde{O}\left(\left(\sum_{\Delta_h(s,a)>0}
\frac{H^2 \log K \land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}
+\sum_{\Delta_h(s,a)=0}\frac{ H^2 \land
\mathtt{Var}_{\max}^{\text{c}}}{\Delta_{\mathrm{min}}} + SAH^4 (S \lor H)
\right) \log K\right),$$ where $H$ is the planning horizon, $S$ is the number
of states, $A$ is the number of actions, and $K$ is the number of episodes.
Here, $\Delta_h(s,a) =V_h^* (a) - Q_h^* (s, a)$ represents the suboptimality
gap and $\Delta_{\mathrm{min}} := \min_{\Delta_h (s,a) > 0} \Delta_h(s,a)$. The
term $\mathtt{Var}_{\max}^{\text{c}}$ denotes the maximum conditional total
variance, calculated as the maximum over all $(\pi, h, s)$ tuples of the
expected total variance under policy $\pi$ conditioned on trajectories visiting
state $s$ at step $h$. $\mathtt{Var}_{\max}^{\text{c}}$ characterizes the
maximum randomness encountered when learning any $(h, s)$ pair. Our result
stems from a novel analysis of the weighted sum of the suboptimality gap and
can be potentially adapted for other algorithms. To complement the study, we
establish a lower bound of $$\Omega \left( \sum_{\Delta_h(s,a)>0} \frac{H^2
\land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}\cdot \log K\right),$$
demonstrating the necessity of dependence on $\mathtt{Var}_{\max}^{\text{c}}$
even when the maximum unconditional total variance (without conditioning on
$(h, s)$) approaches zero.

</details>


### [290] [Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.06532)
*Zijiang Yan,Hao Zhou,Jianhua Pei,Hina Tabassum*

Main category: cs.LG

TL;DR: 本文提出了一种基于大语言模型（LLM）的分层协作方法，用于多无人机（UAV）在动态和受限环境中的联合运动与通信控制，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 多无人机系统在动态和受限环境中的控制与优化是一个重要挑战，尤其是在集成地面和非地面网络（如高空平台站HAPS）的场景中。

Method: 采用分层协作的LLM框架，HAPS上的LLM负责无人机接入控制，而每架无人机上的LLM处理运动规划与控制。

Result: 实验表明，该方法在系统奖励、运营成本和无人机碰撞率方面优于基线方法。

Conclusion: 基于LLM的知识驱动范式为下一代3D空中高速公路系统的发展提供了潜力。

Abstract: Unmanned aerial vehicles (UAVs) have been widely adopted in various
real-world applications. However, the control and optimization of multi-UAV
systems remain a significant challenge, particularly in dynamic and constrained
environments. This work explores the joint motion and communication control of
multiple UAVs operating within integrated terrestrial and non-terrestrial
networks that include high-altitude platform stations (HAPS). Specifically, we
consider an aerial highway scenario in which UAVs must accelerate, decelerate,
and change lanes to avoid collisions and maintain overall traffic flow.
Different from existing studies, we propose a novel hierarchical and
collaborative method based on large language models (LLMs). In our approach, an
LLM deployed on the HAPS performs UAV access control, while another LLM onboard
each UAV handles motion planning and control. This LLM-based framework
leverages the rich knowledge embedded in pre-trained models to enable both
high-level strategic planning and low-level tactical decisions. This
knowledge-driven paradigm holds great potential for the development of
next-generation 3D aerial highway systems. Experimental results demonstrate
that our proposed collaborative LLM-based method achieves higher system
rewards, lower operational costs, and significantly reduced UAV collision rates
compared to baseline approaches.

</details>


### [291] [GeoClip: Geometry-Aware Clipping for Differentially Private SGD](https://arxiv.org/abs/2506.06549)
*Atefeh Gilani,Naima Tasnim,Lalitha Sankar,Oliver Kosut*

Main category: cs.LG

TL;DR: GeoClip是一种几何感知框架，通过变换梯度分布的基础来优化DP-SGD中的梯度裁剪和扰动，显著提升了隐私与效用的权衡。


<details>
  <summary>Details</summary>
Motivation: DP-SGD中梯度裁剪阈值的设置对隐私与效用权衡至关重要，现有方法未考虑梯度坐标间的相关性。

Method: 提出GeoClip框架，在变换后的基础上裁剪和扰动梯度，自适应估计变换且不增加隐私成本。

Result: 实验表明，GeoClip在相同隐私预算下优于现有自适应裁剪方法。

Conclusion: GeoClip通过几何感知的梯度处理，显著提升了DP-SGD的性能。

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the most
widely used method for training machine learning models with provable privacy
guarantees. A key challenge in DP-SGD is setting the per-sample gradient
clipping threshold, which significantly affects the trade-off between privacy
and utility. While recent adaptive methods improve performance by adjusting
this threshold during training, they operate in the standard coordinate system
and fail to account for correlations across the coordinates of the gradient. We
propose GeoClip, a geometry-aware framework that clips and perturbs gradients
in a transformed basis aligned with the geometry of the gradient distribution.
GeoClip adaptively estimates this transformation using only previously released
noisy gradients, incurring no additional privacy cost. We provide convergence
guarantees for GeoClip and derive a closed-form solution for the optimal
transformation that minimizes the amount of noise added while keeping the
probability of gradient clipping under control. Experiments on both tabular and
image datasets demonstrate that GeoClip consistently outperforms existing
adaptive clipping methods under the same privacy budget.

</details>


### [292] [SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks](https://arxiv.org/abs/2506.06556)
*Long Dang,Thushari Hapuarachchi,Kaiqi Xiong,Yi Li*

Main category: cs.LG

TL;DR: 提出了一种基于SDN的虚假数据检测与缓解系统（FDDMS），用于实时检测和缓解车内网络中的虚假数据注入攻击。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶和联网车辆的发展，车内网络的安全性至关重要，尤其是ECU之间的通信安全。

Method: 利用SDN技术，结合LSTM检测模型和对抗攻击评估，设计并实现FDDMS系统。

Result: 实验表明，FDDMS能有效对抗多种对抗攻击，实时检测和缓解虚假数据注入。

Conclusion: FDDMS为车内网络安全提供了一种高效且鲁棒的解决方案。

Abstract: As the development of autonomous and connected vehicles advances, the
complexity of modern vehicles increases, with numerous Electronic Control Units
(ECUs) integrated into the system. In an in-vehicle network, these ECUs
communicate with one another using an standard protocol called Controller Area
Network (CAN). Securing communication among ECUs plays a vital role in
maintaining the safety and security of the vehicle. This paper proposes a
robust SDN-based False Data Detection and Mitigation System (FDDMS) for
in-vehicle networks. Leveraging the unique capabilities of Software-Defined
Networking (SDN), FDDMS is designed to monitor and detect false data injection
attacks in real-time. Specifically, we focus on brake-related ECUs within an
SDN-enabled in-vehicle network. First, we decode raw CAN data to create an
attack model that illustrates how false data can be injected into the system.
Then, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection
model, is used to identify false data injection attacks. We further propose an
effective variant of DeepFool attack to evaluate the model's robustness. To
countermeasure the impacts of four adversarial attacks including Fast gradient
descent method, Basic iterative method, DeepFool, and the DeepFool variant, we
further enhance a re-training technique method with a threshold based selection
strategy. Finally, a mitigation scheme is implemented to redirect attack
traffic by dynamically updating flow rules through SDN. Our experimental
results show that the proposed FDDMS is robust against adversarial attacks and
effectively detects and mitigates false data injection attacks in real-time.

</details>


### [293] [Rapid training of Hamiltonian graph networks without gradient descent](https://arxiv.org/abs/2506.06558)
*Atamert Rahma,Chinmay Datar,Ana Cukarska,Felix Dietrich*

Main category: cs.LG

TL;DR: 论文提出了一种基于哈密顿图网络（HGN）的方法，通过随机特征参数构造替代传统迭代优化算法，显著提升了训练速度（最高600倍），同时保持了物理对称性和约束。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动建模中如何高效学习物理对称性和约束的动态系统的挑战。

Method: 使用哈密顿图网络（HGN）结合随机特征参数构造，替代传统的梯度优化算法（如Adam、RMSProp）。

Result: 在多种模拟中表现稳健，包括3维N体质量-弹簧系统，且无需重新训练即可泛化到更大系统（如4096节点）。

Conclusion: 该方法挑战了传统梯度优化算法在物理系统神经网络训练中的主导地位，提供了一种更高效的替代方案。

Abstract: Learning dynamical systems that respect physical symmetries and constraints
remains a fundamental challenge in data-driven modeling. Integrating physical
laws with graph neural networks facilitates principled modeling of complex
N-body dynamics and yields accurate and permutation-invariant models. However,
training graph neural networks with iterative, gradient-based optimization
algorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training,
especially for large, complex systems. In comparison to 15 different
optimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained
up to 600x faster--but with comparable accuracy--by replacing iterative
optimization with random feature-based parameter construction. We show robust
performance in diverse simulations, including N-body mass-spring systems in up
to 3 dimensions with different geometries, while retaining essential physical
invariances with respect to permutation, rotation, and translation. We reveal
that even when trained on minimal 8-node systems, the model can generalize in a
zero-shot manner to systems as large as 4096 nodes without retraining. Our work
challenges the dominance of iterative gradient-descent-based optimization
algorithms for training neural network models for physical systems.

</details>


### [294] [Graph Persistence goes Spectral](https://arxiv.org/abs/2506.06571)
*Mattie Ji,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: SpectRe是一种新的图拓扑描述符，通过将谱信息融入持久同调图，显著提升了表达能力，并证明了其局部稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖顶点和边特征，无法捕捉基本图结构信息，因此需要一种更强大的描述符。

Method: 提出SpectRe，将谱信息集成到持久同调图中，并分析其全局和局部稳定性。

Result: SpectRe在表达能力和稳定性上优于现有方法，实验验证了其有效性。

Conclusion: SpectRe为图表示学习提供了更强大的工具，有望提升相关学习任务的性能。

Abstract: Including intricate topological information (e.g., cycles) provably enhances
the expressivity of message-passing graph neural networks (GNNs) beyond the
Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods
are increasingly employed for graph representation learning. In this context,
recent works have proposed decorating classical PH diagrams with vertex and
edge features for improved expressivity. However, due to their dependence on
features, these methods still fail to capture basic graph structural
information. In this paper, we propose SpectRe -- a new topological descriptor
for graphs that integrates spectral information into PH diagrams. Notably,
SpectRe is strictly more expressive than existing descriptors on graphs. We
also introduce notions of global and local stability to analyze existing
descriptors and establish that SpectRe is locally stable. Finally, experiments
on synthetic and real-world datasets demonstrate the effectiveness of SpectRe
and its potential to enhance the capabilities of graph models in relevant
learning tasks.

</details>


### [295] [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
*Adarsh Prasad Behera,Jaya Prakash Champati,Roberto Morabito,Sasu Tarkoma,James Gross*

Main category: cs.LG

TL;DR: 论文探讨了如何通过动态模型选择和分层推理策略优化语言模型的计算效率，以在资源受限的环境中部署。


<details>
  <summary>Details</summary>
Motivation: 语言模型的计算和能源消耗高，难以在移动、边缘或成本敏感环境中部署，需要高效推理方法。

Method: 采用路由和分层推理（HI）策略，动态选择模型或逐步升级模型以处理查询。

Result: 这些策略能显著减少计算量，同时保持性能，适用于不同复杂度的任务。

Conclusion: 未来研究方向包括更快响应时间、自适应模型选择和可扩展部署，以提升语言模型的实用性和效率。

Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field
of natural language processing (NLP), excelling at tasks like text generation,
summarization, and question answering. However, their inference remains
computationally expensive and energy intensive, especially in settings with
limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in
mobile, edge, or cost sensitive environments. To address these challenges,
recent approaches have introduced multi LLM intelligent model selection
strategies that dynamically allocate computational resources based on query
complexity -- using lightweight models for simpler queries and escalating to
larger models only when necessary. This survey explores two complementary
strategies for efficient LLM inference: (i) routing, which selects the most
suitable model based on the query, and (ii) cascading or hierarchical inference
(HI), which escalates queries through a sequence of models until a confident
response is found. Both approaches aim to reduce computation by using
lightweight models for simpler tasks while offloading only when needed. We
provide a comparative analysis of these techniques across key performance
metrics, discuss benchmarking efforts, and outline open challenges. Finally, we
outline future research directions to enable faster response times, adaptive
model selection based on task complexity, and scalable deployment across
heterogeneous environments, making LLM based systems more efficient and
accessible for real world applications.

</details>


### [296] [Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing](https://arxiv.org/abs/2506.06582)
*Diaaeldin Taha,James Chapman,Marzieh Eidi,Karel Devriendt,Guido Montúfar*

Main category: cs.LG

TL;DR: 论文提出了一种统一的公理化框架，将图和拓扑消息传递联系起来，以分析并缓解拓扑消息传递中的过度压缩问题。


<details>
  <summary>Details</summary>
Motivation: 拓扑深度学习（TDL）在建模关系数据中的高阶交互方面表现出强大能力，但拓扑消息传递中的过度压缩现象缺乏理论分析。

Method: 通过将单纯和细胞复形及其消息传递方案视为关系结构，扩展图论结果和算法到高阶结构。

Result: 理论分析和单纯网络的实证研究表明，该框架在推动TDL方面具有潜力。

Conclusion: 该框架为拓扑消息传递中的问题提供了理论支持，并展示了其在TDL中的实际应用价值。

Abstract: Topological deep learning (TDL) has emerged as a powerful tool for modeling
higher-order interactions in relational data. However, phenomena such as
oversquashing in topological message-passing remain understudied and lack
theoretical analysis. We propose a unifying axiomatic framework that bridges
graph and topological message-passing by viewing simplicial and cellular
complexes and their message-passing schemes through the lens of relational
structures. This approach extends graph-theoretic results and algorithms to
higher-order structures, facilitating the analysis and mitigation of
oversquashing in topological message-passing networks. Through theoretical
analysis and empirical studies on simplicial networks, we demonstrate the
potential of this framework to advance TDL.

</details>


### [297] [Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixtures](https://arxiv.org/abs/2506.06584)
*Mo Zhou,Weihang Xu,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 论文研究了高斯混合模型（GMM）在过参数化设置下的全局收敛性，证明了梯度EM算法在随机初始化下可以全局收敛到真实参数。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅证明了EM算法在m=2时的全局收敛性，而在m≥3时失败。本文旨在探讨过参数化设置下梯度EM的全局收敛性。

Method: 使用Hermite多项式分析梯度EM的动态，并利用张量分解刻画似然损失函数的几何特性。

Result: 证明了对于任何良好分离的GMM，在n=Ω(mlogm)的过参数化条件下，梯度EM能全局收敛到真实参数。

Conclusion: 这是首次在m>2的情况下证明EM或梯度EM的全局收敛性，为GMM学习提供了新的理论支持。

Abstract: Learning Gaussian Mixture Models (GMMs) is a fundamental problem in machine
learning, with the Expectation-Maximization (EM) algorithm and its popular
variant gradient EM being arguably the most widely used algorithms in practice.
In the exact-parameterized setting, where both the ground truth GMM and the
learning model have the same number of components $m$, a vast line of work has
aimed to establish rigorous recovery guarantees for EM. However, global
convergence has only been proven for the case of $m=2$, and EM is known to fail
to recover the ground truth when $m\geq 3$.
  In this paper, we consider the $\textit{over-parameterized}$ setting, where
the learning model uses $n>m$ components to fit an $m$-component ground truth
GMM. In contrast to the exact-parameterized case, we provide a rigorous global
convergence guarantee for gradient EM. Specifically, for any well separated
GMMs in general position, we prove that with only mild over-parameterization $n
= \Omega(m\log m)$, randomly initialized gradient EM converges globally to the
ground truth at a polynomial rate with polynomial samples. Our analysis
proceeds in two stages and introduces a suite of novel tools for Gaussian
Mixture analysis. We use Hermite polynomials to study the dynamics of gradient
EM and employ tensor decomposition to characterize the geometric landscape of
the likelihood loss. This is the first global convergence and recovery result
for EM or Gradient EM beyond the special case of $m=2$.

</details>


### [298] [Direct Prediction Set Minimization via Bilevel Conformal Classifier Training](https://arxiv.org/abs/2506.06599)
*Yuanjie Shi,Hooman Shahrokhi,Xuesong Jia,Xiongzhi Chen,Janardhan Rao Doppa,Yan Yan*

Main category: cs.LG

TL;DR: 本文提出了一种名为DPSM的算法，通过将共形预测原则融入深度分类器的训练过程，直接最小化预测集的大小，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准共形预测校准方法生成的预测集通常过大，限制了其实际应用。本文旨在通过训练过程直接优化预测集大小。

Method: 将共形训练建模为双层优化问题，提出DPSM算法，通过最小化预测集大小的度量（上层）并基于一致性分数的分位数（下层）实现。

Result: DPSM在多个基准数据集和深度模型上表现优异，预测集大小减少了20.46%，且理论分析显示其学习边界优于现有方法。

Conclusion: DPSM通过直接优化预测集大小，显著提升了共形预测的实用性，并验证了其理论优势。

Abstract: Conformal prediction (CP) is a promising uncertainty quantification framework
which works as a wrapper around a black-box classifier to construct prediction
sets (i.e., subset of candidate classes) with provable guarantees. However,
standard calibration methods for CP tend to produce large prediction sets which
makes them less useful in practice. This paper considers the problem of
integrating conformal principles into the training process of deep classifiers
to directly minimize the size of prediction sets. We formulate conformal
training as a bilevel optimization problem and propose the {\em Direct
Prediction Set Minimization (DPSM)} algorithm to solve it. The key insight
behind DPSM is to minimize a measure of the prediction set size (upper level)
that is conditioned on the learned quantile of conformity scores (lower level).
We analyze that DPSM has a learning bound of $O(1/\sqrt{n})$ (with $n$ training
samples), while prior conformal training methods based on stochastic
approximation for the quantile has a bound of $\Omega(1/s)$ (with batch size
$s$ and typically $s \ll \sqrt{n}$). Experiments on various benchmark datasets
and deep models show that DPSM significantly outperforms the best prior
conformal training baseline with $20.46\%\downarrow$ in the prediction set size
and validates our theory.

</details>


### [299] [CAtCh: Cognitive Assessment through Cookie Thief](https://arxiv.org/abs/2506.06603)
*Joseph T Colonel,Carolyn Hagler,Guiselle Wismer,Laura Curtis,Jacqueline Becker,Juan Wisnivesky,Alex Federman,Gaurav Pandey*

Main category: cs.LG

TL;DR: 论文研究了从语音中预测认知障碍（CI）的方法，比较了多模态和单模态方法，发现多模态方法表现更优，且声学特征优于语言学特征。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习算法主要针对阿尔茨海默病及相关痴呆（ADRD）的预测，但未扩展到更广泛的认知障碍（CI）预测，而CI可能是ADRD的前兆和风险因素。

Method: 评估了基于语音的开源方法（原用于ADRD预测）和多模态情感分析方法，用于从患者音频中预测CI。

Result: 多模态方法优于单模态方法，声学特征（尤其是情感和韵律相关特征）显著优于BERT语言学特征和可解释语言学特征。

Conclusion: 声学特征在多模态方法中对CI预测更有效，研究代码已开源。

Abstract: Several machine learning algorithms have been developed for the prediction of
Alzheimer's disease and related dementia (ADRD) from spontaneous speech.
However, none of these algorithms have been translated for the prediction of
broader cognitive impairment (CI), which in some cases is a precursor and risk
factor of ADRD. In this paper, we evaluated several speech-based open-source
methods originally proposed for the prediction of ADRD, as well as methods from
multimodal sentiment analysis for the task of predicting CI from patient audio
recordings. Results demonstrated that multimodal methods outperformed unimodal
ones for CI prediction, and that acoustics-based approaches performed better
than linguistics-based ones. Specifically, interpretable acoustic features
relating to affect and prosody were found to significantly outperform
BERT-based linguistic features and interpretable linguistic features,
respectively. All the code developed for this study is available at
https://github.com/JTColonel/catch.

</details>


### [300] [Stacey: Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization](https://arxiv.org/abs/2506.06606)
*Xinyu Luo,Cedar Site Bai,Bolian Li,Petros Drineas,Ruqi Zhang,Brian Bullins*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While popular optimization methods such as SGD, AdamW, and Lion depend on
steepest descent updates in either $\ell_2$ or $\ell_\infty$ norms, there
remains a critical gap in handling the non-Euclidean structure observed in
modern deep networks training. In this work, we address this need by
introducing a new accelerated $\ell_p$ steepest descent algorithm, called
Stacey, which uses interpolated primal-dual iterate sequences to effectively
navigate non-Euclidean smooth optimization tasks. In addition to providing
novel theoretical guarantees for the foundations of our algorithm, we
empirically compare our approach against these popular methods on tasks
including image classification and language model (LLM) pretraining,
demonstrating both faster convergence and higher final accuracy. We further
evaluate different values of $p$ across various models and datasets,
underscoring the importance and efficiency of non-Euclidean approaches over
standard Euclidean methods. Code can be found at
https://github.com/xinyuluo8561/Stacey .

</details>


### [301] [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
*Shubham Parashar,Shurui Gui,Xiner Li,Hongyi Ling,Sushil Vemuri,Blake Olson,Eric Li,Yu Zhang,James Caverlee,Dileep Kalathil,Shuiwang Ji*

Main category: cs.LG

TL;DR: 论文提出了一种名为E2H Reasoner的方法，通过从易到难的任务调度（E2H）提升语言模型的推理能力，证明了其在小型语言模型上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，仅使用强化学习（RL）难以有效提升语言模型在复杂任务上的推理能力，因此需要一种更有效的方法。

Method: 采用课程学习思路，设计从易到难的任务调度（E2H），并通过适当的调度防止过拟合。

Result: 实验证明，E2H Reasoner显著提升了小型语言模型（1.5B到3B）的推理能力，且样本效率更高。

Conclusion: E2H Reasoner是一种有效的方法，通过任务调度逐步提升语言模型的推理能力，尤其适用于小型模型。

Abstract: We aim to improve the reasoning capabilities of language models via
reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1
have demonstrated reasoning abilities on mathematical and coding tasks.
However, prior studies suggest that using RL alone to improve reasoning on
inherently difficult tasks is less effective. Here, we draw inspiration from
curriculum learning and propose to schedule tasks from easy to hard (E2H),
allowing LLMs to build reasoning skills gradually. Our method is termed E2H
Reasoner. Empirically, we observe that, although easy tasks are important
initially, fading them out through appropriate scheduling is essential in
preventing overfitting. Theoretically, we establish convergence guarantees for
E2H Reasoner within an approximate policy iteration framework. We derive
finite-sample complexity bounds and show that when tasks are appropriately
decomposed and conditioned, learning through curriculum stages requires fewer
total samples than direct learning. Experiments across multiple domains show
that E2H Reasoner significantly improves the reasoning ability of small LLMs
(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,
highlighting the effectiveness of our method.

</details>


### [302] [Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification](https://arxiv.org/abs/2506.06633)
*Chi-Sheng Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为Vision-QRWKV的混合量子-经典模型，用于图像分类任务，通过将变分量子电路集成到RWKV架构中，提升了非线性特征转换能力。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在复杂高维数据领域展现出潜力，本文旨在探索量子增强的RWKV架构在视觉任务中的应用。

Method: 将变分量子电路（VQC）集成到RWKV的通道混合组件中，构建了Vision-QRWKV模型。

Result: 在14个医学和标准图像分类基准测试中，量子增强模型在多数数据集上优于经典模型，尤其在具有细微或噪声类别区分的数据集上表现突出。

Conclusion: 该研究首次系统地将量子增强RWKV应用于视觉领域，为轻量高效的量子视觉模型提供了未来发展方向。

Abstract: Recent advancements in quantum machine learning have shown promise in
enhancing classical neural network architectures, particularly in domains
involving complex, high-dimensional data. Building upon prior work in temporal
sequence modeling, this paper introduces Vision-QRWKV, a hybrid
quantum-classical extension of the Receptance Weighted Key Value (RWKV)
architecture, applied for the first time to image classification tasks. By
integrating a variational quantum circuit (VQC) into the channel mixing
component of RWKV, our model aims to improve nonlinear feature transformation
and enhance the expressive capacity of visual representations.
  We evaluate both classical and quantum RWKV models on a diverse collection of
14 medical and standard image classification benchmarks, including MedMNIST
datasets, MNIST, and FashionMNIST. Our results demonstrate that the
quantum-enhanced model outperforms its classical counterpart on a majority of
datasets, particularly those with subtle or noisy class distinctions (e.g.,
ChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first
systematic application of quantum-enhanced RWKV in the visual domain, offering
insights into the architectural trade-offs and future potential of quantum
models for lightweight and efficient vision tasks.

</details>


### [303] [Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning](https://arxiv.org/abs/2506.06637)
*Olimjon Toirov,Wei Yu*

Main category: cs.LG

TL;DR: 提出了一种结合图像负载特征和持续学习的非侵入式负载监测方法，显著提高了识别精度。


<details>
  <summary>Details</summary>
Motivation: 传统NILM方法在复杂负载组合和应用环境下特征鲁棒性和模型泛化能力不足。

Method: 将多维电力信号转换为图像负载特征，结合深度卷积神经网络和持续在线学习策略。

Result: 在高采样率负载数据集上实验表明，识别精度显著提升。

Conclusion: 该方法有效解决了传统NILM方法的局限性，适应新负载的出现。

Abstract: Non-Intrusive Load Monitoring (NILM) identifies the operating status and
energy consumption of each electrical device in the circuit by analyzing the
electrical signals at the bus, which is of great significance for smart power
management. However, the complex and changeable load combinations and
application environments lead to the challenges of poor feature robustness and
insufficient model generalization of traditional NILM methods. To this end,
this paper proposes a new non-intrusive load monitoring method that integrates
"image load signature" and continual learning. This method converts
multi-dimensional power signals such as current, voltage, and power factor into
visual image load feature signatures, and combines deep convolutional neural
networks to realize the identification and classification of multiple devices;
at the same time, self-supervised pre-training is introduced to improve feature
generalization, and continual online learning strategies are used to overcome
model forgetting to adapt to the emergence of new loads. This paper conducts a
large number of experiments on high-sampling rate load datasets, and compares a
variety of existing methods and model variants. The results show that the
proposed method has achieved significant improvements in recognition accuracy.

</details>


### [304] [Spark Transformer: Reactivating Sparsity in FFN and Attention](https://arxiv.org/abs/2506.06644)
*Chong You,Kan Wu,Zhipeng Jia,Lin Chen,Srinadh Bhojanapalli,Jiaxian Guo,Utku Evci,Jan Wassenberg,Praneeth Netrapalli,Jeremiah J. Willcock,Suvinay Subramanian,Felix Chern,Alek Andreev,Shreya Pathak,Felix Yu,Prateek Jain,David E. Culler,Henry M. Levy,Sanjiv Kumar*

Main category: cs.LG

TL;DR: Spark Transformer通过top-k掩码和统计top-k算法实现高激活稀疏性，保持模型质量的同时显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 解决现有Transformer中激活稀疏性引入导致的模型质量下降、参数增加或训练复杂化问题。

Method: 采用top-k掩码和统计top-k算法，重新分配FFN参数和注意力键嵌入以预测激活条目。

Result: 在标准基准测试中表现优异，FFN神经元激活率仅8%，计算量减少2.5倍，解码速度提升1.79x（CPU）和1.40x（GPU）。

Conclusion: Spark Transformer在保持高效训练和模型质量的同时，实现了显著的激活稀疏性和计算效率提升。

Abstract: The discovery of the lazy neuron phenomenon in trained Transformers, where
the vast majority of neurons in their feed-forward networks (FFN) are inactive
for each token, has spurred tremendous interests in activation sparsity for
enhancing large model efficiency. While notable progress has been made in
translating such sparsity to wall-time benefits, modern Transformers have moved
away from the ReLU activation function crucial to this phenomenon. Existing
efforts on re-introducing activation sparsity often degrade model quality,
increase parameter count, complicate or slow down training. Sparse attention,
the application of sparse activation to the attention mechanism, often faces
similar challenges.
  This paper introduces the Spark Transformer, a novel architecture that
achieves a high level of activation sparsity in both FFN and the attention
mechanism while maintaining model quality, parameter count, and standard
training procedures. Our method realizes sparsity via top-k masking for
explicit control over sparsity level. Crucially, we introduce statistical
top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that
avoids costly sorting and mitigates significant training slowdown from standard
top-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN
parameters and attention key embeddings to form a low-cost predictor for
identifying activated entries. This design not only mitigates quality loss from
enforced sparsity, but also enhances wall-time benefit. Pretrained with the
Gemma-2 recipe, Spark Transformer demonstrates competitive performance on
standard benchmarks while exhibiting significant sparsity: only 8% of FFN
neurons are activated, and each token attends to a maximum of 256 tokens. This
sparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time
speedups of up to 1.79x on CPU and 1.40x on GPU.

</details>


### [305] [SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.06649)
*Yishan Shen,Yuyang Ye,Hui Xiong,Yong Chen*

Main category: cs.LG

TL;DR: SAFER是一个结合结构化电子健康记录和临床笔记的风险感知推荐框架，用于动态治疗策略（DTR），提供统计保证和更优的治疗建议。


<details>
  <summary>Details</summary>
Motivation: 动态治疗策略在精准医学中至关重要，但现有方法依赖临床标准且未充分利用临床笔记，限制了可靠性。

Method: SAFER整合结构化数据和临床笔记，通过假设模糊最优治疗解决标签不确定性，并采用保形预测提供统计保证。

Result: 在公开的败血症数据集上，SAFER在推荐指标和反事实死亡率上优于现有方法，并提供稳健的统计保证。

Conclusion: SAFER是一个可信赖且理论扎实的高风险DTR应用解决方案。

Abstract: Dynamic treatment regimes (DTRs) are critical to precision medicine,
optimizing long-term outcomes through personalized, real-time decision-making
in evolving clinical contexts, but require careful supervision for unsafe
treatment risks. Existing efforts rely primarily on clinician-prescribed gold
standards despite the absence of a known optimal strategy, and predominantly
using structured EHR data without extracting valuable insights from clinical
notes, limiting their reliability for treatment recommendations. In this work,
we introduce SAFER, a calibrated risk-aware tabular-language recommendation
framework for DTR that integrates both structured EHR and clinical notes,
enabling them to learn from each other, and addresses inherent label
uncertainty by assuming ambiguous optimal treatment solution for deceased
patients. Moreover, SAFER employs conformal prediction to provide statistical
guarantees, ensuring safe treatment recommendations while filtering out
uncertain predictions. Experiments on two publicly available sepsis datasets
demonstrate that SAFER outperforms state-of-the-art baselines across multiple
recommendation metrics and counterfactual mortality rate, while offering robust
formal assurances. These findings underscore SAFER potential as a trustworthy
and theoretically grounded solution for high-stakes DTR applications.

</details>


### [306] [Rescaled Influence Functions: Accurate Data Attribution in High Dimension](https://arxiv.org/abs/2506.06656)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: 论文提出了一种改进的影响函数方法（RIF），用于更准确地评估训练数据对模型行为的影响，解决了传统影响函数（IF）在高维数据中的不精确问题。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据如何影响模型行为，传统影响函数（IF）在高维数据中表现不佳，需要更精确的数据归因方法。

Method: 提出了一种改进的影响函数方法（RIF），通过重新缩放提升精度，并验证其在真实数据集上的表现。

Result: RIF在多种真实数据集上显著优于IF，且能检测到IF无法识别的数据投毒攻击。

Conclusion: RIF是一种高效且准确的数据归因工具，可作为IF的替代方案，适用于多种机器学习应用。

Abstract: How does the training data affect a model's behavior? This is the question we
seek to answer with data attribution. The leading practical approaches to data
attribution are based on influence functions (IF). IFs utilize a first-order
Taylor approximation to efficiently predict the effect of removing a set of
samples from the training set without retraining the model, and are used in a
wide variety of machine learning applications. However, especially in the
high-dimensional regime (# params $\geq \Omega($# samples$)$), they are often
imprecise and tend to underestimate the effect of sample removals, even for
simple models such as logistic regression. We present rescaled influence
functions (RIF), a new tool for data attribution which can be used as a drop-in
replacement for influence functions, with little computational overhead but
significant improvement in accuracy. We compare IF and RIF on a range of
real-world datasets, showing that RIFs offer significantly better predictions
in practice, and present a theoretical analysis explaining this improvement.
Finally, we present a simple class of data poisoning attacks that would fool
IF-based detections but would be detected by RIF.

</details>


### [307] [SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming](https://arxiv.org/abs/2506.06665)
*Hong-Ming Chiu,Hao Chen,Huan Zhang,Richard Y. Zhang*

Main category: cs.LG

TL;DR: SDP-CROWN是一种新型混合验证框架，结合了SDP松弛的紧致性和边界传播验证器的可扩展性，显著提升了大规模神经网络的验证性能。


<details>
  <summary>Details</summary>
Motivation: 现有的线性边界传播验证器在大规模模型上表现良好但紧致性不足，而SDP验证器虽能捕捉神经元耦合但计算复杂度高。

Method: 提出SDP-CROWN框架，通过SDP原理推导新的线性边界，显式捕捉神经元间的耦合，同时保持可扩展性。

Result: 理论证明新边界比传统方法紧致√n倍；实践验证中，在大型模型上性能显著提升，接近SDP方法的紧致性。

Conclusion: SDP-CROWN成功结合了SDP的紧致性和边界传播的可扩展性，为大规模神经网络验证提供了高效解决方案。

Abstract: Neural network verifiers based on linear bound propagation scale impressively
to massive models but can be surprisingly loose when neuron coupling is
crucial. Conversely, semidefinite programming (SDP) verifiers capture
inter-neuron coupling naturally, but their cubic complexity restricts them to
only small models. In this paper, we propose SDP-CROWN, a novel hybrid
verification framework that combines the tightness of SDP relaxations with the
scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new
linear bound, derived via SDP principles, that explicitly captures
$\ell_{2}$-norm-based inter-neuron coupling while adding only one extra
parameter per layer. This bound can be integrated seamlessly into any linear
bound-propagation pipeline, preserving the inherent scalability of such methods
yet significantly improving tightness. In theory, we prove that our
inter-neuron bound can be up to a factor of $\sqrt{n}$ tighter than traditional
per-neuron bounds. In practice, when incorporated into the state-of-the-art
$\alpha$-CROWN verifier, we observe markedly improved verification performance
on large models with up to 65 thousand neurons and 2.47 million parameters,
achieving tightness that approaches that of costly SDP-based methods.

</details>


### [308] [Through the Gaps: Uncovering Tactical Line-Breaking Passes with Clustering](https://arxiv.org/abs/2506.06666)
*Oktay Karakuş,Hasan Arkadaş*

Main category: cs.LG

TL;DR: 提出了一种基于聚类的无监督框架，用于检测和分析足球比赛中的线突破传球（LBPs），并引入战术指标评估其效果。


<details>
  <summary>Details</summary>
Motivation: 线突破传球（LBPs）是足球中关键的战术动作，能够穿透防线并进入高价值区域，但缺乏系统的分析方法。

Method: 通过垂直空间分割建模对手队形，利用同步事件和追踪数据检测LBPs，并引入空间构建比（SBR）和链式指标（LBPCh^1、LBPCh^2）量化其效果。

Result: 在2022年世界杯数据中验证了方法的有效性，揭示了不同球队和球员在垂直推进和结构破坏上的风格差异。

Conclusion: 该方法可解释、可扩展，适用于现代表现分析和球探工作流程。

Abstract: Line-breaking passes (LBPs) are crucial tactical actions in football,
allowing teams to penetrate defensive lines and access high-value spaces. In
this study, we present an unsupervised, clustering-based framework for
detecting and analysing LBPs using synchronised event and tracking data from
elite matches. Our approach models opponent team shape through vertical spatial
segmentation and identifies passes that disrupt defensive lines within open
play. Beyond detection, we introduce several tactical metrics, including the
space build-up ratio (SBR) and two chain-based variants, LBPCh$^1$ and
LBPCh$^2$, which quantify the effectiveness of LBPs in generating immediate or
sustained attacking threats. We evaluate these metrics across teams and players
in the 2022 FIFA World Cup, revealing stylistic differences in vertical
progression and structural disruption. The proposed methodology is explainable,
scalable, and directly applicable to modern performance analysis and scouting
workflows.

</details>


### [309] [Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics](https://arxiv.org/abs/2506.06682)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: HetCRF是一种针对异构图的双通道自监督学习框架，通过两阶段聚合策略和正样本增强策略，解决了MAE和CL在异构图中的兼容性问题，并在节点分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决异构图自监督学习中MAE和CL的互补性问题，尤其是在语义稀疏场景下CL的视图构建困难和梯度不平衡问题。

Method: 提出HetCRF框架，采用两阶段聚合策略适应嵌入语义，增强编码器输出以改进视图构建，并提出两种正样本增强策略平衡梯度贡献。

Result: 在四个真实异构图数据集上的节点分类实验中，HetCRF在缺失节点特征的情况下显著优于现有基线方法，Macro-F1分数提升显著。

Conclusion: HetCRF通过创新的双通道设计和正样本增强策略，有效提升了异构图自监督学习的性能，尤其在语义稀疏场景下表现突出。

Abstract: In graph self-supervised learning, masked autoencoders (MAE) and contrastive
learning (CL) are two prominent paradigms. MAE focuses on reconstructing masked
elements, while CL maximizes similarity between augmented graph views. Recent
studies highlight their complementarity: MAE excels at local feature capture,
and CL at global information extraction. Hybrid frameworks for homogeneous
graphs have been proposed, but face challenges in designing shared encoders to
meet the semantic requirements of both tasks. In semantically sparse scenarios,
CL struggles with view construction, and gradient imbalance between positive
and negative samples persists. This paper introduces HetCRF, a novel
dual-channel self-supervised learning framework for heterogeneous graphs.
HetCRF uses a two-stage aggregation strategy to adapt embedding semantics,
making it compatible with both MAE and CL. To address semantic sparsity, it
enhances encoder output for view construction instead of relying on raw
features, improving efficiency. Two positive sample augmentation strategies are
also proposed to balance gradient contributions. Node classification
experiments on four real-world heterogeneous graph datasets demonstrate that
HetCRF outperforms state-of-the-art baselines. On datasets with missing node
features, such as Aminer and Freebase, at a 40% label rate in node
classification, HetCRF improves the Macro-F1 score by 2.75% and 2.2%
respectively compared to the second-best baseline, validating its effectiveness
and superiority.

</details>


### [310] [Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning](https://arxiv.org/abs/2506.06694)
*Yuan Yuan,Yukun Liu,Chonghua Han,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: MoveGCL是一个隐私保护的框架，通过生成式持续学习训练移动性基础模型，解决了数据孤岛和隐私问题，性能接近联合训练，优于联邦学习。


<details>
  <summary>Details</summary>
Motivation: 由于移动数据的隐私敏感性和数据孤岛问题，构建类似基础模型具有挑战性。

Method: MoveGCL采用生成式持续学习，通过冻结教师模型生成合成轨迹，结合知识蒸馏和Mixture-of-Experts Transformer。

Result: 在六个真实数据集上，MoveGCL性能接近联合训练，显著优于联邦学习基线。

Conclusion: MoveGCL为移动性基础模型的发展提供了开放、可扩展且隐私保护的蓝图。

Abstract: Foundation models have revolutionized fields such as natural language
processing and computer vision by enabling general-purpose learning across
diverse tasks and datasets. However, building analogous models for human
mobility remains challenging due to the privacy-sensitive nature of mobility
data and the resulting data silos across institutions. To bridge this gap, we
propose MoveGCL, a scalable and privacy-preserving framework for training
mobility foundation models via generative continual learning. Without sharing
raw data, MoveGCL enables decentralized and progressive model evolution by
replaying synthetic trajectories generated from a frozen teacher model, and
reinforces knowledge retention through a tailored distillation strategy that
mitigates catastrophic forgetting. To address the heterogeneity of mobility
patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a
mobility-aware expert routing mechanism, and employs a layer-wise progressive
adaptation strategy to stabilize continual updates. Experiments on six
real-world urban datasets demonstrate that MoveGCL achieves performance
comparable to joint training and significantly outperforms federated learning
baselines, while offering strong privacy protection. MoveGCL marks a crucial
step toward unlocking foundation models for mobility, offering a practical
blueprint for open, scalable, and privacy-preserving model development in the
era of foundation models.

</details>


### [311] [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)
*Rajeev Bhatt Ambati,James Lester,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.LG

TL;DR: MarginSel是一种通过选择困难示例提升LLM上下文学习效果的方法，显著提高了分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习的效果对示例选择和顺序敏感，需要一种自适应的方法来优化演示示例的选择。

Method: 提出MarginSel，一种两步法，为每个测试实例选择困难示例，以最大化模型性能。

Result: 在分类任务中，MarginSel相比随机选择示例，F1分数绝对提升2-7%。

Conclusion: MarginSel通过增加困难示例的边界，有效改善了LLM的决策边界，提升了性能。

Abstract: Large Language Models (LLMs) excel at few-shot learning via in-context
learning (ICL). However, the effectiveness of ICL is often sensitive to the
selection and ordering of demonstration examples. To address this, we present
MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that
selects hard demonstration examples for the ICL prompt, adapting to each test
instance. Our approach achieves 2-7% absolute improvement in F1-score across
classification tasks, compared to a random selection of examples. We also
provide theoretical insights and empirical evidence showing that MarginSel
induces max-margin behavior in LLMs by effectively increasing the margin for
hard examples, analogous to support vectors, thereby shifting the decision
boundary in a beneficial direction.

</details>


### [312] [Do Protein Transformers Have Biological Intelligence?](https://arxiv.org/abs/2506.06701)
*Fudong Lin,Wanrou Du,Jinchan Liu,Tarikul Milon,Shelby Meche,Wu Xu,Xiaoqi Qin,Xu Yuan*

Main category: cs.LG

TL;DR: 本文提出了一种新的Transformer架构（SPT）和可解释AI技术（Sequence Score），用于高效预测蛋白质功能并揭示其生物学智能。


<details>
  <summary>Details</summary>
Motivation: 探索蛋白质Transformer是否能捕捉蛋白质序列中的生物学智能。

Method: 1. 引入Protein-FN数据集；2. 设计SPT架构；3. 开发Sequence Score技术。

Result: SPT-Tiny模型在AR和Protein-FN数据集上分别达到94.3%和99.6%的准确率。Sequence Score揭示了与生物学知识一致的序列模式。

Conclusion: SPT和Sequence Score技术成功捕捉了蛋白质序列中的生物学智能，并提供了高效预测和解释能力。

Abstract: Deep neural networks, particularly Transformers, have been widely adopted for
predicting the functional properties of proteins. In this work, we focus on
exploring whether Protein Transformers can capture biological intelligence
among protein sequences. To achieve our goal, we first introduce a protein
function dataset, namely Protein-FN, providing over 9000 protein data with
meaningful labels. Second, we devise a new Transformer architecture, namely
Sequence Protein Transformers (SPT), for computationally efficient protein
function predictions. Third, we develop a novel Explainable Artificial
Intelligence (XAI) technique called Sequence Score, which can efficiently
interpret the decision-making processes of protein models, thereby overcoming
the difficulty of deciphering biological intelligence bided in Protein
Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only
5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3%
on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset,
all accomplished by training from scratch. Besides, our Sequence Score
technique helps reveal that our SPT models can discover several meaningful
patterns underlying the sequence structures of protein data, with these
patterns aligning closely with the domain knowledge in the biology community.
We have officially released our Protein-FN dataset on Hugging Face Datasets
https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at
https://github.com/fudong03/BioIntelligence.

</details>


### [313] [A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks](https://arxiv.org/abs/2506.06715)
*Minh-Duc Nguyen,Dung D. Le*

Main category: cs.LG

TL;DR: 本文提出了一种基于Stein变分梯度下降（SVGD）的新方法SVH-MOL，用于解决多目标学习中Pareto解的多样性与超体积值最大化问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在Pareto解的多样性与超体积值最大化之间存在挑战，需要一种更高效的方法来平衡这两者。

Method: 采用SVGD逼近整个Pareto集，通过功能梯度下降推动粒子集向Pareto集收敛并多样化。结合多样化梯度方向策略和退火调度以提升稳定性。

Result: 在多种多目标问题和多任务学习实验中验证了SVH-MOL的优越性能。

Conclusion: SVH-MOL是一种有效的方法，能够同时实现Pareto解的多样性和超体积值最大化。

Abstract: Pareto Set Learning (PSL) is popular as an efficient approach to obtaining
the complete optimal solution in Multi-objective Learning (MOL). A set of
optimal solutions approximates the Pareto set, and its mapping is a set of
dense points in the Pareto front in objective space. However, some current
methods face a challenge: how to make the Pareto solution is diverse while
maximizing the hypervolume value. In this paper, we propose a novel method to
address this challenge, which employs Stein Variational Gradient Descent (SVGD)
to approximate the entire Pareto set. SVGD pushes a set of particles towards
the Pareto set by applying a form of functional gradient descent, which helps
to converge and diversify optimal solutions. Additionally, we employ diverse
gradient direction strategies to thoroughly investigate a unified framework for
SVGD in multi-objective optimization and adapt this framework with an annealing
schedule to promote stability. We introduce our method, SVH-MOL, and validate
its effectiveness through extensive experiments on multi-objective problems and
multi-task learning, demonstrating its superior performance.

</details>


### [314] [The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing](https://arxiv.org/abs/2506.06761)
*Adrià Molina Rodríguez,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，通过模型编辑技术提升低资源语言（如古代手稿和非西方语言）的识别能力，优于传统的元学习方法，并在跨领域评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言在识别系统中代表性不足的问题，提升模型对新数据分布（如新字母表）的泛化能力。

Method: 利用模型编辑技术，结合领域合并策略，无需依赖大量数据或原型设计。

Result: 实验表明，即使在相同训练数据下，该方法在跨字母表迁移学习和领域外评估中显著提升性能。

Conclusion: 该方法为低资源语言的文档识别提供了新思路，扩展了识别系统的适用场景和文化覆盖范围。

Abstract: Achieving robustness in recognition systems across diverse domains is crucial
for their practical utility. While ample data availability is usually assumed,
low-resource languages, such as ancient manuscripts and non-western languages,
tend to be kept out of the equations of massive pretraining and foundational
techniques due to an under representation. In this work, we aim for building
models which can generalize to new distributions of data, such as alphabets,
faster than centralized fine-tune strategies. For doing so, we take advantage
of the recent advancements in model editing to enhance the incorporation of
unseen scripts (low-resource learning). In contrast to state-of-the-art
meta-learning, we showcase the effectiveness of domain merging in sparse
distributions of data, with agnosticity of its relation to the overall
distribution or any other prototyping necessity. Even when using the same exact
training data, our experiments showcase significant performance boosts in
\textbf{transfer learning} to new alphabets and \textbf{out-of-domain
evaluation} in challenging domain shifts, including historical ciphered texts
and non-Latin scripts. This research contributes a novel approach into building
models that can easily adopt under-represented alphabets and, therefore, enable
document recognition to a wider set of contexts and cultures.

</details>


### [315] [Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World](https://arxiv.org/abs/2506.06782)
*Qinting Jiang,Chuyang Ye,Dongyan Wei,Bingli Wang,Yuan Xue,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为FIND的新方法，通过特征聚类和动态归一化策略，显著提升了深度神经网络在动态分布偏移下的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练和测试域之间的分布偏移下性能下降，影响应用体验。现有测试时适应方法难以应对动态、多分布的测试数据。

Method: FIND方法包括三个关键组件：层间特征解耦（LFD）、特征感知批量归一化（FABN）和选择性FABN（S-FABN），分别用于特征聚类、动态归一化和优化效率。

Result: 实验表明，FIND在动态场景中显著优于现有方法，准确率提升30%，同时保持计算效率。

Conclusion: FIND通过特征聚类和动态归一化策略，有效解决了动态分布偏移问题，提升了模型鲁棒性和效率。

Abstract: Despite progress, deep neural networks still suffer performance declines
under distribution shifts between training and test domains, leading to a
substantial decrease in Quality of Experience (QoE) for applications. Existing
test-time adaptation (TTA) methods are challenged by dynamic, multiple test
distributions within batches. We observe that feature distributions across
different domains inherently cluster into distinct groups with varying means
and variances. This divergence reveals a critical limitation of previous global
normalization strategies in TTA, which inevitably distort the original data
characteristics. Based on this insight, we propose Feature-based Instance
Neighbor Discovery (FIND), which comprises three key components: Layer-wise
Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and
Selective FABN (S-FABN). LFD stably captures features with similar
distributions at each layer by constructing graph structures. While FABN
optimally combines source statistics with test-time distribution specific
statistics for robust feature representation. Finally, S-FABN determines which
layers require feature partitioning and which can remain unified, thereby
enhancing inference efficiency. Extensive experiments demonstrate that FIND
significantly outperforms existing methods, achieving a 30\% accuracy
improvement in dynamic scenarios while maintaining computational efficiency.

</details>


### [316] [Caterpillar GNN: Replacing Message Passing with Efficient Aggregation](https://arxiv.org/abs/2506.06784)
*Marek Černý*

Main category: cs.LG

TL;DR: 论文提出了一种高效的聚合机制，牺牲部分表达能力以增强结构化聚合能力，并基于此设计了Caterpillar GNN，在合成和真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的消息传递图神经网络（MPGNNs）通常追求最大表达能力，但本文希望通过牺牲部分表达能力来增强聚合能力，并实现更高效的图学习。

Method: 引入了一种高效的聚合机制，通过广义的caterpillar图的同态计数来严格表征表达能力，并设计了Caterpillar GNN。

Result: Caterpillar GNN在合成图级任务上表现优于传统MPGNNs，在真实数据集上预测性能相当但显著减少了隐藏层节点数。

Conclusion: 通过高效聚合机制，Caterpillar GNN在保持性能的同时实现了更高的计算效率。

Abstract: Message-passing graph neural networks (MPGNNs) dominate modern graph
learning, typically prioritizing maximal expressive power. In contrast, we
introduce an \emph{efficient aggregation} mechanism, deliberately trading off
some expressivity for stronger and more structured aggregation capabilities.
Our approach allows seamless scaling between classical message-passing and
simpler methods based on colored or plain walks. We rigorously characterize the
expressive power at each intermediate step using homomorphism counts from a
hierarchy of generalized \emph{caterpillar graphs}. Based on this foundation,
we propose the \emph{Caterpillar GNN}, whose robust graph-level aggregation
enables it to successfully tackle synthetic graph-level task specifically
designed to be challenging for classical MPGNNs. Moreover, we demonstrate that,
on real-world datasets, the Caterpillar GNN achieves comparable predictive
performance while significantly reducing the number of nodes in the hidden
layers of the computational graph.

</details>


### [317] [FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks](https://arxiv.org/abs/2506.06787)
*Qiyun Zhao*

Main category: cs.LG

TL;DR: FuncGNN提出了一种改进的电路表示方法，通过多粒度拓扑特征提取和门感知归一化，解决了AIGs中的结构异质性和全局信息丢失问题，显著提升了逻辑电路的表示效果。


<details>
  <summary>Details</summary>
Motivation: 随着集成电路规模和设计复杂度的增加，传统AIGs在表示布尔逻辑时面临结构异质性和全局信息丢失的挑战，亟需更有效的表示方法。

Method: FuncGNN结合了混合特征聚合、门感知归一化和多层集成技术，以提取多粒度拓扑模式并综合局部与全局语义信息。

Result: 在信号概率预测和真值表距离预测任务中，FuncGNN分别提升了2.06%和18.71%的性能，同时减少了50.6%的训练时间和32.8%的GPU内存使用。

Conclusion: FuncGNN通过创新的特征提取和集成方法，显著提升了逻辑电路的表示能力，为电子设计自动化提供了更高效的解决方案。

Abstract: As integrated circuit scale grows and design complexity rises, effective
circuit representation helps support logic synthesis, formal verification, and
other automated processes in electronic design automation. And-Inverter Graphs
(AIGs), as a compact and canonical structure, are widely adopted for
representing Boolean logic in these workflows. However, the increasing
complexity and integration density of modern circuits introduce structural
heterogeneity and global logic information loss in AIGs, posing significant
challenges to accurate circuit modeling. To address these issues, we propose
FuncGNN, which integrates hybrid feature aggregation to extract
multi-granularity topological patterns, thereby mitigating structural
heterogeneity and enhancing logic circuit representations. FuncGNN further
introduces gate-aware normalization that adapts to circuit-specific gate
distributions, improving robustness to structural heterogeneity. Finally,
FuncGNN employs multi-layer integration to merge intermediate features across
layers, effectively synthesizing local and global semantic information for
comprehensive logic representations. Experimental results on two logic-level
analysis tasks (i.e., signal probability prediction and truth-table distance
prediction) demonstrate that FuncGNN outperforms existing state-of-the-art
methods, achieving improvements of 2.06% and 18.71%, respectively, while
reducing training time by approximately 50.6% and GPU memory usage by about
32.8%.

</details>


### [318] [Is Optimal Transport Necessary for Inverse Reinforcement Learning?](https://arxiv.org/abs/2506.06793)
*Zixuan Dong,Yumi Omori,Keith Ross*

Main category: cs.LG

TL;DR: 论文提出两种简单的启发式替代方法（最小距离奖励和分段匹配奖励），挑战了逆强化学习中最优传输的必要性，并在实验中表现优于或匹配基于最优传输的方法。


<details>
  <summary>Details</summary>
Motivation: 最优传输方法在逆强化学习中虽然有效，但引入了算法复杂性、超参数敏感性等问题，作者质疑其必要性。

Method: 提出两种简单方法：最小距离奖励（忽略时间顺序）和分段匹配奖励（轻量级时间对齐），避免优化问题，复杂度低。

Result: 在32个在线和离线基准测试中，两种简单方法与三种强化学习算法结合，表现优于或匹配最优传输方法。

Conclusion: 最优传输的核心优势可能源于基本接近对齐而非其最优耦合公式，建议未来逆强化学习设计重新评估复杂性。

Abstract: Inverse Reinforcement Learning (IRL) aims to recover a reward function from
expert demonstrations. Recently, Optimal Transport (OT) methods have been
successfully deployed to align trajectories and infer rewards. While OT-based
methods have shown strong empirical results, they introduce algorithmic
complexity, hyperparameter sensitivity, and require solving the OT optimization
problems. In this work, we challenge the necessity of OT in IRL by proposing
two simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns
rewards based on the nearest expert state regardless of temporal order; and (2)
Segment-Matching Reward, which incorporates lightweight temporal alignment by
matching agent states to corresponding segments in the expert trajectory. These
methods avoid optimization, exhibit linear-time complexity, and are easy to
implement. Through extensive evaluations across 32 online and offline
benchmarks with three reinforcement learning algorithms, we show that our
simple rewards match or outperform recent OT-based approaches. Our findings
suggest that the core benefits of OT may arise from basic proximity alignment
rather than its optimal coupling formulation, advocating for reevaluation of
complexity in future IRL design.

</details>


### [319] [IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2506.06809)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为IMPA-HGAE的新框架，通过充分利用元路径上的内部节点信息来增强目标节点嵌入，解决了现有异构图自监督学习模型信息利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有异构图自监督学习方法通过元路径将异构图转换为同构图进行训练，仅利用元路径两端节点的信息，而忽略了元路径上的异构节点信息。

Method: 提出IMPA-HGAE框架，通过创新的掩码策略和生成式自监督学习方法，充分利用元路径上的内部节点信息。

Result: 实验证明IMPA-HGAE在异构数据集上表现优异，并增强了生成式自监督学习模型的表示能力。

Conclusion: 该工作为复杂图场景中利用元路径引导的结构语义进行鲁棒表示学习提供了见解，并探讨了未来研究方向。

Abstract: Self-supervised learning (SSL) methods have been increasingly applied to
diverse downstream tasks due to their superior generalization capabilities and
low annotation costs. However, most existing heterogeneous graph SSL models
convert heterogeneous graphs into homogeneous ones via meta-paths for training,
which only leverage information from nodes at both ends of meta-paths while
underutilizing the heterogeneous node information along the meta-paths. To
address this limitation, this paper proposes a novel framework named IMPA-HGAE
to enhance target node embeddings by fully exploiting internal node information
along meta-paths. Experimental results validate that IMPA-HGAE achieves
superior performance on heterogeneous datasets. Furthermore, this paper
introduce innovative masking strategies to strengthen the representational
capacity of generative SSL models on heterogeneous graph data. Additionally,
this paper discuss the interpretability of the proposed method and potential
future directions for generative self-supervised learning in heterogeneous
graphs. This work provides insights into leveraging meta-path-guided structural
semantics for robust representation learning in complex graph scenarios.

</details>


### [320] [Path Integral Optimiser: Global Optimisation via Neural Schrödinger-Föllmer Diffusion](https://arxiv.org/abs/2506.06815)
*Max McGuinness,Eirik Fladmark,Francisco Vargas*

Main category: cs.LG

TL;DR: 本文研究了神经扩散过程在全局优化中的应用，基于Zhang等人的Path Integral Sampler，通过Boltzmann分布将优化问题转化为Schrödinger桥采样问题，并利用Girsanov定理和神经近似（Fourier MLP）求解。实验表明该方法在2至1,247维任务中表现良好，但在15.9k参数模型中探索高维空间时存在困难。


<details>
  <summary>Details</summary>
Motivation: 探索神经扩散过程在全局优化中的潜力，特别是如何通过Boltzmann分布和Schrödinger桥采样问题来优化高维任务。

Method: 使用Boltzmann分布将优化问题转化为Schrödinger桥采样问题，应用Girsanov定理和神经近似（Fourier MLP）求解。

Result: 在2至1,247维任务中表现良好，但在15.9k参数模型中探索高维空间时表现不佳。

Conclusion: 该方法在低维任务中具有潜力，但在高维环境中需要进一步改进以适应更复杂的优化需求。

Abstract: We present an early investigation into the use of neural diffusion processes
for global optimisation, focusing on Zhang et al.'s Path Integral Sampler. One
can use the Boltzmann distribution to formulate optimization as solving a
Schr\"odinger bridge sampling problem, then apply Girsanov's theorem with a
simple (single-point) prior to frame it in stochastic control terms, and
compute the solution's integral terms via a neural approximation (a Fourier
MLP). We provide theoretical bounds for this optimiser, results on toy
optimisation tasks, and a summary of the stochastic theory motivating the
model. Ultimately, we found the optimiser to display promising per-step
performance at optimisation tasks between 2 and 1,247 dimensions, but struggle
to explore higher-dimensional spaces when faced with a 15.9k parameter model,
indicating a need for work on adaptation in such environments.

</details>


### [321] [Curvature Enhanced Data Augmentation for Regression](https://arxiv.org/abs/2506.06853)
*Ilya Kaufman Sirot,Omri Azencot*

Main category: cs.LG

TL;DR: 论文提出了一种基于二阶数据流形表示的Curvature-Enhanced Manifold Sampling (CEMS)方法，用于回归任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管数据增强在分类任务中表现优异，但在回归问题中应用较少，因此需要一种新的方法来生成合成数据以提升回归任务的性能。

Method: 利用二阶数据流形表示，提出CEMS方法，通过高效采样和重建新数据点来增强回归任务的性能。

Result: 在多个数据集上的评估表明，CEMS在分布内和分布外场景下均优于现有方法，且计算开销极小。

Conclusion: CEMS为回归任务提供了一种高效的数据增强方法，显著提升了模型的泛化能力。

Abstract: Deep learning models with a large number of parameters, often referred to as
over-parameterized models, have achieved exceptional performance across various
tasks. Despite concerns about overfitting, these models frequently generalize
well to unseen data, thanks to effective regularization techniques, with data
augmentation being among the most widely used. While data augmentation has
shown great success in classification tasks using label-preserving
transformations, its application in regression problems has received less
attention. Recently, a novel \emph{manifold learning} approach for generating
synthetic data was proposed, utilizing a first-order approximation of the data
manifold. Building on this foundation, we present a theoretical framework and
practical tools for approximating and sampling general data manifolds.
Furthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS)
method for regression tasks. CEMS leverages a second-order representation of
the data manifold to enable efficient sampling and reconstruction of new data
points. Extensive evaluations across multiple datasets and comparisons with
state-of-the-art methods demonstrate that CEMS delivers superior performance in
both in-distribution and out-of-distribution scenarios, while introducing only
minimal computational overhead. Code is available at
https://github.com/azencot-group/CEMS.

</details>


### [322] [High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations](https://arxiv.org/abs/2506.06858)
*Ziwei Li,Yuhan Duan,Tianyu Xiong,Yi-Tang Chen,Wei-Lun Chao,Han-Wei Shen*

Main category: cs.LG

TL;DR: FA-INR是一种基于交叉注意力和记忆库的隐式神经表示方法，通过自适应特征分配和专家混合提升模型效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有隐式神经表示（INR）在处理复杂科学数据时，常因局部高频变化而表现不佳，且传统方法引入刚性结构特征会增加模型复杂度。

Method: 提出FA-INR，利用交叉注意力从记忆库学习自适应特征表示，并结合坐标引导的专家混合（MoE）提升可扩展性。

Result: 在三个大规模仿真数据集上，FA-INR在保持高精度的同时显著减小模型规模，实现了精度与紧凑性的新平衡。

Conclusion: FA-INR为基于INR的代理模型提供了一种高效且灵活的新方法，适用于复杂科学数据的建模。

Abstract: Effective surrogate models are critical for accelerating scientific
simulations. Implicit neural representations (INRs) offer a compact and
continuous framework for modeling spatially structured data, but they often
struggle with complex scientific fields exhibiting localized, high-frequency
variations. Recent approaches address this by introducing additional features
along rigid geometric structures (e.g., grids), but at the cost of flexibility
and increased model size. In this paper, we propose a simple yet effective
alternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to
an augmented memory bank to learn flexible feature representations, enabling
adaptive allocation of model capacity based on data characteristics, rather
than rigid structural assumptions. To further improve scalability, we introduce
a coordinate-guided mixture of experts (MoE) that enhances the specialization
and efficiency of feature representations. Experiments on three large-scale
ensemble simulation datasets show that FA-INR achieves state-of-the-art
fidelity while significantly reducing model size, establishing a new trade-off
frontier between accuracy and compactness for INR-based surrogates.

</details>


### [323] [Differentially Private Sparse Linear Regression with Heavy-tailed Responses](https://arxiv.org/abs/2506.06861)
*Xizhi Tian,Meng Ding,Touming Tao,Zihang Xiang,Di Wang*

Main category: cs.LG

TL;DR: 本文研究了高维设置下具有重尾响应的差分隐私稀疏线性回归问题，提出了两种方法（DP-IHT-H和DP-IHT-L），分别在无和有额外假设下优化了误差界，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对常规数据分布或低维不规则数据，无法满足高维重尾数据的需求，因此需要新的解决方案。

Method: 提出DP-IHT-H（基于Huber损失和私有迭代硬阈值）和DP-IHT-L（在额外假设下进一步优化），分别针对不同数据特性。

Result: DP-IHT-H在(ε, δ)-DP模型下达到特定误差界，DP-IHT-L在额外假设下进一步优化误差界。实验表明方法优于常规DP算法。

Conclusion: 本文提出的方法在高维重尾数据下表现优越，扩展了差分隐私线性回归的应用范围。

Abstract: As a fundamental problem in machine learning and differential privacy (DP),
DP linear regression has been extensively studied. However, most existing
methods focus primarily on either regular data distributions or low-dimensional
cases with irregular data. To address these limitations, this paper provides a
comprehensive study of DP sparse linear regression with heavy-tailed responses
in high-dimensional settings. In the first part, we introduce the DP-IHT-H
method, which leverages the Huber loss and private iterative hard thresholding
to achieve an estimation error bound of \(
  \tilde{O}\biggl(
  s^{* \frac{1 }{2}}
  \cdot \biggl(\frac{\log d}{n}\biggr)^{\frac{\zeta}{1 + \zeta}}
  +
  s^{* \frac{1 + 2\zeta}{2 + 2\zeta}}
  \cdot \biggl(\frac{\log^2 d}{n \varepsilon}\biggr)^{\frac{\zeta}{1 + \zeta}}
  \biggr) \) under the $(\varepsilon, \delta)$-DP model, where $n$ is the
sample size, $d$ is the dimensionality, $s^*$ is the sparsity of the parameter,
and $\zeta \in (0, 1]$ characterizes the tail heaviness of the data. In the
second part, we propose DP-IHT-L, which further improves the error bound under
additional assumptions on the response and achieves \(
  \tilde{O}\Bigl(\frac{(s^*)^{3/2} \log d}{n \varepsilon}\Bigr). \) Compared to
the first result, this bound is independent of the tail parameter $\zeta$.
Finally, through experiments on synthetic and real-world datasets, we
demonstrate that our methods outperform standard DP algorithms designed for
``regular'' data.

</details>


### [324] [SAFE: Finding Sparse and Flat Minima to Improve Pruning](https://arxiv.org/abs/2506.06866)
*Dongyeop Lee,Kwanhee Lee,Jinseok Chung,Namhoon Lee*

Main category: cs.LG

TL;DR: 论文提出了一种名为SAFE的新剪枝方法，通过同时追求稀疏性和平坦性来恢复神经网络性能，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 稀疏化神经网络常伴随性能下降，现有方法难以恢复原始性能。受鲁棒优化研究启发，作者希望通过同时优化稀疏性和平坦性来解决这一问题。

Method: 将剪枝问题建模为稀疏性约束的优化问题，平坦性作为目标之一。采用增强拉格朗日对偶方法求解，并扩展为广义投影操作，提出了SAFE及其扩展SAFE$^+$。

Result: 在图像分类和语言建模任务中，SAFE生成的稀疏网络泛化性能优于基线方法，且对噪声数据具有鲁棒性。

Conclusion: SAFE方法通过结合稀疏性和平坦性，有效提升了剪枝网络的性能，适用于现实场景。

Abstract: Sparsifying neural networks often suffers from seemingly inevitable
performance degradation, and it remains challenging to restore the original
performance despite much recent progress. Motivated by recent studies in robust
optimization, we aim to tackle this problem by finding subnetworks that are
both sparse and flat at the same time. Specifically, we formulate pruning as a
sparsity-constrained optimization problem where flatness is encouraged as an
objective. We solve it explicitly via an augmented Lagrange dual approach and
extend it further by proposing a generalized projection operation, resulting in
novel pruning methods called SAFE and its extension, SAFE$^+$. Extensive
evaluations on standard image classification and language modeling tasks reveal
that SAFE consistently yields sparse networks with improved generalization
performance, which compares competitively to well-established baselines. In
addition, SAFE demonstrates resilience to noisy data, making it well-suited for
real-world conditions.

</details>


### [325] [Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning](https://arxiv.org/abs/2506.06873)
*Armin Behnamnia,Gholamali Aminian,Alireza Aghaei,Chengchun Shi,Vincent Y. F. Tan,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 论文提出了一种基于log-sum-exponential（LSE）算子的新估计器，用于解决离线学习和评估中的高方差和重尾奖励分布问题，并在理论和实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 离线学习和评估中，传统逆倾向得分估计器在高方差和低质量倾向得分下表现不佳，且对重尾奖励分布敏感。

Method: 引入LSE算子作为新估计器，推导其偏差和方差上界，并在离线学习和评估场景中分析其性能。

Result: LSE估计器在方差减少和重尾条件下表现出鲁棒性，理论分析显示其遗憾界限收敛速度为O(n^(-ε/(1+ε)))。

Conclusion: LSE估计器在理论和实践中均优于传统方法，适用于离线学习和评估场景。

Abstract: Off-policy learning and evaluation leverage logged bandit feedback datasets,
which contain context, action, propensity score, and feedback for each data
point. These scenarios face significant challenges due to high variance and
poor performance with low-quality propensity scores and heavy-tailed reward
distributions. We address these issues by introducing a novel estimator based
on the log-sum-exponential (LSE) operator, which outperforms traditional
inverse propensity score estimators. Our LSE estimator demonstrates variance
reduction and robustness under heavy-tailed conditions. For off-policy
evaluation, we derive upper bounds on the estimator's bias and variance. In the
off-policy learning scenario, we establish bounds on the regret -- the
performance gap between our LSE estimator and the optimal policy -- assuming
bounded $(1+\epsilon)$-th moment of weighted reward. Notably, we achieve a
convergence rate of $O(n^{-\epsilon/(1+ \epsilon)})$ for the regret bounds,
where $\epsilon \in [0,1]$ and $n$ is the size of logged bandit feedback
dataset. Theoretical analysis is complemented by comprehensive empirical
evaluations in both off-policy learning and evaluation scenarios, confirming
the practical advantages of our approach. The code for our estimator is
available at the following link:
https://github.com/armin-behnamnia/lse-offpolicy-learning.

</details>


### [326] [FREE: Fast and Robust Vision Language Models with Early Exits](https://arxiv.org/abs/2506.06884)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: 论文提出了一种名为FREE的方法，通过对抗训练和GAN框架在视觉语言模型中实现早期退出策略，以提高推理速度并保持性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在性能提升的同时，其大尺寸导致推理延迟问题，限制了实际应用。

Method: 采用对抗训练方法（FREE），每个退出点包含一个Transformer层和一个分类器，通过生成类似最终层的特征表示来优化推理。

Result: 实验表明，该方法在保持性能的同时，推理速度提升超过1.51倍，并增强了模型鲁棒性。

Conclusion: FREE方法有效解决了VLMs的推理延迟问题，为实际应用提供了高效解决方案。

Abstract: In recent years, Vision-Language Models (VLMs) have shown remarkable
performance improvements in Vision-Language tasks. However, their large size
poses challenges for real-world applications where inference latency is a
concern. To tackle this issue, we propose employing Early Exit (EE) strategies
in VLMs. However, training exit classifiers in VLMs is challenging,
particularly with limited labeled training data. To address this, we introduce
FREE, an adversarial training approach within a GAN-based framework. Here, each
exit consists of a transformer layer and a classifier. The transformer layer is
adversarially trained to produce feature representations similar to the final
layer, while a feature classifier serves as the discriminator. Our method
focuses on performing input-adaptive inference that increases inference speed
with minimal drop in performance. Experimental results demonstrate the
effectiveness of our approach in enhancing accuracy and model robustness by
mitigating overthinking and the phenomenon of mid-crisis that we highlight. We
experimentally validate that our method speeds up the inference process by more
than 1.51x while retaining comparable performance. The source code is available
at https://github.com/Div290/FREE.

</details>


### [327] [Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?](https://arxiv.org/abs/2506.06891)
*Paulius Sasnauskas,Yiğit Yalın,Goran Radanović*

Main category: cs.LG

TL;DR: 本文研究了上下文强化学习（ICRL）的抗干扰性，提出了一种对抗训练框架AT-DPT，以应对针对DPT的奖励投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 针对DPT在奖励投毒攻击下的脆弱性，研究如何提升其抗干扰能力。

Method: 提出AT-DPT框架，同时训练攻击者和DPT模型，攻击者通过污染环境奖励来最小化真实奖励，DPT则从污染数据中推断最优动作。

Result: 在强盗问题和MDP设置中，AT-DPT显著优于现有基线方法，表现出更强的抗干扰能力。

Conclusion: AT-DPT在复杂环境中展现了良好的抗干扰性，验证了其在强盗问题中的鲁棒性可以推广到更复杂场景。

Abstract: We study the corruption-robustness of in-context reinforcement learning
(ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al.,
2023). To address the challenge of reward poisoning attacks targeting the DPT,
we propose a novel adversarial training framework, called Adversarially Trained
Decision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an
attacker to minimize the true reward of the DPT by poisoning environment
rewards, and a DPT model to infer optimal actions from the poisoned data. We
evaluate the effectiveness of our approach against standard bandit algorithms,
including robust baselines designed to handle reward contamination. Our results
show that the proposed method significantly outperforms these baselines in
bandit settings, under a learned attacker. We additionally evaluate AT-DPT on
an adaptive attacker, and observe similar results. Furthermore, we extend our
evaluation to the MDP setting, confirming that the robustness observed in
bandit scenarios generalizes to more complex environments.

</details>


### [328] [Scalable Gaussian Processes with Latent Kronecker Structure](https://arxiv.org/abs/2506.06895)
*Jihao Andreas Lin,Sebastian Ament,Maximilian Balandat,David Eriksson,José Miguel Hernández-Lobato,Eytan Bakshy*

Main category: cs.LG

TL;DR: 论文提出了一种利用潜在Kronecker结构的方法，解决了高斯过程在大数据集上的计算扩展性问题，显著减少了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在大数据集上的计算扩展性受限，现有方法（如Kronecker积）常需近似或不现实假设，且缺失数据会破坏结构。

Method: 通过将观测值的核矩阵表示为潜在Kronecker积的投影，结合迭代线性系统求解器和路径条件，实现精确高斯过程推断。

Result: 方法在多达五百万样本的真实数据集上表现优于现有稀疏和变分高斯过程，适用于机器人、自动机器学习和气候应用。

Conclusion: 提出的方法显著提升了高斯过程在大规模数据集上的计算效率，同时保持了精确性。

Abstract: Applying Gaussian processes (GPs) to very large datasets remains a challenge
due to limited computational scalability. Matrix structures, such as the
Kronecker product, can accelerate operations significantly, but their
application commonly entails approximations or unrealistic assumptions. In
particular, the most common path to creating a Kronecker-structured kernel
matrix is by evaluating a product kernel on gridded inputs that can be
expressed as a Cartesian product. However, this structure is lost if any
observation is missing, breaking the Cartesian product structure, which
frequently occurs in real-world data such as time series. To address this
limitation, we propose leveraging latent Kronecker structure, by expressing the
kernel matrix of observed values as the projection of a latent Kronecker
product. In combination with iterative linear system solvers and pathwise
conditioning, our method facilitates inference of exact GPs while requiring
substantially fewer computational resources than standard iterative methods. We
demonstrate that our method outperforms state-of-the-art sparse and variational
GPs on real-world datasets with up to five million examples, including
robotics, automated machine learning, and climate applications.

</details>


### [329] [Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations](https://arxiv.org/abs/2506.06907)
*Fred Xu,Thomas Markovich*

Main category: cs.LG

TL;DR: 提出了一种基于高斯过程的图神经网络消息传递方法，用于改进图数据中的不确定性估计，尤其在分布偏移情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统不确定性估计方法难以处理图数据中结构和标签分布带来的随机性，需要一种更全面的方法。

Method: 将随机偏微分方程（SPDE）与高斯过程类比，设计了一种新的消息传递方案，结合时空噪声。

Result: 在分布外检测任务中，该方法在多种标签信息量的图数据集上表现优于现有方法。

Conclusion: 该方法通过控制协方差核平滑度，显著提升了图数据不确定性估计的准确性。

Abstract: Graph Neural Networks have achieved impressive results across diverse network
modeling tasks, but accurately estimating uncertainty on graphs remains
difficult, especially under distributional shifts. Unlike traditional
uncertainty estimation, graph-based uncertainty must account for randomness
arising from both the graph's structure and its label distribution, which adds
complexity. In this paper, making an analogy between the evolution of a
stochastic partial differential equation (SPDE) driven by Matern Gaussian
Process and message passing using GNN layers, we present a principled way to
design a novel message passing scheme that incorporates spatial-temporal noises
motivated by the Gaussian Process approach to SPDE. Our method simultaneously
captures uncertainty across space and time and allows explicit control over the
covariance kernel smoothness, thereby enhancing uncertainty estimates on graphs
with both low and high label informativeness. Our extensive experiments on
Out-of-Distribution (OOD) detection on graph datasets with varying label
informativeness demonstrate the soundness and superiority of our model to
existing approaches.

</details>


### [330] [Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data](https://arxiv.org/abs/2506.06917)
*Shangjie Du,Hui Wei,Dong Yoon Lee,Zhizhang Hu,Shijia Pan*

Main category: cs.LG

TL;DR: GraPhy是一种基于图的物理引导学习框架，用于在监测数据有限的城区进行高分辨率、准确的空气质量建模。


<details>
  <summary>Details</summary>
Motivation: 细粒度空气质量监测对减少公众污染物暴露至关重要，但社会经济弱势地区监测网络稀疏，限制了建模的准确性和分辨率。

Method: 提出了一种物理引导的图神经网络架构GraPhy，专为低分辨率监测数据设计，包含特定层和边特征。

Result: 在加州圣华金谷的实验显示，GraPhy在MSE、MAE和R2指标上表现最佳，性能提升9%-56%。

Conclusion: GraPhy在不同空间异质性水平下均优于基线模型，验证了其设计的有效性。

Abstract: This work introduces GraPhy, a graph-based, physics-guided learning framework
for high-resolution and accurate air quality modeling in urban areas with
limited monitoring data. Fine-grained air quality monitoring information is
essential for reducing public exposure to pollutants. However, monitoring
networks are often sparse in socioeconomically disadvantaged regions, limiting
the accuracy and resolution of air quality modeling. To address this, we
propose a physics-guided graph neural network architecture called GraPhy with
layers and edge features designed specifically for low-resolution monitoring
data. Experiments using data from California's socioeconomically disadvantaged
San Joaquin Valley show that GraPhy achieves the overall best performance
evaluated by mean squared error (MSE), mean absolute error (MAE), and R-square
value (R2), improving the performance by 9%-56% compared to various baseline
models. Moreover, GraPhy consistently outperforms baselines across different
spatial heterogeneity levels, demonstrating the effectiveness of our model
design.

</details>


### [331] [Basis Transformers for Multi-Task Tabular Regression](https://arxiv.org/abs/2506.06926)
*Wei Min Loh,Jiaqi Shang,Pascal Poupart*

Main category: cs.LG

TL;DR: 论文提出了一种名为“基础变换器”的新架构，用于解决表格数据中的部分信息、噪声和异构结构问题，显著提升了多任务表格回归性能。


<details>
  <summary>Details</summary>
Motivation: 处理表格数据时，现有技术难以同时处理文本信息、可变列数和无元数据等问题，因此需要一种新方法。

Method: 设计了“基础变换器”架构，考虑了表格数据的固有不变性（如层次结构和数值表示）。

Result: 在OpenML-CTR23基准测试中，中位数R²得分提高了0.338，参数数量比最佳基线少五倍，且优于预训练大型语言模型。

Conclusion: 基础变换器在表格数据处理中表现出色，具有高效性和鲁棒性。

Abstract: Dealing with tabular data is challenging due to partial information, noise,
and heterogeneous structure. Existing techniques often struggle to
simultaneously address key aspects of tabular data such as textual information,
a variable number of columns, and unseen data without metadata besides column
names. We propose a novel architecture, \textit{basis transformers},
specifically designed to tackle these challenges while respecting inherent
invariances in tabular data, including hierarchical structure and the
representation of numeric values. We evaluate our design on a multi-task
tabular regression benchmark, achieving an improvement of 0.338 in the median
$R^2$ score and the lowest standard deviation across 34 tasks from the
OpenML-CTR23 benchmark. Furthermore, our model has five times fewer parameters
than the best-performing baseline and surpasses pretrained large language model
baselines -- even when initialized from randomized weights.

</details>


### [332] [Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry](https://arxiv.org/abs/2506.06933)
*Mahdi Salmani,Alireza Abdollahpoorrostam,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: 本文提出了一种针对图像分类器的非对称黑盒攻击框架，通过优化搜索策略和梯度估计过程，显著降低了攻击的总查询成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设所有查询成本相同，但实际中某些查询可能成本更高（如触发额外审查的类别）。针对这种非对称成本场景，现有算法效果不佳。

Method: 提出非对称搜索（AS）和非对称梯度估计（AGREST），减少对高成本查询的依赖，并通过采样分布优化降低成本。

Result: 在多种成本设置下，该方法总查询成本和扰动幅度均优于现有方法，某些场景下提升达40%。

Conclusion: 本文提出的框架能有效降低非对称查询成本，且易于集成到现有黑盒攻击方法中。

Abstract: Traditional decision-based black-box adversarial attacks on image classifiers
aim to generate adversarial examples by slightly modifying input images while
keeping the number of queries low, where each query involves sending an input
to the model and observing its output. Most existing methods assume that all
queries have equal cost. However, in practice, queries may incur asymmetric
costs; for example, in content moderation systems, certain output classes may
trigger additional review, enforcement, or penalties, making them more costly
than others. While prior work has considered such asymmetric cost settings,
effective algorithms for this scenario remain underdeveloped. In this paper, we
propose a general framework for decision-based attacks under asymmetric query
costs, which we refer to as asymmetric black-box attacks. We modify two core
components of existing attacks: the search strategy and the gradient estimation
process. Specifically, we propose Asymmetric Search (AS), a more conservative
variant of binary search that reduces reliance on high-cost queries, and
Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution
to favor low-cost queries. We design efficient algorithms that minimize total
attack cost by balancing different query types, in contrast to earlier methods
such as stealthy attacks that focus only on limiting expensive (high-cost)
queries. Our method can be integrated into a range of existing black-box
attacks with minimal changes. We perform both theoretical analysis and
empirical evaluation on standard image classification benchmarks. Across
various cost regimes, our method consistently achieves lower total query cost
and smaller perturbations than existing approaches, with improvements of up to
40% in some settings.

</details>


### [333] [Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More](https://arxiv.org/abs/2506.06940)
*Geonhui Yoo,Minhak Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文研究了深度神经网络训练中梯度下降导致的渐进锐化现象，通过简化模型（深度线性网络）分析其机制，并探讨了数据集、网络深度、优化器随机性和步长的影响。


<details>
  <summary>Details</summary>
Motivation: 渐进锐化现象在实践中常见，但其机制尚不明确，作者希望通过简化模型揭示其背后的原理。

Method: 使用单神经元每层的深度线性网络作为简化模型，分析其锐化动态，并理论探讨数据集、网络深度、优化器和步长的影响。

Result: 简化模型成功捕捉了实际观察到的锐化动态，理论分析揭示了这些因素对渐进锐化的影响，并通过实验验证了理论结果的普适性。

Conclusion: 研究深化了对神经网络训练中锐化动态的理解，强调了深度、训练数据和优化器之间的相互作用。

Abstract: When training deep neural networks with gradient descent, sharpness often
increases -- a phenomenon known as progressive sharpening -- before saturating
at the edge of stability. Although commonly observed in practice, the
underlying mechanisms behind progressive sharpening remain poorly understood.
In this work, we study this phenomenon using a minimalist model: a deep linear
network with a single neuron per layer. We show that this simple model
effectively captures the sharpness dynamics observed in recent empirical
studies, offering a simple testbed to better understand neural network
training. Moreover, we theoretically analyze how dataset properties, network
depth, stochasticity of optimizers, and step size affect the degree of
progressive sharpening in the minimalist model. We then empirically demonstrate
how these theoretical insights extend to practical scenarios. This study offers
a deeper understanding of sharpness dynamics in neural network training,
highlighting the interplay between depth, training data, and optimizers.

</details>


### [334] [Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression](https://arxiv.org/abs/2506.06954)
*Clinton Enwerem,Aniruddh G. Puranic,John S. Baras,Calin Belta*

Main category: cs.LG

TL;DR: 提出了一种基于分位数和CVaR的风险正则化强化学习算法，解决了传统方法在安全约束下的过估计问题，并通过理论保证和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统近似动作值迭代强化学习算法在高方差随机环境中存在过估计偏差，导致策略次优，且难以满足安全约束。

Method: 提出了一种结合分位数回归和CVaR的风险正则化算法，无需复杂架构即可实现安全约束。

Result: 理论证明了风险敏感分布Bellman算子的收敛性，实验显示在动态任务中实现了更高的目标达成率和更少碰撞。

Conclusion: 该方法在安全性和性能之间取得了更好的平衡，优于风险中性方法。

Abstract: Mainstream approximate action-value iteration reinforcement learning (RL)
algorithms suffer from overestimation bias, leading to suboptimal policies in
high-variance stochastic environments. Quantile-based action-value iteration
methods reduce this bias by learning a distribution of the expected cost-to-go
using quantile regression. However, ensuring that the learned policy satisfies
safety constraints remains a challenge when these constraints are not
explicitly integrated into the RL framework. Existing methods often require
complex neural architectures or manual tradeoffs due to combined cost
functions. To address this, we propose a risk-regularized quantile-based
algorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety
without complex architectures. We also provide theoretical guarantees on the
contraction properties of the risk-sensitive distributional Bellman operator in
Wasserstein space, ensuring convergence to a unique cost distribution.
Simulations of a mobile robot in a dynamic reach-avoid task show that our
approach leads to more goal successes, fewer collisions, and better
safety-performance trade-offs compared to risk-neutral methods.

</details>


### [335] [UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare](https://arxiv.org/abs/2506.06977)
*Pengfei Hu,Xiaoxue Han,Fei Wang,Yue Ning*

Main category: cs.LG

TL;DR: 论文提出UdonCare框架，利用医学本体论（如ICD-9-CM）发现潜在领域，结合层次化方法提升临床预测的领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决临床预测中数据分布变化导致的模型性能下降问题，传统方法因缺乏领域标签和忽视医学知识而效果不佳。

Method: 提出UdonCare框架，通过层次化医学本体论分组疾病，迭代修剪细粒度领域，并采用Siamese型推理机制分离领域信号与患者特征。

Result: 在MIMIC-III和MIMIC-IV数据集上表现优于其他领域泛化基线，尤其在领域差异显著时。

Conclusion: UdonCare展示了医学知识在提升领域泛化能力中的潜力，适用于实际医疗应用。

Abstract: Domain generalization has become a critical challenge in clinical prediction,
where patient cohorts often exhibit shifting data distributions that degrade
model performance. Typical domain generalization approaches struggle in
real-world healthcare settings for two main reasons: (1) patient-specific
domain labels are typically unavailable, making domain discovery especially
difficult; (2) purely data-driven approaches overlook key clinical insights,
leading to a gap in medical knowledge integration. To address these problems,
we leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to
group diseases into higher-level categories and discover more flexible latent
domains. In this paper, we introduce UdonCare, a hierarchy-guided framework
that iteratively prunes fine-grained domains, encodes these refined domains,
and applies a Siamese-type inference mechanism to separate domain-related
signals from patient-level features. Experimental results on clinical datasets
(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher
performance compared to other domain generalization baselines when substantial
domain gaps presents, highlighting the untapped potential of medical knowledge
for enhancing domain generalization in practical healthcare applications.

</details>


### [336] [Near Optimal Non-asymptotic Sample Complexity of 1-Identification](https://arxiv.org/abs/2506.06978)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 论文研究了1-identification问题，提出了一种新算法SEE，并在非渐近视角下进行了理论分析，实现了近乎最优的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有文献中关于1-identification问题的非渐近分析尚不明确，本文填补了这一空白。

Method: 设计了新算法Sequential-Exploration-Exploitation (SEE)，并从非渐近视角进行了理论分析。

Result: 算法SEE在样本复杂度上实现了近乎最优的上界和下界匹配，差距仅为多项式对数因子。数值实验也验证了其有效性。

Conclusion: SEE算法在1-identification问题上表现优异，填补了非渐近分析的空白。

Abstract: Motivated by an open direction in existing literature, we study the
1-identification problem, a fundamental multi-armed bandit formulation on pure
exploration. The goal is to determine whether there exists an arm whose mean
reward is at least a known threshold $\mu_0$, or to output None if it believes
such an arm does not exist. The agent needs to guarantee its output is correct
with probability at least $1-\delta$. Degenne & Koolen 2019 has established the
asymptotically tight sample complexity for the 1-identification problem, but
they commented that the non-asymptotic analysis remains unclear. We design a
new algorithm Sequential-Exploration-Exploitation (SEE), and conduct
theoretical analysis from the non-asymptotic perspective. Novel to the
literature, we achieve near optimality, in the sense of matching upper and
lower bounds on the pulling complexity. The gap between the upper and lower
bounds is up to a polynomial logarithmic factor. The numerical result also
indicates the effectiveness of our algorithm, compared to existing benchmarks.

</details>


### [337] [MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification](https://arxiv.org/abs/2506.06980)
*Sajib Acharjee Dip,Uddip Acharjee Shuvo,Dipanwita Mallick,Abrar Rahman Abir,Liqing Zhang*

Main category: cs.LG

TL;DR: MoXGATE是一种基于交叉注意力和可学习模态权重的深度学习框架，用于多组学数据融合，显著提高了癌症亚型分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 癌症亚型分类对个性化治疗至关重要，但多组学数据的异质性使其整合具有挑战性。

Method: 提出Modality-Aware Cross-Attention MoXGATE框架，利用交叉注意力和模态权重实现多组学特征融合。

Result: 在GIAC和BRCA数据集上达到95%的分类准确率，优于现有方法。

Conclusion: MoXGATE在多组学癌症亚型分类中表现出色，具有高性能和生物学通用性。

Abstract: Cancer subtype classification is crucial for personalized treatment and
prognostic assessment. However, effectively integrating multi-omic data remains
challenging due to the heterogeneous nature of genomic, epigenomic, and
transcriptomic features. In this work, we propose Modality-Aware
Cross-Attention MoXGATE, a novel deep-learning framework that leverages
cross-attention and learnable modality weights to enhance feature fusion across
multiple omics sources. Our approach effectively captures inter-modality
dependencies, ensuring robust and interpretable integration. Through
experiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA)
datasets from TCGA, we demonstrate that MoXGATE outperforms existing methods,
achieving 95\% classification accuracy. Ablation studies validate the
effectiveness of cross-attention over simple concatenation and highlight the
importance of different omics modalities. Moreover, our model generalizes well
to unseen cancer types e.g., breast cancer, underscoring its adaptability. Key
contributions include (1) a cross-attention-based multi-omic integration
framework, (2) modality-weighted fusion for enhanced interpretability, (3)
application of focal loss to mitigate data imbalance, and (4) validation across
multiple cancer subtypes. Our results indicate that MoXGATE is a promising
approach for multi-omic cancer subtype classification, offering improved
performance and biological generalizability.

</details>


### [338] [Certified Unlearning for Neural Networks](https://arxiv.org/abs/2506.06985)
*Anastasia Koloskova,Youssef Allouah,Animesh Jha,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 提出了一种新的认证机器遗忘方法，通过噪声微调保留数据实现可证明的遗忘保证，无需对损失函数做假设，并在实践中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决机器遗忘问题，满足隐私需求和法规要求（如“被遗忘权”），现有方法假设严格或缺乏形式化保证。

Method: 利用遗忘与随机后处理的隐私放大之间的联系，提出基于噪声微调保留数据的认证遗忘方法。

Result: 方法在理论和实践中均表现良好，提供形式化遗忘保证且效率与准确性平衡，优于现有基线。

Conclusion: 该方法为机器遗忘提供了通用且有效的解决方案，适用于多样化场景。

Abstract: We address the problem of machine unlearning, where the goal is to remove the
influence of specific training data from a model upon request, motivated by
privacy concerns and regulatory requirements such as the "right to be
forgotten." Unfortunately, existing methods rely on restrictive assumptions or
lack formal guarantees. To this end, we propose a novel method for certified
machine unlearning, leveraging the connection between unlearning and privacy
amplification by stochastic post-processing. Our method uses noisy fine-tuning
on the retain data, i.e., data that does not need to be removed, to ensure
provable unlearning guarantees. This approach requires no assumptions about the
underlying loss function, making it broadly applicable across diverse settings.
We analyze the theoretical trade-offs in efficiency and accuracy and
demonstrate empirically that our method not only achieves formal unlearning
guarantees but also performs effectively in practice, outperforming existing
baselines. Our code is available at
https://github.com/stair-lab/certified-unlearningneural-networks-icml-2025

</details>


### [339] [Fully Explainable Classification Models Using Hyperblocks](https://arxiv.org/abs/2506.06986)
*Austin Snyder,Ryan Gallagher,Boris Kovalerchuk*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Hyperblocks的改进方法，旨在提升模型的可解释性、减少训练时间并降低复杂度，同时保持准确性。通过简化算法和引入k-NN回退机制，实现了高性能且透明的分类模型。


<details>
  <summary>Details</summary>
Motivation: 现有的Hyperblocks方法虽然能分类数据，但在可解释性、训练时间和模型复杂度方面存在不足。论文旨在解决这些问题，使领域专家能直接理解模型逻辑。

Method: 提出了一系列Hyperblock简化算法，包括去除冗余属性、分析重叠块以去除冗余块，以及创建分离单元。同时引入k-NN回退机制以覆盖未被块覆盖的数据点。

Result: 在WBC（9维）和MNIST（784维）等基准数据集上，模型在保持高准确性的同时显著降低了复杂度。

Conclusion: 该方法证明了可解释模型在高维大数据集上的可扩展性，为需要透明性和可控性的领域提供了替代黑盒模型的可行方案。

Abstract: Building on existing work with Hyperblocks, which classify data using minimum
and maximum bounds for each attribute, we focus on enhancing interpretability,
decreasing training time, and reducing model complexity without sacrificing
accuracy. This system allows subject matter experts (SMEs) to directly inspect
and understand the model's decision logic without requiring extensive machine
learning expertise. To reduce Hyperblock complexity while retaining
performance, we introduce a suite of algorithms for Hyperblock simplification.
These include removing redundant attributes, removing redundant blocks through
overlap analysis, and creating disjunctive units. These methods eliminate
unnecessary parameters, dramatically reducing model size without harming
classification power. We increase robustness by introducing an interpretable
fallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not
covered by any block, ensuring complete data coverage while preserving model
transparency. Our results demonstrate that interpretable models can scale to
high-dimensional, large-volume datasets while maintaining competitive accuracy.
On benchmark datasets such as WBC (9-D), we achieve strong predictive
performance with significantly reduced complexity. On MNIST (784-D), our method
continues to improve through tuning and simplification, showing promise as a
transparent alternative to black-box models in domains where trust, clarity,
and control are crucial.

</details>


### [340] [Modified K-means Algorithm with Local Optimality Guarantees](https://arxiv.org/abs/2506.06990)
*Mingyi Li,Michael R. Metel,Akiko Takeda*

Main category: cs.LG

TL;DR: 本文分析了K-means算法的局部最优性，提出了改进方法以确保局部最优解，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管K-means算法在机器学习中被广泛研究，但其局部最优性缺乏严格分析。本文旨在填补这一空白。

Method: 提出了对K-means算法的简单修改，确保在连续和离散意义上达到局部最优，同时保持与原算法相同的计算复杂度。

Result: 实验表明，原始K-means算法在实践中不一定找到局部最优解，而改进方法能提供更优的聚类结果。

Conclusion: 本文提出的方法在保证计算效率的同时，显著提升了K-means算法的局部最优性。

Abstract: The K-means algorithm is one of the most widely studied clustering algorithms
in machine learning. While extensive research has focused on its ability to
achieve a globally optimal solution, there still lacks a rigorous analysis of
its local optimality guarantees. In this paper, we first present conditions
under which the K-means algorithm converges to a locally optimal solution.
Based on this, we propose simple modifications to the K-means algorithm which
ensure local optimality in both the continuous and discrete sense, with the
same computational complexity as the original K-means algorithm. As the
dissimilarity measure, we consider a general Bregman divergence, which is an
extension of the squared Euclidean distance often used in the K-means
algorithm. Numerical experiments confirm that the K-means algorithm does not
always find a locally optimal solution in practice, while our proposed methods
provide improved locally optimal solutions with reduced clustering loss. Our
code is available at https://github.com/lmingyi/LO-K-means.

</details>


### [341] [Towards Physics-informed Diffusion for Anomaly Detection in Trajectories](https://arxiv.org/abs/2506.06999)
*Arun Sharma,Mingzhou Yang,Majid Farhadloo,Subhankar Ghosh,Bharat Jayaprakash,Shashi Shekhar*

Main category: cs.LG

TL;DR: 提出了一种基于物理约束的扩散模型，用于检测异常轨迹，特别是在GPS欺骗场景中，提高了准确率并降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 解决国际水域非法活动（如非法捕鱼和石油走私）中的GPS欺骗问题，同时应对AI生成的假轨迹和标记数据不足的挑战。

Method: 提出了一种物理信息扩散模型，结合运动学约束，识别不符合物理规律的轨迹。

Result: 在真实数据集（海事和城市领域）上实验，显示出更高的异常检测准确率和更低的轨迹生成误差。

Conclusion: 该方法通过整合物理知识，显著提升了异常轨迹检测的性能，且代码已开源。

Abstract: Given trajectory data, a domain-specific study area, and a user-defined
threshold, we aim to find anomalous trajectories indicative of possible GPS
spoofing (e.g., fake trajectory). The problem is societally important to curb
illegal activities in international waters, such as unauthorized fishing and
illicit oil transfers. The problem is challenging due to advances in AI
generated in deep fakes generation (e.g., additive noise, fake trajectories)
and lack of adequate amount of labeled samples for ground-truth verification.
Recent literature shows promising results for anomalous trajectory detection
using generative models despite data sparsity. However, they do not consider
fine-scale spatiotemporal dependencies and prior physical knowledge, resulting
in higher false-positive rates. To address these limitations, we propose a
physics-informed diffusion model that integrates kinematic constraints to
identify trajectories that do not adhere to physical laws. Experimental results
on real-world datasets in the maritime and urban domains show that the proposed
framework results in higher prediction accuracy and lower estimation error rate
for anomaly detection and trajectory generation methods, respectively. Our
implementation is available at
https://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.

</details>


### [342] [End-to-End Probabilistic Framework for Learning with Hard Constraints](https://arxiv.org/abs/2506.07003)
*Utkarsh Utkarsh,Danielle C. Maddix,Ruijun Ma,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: ProbHardE2E是一个通用的概率预测框架，通过新颖的可微分概率投影层（DPPL）实现硬约束，支持端到端学习，并优化严格评分规则，适用于多种非线性约束问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过后处理或推理阶段满足约束，且依赖分布假设，限制了模型的灵活性和鲁棒性。ProbHardE2E旨在解决这些问题。

Method: 利用DPPL层结合神经网络架构，端到端学习系统，并通过方差信息实现硬约束和不确定性量化。

Result: 在偏微分方程学习和时间序列预测中展示了广泛适用性，优于依赖分布假设的现有方法。

Conclusion: ProbHardE2E是一个灵活且强大的框架，适用于多种约束问题的概率预测。

Abstract: We present a general purpose probabilistic forecasting framework,
ProbHardE2E, to learn systems that can incorporate operational/physical
constraints as hard requirements. ProbHardE2E enforces hard constraints by
exploiting variance information in a novel way; and thus it is also capable of
performing uncertainty quantification (UQ) on the model. Our methodology uses a
novel differentiable probabilistic projection layer (DPPL) that can be combined
with a wide range of neural network architectures. This DPPL allows the model
to learn the system in an end-to-end manner, compared to other approaches where
the constraints are satisfied either through a post-processing step or at
inference. In addition, ProbHardE2E can optimize a strictly proper scoring
rule, without making any distributional assumptions on the target, which
enables it to obtain robust distributional estimates (in contrast to existing
approaches that generally optimize likelihood-based objectives, which are
heavily biased by their distributional assumptions and model choices); and it
can incorporate a range of non-linear constraints (increasing the power of
modeling and flexibility). We apply ProbHardE2E to problems in learning partial
differential equations with uncertainty estimates and to probabilistic
time-series forecasting, showcasing it as a broadly applicable general setup
that connects these seemingly disparate domains.

</details>


### [343] [Comparison of Lightweight Methods for Vehicle Dynamics-Based Driver Drowsiness Detection](https://arxiv.org/abs/2506.07014)
*Yutaro Nakagama,Daisuke Ishii,Kazuki Yoshizoe*

Main category: cs.LG

TL;DR: 本文提出了一种透明公平的框架，用于比较基于车辆动力学的驾驶员疲劳检测（DDD）方法，并验证其可重复性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有DDD方法存在性能指标不可靠和可重复性差的问题，部分研究存在数据泄露或未公开数据集。本文旨在通过公开数据集和标准化框架解决这些问题。

Method: 开发了一个框架，从公开数据集中提取特征并使用轻量级机器学习模型进行DDD。实现了三种现有代表性方法和一种基于随机森林（RF）的新方法。

Result: 在评估的方法中，基于RF的方法达到了88%的最高准确率。

Conclusion: 研究揭示了非标准化开发的DDD方法的固有问题，并展示了一种高性能且实现得当的方法。

Abstract: Driver drowsiness detection (DDD) prevents road accidents caused by driver
fatigue. Vehicle dynamics-based DDD has been proposed as a method that is both
economical and high performance. However, there are concerns about the
reliability of performance metrics and the reproducibility of many of the
existing methods. For instance, some previous studies seem to have a data
leakage issue among training and test datasets, and many do not openly provide
the datasets they used. To this end, this paper aims to compare the performance
of representative vehicle dynamics-based DDD methods under a transparent and
fair framework that uses a public dataset. We first develop a framework for
extracting features from an open dataset by Aygun et al. and performing DDD
with lightweight ML models; the framework is carefully designed to support a
variety of onfigurations. Second, we implement three existing representative
methods and a concise random forest (RF)-based method in the framework.
Finally, we report the results of experiments to verify the reproducibility and
clarify the performance of DDD based on common metrics. Among the evaluated
methods, the RF-based method achieved the highest accuracy of 88 %. Our
findings imply the issues inherent in DDD methods developed in a non-standard
manner, and demonstrate a high performance method implemented appropriately.

</details>


### [344] [AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint](https://arxiv.org/abs/2506.07022)
*Leheng Sheng,Changshuo Shen,Weixiang Zhao,Junfeng Fang,Xiaohao Liu,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: AlphaSteer是一种基于理论的方法，通过优化激活导向向量来平衡LLMs的安全性和实用性，显著提升了模型的安全性而不损害其通用能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在现实应用中的广泛部署，确保其能够拒绝恶意提示（如越狱攻击）是安全可靠使用的关键。现有方法在安全性和实用性之间存在权衡，缺乏理论支持。

Method: AlphaSteer将激活导向视为可学习过程，通过两个目标优化：实用性保持（构造接近零的导向向量）和安全性增强（构造拒绝导向向量）。

Result: 实验表明，AlphaSteer在多种越狱攻击和实用性基准测试中显著提升了LLMs的安全性，同时保持了其通用能力。

Conclusion: AlphaSteer是一种理论支持且实证有效的方法，成功解决了LLMs在安全性和实用性之间的权衡问题。

Abstract: As LLMs are increasingly deployed in real-world applications, ensuring their
ability to refuse malicious prompts, especially jailbreak attacks, is essential
for safe and reliable use. Recently, activation steering has emerged as an
effective approach for enhancing LLM safety by adding a refusal direction
vector to internal activations of LLMs during inference, which will further
induce the refusal behaviors of LLMs. However, indiscriminately applying
activation steering fundamentally suffers from the trade-off between safety and
utility, since the same steering vector can also lead to over-refusal and
degraded performance on benign prompts. Although prior efforts, such as vector
calibration and conditional steering, have attempted to mitigate this
trade-off, their lack of theoretical grounding limits their robustness and
effectiveness. To better address the trade-off between safety and utility, we
present a theoretically grounded and empirically effective activation steering
method called AlphaSteer. Specifically, it considers activation steering as a
learnable process with two principled learning objectives: utility preservation
and safety enhancement. For utility preservation, it learns to construct a
nearly zero vector for steering benign data, with the null-space constraints.
For safety enhancement, it learns to construct a refusal direction vector for
steering malicious data, with the help of linear regression. Experiments across
multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness
of AlphaSteer, which significantly improves the safety of LLMs without
compromising general capabilities. Our codes are available at
https://github.com/AlphaLab-USTC/AlphaSteer.

</details>


### [345] [Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalanced Regression](https://arxiv.org/abs/2506.07033)
*Yung-Chien Wang,Kuang-Da Wang,Wei-Yao Wang,Wen-Chih Peng*

Main category: cs.LG

TL;DR: MATI提出了一种针对表格不平衡回归问题的新方法，通过区域感知混合专家和测试时自监督专家聚合，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 表格数据在现实应用中广泛存在，但表格回归任务中的数据不平衡问题研究不足，现有方法假设测试分布已知且平衡，实际中可能导致性能下降。

Method: MATI采用高斯混合模型捕获相关区域，训练区域特定专家，并通过测试时自监督动态调整专家权重以适应不同测试分布。

Result: 在四种真实数据集上，MATI在三种测试分布下平均MAE提升7.1%。

Conclusion: MATI有效解决了表格不平衡回归问题，具有实际应用价值。

Abstract: Tabular data serve as a fundamental and ubiquitous representation of
structured information in numerous real-world applications, e.g., finance and
urban planning. In the realm of tabular imbalanced applications, data imbalance
has been investigated in classification tasks with insufficient instances in
certain labels, causing the model's ineffective generalizability. However, the
imbalance issue of tabular regression tasks is underexplored, and yet is
critical due to unclear boundaries for continuous labels and simplifying
assumptions in existing imbalance regression work, which often rely on known
and balanced test distributions. Such assumptions may not hold in practice and
can lead to performance degradation. To address these issues, we propose MATI:
Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular
Imbalance Regression, featuring two key innovations: (i) the Region-Aware
Mixture Expert, which adopts a Gaussian Mixture Model to capture the underlying
related regions. The statistical information of each Gaussian component is then
used to synthesize and train region-specific experts to capture the unique
characteristics of their respective regions. (ii) Test-Time Self-Supervised
Expert Aggregation, which dynamically adjusts region expert weights based on
test data features to reinforce expert adaptation across varying test
distributions. We evaluated MATI on four real-world tabular imbalance
regression datasets, including house pricing, bike sharing, and age prediction.
To reflect realistic deployment scenarios, we adopted three types of test
distributions: a balanced distribution with uniform target frequencies, a
normal distribution that follows the training data, and an inverse distribution
that emphasizes rare target regions. On average across these three test
distributions, MATI achieved a 7.1% improvement in MAE compared to existing
methods.

</details>


### [346] [Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning](https://arxiv.org/abs/2506.07040)
*Yang Xu,Swetha Ganesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the first $Q$-learning and actor-critic algorithms for robust
average reward Markov Decision Processes (MDPs) with non-asymptotic convergence
under contamination, TV distance and Wasserstein distance uncertainty sets. We
show that the robust $Q$ Bellman operator is a strict contractive mapping with
respect to a carefully constructed semi-norm with constant functions being
quotiented out. This property supports a stochastic approximation update, that
learns the optimal robust $Q$ function in $\tilde{\cO}(\epsilon^{-2})$ samples.
We also show that the same idea can be used for robust $Q$ function estimation,
which can be further used for critic estimation. Coupling it with theories in
robust policy mirror descent update, we present a natural actor-critic
algorithm that attains an $\epsilon$-optimal robust policy in
$\tilde{\cO}(\epsilon^{-3})$ samples. These results advance the theory of
distributionally robust reinforcement learning in the average reward setting.

</details>


### [347] [FairPFN: A Tabular Foundation Model for Causal Fairness](https://arxiv.org/abs/2506.07049)
*Jake Robertson,Noah Hollmann,Samuel Müller,Noor Awad,Frank Hutter*

Main category: cs.LG

TL;DR: FairPFN是一种无需因果模型先验知识的表格基础模型，用于识别和减轻受保护属性的因果效应。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在关键领域中使用，但历史数据中的偏见可能导致社会不平等。因果公平性框架通常需要已知因果模型，限制了其适用性。

Method: 提出FairPFN，通过预训练合成因果公平数据，无需因果模型知识即可识别和消除受保护属性的因果效应。

Result: FairPFN在多样化的手工和真实场景中表现优于基线方法。

Conclusion: FairPFN为复杂公平问题提供了更广泛的因果公平性解决方案。

Abstract: Machine learning (ML) systems are utilized in critical sectors, such as
healthcare, law enforcement, and finance. However, these systems are often
trained on historical data that contains demographic biases, leading to ML
decisions that perpetuate or exacerbate existing social inequalities. Causal
fairness provides a transparent, human-in-the-loop framework to mitigate
algorithmic discrimination, aligning closely with legal doctrines of direct and
indirect discrimination. However, current causal fairness frameworks hold a key
limitation in that they assume prior knowledge of the correct causal model,
restricting their applicability in complex fairness scenarios where causal
models are unknown or difficult to identify. To bridge this gap, we propose
FairPFN, a tabular foundation model pre-trained on synthetic causal fairness
data to identify and mitigate the causal effects of protected attributes in its
predictions. FairPFN's key contribution is that it requires no knowledge of the
causal model and still demonstrates strong performance in identifying and
removing protected causal effects across a diverse set of hand-crafted and
real-world scenarios relative to robust baseline methods. FairPFN paves the way
for promising future research, making causal fairness more accessible to a
wider variety of complex fairness problems.

</details>


### [348] [Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead](https://arxiv.org/abs/2506.07054)
*Uri Koren,Navdeep Kumar,Uri Gadot,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: PGTS方法通过结合树搜索机制改进策略梯度方法，减少局部最优问题，提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统策略梯度方法在复杂环境中易陷入局部最优，PGTS旨在通过树搜索机制解决这一问题。

Method: PGTS引入m步前瞻机制，理论分析表明增加搜索深度可减少不良驻点，提升最坏情况性能。

Result: 实验表明PGTS在多种MDP结构中表现优异，能规避局部陷阱并找到更优解。

Conclusion: PGTS通过树搜索机制显著提升了策略梯度方法的性能，适用于复杂环境。

Abstract: Classical policy gradient (PG) methods in reinforcement learning frequently
converge to suboptimal local optima, a challenge exacerbated in large or
complex environments. This work investigates Policy Gradient with Tree Search
(PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance
policy optimization. We provide theoretical analysis demonstrating that
increasing the tree search depth $m$-monotonically reduces the set of
undesirable stationary points and, consequently, improves the worst-case
performance of any resulting stationary policy. Critically, our analysis
accommodates practical scenarios where policy updates are restricted to states
visited by the current policy, rather than requiring updates across the entire
state space. Empirical evaluations on diverse MDP structures, including Ladder,
Tightrope, and Gridworld environments, illustrate PGTS's ability to exhibit
"farsightedness," navigate challenging reward landscapes, escape local traps
where standard PG fails, and achieve superior solutions.

</details>


### [349] [E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models](https://arxiv.org/abs/2506.07078)
*Jiaheng Dong,Hong Jia,Soumyajit Chatterjee,Abhirup Ghosh,James Bailey,Ting Dang*

Main category: cs.LG

TL;DR: E-BATS是一种高效的无反向传播测试时间适应框架，专为语音基础模型设计，解决了现有方法在内存效率和准确性上的不足。


<details>
  <summary>Details</summary>
Motivation: 语音基础模型在真实场景中因声学领域偏移（如背景噪声和口音）性能下降，现有测试时间适应方法要么内存消耗大，要么准确性低。

Method: E-BATS通过轻量级提示适应、多尺度损失和测试时间指数移动平均机制，实现高效且稳定的适应。

Result: 在四个噪声语音数据集上的实验显示，E-BATS比无反向传播基线提高了4.1%-13.5%的准确率，并节省了2.0-6.4倍的GPU内存。

Conclusion: E-BATS为实际语音处理系统在声学变化下的高效适应提供了可行方案。

Abstract: Speech Foundation Models encounter significant performance degradation when
deployed in real-world scenarios involving acoustic domain shifts, such as
background noise and speaker accents. Test-time adaptation (TTA) has recently
emerged as a viable strategy to address such domain shifts at inference time
without requiring access to source data or labels. However, existing TTA
approaches, particularly those relying on backpropagation, are
memory-intensive, limiting their applicability in speech tasks and
resource-constrained settings. Although backpropagation-free methods offer
improved efficiency, existing ones exhibit poor accuracy. This is because they
are predominantly developed for vision tasks, which fundamentally differ from
speech task formulations, noise characteristics, and model architecture, posing
unique transferability challenges. In this paper, we introduce E-BATS, the
first Efficient BAckpropagation-free TTA framework designed explicitly for
speech foundation models. E-BATS achieves a balance between adaptation
effectiveness and memory efficiency through three key components: (i)
lightweight prompt adaptation for a forward-pass-based feature alignment, (ii)
a multi-scale loss to capture both global (utterance-level) and local
distribution shifts (token-level) and (iii) a test-time exponential moving
average mechanism for stable adaptation across utterances. Experiments
conducted on four noisy speech datasets spanning sixteen acoustic conditions
demonstrate consistent improvements, with 4.1%-13.5% accuracy gains over
backpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to
backpropagation-based methods. By enabling scalable and robust adaptation under
acoustic variability, this work paves the way for developing more efficient
adaptation approaches for practical speech processing systems in real-world
environments.

</details>


### [350] [State Entropy Regularization for Robust Reinforcement Learning](https://arxiv.org/abs/2506.07085)
*Uri Koren,Yonatan Ashlag,Mirco Mutti,Esther Derman,Pierre-Luc Bacon,Shie Mannor*

Main category: cs.LG

TL;DR: 状态熵正则化在强化学习中表现出更好的探索性和样本效率，但其理论保证尚未被研究。本文证明了状态熵正则化对结构和空间相关扰动的鲁棒性，这些扰动在迁移学习中常见但常被标准鲁棒RL方法忽视。


<details>
  <summary>Details</summary>
Motivation: 研究状态熵正则化在强化学习中的理论保证，特别是其对结构和空间相关扰动的鲁棒性，填补现有方法的不足。

Method: 通过理论分析和实验对比状态熵与策略熵正则化，揭示其在不同扰动下的表现差异。

Result: 状态熵正则化在结构和空间相关扰动下表现更鲁棒，但其优势对策略评估的rollout数量更敏感。

Conclusion: 状态熵正则化在特定扰动下具有显著优势，但实际应用中需注意其对评估条件的敏感性。

Abstract: State entropy regularization has empirically shown better exploration and
sample complexity in reinforcement learning (RL). However, its theoretical
guarantees have not been studied. In this paper, we show that state entropy
regularization improves robustness to structured and spatially correlated
perturbations. These types of variation are common in transfer learning but
often overlooked by standard robust RL methods, which typically focus on small,
uncorrelated changes. We provide a comprehensive characterization of these
robustness properties, including formal guarantees under reward and transition
uncertainty, as well as settings where the method performs poorly. Much of our
analysis contrasts state entropy with the widely used policy entropy
regularization, highlighting their different benefits. Finally, from a
practical standpoint, we illustrate that compared with policy entropy, the
robustness advantages of state entropy are more sensitive to the number of
rollouts used for policy evaluation.

</details>


### [351] [Pointwise confidence estimation in the non-linear $\ell^2$-regularized least squares](https://arxiv.org/abs/2506.07088)
*Ilja Kuzborskij,Yasin Abbasi Yadkori*

Main category: cs.LG

TL;DR: 该论文提出了一种在高概率非渐近情况下对ℓ²正则化非线性最小二乘问题的置信度估计方法，适用于固定设计。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于为非线性预测模型（如神经网络）提供更高效的置信度估计方法，以解决传统方法（如自助法）在覆盖率和宽度之间的权衡问题。

Method: 方法包括推导局部极小值点的点态置信边界，利用隐式特征空间中的加权范数（涉及目标函数的逆Hessian矩阵）来量化测试输入与训练数据的相似性。

Result: 结果显示，所提出的置信边界在覆盖率和宽度之间表现优于自助法，且计算效率高，仅略高于损失函数的梯度计算成本。

Conclusion: 结论表明，该方法为非渐近置信区间提供了一种高效且实用的替代方案，尤其适用于非线性预测模型。

Abstract: We consider a high-probability non-asymptotic confidence estimation in the
$\ell^2$-regularized non-linear least-squares setting with fixed design. In
particular, we study confidence estimation for local minimizers of the
regularized training loss. We show a pointwise confidence bound, meaning that
it holds for the prediction on any given fixed test input $x$. Importantly, the
proposed confidence bound scales with similarity of the test input to the
training data in the implicit feature space of the predictor (for instance,
becoming very large when the test input lies far outside of the training data).
This desirable last feature is captured by the weighted norm involving the
inverse-Hessian matrix of the objective function, which is a generalized
version of its counterpart in the linear setting, $x^{\top} \text{Cov}^{-1} x$.
Our generalized result can be regarded as a non-asymptotic counterpart of the
classical confidence interval based on asymptotic normality of the MLE
estimator. We propose an efficient method for computing the weighted norm,
which only mildly exceeds the cost of a gradient computation of the loss
function. Finally, we complement our analysis with empirical evidence showing
that the proposed confidence bound provides better coverage/width trade-off
compared to a confidence estimation by bootstrapping, which is a gold-standard
method in many applications involving non-linear predictors such as neural
networks.

</details>


### [352] [Patient Similarity Computation for Clinical Decision Support: An Efficient Use of Data Transformation, Combining Static and Time Series Data](https://arxiv.org/abs/2506.07092)
*Joydeb Kumar Sana,Mohammad M. Masud,M Sohel Rahman,M Saifur Rahman*

Main category: cs.LG

TL;DR: 本文提出了一种基于数据转换方法的分布式患者相似性计算（DPSC）技术，结合时间序列和静态数据，显著提升了预测性能并减少了计算时间。


<details>
  <summary>Details</summary>
Motivation: 患者相似性计算（PSC）是医疗信息学中的核心问题，旨在通过历史临床记录衡量患者相似性以支持临床决策。

Method: 采用数据转换方法（aWOE和Z-score）处理静态数据，结合动态时间规整（DTW）处理时间序列数据，并通过分布式计算优化DTW。

Result: 在冠状动脉疾病和充血性心力衰竭的预测中，AUC、准确率和F-measure分别提升高达11.4%-21.9%，计算时间减少40%。

Conclusion: 提出的DPSC方法在预测性能和计算效率上均有显著提升，适用于大规模医疗数据分析。

Abstract: Patient similarity computation (PSC) is a fundamental problem in healthcare
informatics. The aim of the patient similarity computation is to measure the
similarity among patients according to their historical clinical records, which
helps to improve clinical decision support. This paper presents a novel
distributed patient similarity computation (DPSC) technique based on data
transformation (DT) methods, utilizing an effective combination of time series
and static data. Time series data are sensor-collected patients' information,
including metrics like heart rate, blood pressure, Oxygen saturation,
respiration, etc. The static data are mainly patient background and demographic
data, including age, weight, height, gender, etc. Static data has been used for
clustering the patients. Before feeding the static data to the machine learning
model adaptive Weight-of-Evidence (aWOE) and Z-score data transformation (DT)
methods have been performed, which improve the prediction performances. In
aWOE-based patient similarity models, sensitive patient information has been
processed using aWOE which preserves the data privacy of the trained models. We
used the Dynamic Time Warping (DTW) approach, which is robust and very popular,
for time series similarity. However, DTW is not suitable for big data due to
the significant computational run-time. To overcome this problem, distributed
DTW computation is used in this study. For Coronary Artery Disease, our DT
based approach boosts prediction performance by as much as 11.4%, 10.20%, and
12.6% in terms of AUC, accuracy, and F-measure, respectively. In the case of
Congestive Heart Failure (CHF), our proposed method achieves performance
enhancement up to 15.9%, 10.5%, and 21.9% for the same measures, respectively.
The proposed method reduces the computation time by as high as 40%.

</details>


### [353] [Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion](https://arxiv.org/abs/2506.07099)
*Wenying He,Jieling Huang,Junhua Gu,Ji Zhang,Yude Bai*

Main category: cs.LG

TL;DR: CoFILL是一种基于条件扩散模型的新型时空数据填补方法，通过双流架构处理时空和频域特征，显著提升了填补精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效建模时空维度间的复杂依赖关系，且在填补过程中存在累积误差问题。

Method: 提出CoFILL，利用扩散模型的优势，结合双流架构并行处理时空和频域特征。

Result: 实验表明CoFILL在填补精度上优于现有方法，并能生成与真实数据分布一致的高质量填补值。

Conclusion: CoFILL为时空数据填补提供了一种更鲁棒的解决方案，代码已开源。

Abstract: Missing data in spatiotemporal systems presents a significant challenge for
modern applications, ranging from environmental monitoring to urban traffic
management. The integrity of spatiotemporal data often deteriorates due to
hardware malfunctions and software failures in real-world deployments. Current
approaches based on machine learning and deep learning struggle to model the
intricate interdependencies between spatial and temporal dimensions effectively
and, more importantly, suffer from cumulative errors during the data imputation
process, which propagate and amplify through iterations. To address these
limitations, we propose CoFILL, a novel Conditional Diffusion Model for
spatiotemporal data imputation. CoFILL builds on the inherent advantages of
diffusion models to generate high-quality imputations without relying on
potentially error-prone prior estimates. It incorporates an innovative
dual-stream architecture that processes temporal and frequency domain features
in parallel. By fusing these complementary features, CoFILL captures both rapid
fluctuations and underlying patterns in the data, which enables more robust
imputation. The extensive experiments reveal that CoFILL's noise prediction
network successfully transforms random noise into meaningful values that align
with the true data distribution. The results also show that CoFILL outperforms
state-of-the-art methods in imputation accuracy. The source code is publicly
available at https://github.com/joyHJL/CoFILL.

</details>


### [354] [Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings](https://arxiv.org/abs/2506.07109)
*Rong-Xi Tan,Ming Chen,Ke Xue,Yao Wang,Yaoyuan Wang,Sheng Fu,Chao Qian*

Main category: cs.LG

TL;DR: 论文探讨了利用语言模型的嵌入能力实现跨领域通用黑盒优化（BBO），提出了端到端学习框架和潜在空间学习方法，实验验证了其通用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 传统离线BBO方法受限于单任务和固定维度，缺乏统一表示方法，而语言模型的嵌入能力为解决这一问题提供了可能。

Method: 提出了基于语言模型的端到端学习框架（如next-token预测）和潜在空间学习方法，利用开源数据训练。

Result: 实验证明所提方法在通用性和有效性上表现优异，能够克服传统BBO的局限性。

Conclusion: 结合语言模型先验和学习字符串嵌入空间，为通用BBO算法的发展提供了新方向。

Abstract: The pursuit of universal black-box optimization (BBO) algorithms is a
longstanding goal. However, unlike domains such as language or vision, where
scaling structured data has driven generalization, progress in offline BBO
remains hindered by the lack of unified representations for heterogeneous
numerical spaces. Thus, existing offline BBO approaches are constrained to
single-task and fixed-dimensional settings, failing to achieve cross-domain
universal optimization. Recent advances in language models (LMs) offer a
promising path forward: their embeddings capture latent relationships in a
unifying way, enabling universal optimization across different data types
possible. In this paper, we discuss multiple potential approaches, including an
end-to-end learning framework in the form of next-token prediction, as well as
prioritizing the learning of latent spaces with strong representational
capabilities. To validate the effectiveness of these methods, we collect
offline BBO tasks and data from open-source academic works for training.
Experiments demonstrate the universality and effectiveness of our proposed
methods. Our findings suggest that unifying language model priors and learning
string embedding space can overcome traditional barriers in universal BBO,
paving the way for general-purpose BBO algorithms. The code is provided at
https://github.com/lamda-bbo/universal-offline-bbo.

</details>


### [355] [Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models](https://arxiv.org/abs/2506.07121)
*Ren-Jian Wang,Ke Xue,Zeyu Qin,Ziniu Li,Sheng Tang,Hao-Tian Li,Shengcai Liu,Chao Qian*

Main category: cs.LG

TL;DR: 本文提出了一种名为QDRT的新框架，用于解决现有红队测试方法在多样性和攻击效果上的局限性，通过行为条件训练和多攻击者模型提升攻击的多样性和有效性。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型（LLM）的安全性至关重要，而现有的红队测试方法在攻击多样性和覆盖范围上存在不足。

Method: QDRT框架通过行为条件训练实现目标驱动的多样性，并采用开放式行为重放缓冲区，同时训练多个专业攻击者模型。

Result: 实验表明，QDRT生成的攻击在多样性和有效性上均优于现有方法，适用于多种目标LLM。

Conclusion: QDRT为LLM安全性提供了一种系统且高效的红队测试方法，支持LLM的负责任部署。

Abstract: Ensuring safety of large language models (LLMs) is important. Red teaming--a
systematic approach to identifying adversarial prompts that elicit harmful
responses from target LLMs--has emerged as a crucial safety evaluation method.
Within this framework, the diversity of adversarial prompts is essential for
comprehensive safety assessments. We find that previous approaches to
red-teaming may suffer from two key limitations. First, they often pursue
diversity through simplistic metrics like word frequency or sentence embedding
similarity, which may not capture meaningful variation in attack strategies.
Second, the common practice of training a single attacker model restricts
coverage across potential attack styles and risk categories. This paper
introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to
address these limitations. QDRT achieves goal-driven diversity through
behavior-conditioned training and implements a behavioral replay buffer in an
open-ended manner. Additionally, it trains multiple specialized attackers
capable of generating high-quality attacks across diverse styles and risk
categories. Our empirical evaluation demonstrates that QDRT generates attacks
that are both more diverse and more effective against a wide range of target
LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the
field of LLM safety by providing a systematic and effective approach to
automated red-teaming, ultimately supporting the responsible deployment of
LLMs.

</details>


### [356] [Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning](https://arxiv.org/abs/2506.07134)
*Eshwar S. R.,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.LG

TL;DR: 论文提出了一种名为可靠策略迭代（RPI）的新算法，解决了强化学习中函数逼近下策略迭代的单调性保证问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法在函数逼近下无法保证策略迭代的单调性改进，RPI旨在解决这一问题。

Method: RPI通过基于贝尔曼的约束优化替代传统的投影或贝尔曼误差最小化，确保值估计的单调性和下界保证。

Result: RPI在经典控制任务中不仅保持了下界保证，还匹配或超越了基线方法的性能。

Conclusion: RPI是首个在函数逼近下具有单调性和收敛性保证的算法，适用于模型无关的强化学习实现。

Abstract: Despite decades of research, it remains challenging to correctly use
Reinforcement Learning (RL) algorithms with function approximation. A prime
example is policy iteration, whose fundamental guarantee of monotonic
improvement collapses even under linear function approximation. To address this
issue, we introduce Reliable Policy Iteration (RPI). It replaces the common
projection or Bellman-error minimization during policy evaluation with a
Bellman-based constrained optimization. We prove that not only does RPI confer
textbook monotonicity on its value estimates but these estimates also lower
bound the true return. Also, their limit partially satisfies the unprojected
Bellman equation, emphasizing RPI's natural fit within RL. RPI is the first
algorithm with such monotonicity and convergence guarantees under function
approximation. For practical use, we provide a model-free variant of RPI that
amounts to a novel critic. It can be readily integrated into primary model-free
PI implementations such as DQN and DDPG. In classical control tasks, such
RPI-enhanced variants consistently maintain their lower-bound guarantee while
matching or surpassing the performance of all baseline methods.

</details>


### [357] [AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models](https://arxiv.org/abs/2506.07165)
*Qi Liu,Jingqing Ruan,Hao Li,Haodong Zhao,Desheng Wang,Jiansong Chen,Wan Guanglu,Xunliang Cai,Zhi Zheng,Tong Xu*

Main category: cs.LG

TL;DR: AMoPO是一种新型框架，通过动态平衡多目标偏好优化，解决了现有方法在平衡偏好维度和计算复杂性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效平衡多目标偏好，且依赖辅助模型增加了计算复杂性。

Method: 引入多目标优化范式，使用维度感知生成指标作为隐式奖励，无需额外奖励模型或参考模型。通过自适应权重分配机制动态优先处理偏好维度。

Result: AMoPO在7B、14B和32B模型上表现优异，优于基线方法28.5%，验证了其扩展性和适应性。

Conclusion: AMoPO能够实现维度感知的偏好对齐，展示了其优越性和有效性。

Abstract: Existing multi-objective preference alignment methods for large language
models (LLMs) face limitations: (1) the inability to effectively balance
various preference dimensions, and (2) reliance on auxiliary reward/reference
models introduces computational complexity. To address these challenges, we
propose Adaptive Multi-objective Preference Optimization (AMoPO), a novel
framework that achieves dynamic balance across preference dimensions. By
introducing the multi-objective optimization paradigm to use the
dimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with
diverse preferences without additional reward models or reference models. We
introduce an adaptive weight assignment mechanism that models the generation
space as a Gaussian distribution, allowing dynamic prioritization of preference
dimensions. Empirical results demonstrate that AMoPO outperforms
state-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B
models reveal the scaling ability of AMoPO. Moreover, additional analysis of
multiple dimensions verifies its adaptability and effectiveness. These findings
validate AMoPO's capability to achieve dimension-aware preference alignment,
highlighting its superiority. Our codes and datasets are available at
https://github.com/Javkonline/AMoPO.

</details>


### [358] [Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment](https://arxiv.org/abs/2506.07168)
*Huanyi Xie,Lijie Hu,Lu Yu,Tianhao Huang,Longfei Li,Meng Li,Jun Zhou,Huan Wang,Di Wang*

Main category: cs.LG

TL;DR: GAGA是一个高效的TAG表示学习框架，通过仅标注代表性节点和边来减少时间和成本，并通过两级对齐模块整合标注图与TAG，实验表明其性能优于现有方法且仅需1%的标注数据。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在处理TAG时因复杂文本信息表现不佳，现有方法依赖大量标注或微调，成本高且耗时。

Method: GAGA通过标注代表性节点和边构建标注图，并采用两级对齐模块将其与TAG结构对齐。

Result: GAGA在分类准确率上达到或超越现有方法，且仅需1%的标注数据。

Conclusion: GAGA是一种高效且低成本的TAG表示学习解决方案。

Abstract: In the realm of Text-attributed Graphs (TAGs), traditional graph neural
networks (GNNs) often fall short due to the complex textual information
associated with each node. Recent methods have improved node representations by
leveraging large language models (LLMs) to enhance node text features, but
these approaches typically require extensive annotations or fine-tuning across
all nodes, which is both time-consuming and costly. To overcome these
challenges, we introduce GAGA, an efficient framework for TAG representation
learning. GAGA reduces annotation time and cost by focusing on annotating only
representative nodes and edges. It constructs an annotation graph that captures
the topological relationships among these annotations. Furthermore, GAGA
employs a two-level alignment module to effectively integrate the annotation
graph with the TAG, aligning their underlying structures. Experiments show that
GAGA achieves classification accuracies on par with or surpassing
state-of-the-art methods while requiring only 1% of the data to be annotated,
demonstrating its high efficiency.

</details>


### [359] [Regularized Adaptive Graph Learning for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2506.07179)
*Kaiqi Wu,Weiyang Kong,Sen Zhang,Yubao Liu,Zitong Chen*

Main category: cs.LG

TL;DR: 论文提出了一种正则化自适应图学习（RAGL）模型，通过结合随机共享嵌入（SSE）和残差差异机制，解决了现有交通预测方法中节点嵌入正则化不足和计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自适应图学习方法在交通预测中常忽略节点嵌入的正则化或面临计算效率低的挑战，影响了模型的性能和可扩展性。

Method: 提出RAGL模型，结合SSE和残差差异机制实现嵌入正则化和噪声抑制；开发高效余弦算子（ECO）以线性时间复杂度完成图卷积。

Result: 在四个大规模真实交通数据集上的实验表明，RAGL在预测精度和计算效率上均优于现有方法。

Conclusion: RAGL通过正则化和高效计算机制，显著提升了交通预测的性能和可扩展性。

Abstract: Traffic prediction is a critical task in spatial-temporal forecasting with
broad applications in travel planning and urban management. Adaptive graph
convolution networks have emerged as mainstream solutions due to their ability
to learn node embeddings in a data-driven manner and capture complex latent
dependencies. However, existing adaptive graph learning methods for traffic
forecasting often either ignore the regularization of node embeddings, which
account for a significant proportion of model parameters, or face scalability
issues from expensive graph convolution operations. To address these
challenges, we propose a Regularized Adaptive Graph Learning (RAGL) model.
First, we introduce a regularized adaptive graph learning framework that
synergizes Stochastic Shared Embedding (SSE) and adaptive graph convolution via
a residual difference mechanism, achieving both embedding regularization and
noise suppression. Second, to ensure scalability on large road networks, we
develop the Efficient Cosine Operator (ECO), which performs graph convolution
based on the cosine similarity of regularized embeddings with linear time
complexity. Extensive experiments on four large-scale real-world traffic
datasets show that RAGL consistently outperforms state-of-the-art methods in
terms of prediction accuracy and exhibits competitive computational efficiency.

</details>


### [360] [Learning based on neurovectors for tabular data: a new neural network approach](https://arxiv.org/abs/2506.07185)
*J. C. Husillos,A. Gallego,A. Roma,A. Troncoso*

Main category: cs.LG

TL;DR: 提出了一种基于Neurovectors的新型学习方法，通过向量关系和能量传播实现高效且可解释的学习。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络依赖反向传播调整权重，缺乏可解释性和适应性，Neurovectors旨在解决这些问题。

Method: 利用Neurovectors在向量空间中结构化数据，通过能量传播驱动学习，生成动态知识表示。

Result: 在UCI和Kaggle数据集上的实验表明，Neurovectors在分类和回归任务中具有竞争力。

Conclusion: Neurovectors提供了一种高效、可解释的替代方案，优于传统方法。

Abstract: In this paper, we present a novel learning approach based on Neurovectors, an
innovative paradigm that structures information through interconnected nodes
and vector relationships for tabular data processing. Unlike traditional
artificial neural networks that rely on weight adjustment through
backpropagation, Neurovectors encode information by structuring data in vector
spaces where energy propagation, rather than traditional weight updates, drives
the learning process, enabling a more adaptable and explainable learning
process. Our method generates dynamic representations of knowledge through
neurovectors, thereby improving both the interpretability and efficiency of the
predictive model. Experimental results using datasets from well-established
repositories such as the UCI machine learning repository and Kaggle are
reported both for classification and regression. To evaluate its performance,
we compare our approach with standard machine learning and deep learning
models, showing that Neurovectors achieve competitive accuracy.

</details>


### [361] [Analyzing Breast Cancer Survival Disparities by Race and Demographic Location: A Survival Analysis Approach](https://arxiv.org/abs/2506.07191)
*Ramisa Farha,Joshua O. Olukoya*

Main category: cs.LG

TL;DR: 该研究通过SEER 2021数据集和生存分析技术，揭示了不同种族和地区乳腺癌患者生存结果的差异，为政策制定和医疗干预提供依据。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示乳腺癌患者生存结果的种族和地区差异，为减少治疗不平等提供数据支持。

Method: 结合探索性数据分析（EDA）、Kaplan-Meier估计、log-rank检验和Cox比例风险模型进行分析。

Result: 详细统计了乳腺癌生存率的种族和地区差异，为针对性干预提供基础。

Conclusion: 研究为改善乳腺癌治疗结果和减少不平等提供了重要工具和参考。

Abstract: This study employs a robust analytical framework to uncover patterns in
survival outcomes among breast cancer patients from diverse racial and
geographical backgrounds. This research uses the SEER 2021 dataset to analyze
breast cancer survival outcomes to identify and comprehend dissimilarities. Our
approach integrates exploratory data analysis (EDA), through this we identify
key variables that influence survival rates and employ survival analysis
techniques, including the Kaplan-Meier estimator and log-rank test and the
advanced modeling Cox Proportional Hazards model to determine how survival
rates vary across racial groups and countries. Model validation and
interpretation are undertaken to ensure the reliability of our findings, which
are documented comprehensively to inform policymakers and healthcare
professionals. The outcome of this paper is a detailed version of statistical
analysis that not just highlights disparities in breast cancer treatment and
care but also serves as a foundational tool for developing targeted
interventions to address the inequalities effectively. Through this research,
our aim is to contribute to the global efforts to improve breast cancer
outcomes and reduce treatment disparities.

</details>


### [362] [GGBall: Graph Generative Model on Poincaré Ball](https://arxiv.org/abs/2506.07198)
*Tianci Bu,Chuanrui Wang,Hao Ma,Haoren Zheng,Xin Lu,Tailin Wu*

Main category: cs.LG

TL;DR: GGBall提出了一种基于双曲几何的图生成框架，结合了HVQVAE和黎曼流匹配先验，显著提升了生成图的层次结构保持能力。


<details>
  <summary>Details</summary>
Motivation: 解决欧几里得几何在捕捉指数复杂度时的局限性，为生成具有层次结构的图提供新方法。

Method: 结合双曲向量量化自编码器（HVQVAE）与基于闭式测地线的黎曼流匹配先验，开发了双曲GNN和Transformer层。

Result: 在Community-Small和Ego-Small数据集上，度数MMD分别降低了75%和40%，优于现有方法。

Conclusion: 双曲几何为复杂、结构化且层次化的数据生成提供了强大基础。

Abstract: Generating graphs with hierarchical structures remains a fundamental
challenge due to the limitations of Euclidean geometry in capturing exponential
complexity. Here we introduce \textbf{GGBall}, a novel hyperbolic framework for
graph generation that integrates geometric inductive biases with modern
generative paradigms. GGBall combines a Hyperbolic Vector-Quantized Autoencoder
(HVQVAE) with a Riemannian flow matching prior defined via closed-form
geodesics. This design enables flow-based priors to model complex latent
distributions, while vector quantization helps preserve the curvature-aware
structure of the hyperbolic space. We further develop a suite of hyperbolic GNN
and Transformer layers that operate entirely within the manifold, ensuring
stability and scalability. Empirically, our model reduces degree MMD by over
75\% on Community-Small and over 40\% on Ego-Small compared to state-of-the-art
baselines, demonstrating an improved ability to preserve topological
hierarchies. These results highlight the potential of hyperbolic geometry as a
powerful foundation for the generative modeling of complex, structured, and
hierarchical data domains. Our code is available at
\href{https://github.com/AI4Science-WestlakeU/GGBall}{here}.

</details>


### [363] [Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward](https://arxiv.org/abs/2506.07218)
*Tong Xiao,Xin Xu,Zhenya Huang,Hongyu Gao,Quan Liu,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: 论文提出Perception-R1方法，通过引入视觉感知奖励提升多模态大语言模型（MLLMs）的感知和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法未能有效增强MLLMs的多模态感知能力，限制了其推理能力的进一步提升。

Method: 提出Perception-R1，通过视觉感知奖励激励MLLMs准确感知视觉内容，并利用LLM评估视觉注释与模型响应的一致性。

Result: 在多个多模态推理基准测试中，Perception-R1仅用1,442个训练数据即达到最优性能。

Conclusion: Perception-R1有效提升了MLLMs的感知和推理能力，为多模态领域提供了新思路。

Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language
Models (MLLMs) is a challenging task that has attracted increasing attention in
the community. Recently, several studies have applied Reinforcement Learning
with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the
reasoning abilities of MLLMs. However, these works largely overlook the
enhancement of multimodal perception capabilities in MLLMs, which serve as a
core prerequisite and foundational component of complex multimodal reasoning.
Through McNemar's test, we find that existing RLVR method fails to effectively
enhance the multimodal perception capabilities of MLLMs, thereby limiting their
further improvement in multimodal reasoning. To address this limitation, we
propose Perception-R1, which introduces a novel visual perception reward that
explicitly encourages MLLMs to perceive the visual content accurately, thereby
can effectively incentivizing both their multimodal perception and reasoning
capabilities. Specifically, we first collect textual visual annotations from
the CoT trajectories of multimodal problems, which will serve as visual
references for reward assignment. During RLVR training, we employ a judging LLM
to assess the consistency between the visual annotations and the responses
generated by MLLM, and assign the visual perception reward based on these
consistency judgments. Extensive experiments on several multimodal reasoning
benchmarks demonstrate the effectiveness of our Perception-R1, which achieves
state-of-the-art performance on most benchmarks using only 1,442 training data.

</details>


### [364] [VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution](https://arxiv.org/abs/2506.07229)
*Mateusz Gajewski,Mikołaj Morzy,Adam Karczmarz,Piotr Sankowski*

Main category: cs.LG

TL;DR: VARSHAP是一种新的模型无关局部特征归因方法，通过减少预测方差作为特征重要性指标，优于SHAP和LIME。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因方法（如SHAP）存在全局依赖问题，无法准确捕捉局部模型行为。

Method: 基于Shapley值框架，VARSHAP以预测方差减少为核心指标，满足Shapley公理且对全局数据分布变化鲁棒。

Result: 在合成和真实数据集上，VARSHAP在定量和定性上均优于KernelSHAP和LIME。

Conclusion: VARSHAP是一种更准确且鲁棒的局部特征归因方法。

Abstract: Existing feature attribution methods like SHAP often suffer from global
dependence, failing to capture true local model behavior. This paper introduces
VARSHAP, a novel model-agnostic local feature attribution method which uses the
reduction of prediction variance as the key importance metric of features.
Building upon Shapley value framework, VARSHAP satisfies the key Shapley
axioms, but, unlike SHAP, is resilient to global data distribution shifts.
Experiments on synthetic and real-world datasets demonstrate that VARSHAP
outperforms popular methods such as KernelSHAP or LIME, both quantitatively and
qualitatively.

</details>


### [365] [Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs](https://arxiv.org/abs/2506.07240)
*Roy Eisenstadt,Itamar Zimerman,Lior Wolf*

Main category: cs.LG

TL;DR: 论文研究了LLMs在显式结构化推理中如何理解和调节推理长度，提出了一种通过可视化进度条和操纵内部进度编码来优化推理过程的方法，以减少不必要的步骤并提高性能。


<details>
  <summary>Details</summary>
Motivation: 显式结构化推理中，推理长度对答案质量有重要影响：过短可能无法捕捉任务复杂性，过长则可能导致过度思考和性能下降。

Method: 1. 展示LLMs如何编码推理进度，并引入交互式进度条可视化；2. 在推理过程中操纵内部进度编码以减少冗余步骤。

Result: 实验结果表明，该方法能减少过度思考，提高答案准确性，并降低推理延迟。

Conclusion: 通过优化推理长度，LLMs可以在显式结构化推理中更高效地完成任务。

Abstract: Recently, techniques such as explicit structured reasoning have demonstrated
strong test-time scaling behavior by enforcing a separation between the model's
internal "thinking" process and the final response. A key factor influencing
answer quality in this setting is the length of the thinking stage. When the
reasoning is too short, the model may fail to capture the complexity of the
task. Conversely, when it is too long, the model may overthink, leading to
unnecessary computation and degraded performance. This paper explores and
exploits the underlying mechanisms by which LLMs understand and regulate the
length of their reasoning during explicit thought processes. First, we show
that LLMs encode their progress through the reasoning process and introduce an
interactive progress bar visualization, which is then used to reveal insights
on the model's planning dynamics. Second, we manipulate the internal progress
encoding during inference to reduce unnecessary steps and generate a more
concise and decisive chain of thoughts. Our empirical results demonstrate that
this "overclocking" method mitigates overthinking, improves answer accuracy,
and reduces inference latency. Our code is publicly available.

</details>


### [366] [Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models](https://arxiv.org/abs/2506.07247)
*Ngoc-Quan Pham,Tuan Truong,Quyen Tran,Tan Nguyen,Dinh Phung,Trung Le*

Main category: cs.LG

TL;DR: IBDR是一种新型贝叶斯推理框架，通过增强粒子多样性提升集成质量，并在实际应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过建模粒子间交互作用，提升贝叶斯推理的分布鲁棒性和粒子多样性。

Method: IBDR基于广义理论框架，连接分布总体损失与近似后验，采用双重优化程序实现分布鲁棒性和粒子多样性。

Result: 在VTAB-1K基准测试和语言推理任务中，IBDR表现优于基线方法。

Conclusion: IBDR在实际应用中表现出色，验证了其有效性。

Abstract: We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel
Bayesian inference framework that allows modeling the interactions between
particles, thereby enhancing ensemble quality through increased particle
diversity. IBDR is grounded in a generalized theoretical framework that
connects the distributional population loss with the approximate posterior,
motivating a practical dual optimization procedure that enforces distributional
robustness while fostering particle diversity. We evaluate IBDR's performance
against various baseline methods using the VTAB-1K benchmark and the common
reasoning language task. The results consistently show that IBDR outperforms
these baselines, underscoring its effectiveness in real-world applications.

</details>


### [367] [A Stable Whitening Optimizer for Efficient Neural Network Training](https://arxiv.org/abs/2506.07254)
*Kevin Frans,Sergey Levine,Pieter Abbeel*

Main category: cs.LG

TL;DR: 论文提出SPlus方法，解决了Shampoo算法的三个关键问题，包括稳定性、学习率适应性和参数噪声问题，实验表明其性能优于Adam。


<details>
  <summary>Details</summary>
Motivation: 解决Shampoo算法在实际应用中的稳定性、计算效率和适应性不足的问题。

Method: 结合历史特征基与瞬时归一化的有界更新、形状感知的学习率调整和迭代平均方案。

Result: SPlus在验证性能上比Adam节省44%的梯度步数和62%的墙钟时间。

Conclusion: SPlus是一种高效且稳定的优化方法，适用于多种任务。

Abstract: In this work, we take an experimentally grounded look at neural network
optimization. Building on the Shampoo family of algorithms, we identify and
alleviate three key issues, resulting in the proposed SPlus method. First, we
find that naive Shampoo is prone to divergence when matrix-inverses are cached
for long periods. We introduce an alternate bounded update combining a
historical eigenbasis with instantaneous normalization, resulting in
across-the-board stability and significantly lower computational requirements.
Second, we adapt a shape-aware scaling to enable learning rate transfer across
network width. Third, we find that high learning rates result in large
parameter noise, and propose a simple iterate-averaging scheme which unblocks
faster learning. To properly confirm these findings, we introduce a pointed
Transformer training benchmark, considering three objectives (language
modelling, image classification, and diffusion modelling) across different
stages of training. On average, SPlus is able to reach the validation
performance of Adam within 44% of the gradient steps and 62% of the wallclock
time.

</details>


### [368] [A Cramér-von Mises Approach to Incentivizing Truthful Data Sharing](https://arxiv.org/abs/2506.07272)
*Alex Clinton,Thomas Zeng,Yiding Chen,Xiaojin Zhu,Kirthevasan Kandasamy*

Main category: cs.LG

TL;DR: 提出一种基于Cramér-von Mises统计的新型奖励机制，严格激励真实数据提交，减少虚假数据。


<details>
  <summary>Details</summary>
Motivation: 传统基于数据量的奖励机制易被操纵，现有方法依赖强数据分布假设，适用性有限。

Method: 开发基于两样本检验的奖励机制，适用于贝叶斯和无先验设置。

Result: 理论证明真实报告为纳什均衡，实验验证机制在语言和图像数据上的有效性。

Conclusion: 新机制放宽假设，有效激励真实数据共享。

Abstract: Modern data marketplaces and data sharing consortia increasingly rely on
incentive mechanisms to encourage agents to contribute data. However, schemes
that reward agents based on the quantity of submitted data are vulnerable to
manipulation, as agents may submit fabricated or low-quality data to inflate
their rewards. Prior work has proposed comparing each agent's data against
others' to promote honesty: when others contribute genuine data, the best way
to minimize discrepancy is to do the same. Yet prior implementations of this
idea rely on very strong assumptions about the data distribution (e.g.
Gaussian), limiting their applicability. In this work, we develop reward
mechanisms based on a novel, two-sample test inspired by the Cram\'er-von Mises
statistic. Our methods strictly incentivize agents to submit more genuine data,
while disincentivizing data fabrication and other types of untruthful
reporting. We establish that truthful reporting constitutes a (possibly
approximate) Nash equilibrium in both Bayesian and prior-agnostic settings. We
theoretically instantiate our method in three canonical data sharing problems
and show that it relaxes key assumptions made by prior work. Empirically, we
demonstrate that our mechanism incentivizes truthful data sharing via
simulations and on real-world language and image data.

</details>


### [369] [Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models](https://arxiv.org/abs/2506.07275)
*Haochen Song,Dominik Hofer,Rania Islambouli,Laura Hawkins,Ananya Bhattacharjee,Meredith Franklin,Joseph Jay Williams*

Main category: cs.LG

TL;DR: 提出了一种结合cMAB和LLM的混合方法，用于个性化干预以减少久坐行为，并通过实验验证其效果。


<details>
  <summary>Details</summary>
Motivation: 传统cMAB算法需要大样本且可能忽略心理因素，因此探索结合LLM的个性化方法。

Method: 结合cMAB选择干预类型和LLM个性化消息内容，评估四种干预类型，并通过七天的试验比较四种模型的效果。

Result: 研究揭示了LLM个性化和cMAB适应在促进体育活动中的互补作用。

Conclusion: 混合方法在个性化行为干预中具有潜力，为未来研究提供了新方向。

Abstract: Machine learning approaches, such as contextual multi-armed bandit (cMAB)
algorithms, offer a promising strategy to reduce sedentary behavior by
delivering personalized interventions to encourage physical activity. However,
cMAB algorithms typically require large participant samples to learn
effectively and may overlook key psychological factors that are not explicitly
encoded in the model. In this study, we propose a hybrid approach that combines
cMAB for selecting intervention types with large language models (LLMs) to
personalize message content. We evaluate four intervention types: behavioral
self-monitoring, gain-framed, loss-framed, and social comparison, each
delivered as a motivational message aimed at increasing motivation for physical
activity and daily step count. Message content is further personalized using
dynamic contextual factors including daily fluctuations in self-efficacy,
social influence, and regulatory focus. Over a seven-day trial, participants
receive daily messages assigned by one of four models: cMAB alone, LLM alone,
combined cMAB with LLM personalization (cMABxLLM), or equal randomization
(RCT). Outcomes include daily step count and message acceptance, assessed via
ecological momentary assessments (EMAs). We apply a causal inference framework
to evaluate the effects of each model. Our findings offer new insights into the
complementary roles of LLM-based personalization and cMAB adaptation in
promoting physical activity through personalized behavioral messaging.

</details>


### [370] [Tokenized Bandit for LLM Decoding and Alignment](https://arxiv.org/abs/2506.07276)
*Suho Shin,Chenghao Yang,Haifeng Xu,Mohammad T. Hajiaghayi*

Main category: cs.LG

TL;DR: 论文提出了基于LLM解码和对齐的Tokenized线性赌博机（TLB）和多臂赌博机（TMAB）问题，并证明了在无结构假设下学习不可行。通过引入DDMC假设，设计了高效算法，验证了贪婪解码的合理性，并应用于LLM对齐。


<details>
  <summary>Details</summary>
Motivation: 研究LLM解码和对齐中的序列决策问题，解决传统方法在无结构假设下的学习困难。

Method: 提出TLB和TMAB问题，引入DDMC假设，设计算法实现O~(L√T)和O~(L√T^(2/3))的遗憾界。

Result: 算法在合成和真实数据集上表现良好，验证了DDMC假设和贪婪解码的有效性。

Conclusion: DDMC假设和提出的算法为LLM解码和对齐提供了理论基础和实用工具。

Abstract: We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB),
variants of linear and stochastic multi-armed bandit problems inspired by LLM
decoding and alignment. In these problems, at each round $t \in [T]$, a user
submits a query (context), and the decision maker (DM) sequentially selects a
token irrevocably from a token set. Once the sequence is complete, the DM
observes a random utility from the user, whose expectation is presented by a
sequence function mapping the chosen token sequence to a nonnegative real value
that depends on the query.
  In both problems, we first show that learning is impossible without any
structure on the sequence function. We introduce a natural assumption,
diminishing distance with more commons (DDMC), and propose algorithms with
regret $\tilde{O}(L\sqrt{T})$ and $\tilde{O}(L\sqrt{T^{2/3}})$ for TLB and
TMAB, respectively. As a side product, we obtain an (almost) optimality of the
greedy decoding for LLM decoding algorithm under DDMC, which justifies the
unresaonable effectiveness of greedy decoding in several tasks. This also has
an immediate application to decoding-time LLM alignment, when the misaligned
utility can be represented as the frozen LLM's utility and a linearly
realizable latent function. We finally validate our algorithm's performance
empirically as well as verify our assumptions using synthetic and real-world
datasets.

</details>


### [371] [EviNet: Evidential Reasoning Network for Resilient Graph Learning in the Open and Noisy Environments](https://arxiv.org/abs/2506.07288)
*Weijie Guan,Haohui Wang,Jian Kang,Lihui Liu,Dawei Zhou*

Main category: cs.LG

TL;DR: EVINET框架通过Beta嵌入和主观逻辑解决了图学习中的误分类检测和分布外检测问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图学习通常基于封闭世界假设，而实际应用中需要处理开放和噪声环境中的误分类和分布外数据。

Method: EVINET结合Beta嵌入和主观逻辑，包含Dissonance Reasoning（误分类检测）和Vacuity Reasoning（分布外检测）模块。

Result: 实验表明EVINET在多个任务和指标上优于现有方法。

Conclusion: EVINET证明了不确定性估计和逻辑推理在开放世界图学习中的重要性。

Abstract: Graph learning has been crucial to many real-world tasks, but they are often
studied with a closed-world assumption, with all possible labels of data known
a priori. To enable effective graph learning in an open and noisy environment,
it is critical to inform the model users when the model makes a wrong
prediction to in-distribution data of a known class, i.e., misclassification
detection or when the model encounters out-of-distribution from novel classes,
i.e., out-of-distribution detection. This paper introduces Evidential Reasoning
Network (EVINET), a framework that addresses these two challenges by
integrating Beta embedding within a subjective logic framework. EVINET includes
two key modules: Dissonance Reasoning for misclassification detection and
Vacuity Reasoning for out-of-distribution detection. Extensive experiments
demonstrate that EVINET outperforms state-of-the-art methods across multiple
metrics in the tasks of in-distribution classification, misclassification
detection, and out-of-distribution detection. EVINET demonstrates the necessity
of uncertainty estimation and logical reasoning for misclassification detection
and out-of-distribution detection and paves the way for open-world graph
learning. Our code and data are available at https://github.com/SSSKJ/EviNET.

</details>


### [372] [Pre-trained Large Language Models Learn Hidden Markov Models In-context](https://arxiv.org/abs/2506.07298)
*Yijia Dai,Zhaolin Gao,Yahya Satter,Sarah Dean,Jennifer J. Sun*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）通过上下文学习（ICL）有效建模隐马尔可夫模型（HMMs）生成的数据，预测精度接近理论最优。


<details>
  <summary>Details</summary>
Motivation: 解决HMMs在真实数据中计算复杂度高的问题，探索LLMs在建模序列数据中的潜力。

Method: 利用预训练的LLMs通过ICL学习HMMs生成的序列数据，分析其在不同HMMs上的表现。

Result: LLMs在合成HMMs上表现优异，接近理论最优；在真实动物决策任务中与专家设计模型竞争。

Conclusion: ICL能有效学习HMMs序列，为复杂科学数据中隐藏结构的发现提供新工具。

Abstract: Hidden Markov Models (HMMs) are foundational tools for modeling sequential
data with latent Markovian structure, yet fitting them to real-world data
remains computationally challenging. In this work, we show that pre-trained
large language models (LLMs) can effectively model data generated by HMMs via
in-context learning (ICL)$\unicode{x2013}$their ability to infer patterns from
examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve
predictive accuracy approaching the theoretical optimum. We uncover novel
scaling trends influenced by HMM properties, and offer theoretical conjectures
for these empirical observations. We also provide practical guidelines for
scientists on using ICL as a diagnostic tool for complex data. On real-world
animal decision-making tasks, ICL achieves competitive performance with models
designed by human experts. To our knowledge, this is the first demonstration
that ICL can learn and predict HMM-generated sequences$\unicode{x2013}$an
advance that deepens our understanding of in-context learning in LLMs and
establishes its potential as a powerful tool for uncovering hidden structure in
complex scientific data.

</details>


### [373] [PASS: Private Attributes Protection with Stochastic Data Substitution](https://arxiv.org/abs/2506.07308)
*Yizhuo Chen,Chun-Fu,Chen,Hsiang Hsu,Shaohan Hu,Tarek Abdelzaher*

Main category: cs.LG

TL;DR: 论文提出了一种名为PASS的新方法，用于保护用户隐私数据，同时保持数据的实用性，克服了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习服务的普及，用户数据中可能包含与服务无关的隐私信息，现有方法因对抗训练策略存在严重漏洞。

Method: PASS通过概率性替换原始样本，并采用基于信息论目标的新损失函数进行训练。

Result: 在多种数据集上的综合评估验证了PASS的有效性和通用性。

Conclusion: PASS是一种高效且通用的隐私保护方法，优于现有技术。

Abstract: The growing Machine Learning (ML) services require extensive collections of
user data, which may inadvertently include people's private information
irrelevant to the services. Various studies have been proposed to protect
private attributes by removing them from the data while maintaining the
utilities of the data for downstream tasks. Nevertheless, as we theoretically
and empirically show in the paper, these methods reveal severe vulnerability
because of a common weakness rooted in their adversarial training based
strategies. To overcome this limitation, we propose a novel approach, PASS,
designed to stochastically substitute the original sample with another one
according to certain probabilities, which is trained with a novel loss function
soundly derived from information-theoretic objective defined for
utility-preserving private attributes protection. The comprehensive evaluation
of PASS on various datasets of different modalities, including facial images,
human activity sensory signals, and voice recording datasets, substantiates
PASS's effectiveness and generalizability.

</details>


### [374] [Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference](https://arxiv.org/abs/2506.07311)
*Thomas Joshi,Herman Saini,Neil Dhillon,Antoni Viros i Martin,Kaoutar El Maghraoui*

Main category: cs.LG

TL;DR: 论文提出了一种结合PagedAttention和PyTorch FlexAttention的方法，解决了LLMs在长上下文推理中的内存效率问题，显著降低了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存处理方式导致LLMs在长上下文推理中存在严重的内存效率问题，需要改进。

Method: 通过集成PagedAttention与PyTorch的FlexAttention，优化KV缓存分配，减少内部碎片化。

Result: 在NVIDIA L4 GPU上测试，推理延迟显著降低，且随序列长度线性增长，而非指数增长。

Conclusion: 该方法为未来长上下文模型部署提供了高效解决方案，并开源了完整实现。

Abstract: Large Language Models (LLMs) encounter severe memory inefficiencies during
long-context inference due to conventional handling of key-value (KV) caches.
In this work, we introduce a novel integration of PagedAttention with PyTorch's
FlexAttention, addressing internal fragmentation and inefficiencies associated
with monolithic KV cache allocations. Implemented within IBM's Foundation Model
Stack (FMS), our fused attention kernel efficiently gathers scattered KV data.
Our benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced
inference latency, growing only linearly (~2x) with sequence length from 128 to
2048 tokens when utilizing a global KV cache, compared to exponential latency
increases without caching. While peak memory usage remains largely unchanged
for single-step evaluations (dominated by model weights and activations), paged
attention causes minimal incremental memory usage, observable only at sequence
lengths exceeding 2048 tokens due to its power-of-two cache allocations. We
open-source the full implementation and discuss its implications for future
long-context model deployment.

</details>


### [375] [Generative Modeling of Networked Time-Series via Transformer Architectures](https://arxiv.org/abs/2506.07312)
*Yusuf Elnady*

Main category: cs.LG

TL;DR: 提出了一种高效的基于Transformer的生成模型，用于合成时间序列数据，以解决安全领域中数据不足的问题，并提升机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 安全领域的数据访问受限是一个常见问题，现有方法合成的数据未能显著提升模型性能。

Method: 设计了一种高效的Transformer生成框架，用于生成高质量的时间序列数据。

Result: 新模型在性能上达到了SOTA水平，并具有通用性和高质量样本生成能力。

Conclusion: 该模型为安全领域的机器学习工作流提供了有效的数据增强解决方案。

Abstract: Many security and network applications require having large datasets to train
the machine learning models. Limited data access is a well-known problem in the
security domain. Recent studies have shown the potential of Transformer models
to enlarge the size of data by synthesizing new samples, but the synthesized
samples don't improve the models over the real data. To address this issue, we
design an efficient transformer-based model as a generative framework to
generate time-series data, that can be used to boost the performance of
existing and new ML workflows. Our new transformer model achieves the SOTA
results. We style our model to be generalizable and work across different
datasets, and produce high-quality samples.

</details>


### [376] [DEF: Diffusion-augmented Ensemble Forecasting](https://arxiv.org/abs/2506.07324)
*David Millard,Arielle Carr,Stéphane Gaudreault,Ali Baheri*

Main category: cs.LG

TL;DR: DEF是一种通过扩散增强的集成预测方法，用于生成初始条件扰动，适用于机器学习天气预测领域。


<details>
  <summary>Details</summary>
Motivation: 现有初始条件扰动方法主要针对数值天气预报（NWP），限制了其在机器学习天气预测中的应用。

Method: 使用简单的条件扩散模型生成结构化扰动，可迭代应用并通过引导项控制扰动水平。

Result: 在ERA5再分析数据集上验证，DEF减少了长期预测误差并生成有意义的预测分布。

Conclusion: DEF能将确定性神经预测系统转化为随机系统，提升预测性能并生成合理的分布估计。

Abstract: We present DEF (\textbf{\ul{D}}iffusion-augmented \textbf{\ul{E}}nsemble
\textbf{\ul{F}}orecasting), a novel approach for generating initial condition
perturbations. Modern approaches to initial condition perturbations are
primarily designed for numerical weather prediction (NWP) solvers, limiting
their applicability in the rapidly growing field of machine learning for
weather prediction. Consequently, stochastic models in this domain are often
developed on a case-by-case basis. We demonstrate that a simple conditional
diffusion model can (1) generate meaningful structured perturbations, (2) be
applied iteratively, and (3) utilize a guidance term to intuitivey control the
level of perturbation. This method enables the transformation of any
deterministic neural forecasting system into a stochastic one. With our
stochastic extended systems, we show that the model accumulates less error over
long-term forecasts while producing meaningful forecast distributions. We
validate our approach on the 5.625$^\circ$ ERA5 reanalysis dataset, which
comprises atmospheric and surface variables over a discretized global grid,
spanning from the 1960s to the present. On this dataset, our method
demonstrates improved predictive performance along with reasonable spread
estimates.

</details>


### [377] [Mobility-Aware Asynchronous Federated Learning with Dynamic Sparsification](https://arxiv.org/abs/2506.07328)
*Jintao Yan,Tan Chen,Yuxuan Sun,Zhaojun Nan,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: 论文提出了一种移动感知的动态稀疏化（MADS）算法，用于优化异步联邦学习（AFL）中的梯度稀疏化，以应对设备移动性带来的连接问题和模型陈旧性。


<details>
  <summary>Details</summary>
Motivation: 设备移动性导致间歇性连接和模型陈旧性，影响AFL的收敛性，需要一种动态调整稀疏化程度的方法。

Method: 开发了理论模型分析稀疏化、模型陈旧性和移动性接触模式的相互作用，并提出了MADS算法，根据接触时间和模型陈旧性动态优化稀疏化程度。

Result: 在低速条件下增加稀疏化程度以提升收敛性，在高速条件下减少稀疏化程度以确保可靠上传。实验显示MADS在CIFAR-10数据集上分类准确率提高8.76%，在Argoverse数据集上平均位移误差降低9.46%。

Conclusion: MADS算法有效解决了移动性对AFL的影响，显著提升了模型性能。

Abstract: Asynchronous Federated Learning (AFL) enables distributed model training
across multiple mobile devices, allowing each device to independently update
its local model without waiting for others. However, device mobility introduces
intermittent connectivity, which necessitates gradient sparsification and leads
to model staleness, jointly affecting AFL convergence. This paper develops a
theoretical model to characterize the interplay among sparsification, model
staleness and mobility-induced contact patterns, and their joint impact on AFL
convergence. Based on the analysis, we propose a mobility-aware dynamic
sparsification (MADS) algorithm that optimizes the sparsification degree based
on contact time and model staleness. Closed-form solutions are derived, showing
that under low-speed conditions, MADS increases the sparsification degree to
enhance convergence, while under high-speed conditions, it reduces the
sparsification degree to guarantee reliable uploads within limited contact
time. Experimental results validate the theoretical findings. Compared with the
state-of-the-art benchmarks, the MADS algorithm increases the image
classification accuracy on the CIFAR-10 dataset by 8.76% and reduces the
average displacement error in the Argoverse trajectory prediction dataset by
9.46%.

</details>


### [378] [JavelinGuard: Low-Cost Transformer Architectures for LLM Security](https://arxiv.org/abs/2506.07330)
*Yash Datta,Sharath Rajasekar*

Main category: cs.LG

TL;DR: JavelinGuard是一套低成本、高性能的模型架构，用于检测大型语言模型（LLM）交互中的恶意意图，专为生产部署优化。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，恶意交互的检测需求日益增长，需要高效且低成本的解决方案。

Method: 探索了五种基于Transformer的架构，包括基础分类器、增强注意力加权池化、混合神经网络集成和高级多任务框架。

Result: 在九个对抗性数据集上测试，展示了优于开源护栏模型和大型解码器LLM的性能与成本平衡。

Conclusion: Raudra多任务设计表现最稳健，但不同架构在速度、可解释性和资源需求上有独特权衡，适用于不同场景。

Abstract: We present JavelinGuard, a suite of low-cost, high-performance model
architectures designed for detecting malicious intent in Large Language Model
(LLM) interactions, optimized specifically for production deployment. Recent
advances in transformer architectures, including compact BERT(Devlin et al.
2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build
highly accurate classifiers with as few as approximately 400M parameters that
achieve rapid inference speeds even on standard CPU hardware. We systematically
explore five progressively sophisticated transformer-based architectures:
Sharanga (baseline transformer classifier), Mahendra (enhanced
attention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid
neural ensemble architectures), and Raudra (an advanced multi-task framework
with specialized loss functions). Our models are rigorously benchmarked across
nine diverse adversarial datasets, including popular sets like the NotInject
series, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly
introduced JavelinBench, specifically crafted to test generalization on
challenging borderline and hard-negative cases. Additionally, we compare our
architectures against leading open-source guardrail models as well as large
decoder-only LLMs such as gpt-4o, demonstrating superior cost-performance
trade-offs in terms of accuracy, and latency. Our findings reveal that while
Raudra's multi-task design offers the most robust performance overall, each
architecture presents unique trade-offs in speed, interpretability, and
resource requirements, guiding practitioners in selecting the optimal balance
of complexity and efficiency for real-world LLM security applications.

</details>


### [379] [Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models](https://arxiv.org/abs/2506.07334)
*Haoyu Wang,Peihao Wang,Mufei Li,Shikun Liu,Siqi Miao,Zhangyang Wang,Pan Li*

Main category: cs.LG

TL;DR: Graph-KV通过利用KV-cache作为压缩表示，并通过结构归纳偏置控制其交互，克服了传统序列化输入对LLMs的限制，显著提升了在RAG和图形结构数据任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型（LLMs）的序列化输入方式无法有效利用结构依赖关系，尤其在需要处理图形结构数据的任务中表现不佳。Graph-KV旨在通过结构归纳偏置解决这一问题。

Method: Graph-KV利用KV-cache作为文本段的压缩表示，并通过选择性注意力机制（仅关注指定的源段）引入图形结构块掩码，同时优化位置编码以减少位置偏差。

Result: Graph-KV在RAG基准测试、学术论文QA任务和论文主题分类任务中均显著优于基线方法，尤其是在减少位置偏差和利用结构偏置方面表现突出。

Conclusion: Graph-KV通过结构化的注意力机制和优化的位置编码，显著提升了LLMs在图形结构数据任务中的性能，为相关领域提供了新的解决方案。

Abstract: Modern large language models (LLMs) are inherently auto-regressive, requiring
input to be serialized into flat sequences regardless of their structural
dependencies. This serialization hinders the model's ability to leverage
structural inductive biases, especially in tasks such as retrieval-augmented
generation (RAG) and reasoning on data with native graph structures, where
inter-segment dependencies are crucial. We introduce Graph-KV with the
potential to overcome this limitation. Graph-KV leverages the KV-cache of text
segments as condensed representations and governs their interaction through
structural inductive biases. In this framework, 'target' segments selectively
attend only to the KV-caches of their designated 'source' segments, rather than
all preceding segments in a serialized sequence. This approach induces a
graph-structured block mask, sparsifying attention and enabling a
message-passing-like step within the LLM. Furthermore, strategically allocated
positional encodings for source and target segments reduce positional bias and
context window consumption. We evaluate Graph-KV across three scenarios: (1)
seven RAG benchmarks spanning direct inference, multi-hop reasoning, and
long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with
full-text scientific papers structured as citation ego-graphs; and (3) paper
topic classification within a citation network. By effectively reducing
positional bias and harnessing structural inductive biases, Graph-KV
substantially outperforms baselines, including standard costly sequential
encoding, across various settings. Code and the Graph-KV data are publicly
available.

</details>


### [380] [SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments](https://arxiv.org/abs/2506.07355)
*Yuya Okada,Takayuki Nishio*

Main category: cs.LG

TL;DR: SALT是一种轻量级模型适应框架，用于在封闭约束下进行分割计算，通过客户端适配器优化特征，无需修改原始模型。


<details>
  <summary>Details</summary>
Motivation: 在封闭环境中，传统适应方法不可行，因为需要访问模型参数或架构。

Method: 引入紧凑、可训练的客户端适配器，优化头部网络的潜在特征。

Result: 在CIFAR-10和CIFAR-100上表现优于微调方法，训练延迟更低。

Conclusion: SALT为边缘AI系统提供了一种实用的个性化推理解决方案。

Abstract: We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model
adaptation framework for Split Computing under closed constraints, where the
head and tail networks are proprietary and inaccessible to users. In such
closed environments, conventional adaptation methods are infeasible since they
require access to model parameters or architectures. SALT addresses this
challenge by introducing a compact, trainable adapter on the client side to
refine latent features from the head network, enabling user-specific adaptation
without modifying the original models or increasing communication overhead. We
evaluate SALT on user-specific classification tasks with CIFAR-10 and
CIFAR-100, demonstrating improved accuracy with lower training latency compared
to fine-tuning methods. Furthermore, SALT facilitates model adaptation for
robust inference over lossy networks, a common challenge in edge-cloud
environments. With minimal deployment overhead, SALT offers a practical
solution for personalized inference in edge AI systems under strict system
constraints.

</details>


### [381] [MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing](https://arxiv.org/abs/2506.07366)
*Haiyue Ma,Zhixu Du,Yiran Chen*

Main category: cs.LG

TL;DR: MoE-GPS框架通过量化预测策略对系统性能的影响，优化多GPU MoE网络的负载均衡，提出仅预测总体token分布的策略，性能提升23%。


<details>
  <summary>Details</summary>
Motivation: 多GPU MoE网络中专家负载不均衡，现有方法需预测token分布，但预测策略存在性能与开销的权衡。

Method: 提出MoE-GPS框架，量化预测策略对系统性能的影响，推荐仅预测总体token分布的策略（Distribution-Only Prediction）。

Result: 在Mixtral 8x7B MMLU数据集上，Distribution-Only Prediction比传统方法性能提升23%。

Conclusion: MoE-GPS能有效指导预测策略选择，显著提升系统性能。

Abstract: In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across
different GPUs, which creates load imbalance as each expert processes different
number of tokens. Recent works improve MoE inference load balance by
dynamically duplicating popular experts to more GPUs to process excessive
tokens, which requires predicting the distribution before routing. In this
paper, we discuss the tradeoff of prediction strategies, accuracies, overhead,
and end-to-end system performance. We propose MoE-GPS, a framework that guides
the selection of the optimal predictor design under various system
configurations, by quantifying the performance impact to system-level model
runtime. Specifically, we advocate for Distribution-Only Prediction, a
prediction strategy that only predicts overall token distribution which
significantly reduces overhead compared to the traditional Token-to-Expert
Prediction. On Mixtral 8x7B MMLU dataset, MoE-GPS suggests Distribution-Only
Prediction which improves end-to-end inference performance by more than 23%
compared with Token-to-Expert Prediction.

</details>


### [382] [Moment Alignment: Unifying Gradient and Hessian Matching for Domain Generalization](https://arxiv.org/abs/2506.07378)
*Yuen Chen,Haozhe Si,Guojun Zhang,Han Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于矩对齐理论的域泛化方法（CMA），通过闭式对齐梯度和Hessian矩阵，解决了现有方法的计算效率问题，并在实验中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 域泛化（DG）旨在解决现实应用中分布偏移问题，现有方法计算效率低且理论不清晰，因此需要一种更高效且理论完备的方法。

Method: 基于转移测度的理论框架，提出闭式矩对齐（CMA）算法，通过闭式对齐梯度和Hessian矩阵，避免重复反向传播或采样估计。

Result: CMA在线性探测和全微调实验中均优于经验风险最小化和现有先进算法。

Conclusion: 矩对齐理论统一了多种DG方法，CMA算法在效率和性能上均有显著提升。

Abstract: Domain generalization (DG) seeks to develop models that generalize well to
unseen target domains, addressing the prevalent issue of distribution shifts in
real-world applications. One line of research in DG focuses on aligning
domain-level gradients and Hessians to enhance generalization. However,
existing methods are computationally inefficient and the underlying principles
of these approaches are not well understood. In this paper, we develop the
theory of moment alignment for DG. Grounded in \textit{transfer measure}, a
principled framework for quantifying generalizability between two domains, we
first extend the definition of transfer measure to domain generalization that
includes multiple source domains and establish a target error bound. Then, we
prove that aligning derivatives across domains improves transfer measure both
when the feature extractor induces an invariant optimal predictor across
domains and when it does not. Notably, moment alignment provides a unifying
understanding of Invariant Risk Minimization, gradient matching, and Hessian
matching, three previously disconnected approaches to DG. We further connect
feature moments and derivatives of the classifier head, and establish the
duality between feature learning and classifier fitting. Building upon our
theory, we introduce \textbf{C}losed-Form \textbf{M}oment \textbf{A}lignment
(CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in
closed-form. Our method overcomes the computational inefficiencies of existing
gradient and Hessian-based techniques by eliminating the need for repeated
backpropagation or sampling-based Hessian estimation. We validate the efficacy
of our approach through two sets of experiments: linear probing and full
fine-tuning. CMA demonstrates superior performance in both settings compared to
Empirical Risk Minimization and state-of-the-art algorithms.

</details>


### [383] [RiemannFormer: A Framework for Attention in Curved Spaces](https://arxiv.org/abs/2506.07405)
*Zhongping Ji*

Main category: cs.LG

TL;DR: 本文提出了一种基于几何视角的Transformer注意力机制解释，通过减少参数和增强局部性提升性能。


<details>
  <summary>Details</summary>
Motivation: 为Transformer的注意力机制提供几何解释，并解决其忽视局部归纳偏置的问题。

Method: 利用度量张量、切空间和内积等几何概念，通过并行传输连接离散位置的结构，并减少参数数量。

Result: 实验表明，该方法显著提升了性能。

Conclusion: 该方法为Transformer的几何解释和性能优化提供了新思路，未来将在视觉和大语言模型中进一步验证。

Abstract: This research endeavors to offer insights into unlocking the further
potential of transformer-based architectures. One of the primary motivations is
to offer a geometric interpretation for the attention mechanism in
transformers. In our framework, the attention mainly involves metric tensors,
tangent spaces, inner product, and how they relate to each other. These
quantities and structures at discrete positions are intricately interconnected
via the parallel transport of tangent vectors. To make the learning process
more efficient, we reduce the number of parameters through ingenious predefined
configurations. Moreover, we introduce an explicit mechanism to highlight a
neighborhood by attenuating the remote values, given that transformers
inherently neglect local inductive bias. Experimental results demonstrate that
our modules deliver significant performance improvements relative to the
baseline. More evaluation experiments on visual and large language models will
be launched successively.

</details>


### [384] [InverseScope: Scalable Activation Inversion for Interpreting Large Language Models](https://arxiv.org/abs/2506.07406)
*Yifan Luo,Zhennan Zhou,Bin Dong*

Main category: cs.LG

TL;DR: InverseScope是一种轻假设、可扩展的框架，通过输入反演解释神经网络激活，显著提高了样本效率，并支持对大型语言模型内部表示的系统定量分析。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLMs）的内部表示是解释性研究的核心挑战，现有方法依赖的强假设在实际中可能不成立。

Method: 提出InverseScope框架，通过定义生成相似激活的输入分布来分析编码特征，采用条件生成架构提高采样效率，并引入定量评估协议。

Result: InverseScope能够扩展到更大模型和实际任务，实现对LLMs内部表示的系统定量分析。

Conclusion: InverseScope为解释神经网络激活提供了一种高效且可扩展的方法，推动了LLMs解释性研究的进展。

Abstract: Understanding the internal representations of large language models (LLMs) is
a central challenge in interpretability research. Existing feature
interpretability methods often rely on strong assumptions about the structure
of representations that may not hold in practice. In this work, we introduce
InverseScope, an assumption-light and scalable framework for interpreting
neural activations via input inversion. Given a target activation, we define a
distribution over inputs that generate similar activations and analyze this
distribution to infer the encoded features. To address the inefficiency of
sampling in high-dimensional spaces, we propose a novel conditional generation
architecture that significantly improves sample efficiency compared to previous
methods. We further introduce a quantitative evaluation protocol that tests
interpretability hypotheses using feature consistency rate computed over the
sampled inputs. InverseScope scales inversion-based interpretability methods to
larger models and practical tasks, enabling systematic and quantitative
analysis of internal representations in real-world LLMs.

</details>


### [385] [Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM](https://arxiv.org/abs/2506.07407)
*Yihong Jin,Ze Yang,Juntian Liu,Xinhe Xu*

Main category: cs.LG

TL;DR: 提出了一种基于大语言模型（LLM）的多云环境智能监控系统异常检测与预警机制，通过结合LLM的自然语言处理能力和传统机器学习方法，提升了检测精度和实时响应效率。


<details>
  <summary>Details</summary>
Motivation: 随着多云环境的快速发展，确保智能监控系统的安全性和可靠性变得愈发重要。

Method: 在现有监控框架基础上，创新性地引入多级特征提取方法，结合LLM的自然语言处理能力和传统机器学习方法，动态适应不同云服务提供商和环境。

Result: 实验结果表明，该模型在检测精度和延迟方面显著优于传统异常检测系统，并显著提升了云基础设施的韧性和主动管理能力。

Conclusion: 该研究为多云环境下的智能监控系统提供了一种高效、可靠的异常检测与预警解决方案。

Abstract: With the rapid development of multi-cloud environments, it is increasingly
important to ensure the security and reliability of intelligent monitoring
systems. In this paper, we propose an anomaly detection and early warning
mechanism for intelligent monitoring system in multi-cloud environment based on
Large-Scale Language Model (LLM). On the basis of the existing monitoring
framework, the proposed model innovatively introduces a multi-level feature
extraction method, which combines the natural language processing ability of
LLM with traditional machine learning methods to enhance the accuracy of
anomaly detection and improve the real-time response efficiency. By introducing
the contextual understanding capabilities of LLMs, the model dynamically adapts
to different cloud service providers and environments, so as to more
effectively detect abnormal patterns and predict potential failures.
Experimental results show that the proposed model is significantly better than
the traditional anomaly detection system in terms of detection accuracy and
latency, and significantly improves the resilience and active management
ability of cloud infrastructure.

</details>


### [386] [Fractional-order Jacobian Matrix Differentiation and Its Application in Artificial Neural Networks](https://arxiv.org/abs/2506.07408)
*Xiaojun zhou,Chunna Zhao,Yaqun Huang,Chengli Zhou,Junjie Ye,Kemeng Xiang*

Main category: cs.LG

TL;DR: 提出了一种基于分数阶雅可比矩阵的自动微分技术，用于深度学习中的分数阶梯度下降，实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 分数阶微分在神经网络优化中有潜力，但缺乏与自动微分技术兼容的分数阶矩阵微分方法。

Method: 通过整数阶雅可比矩阵定义引入分数阶雅可比矩阵，设计分数阶自动微分技术，并在多层感知机中应用。

Result: 实验表明，该方法在训练集和验证集上表现优异，测试集指标和时间、内存消耗均验证其高效性。

Conclusion: 分数阶雅可比矩阵微分是一种优秀的深度学习分数阶梯度下降方法。

Abstract: Fractional-order differentiation has many characteristics different from
integer-order differentiation. These characteristics can be applied to the
optimization algorithms of artificial neural networks to obtain better results.
However, due to insufficient theoretical research, at present, there is no
fractional-order matrix differentiation method that is perfectly compatible
with automatic differentiation (Autograd) technology. Therefore, we propose a
fractional-order matrix differentiation calculation method. This method is
introduced by the definition of the integer-order Jacobian matrix. We denote it
as fractional-order Jacobian matrix differentiation (${{\bf{J}}^\alpha }$).
Through ${{\bf{J}}^\alpha }$, we can carry out the matrix-based
fractional-order chain rule. Based on the Linear module and the
fractional-order differentiation, we design the fractional-order Autograd
technology to enable the use of fractional-order differentiation in hidden
layers, thereby enhancing the practicality of fractional-order differentiation
in deep learning. In the experiment, according to the PyTorch framework, we
design fractional-order Linear (FLinear) and replace nn.Linear in the
multilayer perceptron with FLinear. Through the qualitative analysis of the
training set and validation set $Loss$, the quantitative analysis of the test
set indicators, and the analysis of time consumption and GPU memory usage
during model training, we verify the superior performance of ${{\bf{J}}^\alpha
}$ and prove that it is an excellent fractional-order gradient descent method
in the field of deep learning.

</details>


### [387] [Variational Supervised Contrastive Learning](https://arxiv.org/abs/2506.07413)
*Ziwen Wang,Jiajun Fan,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: VarCon提出了一种变分监督对比学习方法，通过变分推断优化嵌入空间，显著提升了对比学习的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习存在嵌入分布缺乏明确调控和过度依赖大批量负样本的问题，导致语义相关实例被错误分离且泛化能力受限。

Method: VarCon将监督对比学习重新表述为对潜在类别变量的变分推断，最大化后验加权的证据下界（ELBO），实现高效的类感知匹配和细粒度控制。

Result: VarCon在多个数据集上达到最先进性能（如ImageNet-1K上79.36% Top-1准确率），并表现出更清晰的决策边界和语义组织。

Conclusion: VarCon通过变分推断显著提升了对比学习的效率和泛化能力，适用于少样本学习和多种增强策略。

Abstract: Contrastive learning has proven to be highly efficient and adaptable in
shaping representation spaces across diverse modalities by pulling similar
samples together and pushing dissimilar ones apart. However, two key
limitations persist: (1) Without explicit regulation of the embedding
distribution, semantically related instances can inadvertently be pushed apart
unless complementary signals guide pair selection, and (2) excessive reliance
on large in-batch negatives and tailored augmentations hinders generalization.
To address these limitations, we propose Variational Supervised Contrastive
Learning (VarCon), which reformulates supervised contrastive learning as
variational inference over latent class variables and maximizes a
posterior-weighted evidence lower bound (ELBO) that replaces exhaustive
pair-wise comparisons for efficient class-aware matching and grants
fine-grained control over intra-class dispersion in the embedding space.
Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,
ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art
performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy
on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while
converging in just 200 epochs; (2) yields substantially clearer decision
boundaries and semantic organization in the embedding space, as evidenced by
KNN classification, hierarchical clustering results, and transfer-learning
assessments; and (3) demonstrates superior performance in few-shot learning
than supervised baseline and superior robustness across various augmentation
strategies.

</details>


### [388] [LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments](https://arxiv.org/abs/2506.07416)
*Jin Huang,Yuchao Jin,Le An,Josh Park*

Main category: cs.LG

TL;DR: 提出了一种针对嵌入式设备优化的高效视觉语言模型（VLM）流水线，显著降低了计算开销，并在自动驾驶应用中实现了2.5倍的端到端延迟减少。


<details>
  <summary>Details</summary>
Motivation: 为资源受限的环境（如机器人和自动驾驶设备）提供实时视觉语言模型部署的解决方案。

Method: 通过联合利用补丁选择、令牌选择模块和推测解码来减少计算开销。

Result: 在NVIDIA DRIVE Thor平台上，流水线实现了2.5倍的延迟减少，应用FP8量化后提升至3.2倍。

Conclusion: 该流水线是资源受限环境中实时VLM部署的可行解决方案。

Abstract: This paper introduces an efficient Vision-Language Model (VLM) pipeline
specifically optimized for deployment on embedded devices, such as those used
in robotics and autonomous driving. The pipeline significantly reduces the
computational overhead by jointly leveraging patch selection to filter
irrelevant camera views, a token selection module to reduce input sequence
length for the LLM, and speculative decoding to accelerate token generation.
Evaluation on the NVIDIA DRIVE Thor platform for automonous driving
application, our pipeline achieves $2.5\times$ end-to-end latency reduction
without compromising task accuracy. The speed-up further increases to
$3.2\times$ when applying FP8 post-training quantization. These results
demonstrate our pipeline as a viable solution for enabling real-time VLM
deployment in resource-constrained environments.

</details>


### [389] [Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs](https://arxiv.org/abs/2506.07417)
*Nan Sun,Xixun Lin,Zhiheng Zhou,Yanmin Shang,Zhenlin Cheng,Yanan Cao*

Main category: cs.LG

TL;DR: 论文提出了一种基于证据深度学习的动态图OOD检测方法EviSEC，通过不确定性估计和谱感知对比学习解决现有方法的偏差和分数同质化问题。


<details>
  <summary>Details</summary>
Motivation: 动态图中的OOD检测在安全敏感领域备受关注，但现有方法因单点估计和缺乏OOD训练数据导致高偏差和分数同质化问题。

Method: 提出EviSEC，结合证据神经网络和谱感知对比学习模块，通过后验Dirichlet分布解释输入随机性，并生成OOD近似样本以扩大ID与OOD分数差距。

Result: 在真实数据集上的实验表明，EviSEC能有效检测动态图中的OOD样本。

Conclusion: EviSEC通过不确定性估计和谱感知对比学习，显著提升了动态图OOD检测的性能。

Abstract: Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims
to identify whether incoming data deviates from the distribution of the
in-distribution (ID) training set, has garnered considerable attention in
security-sensitive fields. Current OOD detection paradigms primarily focus on
static graphs and confront two critical challenges: i) high bias and high
variance caused by single-point estimation, which makes the predictions
sensitive to randomness in the data; ii) score homogenization resulting from
the lack of OOD training data, where the model only learns ID-specific
patterns, resulting in overall low OOD scores and a narrow score gap between ID
and OOD data. To tackle these issues, we first investigate OOD detection in
dynamic graphs through the lens of Evidential Deep Learning (EDL).
Specifically, we propose EviSEC, an innovative and effective OOD detector via
Evidential Spectrum-awarE Contrastive Learning. We design an evidential neural
network to redefine the output as the posterior Dirichlet distribution,
explaining the randomness of inputs through the uncertainty of distribution,
which is overlooked by single-point estimation. Moreover, spectrum-aware
augmentation module generates OOD approximations to identify patterns with high
OOD scores, thereby widening the score gap between ID and OOD data and
mitigating score homogenization. Extensive experiments on real-world datasets
demonstrate that EviSAC effectively detects OOD samples in dynamic graphs.

</details>


### [390] [Federated In-Context Learning: Iterative Refinement for Improved Answer Quality](https://arxiv.org/abs/2506.07440)
*Ruhan Wang,Zhiyong Wang,Chengkai Huang,Rui Wang,Tong Yu,Lina Yao,John C. S. Lui,Dongruo Zhou*

Main category: cs.LG

TL;DR: Fed-ICL框架通过多轮客户端与服务器交互提升问答任务中的上下文学习效果，无需传输模型参数，降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习方法依赖高质量示例，但数据隐私、标注成本和分布差异导致示例稀缺，而现有解决方案通信开销大或未能充分利用本地数据。

Method: 提出Fed-ICL框架，通过客户端与服务器的多轮交互迭代优化回答，避免模型参数传输。

Result: 理论证明Fed-ICL收敛性，实验表明其在标准问答基准上表现优异且通信成本低。

Conclusion: Fed-ICL为上下文学习提供了一种高效、低通信开销的解决方案。

Abstract: For question-answering (QA) tasks, in-context learning (ICL) enables language
models to generate responses without modifying their parameters by leveraging
examples provided in the input. However, the effectiveness of ICL heavily
depends on the availability of high-quality examples, which are often scarce
due to data privacy constraints, annotation costs, and distribution
disparities. A natural solution is to utilize examples stored on client
devices, but existing approaches either require transmitting model parameters -
incurring significant communication overhead - or fail to fully exploit local
datasets, limiting their effectiveness. To address these challenges, we propose
Federated In-Context Learning (Fed-ICL), a general framework that enhances ICL
through an iterative, collaborative process. Fed-ICL progressively refines
responses by leveraging multi-round interactions between clients and a central
server, improving answer quality without the need to transmit model parameters.
We establish theoretical guarantees for the convergence of Fed-ICL and conduct
extensive experiments on standard QA benchmarks, demonstrating that our
proposed approach achieves strong performance while maintaining low
communication costs.

</details>


### [391] [Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs](https://arxiv.org/abs/2506.07448)
*T. Duy Nguyen-Hien,Desi R. Ivanova,Yee Whye Teh,Wee Sun Lee*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯实验建模的框架，用于主动管理和解决LLM部署中的不确定性，而非被动拒绝高不确定性输出。


<details>
  <summary>Details</summary>
Motivation: 当前LLM部署中缺乏系统区分和应对不同不确定性来源的工具，导致保守策略（如拒绝高不确定性输出）的局限性。

Method: 采用贝叶斯实验建模框架，为不确定性提供一致性推理基础，并支持主动解决措施（如请求澄清、检索外部信息）。

Result: 该框架使LLM及其用户能够采取情境适当的步骤，提升系统的可靠性、透明性和适用性。

Conclusion: 贝叶斯实验建模为LLM部署中的不确定性管理提供了新思路，尤其适用于高风险现实场景。

Abstract: Although large language models (LLMs) are highly interactive and extendable,
current approaches to ensure reliability in deployments remain mostly limited
to rejecting outputs with high uncertainty in order to avoid misinformation.
This conservative strategy reflects the current lack of tools to systematically
distinguish and respond to different sources of uncertainty. In this paper, we
advocate for the adoption of Bayesian Modeling of Experiments -- a framework
that provides a coherent foundation to reason about uncertainty and clarify the
reducibility of uncertainty -- for managing and proactively addressing
uncertainty that arises in LLM deployments. This framework enables LLMs and
their users to take contextually appropriate steps, such as requesting
clarification, retrieving external information, or refining inputs. By
supporting active resolution rather than passive avoidance, it opens the door
to more reliable, transparent, and broadly applicable LLM systems, particularly
in high-stakes, real-world settings.

</details>


### [392] [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)
*Yuxin Xiao,Sana Tonekaboni,Walter Gerych,Vinith Suriyakumar,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 研究发现，特定风格提示会显著增加大型语言模型（LLM）的越狱攻击成功率，且风格长度和模型对其的关注度相关。提出了一种防御策略SafeStyle，能有效提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 探讨风格提示对LLM安全性的潜在影响，以及如何通过防御策略缓解风险。

Method: 评估32个LLM在七个越狱基准测试中的表现，分析风格提示对攻击成功率的影响，并提出SafeStyle防御策略。

Result: 风格提示显著提高了越狱攻击成功率，且与风格长度和模型关注度相关。SafeStyle在多种设置下优于基线方法。

Conclusion: 风格提示会威胁LLM安全性，但通过SafeStyle等防御策略可以有效缓解风险。

Abstract: Large language models (LLMs) can be prompted with specific styles (e.g.,
formatting responses as lists), including in jailbreak queries. Although these
style patterns are semantically unrelated to the malicious intents behind
jailbreak queries, their safety impact remains unclear. In this work, we seek
to understand whether style patterns compromise LLM safety, how superficial
style alignment increases model vulnerability, and how best to mitigate these
risks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,
and find that malicious queries with style patterns inflate the attack success
rate (ASR) for nearly all models. Notably, ASR inflation correlates with both
the length of style patterns and the relative attention an LLM exhibits on
them. We then investigate superficial style alignment, and find that
fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of
those same styles. Finally, we propose SafeStyle, a defense strategy that
incorporates a small amount of safety training data augmented to match the
distribution of style patterns in the fine-tuning data. Across three LLMs and
five fine-tuning style settings, SafeStyle consistently outperforms baselines
in maintaining LLM safety.

</details>


### [393] [ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning](https://arxiv.org/abs/2506.07459)
*Ziwen Wang,Jiajun Fan,Ruihan Guo,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: ProteinZero通过在线强化学习框架提升蛋白质生成模型的性能，显著降低了设计失败率并提高了多样性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质生成模型因高质量数据集稀缺而成功率受限，需要一种可扩展的自动化改进方法。

Method: 提出ProteinZero框架，结合在线强化学习、高效代理奖励模型（ESM-fold和快速ddG预测器）及多样性正则化。

Result: ProteinZero在结构准确性、可设计性、热力学稳定性和序列多样性上全面超越现有方法，失败率降低36%-48%。

Conclusion: ProteinZero为蛋白质设计提供了持续自我改进的新范式，扩展了设计空间探索的可能性。

Abstract: Protein generative models have shown remarkable promise in protein design but
still face limitations in success rate, due to the scarcity of high-quality
protein datasets for supervised pretraining. We present ProteinZero, a novel
framework that enables scalable, automated, and continuous self-improvement of
the inverse folding model through online reinforcement learning. To achieve
computationally tractable online feedback, we introduce efficient proxy reward
models based on ESM-fold and a novel rapid ddG predictor that significantly
accelerates evaluation speed. ProteinZero employs a general RL framework
balancing multi-reward maximization, KL-divergence from a reference model, and
a novel protein-embedding level diversity regularization that prevents mode
collapse while promoting higher sequence diversity. Through extensive
experiments, we demonstrate that ProteinZero substantially outperforms existing
methods across every key metric in protein design, achieving significant
improvements in structural accuracy, designability, thermodynamic stability,
and sequence diversity. Most impressively, ProteinZero reduces design failure
rates by approximately 36% - 48% compared to widely-used methods like
ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates
exceeding 90% across diverse and complex protein folds. Notably, the entire RL
run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days,
including reward computation. Our work establishes a new paradigm for protein
design where models evolve continuously from their own generated outputs,
opening new possibilities for exploring the vast protein design space.

</details>


### [394] [Circumventing Backdoor Space via Weight Symmetry](https://arxiv.org/abs/2506.07467)
*Jie Peng,Hongwei Yang,Jing Zhao,Hengji Dong,Hui He,Weizhe Zhang,Haoyu He*

Main category: cs.LG

TL;DR: 论文提出了一种名为TSC的新型后门净化防御方法，适用于多种学习范式，仅需少量干净样本即可有效防御后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法通常需要标记数据或特定训练流程，限制了其在监督学习以外的应用。后门攻击已扩展到多种学习范式，亟需通用解决方案。

Method: 提出Two-stage Symmetry Connectivity (TSC)，利用神经网络的排列不变性和二次模式连接性，放大中毒样本的损失，同时保持干净样本的准确性。

Result: 实验表明，TSC在监督学习中表现优异，并能推广到自监督学习框架（如SimCLR和CLIP），保持强大的防御能力。

Conclusion: TSC是一种高效且通用的后门净化防御方法，适用于多种学习场景。

Abstract: Deep neural networks are vulnerable to backdoor attacks, where malicious
behaviors are implanted during training. While existing defenses can
effectively purify compromised models, they typically require labeled data or
specific training procedures, making them difficult to apply beyond supervised
learning settings. Notably, recent studies have shown successful backdoor
attacks across various learning paradigms, highlighting a critical security
concern. To address this gap, we propose Two-stage Symmetry Connectivity (TSC),
a novel backdoor purification defense that operates independently of data
format and requires only a small fraction of clean samples. Through theoretical
analysis, we prove that by leveraging permutation invariance in neural networks
and quadratic mode connectivity, TSC amplifies the loss on poisoned samples
while maintaining bounded clean accuracy. Experiments demonstrate that TSC
achieves robust performance comparable to state-of-the-art methods in
supervised learning scenarios. Furthermore, TSC generalizes to self-supervised
learning frameworks, such as SimCLR and CLIP, maintaining its strong defense
capabilities. Our code is available at https://github.com/JiePeng104/TSC.

</details>


### [395] [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
*Mickel Liu,Liwei Jiang,Yancheng Liang,Simon Shaolei Du,Yejin Choi,Tim Althoff,Natasha Jaques*

Main category: cs.LG

TL;DR: 论文提出了一种名为Self-RedTeam的在线自博弈强化学习算法，通过攻击者和防御者的协同进化动态提升语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型安全对齐方法存在滞后性，攻击者会针对静态防御过拟合，而防御者则无法及时应对新威胁。

Method: 将安全对齐建模为零和博弈，攻击者和防御者角色交替，通过奖励模型裁决结果，实现动态协同适应。

Result: Self-RedTeam在多样性攻击（+21.8% SBERT）和安全性基准（如WildJailBreak +65.5%）上表现优于静态方法。

Conclusion: 研究提倡从被动修补转向主动协同进化，通过多智能体强化学习实现语言模型的可扩展、自主和鲁棒的自我提升。

Abstract: Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).

</details>


### [396] [Premise Selection for a Lean Hammer](https://arxiv.org/abs/2506.07477)
*Thomas Zhu,Joshua Clune,Jeremy Avigad,Albert Qiaochu Jiang,Sean Welleck*

Main category: cs.LG

TL;DR: LeanHammer是第一个为Lean证明助手设计的端到端通用工具，结合神经前提选择和符号推理，显著提升了自动化证明能力。


<details>
  <summary>Details</summary>
Motivation: 尽管神经方法在自动推理中取得进展，但将其整合到实际验证工作流中仍具挑战性。Lean作为流行的证明助手，缺乏类似工具。

Method: 开发了基于神经前提选择系统的LeanHammer，动态适应用户上下文，并与符号证明搜索和重构结合。

Result: 评估显示，LeanHammer比现有前提选择器多解决21%的目标，并能泛化到多样领域。

Conclusion: LeanHammer填补了神经检索与符号推理间的鸿沟，使形式验证更易为研究者和实践者所用。

Abstract: Neural methods are transforming automated reasoning for proof assistants, yet
integrating these advances into practical verification workflows remains
challenging. Hammers are tools that interface with external automatic theorem
provers to automate tedious reasoning steps. They have dramatically improved
productivity in proof assistants, but the Lean proof assistant still does not
have a hammer despite its growing popularity. We present LeanHammer, the first
end-to-end domain-general hammer for Lean, built on a novel neural premise
selection system for a hammer in dependent type theory. Unlike existing Lean
premise selectors, our approach dynamically adapts to user-specific contexts
and combines with symbolic proof search and reconstruction to create a
practical hammer. With comprehensive evaluations, we show that our premise
selector enables LeanHammer to solve 21\% more goals relative to existing
premise selectors, and generalize well to diverse domains. Our work bridges the
gap between neural retrieval and symbolic reasoning, making formal verification
more accessible to researchers and practitioners.

</details>


### [397] [Explicit Preference Optimization: No Need for an Implicit Reward Model](https://arxiv.org/abs/2506.07492)
*Xiangkun Hu,Lemin Kong,Tong He,David Wipf*

Main category: cs.LG

TL;DR: 论文提出了一种新的偏好优化框架EXPO，解决了直接偏好优化（DPO）方法中的次优正则化和反直觉插值问题。


<details>
  <summary>Details</summary>
Motivation: RLHF依赖复杂的训练序列，而DPO虽简化了流程，但仍存在正则化和插值问题，需要改进。

Method: 引入EXPO框架，通过显式正则化因子避免DPO的潜在缺陷，无需隐式奖励重参数化。

Result: 实验证明EXPO有效解决了DPO的问题，并满足正则化需求。

Conclusion: EXPO提供了一种更透明、更优的偏好优化方法，优于现有DPO变体。

Abstract: The generated responses of large language models (LLMs) are often fine-tuned
to human preferences through a process called reinforcement learning from human
feedback (RLHF). As RLHF relies on a challenging training sequence, whereby a
separate reward model is independently learned and then later applied to LLM
policy updates, ongoing research effort has targeted more straightforward
alternatives. In this regard, direct preference optimization (DPO) and its many
offshoots circumvent the need for a separate reward training step. Instead,
through the judicious use of a reparameterization trick that induces an
\textit{implicit} reward, DPO and related methods consolidate learning to the
minimization of a single loss function. And yet despite demonstrable success in
some real-world settings, we prove that DPO-based objectives are nonetheless
subject to sub-optimal regularization and counter-intuitive interpolation
behaviors, underappreciated artifacts of the reparameterizations upon which
they are based. To this end, we introduce an \textit{explicit} preference
optimization framework termed EXPO that requires no analogous
reparameterization to achieve an implicit reward. Quite differently, we merely
posit intuitively-appealing regularization factors from scratch that
transparently avoid the potential pitfalls of key DPO variants, provably
satisfying regularization desiderata that prior methods do not. Empirical
results serve to corroborate our analyses and showcase the efficacy of EXPO.

</details>


### [398] [Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks](https://arxiv.org/abs/2506.07500)
*Shakir Yousefi,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 论文提出了一种通过注入Gumbel噪声和使用直通估计器来加速逻辑门网络（LGNs）训练的方法，显著减少了离散化差距和未使用门数量。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在计算和能耗方面的高需求促使研究者寻求更高效的解决方案，但LGNs的训练时间长且存在离散化差距，影响实际部署。

Method: 在训练过程中注入Gumbel噪声并使用直通估计器，通过隐式Hessian正则化改善收敛性。

Result: 训练速度提升4.5倍，离散化差距减少98%，未使用门数量减少100%。

Conclusion: 该方法显著提升了LGNs的训练效率和实际部署性能。

Abstract: Modern neural networks demonstrate state-of-the-art performance on numerous
existing benchmarks; however, their high computational requirements and energy
consumption prompt researchers to seek more efficient solutions for real-world
deployment. Logic gate networks (LGNs) learns a large network of logic gates
for efficient image classification. However, learning a network that can solve
a simple problem like CIFAR-10 can take days to weeks to train. Even then,
almost half of the network remains unused, causing a discretization gap. This
discretization gap hinders real-world deployment of LGNs, as the performance
drop between training and inference negatively impacts accuracy. We inject
Gumbel noise with a straight-through estimator during training to significantly
speed up training, improve neuron utilization, and decrease the discretization
gap. We theoretically show that this results from implicit Hessian
regularization, which improves the convergence properties of LGNs. We train
networks $4.5 \times$ faster in wall-clock time, reduce the discretization gap
by $98\%$, and reduce the number of unused gates by $100\%$.

</details>


### [399] [Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning](https://arxiv.org/abs/2506.07501)
*Libo Wang*

Main category: cs.LG

TL;DR: 论文提出了一种图因果演化（GoCE）方法，解决了链式模型（CoM）中子链仅依赖前一个子链信息而可能丢失长距离依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 解决CoM中因因果掩码导致全局上下文流动受阻的问题，提升长距离因果依赖的捕捉能力。

Method: 通过将隐式令牌表示映射为可微分稀疏因果邻接矩阵，结合因果掩码注意力和因果-MoE，实现因果结构学习与自适应更新的动态平衡。

Result: 实验证明GoCE在多个公开数据集上优于基线LLMs，提升了长距离因果依赖捕捉能力和自演化能力。

Conclusion: GoCE不仅超越了CoM的设计原则，还为因果学习和持续自适应改进提供了经验。

Abstract: In view of the problem that each subchain in the chain-of-model (CoM) relies
only on the information of the previous subchain and may lose long-range
dependencies due to the causal mask blocking the global context flow between
multi-level subchains, this work proposes a graph of causal evolution (GoCE).
Its core principle is to map the implicit token representation into a
differentiable and sparse causal adjacency matrix, then permeate causal
constraints through each layer of calculation using causal-masked attention and
causal-MoE. By combining intervention consistency loss test and self-evolution
gate, the dynamic balance between causal structure learning and adaptive
updating of transformer architecture is realized. The researcher built
experimental environments in sandboxes built with Claude Sonnet 4,
o4-mini-high, and DeepSeek R1 respectively with the transformer variant
architecture introduced in GoCE. It is evaluated on publicly available datasets
including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the
baseline LLMs. The finding proves that GoCE strengthens the transformer's
ability to capture long-range causal dependencies, while the ability to
self-evolve is improved. It not only surpasses the design of CoM in terms of
design principles, but also provides experience for future research on causal
learning and continuous adaptive improvement.

</details>


### [400] [Reinforcement Learning via Implicit Imitation Guidance](https://arxiv.org/abs/2506.07505)
*Perry Dong,Alec M. Lessing,Annie S. Chen,Chelsea Finn*

Main category: cs.LG

TL;DR: 论文提出了一种名为DGN的方法，利用先验数据引导探索而非直接模仿学习，显著提升了样本效率。


<details>
  <summary>Details</summary>
Motivation: 研究如何在强化学习中高效利用先验数据（如演示），避免模仿学习目标对长期性能的负面影响。

Method: 通过向策略添加噪声，利用先验数据引导探索，而非直接约束行为克隆。

Result: 在七个模拟连续控制任务中，性能比现有离线数据强化学习方法提升2-3倍。

Conclusion: DGN框架通过引导探索而非模仿学习，有效提升了强化学习的样本效率。

Abstract: We study the problem of sample efficient reinforcement learning, where prior
data such as demonstrations are provided for initialization in lieu of a dense
reward signal. A natural approach is to incorporate an imitation learning
objective, either as regularization during training or to acquire a reference
policy. However, imitation learning objectives can ultimately degrade long-term
performance, as it does not directly align with reward maximization. In this
work, we propose to use prior data solely for guiding exploration via noise
added to the policy, sidestepping the need for explicit behavior cloning
constraints. The key insight in our framework, Data-Guided Noise (DGN), is that
demonstrations are most useful for identifying which actions should be
explored, rather than forcing the policy to take certain actions. Our approach
achieves up to 2-3x improvement over prior reinforcement learning from offline
data methods across seven simulated continuous control tasks.

</details>


### [401] [Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems](https://arxiv.org/abs/2506.07517)
*Shuqiang Zhang,Yuchao Zhang,Jinkun Chen,Haochen Sui*

Main category: cs.LG

TL;DR: 论文提出了一种基于似然最大化的学习算法，解决推荐系统中因外生变量独立性假设导致的偏差问题，通过建模潜在外生变量和数据生成过程，提高了推荐的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统因用户仅与偏好项目交互的选择偏差导致偏好表示失真，现有方法假设外生变量独立，限制了偏差纠正效果。

Method: 提出基于似然最大化的学习算法，建模潜在外生变量，采用蒙特卡洛算法数值估计似然函数。

Result: 在合成数据集和三个真实数据集上的实验验证了方法的有效性。

Conclusion: 释放外生变量独立性假设，提出统一方法处理潜在变量，显著提升推荐系统的偏差纠正能力。

Abstract: Recommendation systems (RS) aim to provide personalized content, but they
face a challenge in unbiased learning due to selection bias, where users only
interact with items they prefer. This bias leads to a distorted representation
of user preferences, which hinders the accuracy and fairness of
recommendations. To address the issue, various methods such as error imputation
based, inverse propensity scoring, and doubly robust techniques have been
developed. Despite the progress, from the structural causal model perspective,
previous debiasing methods in RS assume the independence of the exogenous
variables. In this paper, we release this assumption and propose a learning
algorithm based on likelihood maximization to learn a prediction model. We
first discuss the correlation and difference between unmeasured confounding and
our scenario, then we propose a unified method that effectively handles latent
exogenous variables. Specifically, our method models the data generation
process with latent exogenous variables under mild normality assumptions. We
then develop a Monte Carlo algorithm to numerically estimate the likelihood
function. Extensive experiments on synthetic datasets and three real-world
datasets demonstrate the effectiveness of our proposed method. The code is at
https://github.com/WallaceSUI/kdd25-background-variable.

</details>


### [402] [Flowing Datasets with Wasserstein over Wasserstein Gradient Flows](https://arxiv.org/abs/2506.07534)
*Clément Bonet,Christophe Vauthier,Anna Korba*

Main category: cs.LG

TL;DR: 论文提出了一种基于最优传输的Wasserstein over Wasserstein (WoW)距离，用于处理概率分布上的梯度流，并应用于迁移学习和数据集蒸馏任务。


<details>
  <summary>Details</summary>
Motivation: 处理机器学习中表示为概率分布的数据，需要设计在无限维对象上的梯度流技术。

Method: 将每个类表示为特征的条件分布，数据集建模为这些类的混合分布，并引入WoW距离和梯度流。

Result: 提出了WoW梯度流框架，并在迁移学习和数据集蒸馏任务中验证了其有效性。

Conclusion: WoW梯度流为处理概率分布数据提供了新的工具，并在实际应用中展示了潜力。

Abstract: Many applications in machine learning involve data represented as probability
distributions. The emergence of such data requires radically novel techniques
to design tractable gradient flows on probability distributions over this type
of (infinite-dimensional) objects. For instance, being able to flow labeled
datasets is a core task for applications ranging from domain adaptation to
transfer learning or dataset distillation. In this setting, we propose to
represent each class by the associated conditional distribution of features,
and to model the dataset as a mixture distribution supported on these classes
(which are themselves probability distributions), meaning that labeled datasets
can be seen as probability distributions over probability distributions. We
endow this space with a metric structure from optimal transport, namely the
Wasserstein over Wasserstein (WoW) distance, derive a differential structure on
this space, and define WoW gradient flows. The latter enables to design
dynamics over this space that decrease a given objective functional. We apply
our framework to transfer learning and dataset distillation tasks, leveraging
our gradient flow construction as well as novel tractable functionals that take
the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels
between probability distributions.

</details>


### [403] [Improving Memory Efficiency for Training KANs via Meta Learning](https://arxiv.org/abs/2506.07549)
*Zhangchi Zhao,Jun Shu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: MetaKANs通过元学习生成KANs的权重，减少可训练参数，提升内存效率，同时保持性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: KANs虽然高效且可解释，但参数过多导致内存和训练成本高，需改进。

Method: 提出MetaKANs，用小型元学习器生成KANs权重，端到端训练。

Result: 在符号回归、偏微分方程求解和图像分类等任务中表现优异，显著减少参数。

Conclusion: MetaKANs为KANs提供了一种高效、可扩展的训练方法，缩小了与MLPs的成本差距。

Abstract: Inspired by the Kolmogorov-Arnold representation theorem, KANs offer a novel
framework for function approximation by replacing traditional neural network
weights with learnable univariate functions. This design demonstrates
significant potential as an efficient and interpretable alternative to
traditional MLPs. However, KANs are characterized by a substantially larger
number of trainable parameters, leading to challenges in memory efficiency and
higher training costs compared to MLPs. To address this limitation, we propose
to generate weights for KANs via a smaller meta-learner, called MetaKANs. By
training KANs and MetaKANs in an end-to-end differentiable manner, MetaKANs
achieve comparable or even superior performance while significantly reducing
the number of trainable parameters and maintaining promising interpretability.
Extensive experiments on diverse benchmark tasks, including symbolic
regression, partial differential equation solving, and image classification,
demonstrate the effectiveness of MetaKANs in improving parameter efficiency and
memory usage. The proposed method provides an alternative technique for
training KANs, that allows for greater scalability and extensibility, and
narrows the training cost gap with MLPs stated in the original paper of KANs.
Our code is available at https://github.com/Murphyzc/MetaKAN.

</details>


### [404] [ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/abs/2506.07551)
*Mengsong Wu,YaFei Wang,Yidong Ming,Yuqi An,Yuwei Wan,Wenliang Chen,Binbin Lin,Yuqiang Li,Tong Xie,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 提出一种基于LLM的化学代理，整合外部化学工具和数据集ChemToolBench，通过HE-MCTS框架优化工具规划与执行，显著提升化学任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在化学任务中因预训练知识过时和难以融入专业化学知识而面临的挑战。

Method: 开发LLM代理，整合137种化学工具和数据集ChemToolBench，采用HE-MCTS框架优化工具规划与执行，并通过自生成数据微调模型。

Result: 实验表明，该方法在化学问答和发现任务中性能显著提升，超越GPT-4o。

Conclusion: 为LLMs与专业化学工具的结合提供了高效解决方案，推动化学应用发展。

Abstract: Large language models (LLMs) have recently demonstrated promising
capabilities in chemistry tasks while still facing challenges due to outdated
pretraining knowledge and the difficulty of incorporating specialized chemical
expertise. To address these issues, we propose an LLM-based agent that
synergistically integrates 137 external chemical tools created ranging from
basic information retrieval to complex reaction predictions, and a dataset
curation pipeline to generate the dataset ChemToolBench that facilitates both
effective tool selection and precise parameter filling during fine-tuning and
evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search
(HE-MCTS) framework, enabling independent optimization of tool planning and
execution. By leveraging self-generated data, our approach supports step-level
fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM
that surpass GPT-4o. Experimental evaluations demonstrate that our approach
significantly improves performance in Chemistry QA and discovery tasks,
offering a robust solution to integrate specialized tools with LLMs for
advanced chemical applications. All datasets and code are available at
https://github.com/AI4Chem/ChemistryAgent .

</details>


### [405] [Denoising the Future: Top-p Distributions for Moving Through Time](https://arxiv.org/abs/2506.07578)
*Florian Andreas Marwitz,Ralf Möller,Magnus Bender,Marcel Gehrke*

Main category: cs.LG

TL;DR: 论文提出了一种通过仅使用概率最高的前p个状态（top-p状态）来加速动态概率模型推断的方法，减少了计算复杂度和噪声。


<details>
  <summary>Details</summary>
Motivation: 在动态概率模型（如隐马尔可夫模型）中，推断过程需要枚举整个状态空间，计算效率低且会传播低概率状态的噪声。

Method: 提出仅使用累积概率为p的最可能状态（top-p状态）来加速推断，并减少噪声。

Result: 实验表明，该方法可实现至少一个数量级的加速，且总变差距离误差低于0.09。

Conclusion: 通过限制状态空间为top-p状态，可以在保证误差可控的情况下显著提升推断效率。

Abstract: Inference in dynamic probabilistic models is a complex task involving
expensive operations. In particular, for Hidden Markov Models, the whole state
space has to be enumerated for advancing in time. Even states with negligible
probabilities are considered, resulting in computational inefficiency and
increased noise due to the propagation of unlikely probability mass. We propose
to denoise the future and speed up inference by using only the top-p states,
i.e., the most probable states with accumulated probability p. We show that the
error introduced by using only the top-p states is bound by p and the so-called
minimal mixing rate of the underlying model. Moreover, in our empirical
evaluation, we show that we can expect speedups of at least an order of
magnitude, while the error in terms of total variation distance is below 0.09.

</details>


### [406] [FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning](https://arxiv.org/abs/2506.07581)
*Tan Chen,Jintao Yan,Yuxuan Sun,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: 本文提出FedCGD算法，通过平衡加权地球移动距离（WEMD）和采样方差，最小化多级集体梯度发散（CGD），从而提升联邦学习的收敛速度和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在无线网络中面临设备数据异构性和带宽限制的问题，现有研究多关注设备级数据异构性，而本文发现收敛速度还受样本级CGD影响。

Method: 通过分析设备级和样本级CGD，将其转化为分类问题中的WEMD和采样方差，并提出FedCGD算法在多项式时间内优化两者。

Result: 在CIFAR-10数据集上，FedCGD将分类准确率提升4.2%，同时减少41.8%的设备调度。

Conclusion: FedCGD通过多级CGD优化，显著提升FL性能，并能灵活调整WEMD和采样方差的平衡。

Abstract: Federated learning (FL) is a promising paradigm for multiple devices to
cooperatively train a model. When applied in wireless networks, two issues
consistently affect the performance of FL, i.e., data heterogeneity of devices
and limited bandwidth. Many papers have investigated device scheduling
strategies considering the two issues. However, most of them recognize data
heterogeneity as a property of individual devices. In this paper, we prove that
the convergence speed of FL is affected by the sum of device-level and
sample-level collective gradient divergence (CGD). The device-level CGD refers
to the gradient divergence of the scheduled device group, instead of the sum of
the individual device divergence. The sample-level CGD is statistically upper
bounded by sampling variance, which is inversely proportional to the total
number of samples scheduled for local update. To derive a tractable form of the
device-level CGD, we further consider a classification problem and transform it
into the weighted earth moving distance (WEMD) between the group distribution
and the global distribution. Then we propose FedCGD algorithm to minimize the
sum of multi-level CGDs by balancing WEMD and sampling variance, within
polynomial time. Simulation shows that the proposed strategy increases
classification accuracy on the CIFAR-10 dataset by up to 4.2\% while scheduling
41.8\% fewer devices, and flexibly switches between reducing WEMD and reducing
sampling variance.

</details>


### [407] [MIRA: Medical Time Series Foundation Model for Real-World Health Data](https://arxiv.org/abs/2506.07584)
*Hao Li,Bowen Deng,Chang Xu,Zhiyuan Feng,Viktor Schlegel,Yu-Hao Huang,Yizheng Sun,Jingyuan Sun,Kailai Yang,Yiyao Yu,Jiang Bian*

Main category: cs.LG

TL;DR: MIRA是一个专为医疗时间序列设计的统一基础模型，通过创新方法解决了数据不规则性等问题，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有通用时间序列基础模型难以处理医疗时间序列的挑战（如不规则间隔、异构采样率和缺失值），因此需要专门设计的模型。

Method: MIRA结合了连续时间旋转位置编码、频率特定专家混合层和基于神经ODE的连续动态外推块，以建模复杂时间动态。

Result: 在公开数据集上预训练的MIRA，预测误差平均减少10%（分布外）和7%（分布内），优于其他基线模型。

Conclusion: MIRA为医疗时间序列建模提供了新基准，并为未来研究奠定了基础。

Abstract: A unified foundation model for medical time series -- pretrained on open
access and ethics board-approved medical corpora -- offers the potential to
reduce annotation burdens, minimize model customization, and enable robust
transfer across clinical institutions, modalities, and tasks, particularly in
data-scarce or privacy-constrained environments. However, existing generalist
time series foundation models struggle to handle medical time series data due
to their inherent challenges, including irregular intervals, heterogeneous
sampling rates, and frequent missing values. To address these challenges, we
introduce MIRA, a unified foundation model specifically designed for medical
time series forecasting. MIRA incorporates a Continuous-Time Rotary Positional
Encoding that enables fine-grained modeling of variable time intervals, a
frequency-specific mixture-of-experts layer that routes computation across
latent frequency regimes to further promote temporal specialization, and a
Continuous Dynamics Extrapolation Block based on Neural ODE that models the
continuous trajectory of latent states, enabling accurate forecasting at
arbitrary target timestamps. Pretrained on a large-scale and diverse medical
corpus comprising over 454 billion time points collect from publicly available
datasets, MIRA achieves reductions in forecasting errors by an average of 10%
and 7% in out-of-distribution and in-distribution scenarios, respectively, when
compared to other zero-shot and fine-tuned baselines. We also introduce a
comprehensive benchmark spanning multiple downstream clinical tasks,
establishing a foundation for future research in medical time series modeling.

</details>


### [408] [Aircraft Trajectory Dataset Augmentation in Latent Space](https://arxiv.org/abs/2506.07585)
*Seokbin Yoon,Keumjin Lee*

Main category: cs.LG

TL;DR: 提出了一种名为ATRADA的新型框架，用于增强飞机轨迹数据集，通过Transformer编码器和GMM生成高质量合成数据。


<details>
  <summary>Details</summary>
Motivation: 飞机轨迹建模对空管至关重要，但现有数据集不足且不平衡，需通过合成数据增强模型鲁棒性。

Method: 使用Transformer编码器学习轨迹模式并转换为潜在空间向量，PCA降维后GMM拟合分布，MLP解码生成新样本。

Result: 实验表明ATRADA能有效生成高质量合成轨迹数据，优于基线方法。

Conclusion: ATRADA框架成功解决了轨迹数据集不足问题，为下游任务提供了更丰富的数据支持。

Abstract: Aircraft trajectory modeling plays a crucial role in Air Traffic Management
(ATM) and is important for various downstream tasks, including conflict
detection and landing time prediction. Dataset augmentation through the
addition of synthetically generated trajectory data is necessary to develop a
more robust aircraft trajectory model and ensure that the trajectory dataset is
sufficient and balanced. In this work, we propose a novel framework called
ATRADA for aircraft trajectory dataset augmentation. In the proposed framework,
a Transformer encoder learns the underlying patterns in the original trajectory
dataset and converts each data point into a context vector in the learned
latent space. The converted dataset in the latent space is projected into
reduced dimensions using principal component analysis (PCA), and a Gaussian
mixture model (GMM) is applied to fit the probability distribution of the data
points in the reduced-dimensional space. Finally, new samples are drawn from
the fitted GMM, the dimension of the samples is reverted to the original
dimension, and they are decoded with a Multi-Layer Perceptron (MLP). Several
experiments demonstrate that the framework effectively generates new,
high-quality synthetic aircraft trajectory data, which were compared to the
results of several baselines.

</details>


### [409] [PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs](https://arxiv.org/abs/2506.07587)
*Tongzhou Yu,Zhuhao Zhang,Guanghui Zhu,Shen Jiang,Meikang Qiu,Yihua Huang*

Main category: cs.LG

TL;DR: PrunePEFT将PEFT策略搜索转化为剪枝问题，通过混合剪枝策略优化配置，显著降低计算负担。


<details>
  <summary>Details</summary>
Motivation: PEFT方法虽高效，但设计空间庞大，配置不当会导致性能不佳，传统搜索方法开销大。

Method: 将PEFT策略搜索转化为剪枝问题，采用混合剪枝策略迭代移除冗余模块。

Result: 显著减少计算负担，优化配置，提升效率。

Conclusion: PrunePEFT为大规模预训练模型微调提供了高效、可扩展的解决方案。

Abstract: Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and
promising approaches for fine-tuning pre-trained language models. Compared with
Full parameter Fine-Tuning (FFT), PEFT achieved comparable task performance
with a substantial reduction of trainable parameters, which largely saved the
training and storage costs. However, using the PEFT method requires considering
a vast design space, such as the type of PEFT modules and their insertion
layers. Inadequate configurations can lead to sub-optimal results. Conventional
solutions such as architectural search techniques, while effective, tend to
introduce substantial additional overhead. In this paper, we propose a novel
approach, PrunePEFT, which formulates the PEFT strategy search as a pruning
problem and introduces a hybrid pruning strategy that capitalizes on the
sensitivity of pruning methods to different PEFT modules. This method extends
traditional pruning techniques by iteratively removing redundant or conflicting
PEFT modules, thereby optimizing the fine-tuned configuration. By efficiently
identifying the most relevant modules, our approach significantly reduces the
computational burden typically associated with architectural search processes,
making it a more scalable and efficient solution for fine-tuning large
pre-trained models.

</details>


### [410] [Exploiting Curvature in Online Convex Optimization with Delayed Feedback](https://arxiv.org/abs/2506.07595)
*Hao Qiu,Emmanuel Esposito,Mengxiao Zhang*

Main category: cs.LG

TL;DR: 论文研究了具有弯曲损失和延迟反馈的在线凸优化问题，提出了改进算法以减少遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有方法在强凸损失下的遗憾界可能不如延迟版本的在线梯度下降，因此需要改进算法以缩小这一差距。

Method: 提出了一种改进的跟随正则化领导者算法，并扩展了在线牛顿步算法以处理延迟，同时设计了带剪裁技巧的Vovk-Azoury-Warmuth预测器变体。

Result: 新算法实现了更优的遗憾界，并在实验中表现出优于现有方法的性能。

Conclusion: 论文提出的算法在理论和实验上均取得了显著改进，为延迟反馈的在线优化问题提供了更高效的解决方案。

Abstract: In this work, we study the online convex optimization problem with curved
losses and delayed feedback. When losses are strongly convex, existing
approaches obtain regret bounds of order $d_{\max} \ln T$, where $d_{\max}$ is
the maximum delay and $T$ is the time horizon. However, in many cases, this
guarantee can be much worse than $\sqrt{d_{\mathrm{tot}}}$ as obtained by a
delayed version of online gradient descent, where $d_{\mathrm{tot}}$ is the
total delay. We bridge this gap by proposing a variant of
follow-the-regularized-leader that obtains regret of order
$\min\{\sigma_{\max}\ln T, \sqrt{d_{\mathrm{tot}}}\}$, where $\sigma_{\max}$ is
the maximum number of missing observations. We then consider exp-concave losses
and extend the Online Newton Step algorithm to handle delays with an adaptive
learning rate tuning, achieving regret $\min\{d_{\max} n\ln T,
\sqrt{d_{\mathrm{tot}}}\}$ where $n$ is the dimension. To our knowledge, this
is the first algorithm to achieve such a regret bound for exp-concave losses.
We further consider the problem of unconstrained online linear regression and
achieve a similar guarantee by designing a variant of the Vovk-Azoury-Warmuth
forecaster with a clipping trick. Finally, we implement our algorithms and
conduct experiments under various types of delay and losses, showing an
improved performance over existing methods.

</details>


### [411] [TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts](https://arxiv.org/abs/2506.07596)
*Torsten Krauß,Hamid Dashtbani,Alexandra Dmitrienko*

Main category: cs.LG

TL;DR: TwinBreak是一种创新的安全对齐移除方法，通过识别和修剪负责安全功能的参数，有效绕过LLM的安全机制，成功率达89%至98%。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM带来许多好处，但恶意用户可能通过有害提示绕过安全机制。现有方法成本高或影响模型性能，因此需要更高效的解决方案。

Method: TwinBreak通过分析中间输出，识别并修剪与安全功能相关的参数，同时保留模型的核心功能。

Result: 实验证明TwinBreak在16种LLM上成功率达89%至98%，且计算成本低。

Conclusion: TwinBreak为绕过LLM安全机制提供了一种高效且低成本的方法，但也揭示了安全机制的脆弱性。

Abstract: Machine learning is advancing rapidly, with applications bringing notable
benefits, such as improvements in translation and code generation. Models like
ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated
into daily life. However, alongside these benefits, LLMs also introduce social
risks. Malicious users can exploit LLMs by submitting harmful prompts, such as
requesting instructions for illegal activities. To mitigate this, models often
include a security mechanism that automatically rejects such harmful prompts.
However, they can be bypassed through LLM jailbreaks. Current jailbreaks often
require significant manual effort, high computational costs, or result in
excessive model modifications that may degrade regular utility.
  We introduce TwinBreak, an innovative safety alignment removal method.
Building on the idea that the safety mechanism operates like an embedded
backdoor, TwinBreak identifies and prunes parameters responsible for this
functionality. By focusing on the most relevant model layers, TwinBreak
performs fine-grained analysis of parameters essential to model utility and
safety. TwinBreak is the first method to analyze intermediate outputs from
prompts with high structural and content similarity to isolate safety
parameters. We present the TwinPrompt dataset containing 100 such twin prompts.
Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success
rates with minimal computational requirements across 16 LLMs from five vendors.

</details>


### [412] [FuXi-Air: Urban Air Quality Forecasting Based on Emission-Meteorology-Pollutant multimodal Machine Learning](https://arxiv.org/abs/2506.07616)
*Zhixin Geng,Xu Fan,Xiqiao Lu,Yan Zhang,Guangyuan Yu,Cheng Huang,Qian Wang,Yuewu Li,Weichun Ma,Qi Yu,Libo Wu,Hao Li*

Main category: cs.LG

TL;DR: FuXi-Air模型通过多模态数据融合，高效完成72小时空气质量预测，优于主流数值模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有空气质量预测方法的高计算成本、低效率和观测数据整合不足的问题。

Method: 结合气象预报、排放清单和污染物监测数据，采用自回归预测框架和帧插值策略。

Result: 在25-30秒内完成多站点6种污染物的72小时预测，计算效率和精度均优于主流模型。

Conclusion: 多模态数据驱动模型为智能城市管理提供技术参考，支持混合预测系统的构建。

Abstract: Air pollution has emerged as a major public health challenge in megacities.
Numerical simulations and single-site machine learning approaches have been
widely applied in air quality forecasting tasks. However, these methods face
multiple limitations, including high computational costs, low operational
efficiency, and limited integration with observational data. With the rapid
advancement of artificial intelligence, there is an urgent need to develop a
low-cost, efficient air quality forecasting model for smart urban management.
An air quality forecasting model, named FuXi-Air, has been constructed in this
study based on multimodal data fusion to support high-precision air quality
forecasting and operated in typical megacities. The model integrates
meteorological forecasts, emission inventories, and pollutant monitoring data
under the guidance of air pollution mechanism. By combining an autoregressive
prediction framework with a frame interpolation strategy, the model
successfully completes 72-hour forecasts for six major air pollutants at an
hourly resolution across multiple monitoring sites within 25-30 seconds. In
terms of both computational efficiency and forecasting accuracy, it outperforms
the mainstream numerical air quality models in operational forecasting work.
Ablation experiments concerning key influencing factors show that although
meteorological data contribute more to model accuracy than emission inventories
do, the integration of multimodal data significantly improves forecasting
precision and ensures that reliable predictions are obtained under differing
pollution mechanisms across megacities. This study provides both a technical
reference and a practical example for applying multimodal data-driven models to
air quality forecasting and offers new insights into building hybrid
forecasting systems to support air pollution risk warning in smart city
management.

</details>


### [413] [The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning](https://arxiv.org/abs/2506.07619)
*Toby Boyne,Juan S. Campos,Becky D. Langdon,Jixiang Qing,Yilin Xie,Shiqiang Zhang,Calvin Tsay,Ruth Misener,Daniel W. Davies,Kim E. Jelfs,Sarah Boyall,Thomas M. Dixon,Linden Schrecker,Jose Pablo Folch*

Main category: cs.LG

TL;DR: 论文介绍了一个用于产率预测的新型数据集，首次提供了用于机器学习基准测试的瞬态流动数据集，覆盖1200多种工艺条件，重点关注溶剂选择任务。


<details>
  <summary>Details</summary>
Motivation: 化学数据集通常难以获取或需要清洗，限制了机器学习在化学领域的应用。本文旨在填补这一空白，提供首个瞬态流动数据集，推动机器学习在溶剂选择和可持续制造中的应用。

Method: 通过实验设置采集大量连续工艺条件数据，并用于回归算法、迁移学习、特征工程和主动学习的基准测试。

Result: 数据集覆盖1200多种工艺条件，为机器学习模型提供了新的挑战，尤其在溶剂选择任务中表现出色。

Conclusion: 该数据集为机器学习在化学领域的应用提供了重要资源，尤其在溶剂替换和可持续制造方面具有潜在价值。

Abstract: Machine learning has promised to change the landscape of laboratory
chemistry, with impressive results in molecular property prediction and
reaction retro-synthesis. However, chemical datasets are often inaccessible to
the machine learning community as they tend to require cleaning, thorough
understanding of the chemistry, or are simply not available. In this paper, we
introduce a novel dataset for yield prediction, providing the first-ever
transient flow dataset for machine learning benchmarking, covering over 1200
process conditions. While previous datasets focus on discrete parameters, our
experimental set-up allow us to sample a large number of continuous process
conditions, generating new challenges for machine learning models. We focus on
solvent selection, a task that is particularly difficult to model theoretically
and therefore ripe for machine learning applications. We showcase benchmarking
for regression algorithms, transfer-learning approaches, feature engineering,
and active learning, with important applications towards solvent replacement
and sustainable manufacturing.

</details>


### [414] [Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks](https://arxiv.org/abs/2506.07624)
*Ali Hariri,Álvaro Arroyo,Alessio Gravina,Moshe Eliasof,Carola-Bibiane Schönlieb,Davide Bacciu,Kamyar Azizzadenesheli,Xiaowen Dong,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: ChebNet在长距离节点交互建模中表现出竞争力，但存在训练不稳定性问题，改进后的Stable-ChebNet解决了这一问题并取得优异性能。


<details>
  <summary>Details</summary>
Motivation: MPNNs和Graph Transformers在捕捉长距离依赖时存在局限性，而ChebNet作为早期光谱GNN可能具备潜力。

Method: 将ChebNet建模为稳定且非耗散的动态系统（Stable-ChebNet），避免多项式扩展的不稳定性。

Result: Stable-ChebNet在多个基准测试中接近最优性能，且无需额外计算开销。

Conclusion: ChebNet及其改进版Stable-ChebNet在长距离依赖建模中具有竞争力，为GNN研究提供了新方向。

Abstract: ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by
Message Passing Neural Networks (MPNNs), which gained popularity for their
simplicity and effectiveness in capturing local graph structure. Despite their
success, MPNNs are limited in their ability to capture long-range dependencies
between nodes. This has led researchers to adapt MPNNs through rewiring or make
use of Graph Transformers, which compromises the computational efficiency that
characterized early spatial message-passing architectures, and typically
disregards the graph structure. Almost a decade after its original
introduction, we revisit ChebNet to shed light on its ability to model distant
node interactions. We find that out-of-box, ChebNet already shows competitive
advantages relative to classical MPNNs and GTs on long-range benchmarks, while
maintaining good scalability properties for high-order polynomials. However, we
uncover that this polynomial expansion leads ChebNet to an unstable regime
during training. To address this limitation, we cast ChebNet as a stable and
non-dissipative dynamical system, which we coin Stable-ChebNet. Our
Stable-ChebNet model allows for stable information propagation, and has
controllable dynamics which do not require the use of eigendecompositions,
positional encodings, or graph rewiring. Across several benchmarks,
Stable-ChebNet achieves near state-of-the-art performance.

</details>


### [415] [The Universality Lens: Why Even Highly Over-Parametrized Models Learn Well](https://arxiv.org/abs/2506.07661)
*Meir Feder,Ruediger Urbanke,Yaniv Fogel*

Main category: cs.LG

TL;DR: 论文通过信息论和贝叶斯混合学习框架，解释了过参数化模型（如深度神经网络）泛化能力强的现象，提出模型‘权重’概念，并表明简单模型因权重高而更易泛化。


<details>
  <summary>Details</summary>
Motivation: 研究为何过参数化模型在参数远超训练样本时仍能良好泛化，试图从理论上解释这一现象。

Method: 采用贝叶斯混合学习框架，基于对数损失和均匀先验，分析假设类的累积概率（权重）对泛化的影响。

Result: 发现泛化能力取决于假设类中接近真实数据生成过程的模型的累积概率（权重），而非假设类总大小。

Conclusion: 过参数化模型通过后验集中在简单假设上避免过拟合，理论与实际方法（如随机梯度下降）结合，为现代AI系统的泛化行为提供了统一解释。

Abstract: A fundamental question in modern machine learning is why large,
over-parameterized models, such as deep neural networks and transformers, tend
to generalize well, even when their number of parameters far exceeds the number
of training samples.
  We investigate this phenomenon through the lens of information theory,
grounded in universal learning theory. Specifically, we study a Bayesian
mixture learner with log-loss and (almost) uniform prior over an expansive
hypothesis class.
  Our key result shows that the learner's regret is not determined by the
overall size of the hypothesis class, but rather by the cumulative probability
of all models that are close, in Kullback-Leibler divergence distance, to the
true data-generating process. We refer to this cumulative probability as the
weight of the hypothesis.
  This leads to a natural notion of model simplicity: simple models are those
with large weight and thus require fewer samples to generalize, while complex
models have small weight and need more data. This perspective provides a
rigorous and intuitive explanation for why over-parameterized models often
avoid overfitting: the presence of simple hypotheses allows the posterior to
concentrate on them when supported by the data.
  We further bridge theory and practice by recalling that stochastic gradient
descent with Langevin dynamics samples from the correct posterior distribution,
enabling our theoretical learner to be approximated using standard machine
learning methods combined with ensemble learning.
  Our analysis yields non-uniform regret bounds and aligns with key practical
concepts such as flat minima and model distillation. The results apply broadly
across online, batch, and supervised learning settings, offering a unified and
principled understanding of the generalization behavior of modern AI systems.

</details>


### [416] [ProARD: progressive adversarial robustness distillation: provide wide range of robust students](https://arxiv.org/abs/2506.07666)
*Seyedhamidreza Mousavi,Seyedali Mousavi,Masoud Daneshtalab*

Main category: cs.LG

TL;DR: ProARD提出了一种动态网络训练方法，通过一次性训练支持多种轻量级学生网络，避免了重复训练的高成本和碳排放。


<details>
  <summary>Details</summary>
Motivation: 当前对抗鲁棒性蒸馏方法需要为不同资源约束的设备重新训练学生网络，导致高计算成本和碳排放。

Method: 基于动态层构建动态网络，通过权重共享机制联合优化动态教师网络及其内部学生网络，并采用采样机制选择学生子集。

Result: 随机采样学生网络无法产生准确且鲁棒的学生网络。

Conclusion: ProARD为高效训练多样化鲁棒学生网络提供了可行方案。

Abstract: Adversarial Robustness Distillation (ARD) has emerged as an effective method
to enhance the robustness of lightweight deep neural networks against
adversarial attacks. Current ARD approaches have leveraged a large robust
teacher network to train one robust lightweight student. However, due to the
diverse range of edge devices and resource constraints, current approaches
require training a new student network from scratch to meet specific
constraints, leading to substantial computational costs and increased CO2
emissions. This paper proposes Progressive Adversarial Robustness Distillation
(ProARD), enabling the efficient one-time training of a dynamic network that
supports a diverse range of accurate and robust student networks without
requiring retraining. We first make a dynamic deep neural network based on
dynamic layers by encompassing variations in width, depth, and expansion in
each design stage to support a wide range of architectures. Then, we consider
the student network with the largest size as the dynamic teacher network.
ProARD trains this dynamic network using a weight-sharing mechanism to jointly
optimize the dynamic teacher network and its internal student networks.
However, due to the high computational cost of calculating exact gradients for
all the students within the dynamic network, a sampling mechanism is required
to select a subset of students. We show that random student sampling in each
iteration fails to produce accurate and robust students.

</details>


### [417] [How Benchmark Prediction from Fewer Data Misses the Mark](https://arxiv.org/abs/2506.07673)
*Guanhua Zhang,Florian E. Dorner,Moritz Hardt*

Main category: cs.LG

TL;DR: 论文研究了11种基准预测方法在19个不同基准上的表现，发现随机采样加回归模型是一个强基线，现有方法依赖于模型相似性，并提出了一种新方法在部分情况下优于随机采样。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）评估成本高昂，需要快速评估方法以减少数据集规模。

Method: 系统评估11种基准预测方法，提出随机采样加回归模型作为基线，并引入基于增强逆倾向加权的新方法。

Result: 现有方法在模型相似性高时有效，但在新模型能力未知时表现不佳；新方法在部分情况下优于随机采样。

Conclusion: 基准预测在评估前沿（新模型能力未知时）效果有限，仍需改进。

Abstract: Large language model (LLM) evaluation is increasingly costly, prompting
interest in methods that speed up evaluation by shrinking benchmark datasets.
Benchmark prediction (also called efficient LLM evaluation) aims to select a
small subset of evaluation points and predict overall benchmark performance
from that subset. In this paper, we systematically assess the strengths and
limitations of 11 benchmark prediction methods across 19 diverse benchmarks.
First, we identify a highly competitive baseline: Take a random sample and fit
a regression model on the sample to predict missing entries. Outperforming most
existing methods, this baseline challenges the assumption that careful subset
selection is necessary for benchmark prediction. Second, we discover that all
existing methods crucially depend on model similarity. They work best when
interpolating scores among similar models. The effectiveness of benchmark
prediction sharply declines when new models have higher accuracy than
previously seen models. In this setting of extrapolation, none of the previous
methods consistently beat a simple average over random samples. To improve over
the sample average, we introduce a new method inspired by augmented inverse
propensity weighting. This method consistently outperforms the random sample
average even for extrapolation. However, its performance still relies on model
similarity and the gains are modest in general. This shows that benchmark
prediction fails just when it is most needed: at the evaluation frontier, where
the goal is to evaluate new models of unknown capabilities.

</details>


### [418] [Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation](https://arxiv.org/abs/2506.07706)
*Boris Martirosyan,Alexey Karmanov*

Main category: cs.LG

TL;DR: 本文提出改进潜在扩散模型（LDMs）鲁棒性的方法，包括分离文本编码器评估、新数据增强技术、基于Dreambooth的微调及专用评估流程。


<details>
  <summary>Details</summary>
Motivation: LDMs在图像生成等任务中表现优异，但鲁棒性不足，现有研究未充分探索此问题。

Method: 1. 分离文本编码器评估；2. 新数据增强技术；3. 用Dreambooth微调Stable Diffusion模型；4. 提出专用评估流程。

Result: 通过实验验证了方法的有效性，提升了LDMs的鲁棒性。

Conclusion: 本文方法为LDMs的鲁棒性研究提供了新方向，并展示了实际应用潜力。

Abstract: Latent diffusion models (LDMs) achieve state-of-the-art performance across
various tasks, including image generation and video synthesis. However, they
generally lack robustness, a limitation that remains not fully explored in
current research. In this paper, we propose several methods to address this
gap. First, we hypothesize that the robustness of LDMs primarily should be
measured without their text encoder, because if we take and explore the whole
architecture, the problems of image generator and text encoders wll be fused.
Second, we introduce novel data augmentation techniques designed to reveal
robustness shortcomings in LDMs when processing diverse textual prompts. We
then fine-tune Stable Diffusion 3 and Stable Diffusion XL models using
Dreambooth, incorporating these proposed augmentation methods across multiple
tasks. Finally, we propose a novel evaluation pipeline specifically tailored to
assess the robustness of LDMs fine-tuned via Dreambooth.

</details>


### [419] [Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning](https://arxiv.org/abs/2506.07735)
*Haizhao Jing,Haokui Zhang,Zhenhao Shang,Rong Xiao,Peng Wang,Yanning Zhang*

Main category: cs.LG

TL;DR: LeDG-Former框架通过语言嵌入和动态图表示学习的结合，解决了现有方法忽略硬件属性和静态邻接矩阵的局限性，实现了跨硬件平台的零样本预测，并在多个基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络架构表示学习方法忽视了硬件属性信息，且依赖静态邻接矩阵，限制了模型的实用性和编码效果。

Method: 提出LeDG-Former框架，结合语言嵌入（利用LLM将架构和硬件映射到统一语义空间）和动态图Transformer，改进架构建模。

Result: 在NNLQP、NAS-Bench-101和NAS-Bench-201基准测试中表现最优，首次实现跨硬件延迟预测。

Conclusion: LeDG-Former通过语言和动态图的结合，显著提升了神经网络架构表示学习的性能和应用范围。

Abstract: Neural Architecture Representation Learning aims to transform network models
into feature representations for predicting network attributes, playing a
crucial role in deploying and designing networks for real-world applications.
Recently, inspired by the success of transformers, transformer-based models
integrated with Graph Neural Networks (GNNs) have achieved significant progress
in representation learning. However, current methods still have some
limitations. First, existing methods overlook hardware attribute information,
which conflicts with the current trend of diversified deep learning hardware
and limits the practical applicability of models. Second, current encoding
approaches rely on static adjacency matrices to represent topological
structures, failing to capture the structural differences between computational
nodes, which ultimately compromises encoding effectiveness. In this paper, we
introduce LeDG-Former, an innovative framework that addresses these limitations
through the synergistic integration of language-based semantic embedding and
dynamic graph representation learning. Specifically, inspired by large language
models (LLMs), we propose a language embedding framework where both neural
architectures and hardware platform specifications are projected into a unified
semantic space through tokenization and LLM processing, enabling zero-shot
prediction across different hardware platforms for the first time. Then, we
propose a dynamic graph-based transformer for modeling neural architectures,
resulting in improved neural architecture modeling performance. On the NNLQP
benchmark, LeDG-Former surpasses previous methods, establishing a new SOTA
while demonstrating the first successful cross-hardware latency prediction
capability. Furthermore, our framework achieves superior performance on the
cell-structured NAS-Bench-101 and NAS-Bench-201 datasets.

</details>


### [420] [Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.07744)
*Seungho Baek,Taegeon Park,Jongchan Park,Seungjun Oh,Yusung Kim*

Main category: cs.LG

TL;DR: 论文提出了一种名为GAS的新框架，通过将子目标选择建模为图搜索问题，而非显式学习高层策略，解决了现有离线分层强化学习方法在任务时间增长时效率下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有离线分层强化学习方法依赖高层策略生成子目标序列，但随着任务时间增长效率下降，且缺乏跨轨迹有效状态转移的策略。

Method: GAS将子目标选择视为图搜索问题，通过Temporal Distance Representation (TDR)空间嵌入状态，聚类语义相似状态为图节点，并应用最短路径算法选择子目标序列。同时引入Temporal Efficiency (TE)指标优化图质量。

Result: GAS在运动、导航和操作任务中表现优于现有方法，尤其在缝合关键任务中得分88.3，远超之前最佳得分1.0。

Conclusion: GAS通过图搜索和TDR空间嵌入，显著提升了离线分层强化学习的效率和性能。

Abstract: Existing offline hierarchical reinforcement learning methods rely on
high-level policy learning to generate subgoal sequences. However, their
efficiency degrades as task horizons increase, and they lack effective
strategies for stitching useful state transitions across different
trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that
formulates subgoal selection as a graph search problem rather than learning an
explicit high-level policy. By embedding states into a Temporal Distance
Representation (TDR) space, GAS clusters semantically similar states from
different trajectories into unified graph nodes, enabling efficient transition
stitching. A shortest-path algorithm is then applied to select subgoal
sequences within the graph, while a low-level policy learns to reach the
subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)
metric, which filters out noisy or inefficient transition states, significantly
enhancing task performance. GAS outperforms prior offline HRL methods across
locomotion, navigation, and manipulation tasks. Notably, in the most
stitching-critical task, it achieves a score of 88.3, dramatically surpassing
the previous state-of-the-art score of 1.0. Our source code is available at:
https://github.com/qortmdgh4141/GAS.

</details>


### [421] [E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time](https://arxiv.org/abs/2506.07747)
*Adam Breuer*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的非梯度组合方法，用于推断LDA主题模型中每个文档的主题分配，具有对数并行计算时间的高效性，并提供了可解释性保证。


<details>
  <summary>Details</summary>
Motivation: 解决LDA主题模型在社会科学、数据探索和因果推断中的主要推断问题，提供高效且可解释的算法。

Method: 采用非梯度组合方法估计主题模型，实现对数并行计算时间的高效收敛。

Result: 算法在多种文本数据集上表现优于现有LDA、神经主题模型和基于LLM的主题模型，提供更高的语义质量。

Conclusion: 该方法不仅高效且可解释，还能保持独立性假设，适用于下游因果推断研究。

Abstract: In this paper, we provide the first practical algorithms with provable
guarantees for the problem of inferring the topics assigned to each document in
an LDA topic model. This is the primary inference problem for many applications
of topic models in social science, data exploration, and causal inference
settings. We obtain this result by showing a novel non-gradient-based,
combinatorial approach to estimating topic models. This yields algorithms that
converge to near-optimal posterior probability in logarithmic parallel
computation time (adaptivity) -- exponentially faster than any known LDA
algorithm. We also show that our approach can provide interpretability
guarantees such that each learned topic is formally associated with a known
keyword. Finally, we show that unlike alternatives, our approach can maintain
the independence assumptions necessary to use the learned topic model for
downstream causal inference methods that allow researchers to study topics as
treatments. In terms of practical performance, our approach consistently
returns solutions of higher semantic quality than solutions from
state-of-the-art LDA algorithms, neural topic models, and LLM-based topic
models across a diverse range of text datasets and evaluation parameters.

</details>


### [422] [Comparing Credit Risk Estimates in the Gen-AI Era](https://arxiv.org/abs/2506.07754)
*Nicola Lavecchia,Sid Fadanelli,Federico Ricciuti,Gennaro Aloe,Enrico Bagli,Pietro Giuffrida,Daniele Vergari*

Main category: cs.LG

TL;DR: 生成式AI在信用评分建模中表现不及传统方法，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在信用评分建模中的应用潜力。

Method: 对比传统信用评分方法与生成式AI技术。

Result: 生成式AI目前表现不如传统方法。

Conclusion: 生成式AI在信用评分领域仍需改进与发展。

Abstract: Generative AI technologies have demonstrated significant potential across
diverse applications. This study provides a comparative analysis of credit
score modeling techniques, contrasting traditional approaches with those
leveraging generative AI. Our findings reveal that current generative AI models
fall short of matching the performance of traditional methods, regardless of
the integration strategy employed. These results highlight the limitations in
the current capabilities of generative AI for credit risk scoring, emphasizing
the need for further research and development before the possibility of
applying generative AI for this specific task, or equivalent ones.

</details>


### [423] [Clustered Federated Learning via Embedding Distributions](https://arxiv.org/abs/2506.07769)
*Dekai Zhang,Matthew Williams,Francesca Toni*

Main category: cs.LG

TL;DR: 论文提出了一种名为EMD-CFL的新型一次性聚类方法，用于解决联邦学习中非独立同分布数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式数据环境中广泛应用，但面临非独立同分布数据的脆弱性问题。

Method: 使用嵌入空间中数据分布的地球移动距离（EMD）进行一次性聚类。

Result: 在16个基线方法和多个挑战性数据集上的实验表明，EMD-CFL具有优越的聚类性能。

Conclusion: EMD-CFL为联邦学习中的非独立同分布数据问题提供了一种有效的解决方案。

Abstract: Federated learning (FL) is a widely used framework for machine learning in
distributed data environments where clients hold data that cannot be easily
centralised, such as for data protection reasons. FL, however, is known to be
vulnerable to non-IID data. Clustered FL addresses this issue by finding more
homogeneous clusters of clients. We propose a novel one-shot clustering method,
EMD-CFL, using the Earth Mover's distance (EMD) between data distributions in
embedding space. We theoretically motivate the use of EMDs using results from
the domain adaptation literature and demonstrate empirically superior
clustering performance in extensive comparisons against 16 baselines and on a
range of challenging datasets.

</details>


### [424] [Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability](https://arxiv.org/abs/2506.07804)
*Jie Bao,Chuangyin Dang,Rui Luo,Hanwei Zhang,Zhixin Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种基于Conformal Prediction的对抗攻击方法OPSA及其防御策略OPSA-AT，通过最大化模型不确定性提升对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在高风险应用中的部署需要更强的对抗防御和性能保证，仅靠准确性不足以提供可靠的不确定性估计。

Method: 开发了OPSA攻击方法，通过最大化模型不确定性削弱Conformal Prediction效率；提出OPSA-AT防御策略，将OPSA整合到新型Conformal训练框架中。

Result: 实验表明OPSA攻击比基线方法更能诱导不确定性，而OPSA-AT防御模型显著提升了对抗鲁棒性并保持可靠预测。

Conclusion: 该集成方法为安全关键领域开发可信赖且鲁棒的深度学习模型提供了有效途径。

Abstract: As deep learning models are increasingly deployed in high-risk applications,
robust defenses against adversarial attacks and reliable performance guarantees
become paramount. Moreover, accuracy alone does not provide sufficient
assurance or reliable uncertainty estimates for these models. This study
advances adversarial training by leveraging principles from Conformal
Prediction. Specifically, we develop an adversarial attack method, termed OPSA
(OPtimal Size Attack), designed to reduce the efficiency of conformal
prediction at any significance level by maximizing model uncertainty without
requiring coverage guarantees. Correspondingly, we introduce OPSA-AT
(Adversarial Training), a defense strategy that integrates OPSA within a novel
conformal training paradigm. Experimental evaluations demonstrate that our OPSA
attack method induces greater uncertainty compared to baseline approaches for
various defenses. Conversely, our OPSA-AT defensive model significantly
enhances robustness not only against OPSA but also other adversarial attacks,
and maintains reliable prediction. Our findings highlight the effectiveness of
this integrated approach for developing trustworthy and resilient deep learning
models for safety-critical domains. Our code is available at
https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.

</details>


### [425] [Identifiable Object Representations under Spatial Ambiguities](https://arxiv.org/abs/2506.07806)
*Avinash Kori,Francesca Toni,Ben Glocker*

Main category: cs.LG

TL;DR: 提出一种多视角概率方法，解决空间模糊性问题，无需视角标注，实现模块化对象中心表示。


<details>
  <summary>Details</summary>
Motivation: 人类推理需要模块化对象中心表示，但空间模糊性（如遮挡和视角模糊）使其难以实现。

Method: 采用多视角概率方法，聚合视角特定槽以捕捉不变内容信息，同时学习解耦的全局视角信息。

Result: 实验验证了方法的鲁棒性和可扩展性，解决了空间模糊性并提供可识别性理论保证。

Conclusion: 该方法在标准基准和新复杂数据集上表现优异，为模块化表示提供了实用解决方案。

Abstract: Modular object-centric representations are essential for *human-like
reasoning* but are challenging to obtain under spatial ambiguities, *e.g. due
to occlusions and view ambiguities*. However, addressing challenges presents
both theoretical and practical difficulties. We introduce a novel multi-view
probabilistic approach that aggregates view-specific slots to capture
*invariant content* information while simultaneously learning disentangled
global *viewpoint-level* information. Unlike prior single-view methods, our
approach resolves spatial ambiguities, provides theoretical guarantees for
identifiability, and requires *no viewpoint annotations*. Extensive experiments
on standard benchmarks and novel complex datasets validate our method's
robustness and scalability.

</details>


### [426] [Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation](https://arxiv.org/abs/2506.07822)
*Xintong Duan,Yutong He,Fahim Tajwar,Ruslan Salakhutdinov,J. Zico Kolter,Jeff Schneider*

Main category: cs.LG

TL;DR: 提出了一种新的离线强化学习一致性蒸馏方法，结合奖励优化，实现单步生成，性能更高且训练更简单。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在决策任务中表现优异，但推理速度慢；一致性模型虽能解决速度问题，但在决策任务中常因次优演示或多网络并发训练而受限。

Method: 将奖励优化直接融入一致性蒸馏过程，实现单步生成，简化训练。

Result: 在Gym MuJoCo基准测试和长时程规划中，性能提升8.7%，推理速度提升142倍。

Conclusion: 新方法在保持高性能的同时显著提升了推理速度，为决策任务提供了更高效的解决方案。

Abstract: Although diffusion models have achieved strong results in decision-making
tasks, their slow inference speed remains a key limitation. While the
consistency model offers a potential solution, its applications to
decision-making often struggle with suboptimal demonstrations or rely on
complex concurrent training of multiple networks. In this work, we propose a
novel approach to consistency distillation for offline reinforcement learning
that directly incorporates reward optimization into the distillation process.
Our method enables single-step generation while maintaining higher performance
and simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and
long horizon planning demonstrate that our approach can achieve an 8.7%
improvement over previous state-of-the-art while offering up to 142x speedup
over diffusion counterparts in inference time.

</details>


### [427] [Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information](https://arxiv.org/abs/2506.07829)
*Jan Corazza,Hadi Partovi Aria,Hyohun Kim,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 本文研究了在分散式多智能体强化学习（DMARL）中，通过提供高层符号知识来解决隐私、通信限制和性能问题，并扩展了理论工具以验证局部策略与团队任务的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现实问题常需多智能体协作，但DMARL中智能体独立学习可能导致策略兼容性问题，需解决隐私、通信和性能挑战。

Method: 引入高层符号知识，扩展理论工具验证局部策略与团队任务的兼容性，并利用符号知识加速学习过程。

Result: 符号知识显著加速了DMARL的学习过程，扩展的理论工具使分散式训练在更多场景中具备理论保障。

Conclusion: 高层符号知识能有效解决DMARL中的兼容性和学习效率问题，为多智能体协作提供了新思路。

Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a
single agent to accomplish a particular task. However, many real-world problems
require multiple agents to collaborate in order to achieve a common goal. For
example, a robot executing a task in a warehouse may require the assistance of
a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL
(DMARL), agents learn independently and then combine their policies at
execution time, but often must satisfy constraints on compatibility of local
policies to ensure that they can achieve the global task when combined. In this
paper, we study how providing high-level symbolic knowledge to agents can help
address unique challenges of this setting, such as privacy constraints,
communication limitations, and performance concerns. In particular, we extend
the formal tools used to check the compatibility of local policies with the
team task, making decentralized training with theoretical guarantees usable in
more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge
about the temporal evolution of events in the environment can significantly
expedite the learning process in DMARL.

</details>


### [428] [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833)
*Michael K. Chen,Xikun Zhang,Jiaxing Huang,Dacheng Tao*

Main category: cs.LG

TL;DR: 论文提出了一种名为CAFT的新方法，通过多令牌训练改进LLMs的概念学习能力，显著提升了任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的逐令牌预测范式限制了其对高层次概念的理解能力，阻碍了智能系统的进一步发展。

Method: 引入Concept-Aware Fine-Tuning (CAFT)，一种多令牌训练方法，支持跨令牌序列学习。

Result: 实验表明，CAFT在文本摘要和蛋白质设计等任务中显著优于传统方法。

Conclusion: CAFT首次将多令牌预测引入后训练阶段，为研究社区提供了更广泛的应用潜力。

Abstract: Large language models (LLMs) have become the cornerstone of modern AI.
However, the existing paradigm of next-token prediction fundamentally limits
their ability to form coherent, high-level concepts, making it a critical
barrier to human-like understanding and reasoning. Take the phrase "ribonucleic
acid" as an example: an LLM will first decompose it into tokens, i.e.,
artificial text fragments ("rib", "on", ...), then learn each token
sequentially, rather than grasping the phrase as a unified, coherent semantic
entity. This fragmented representation hinders deeper conceptual understanding
and, ultimately, the development of truly intelligent systems. In response, we
introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method
that redefines how LLMs are fine-tuned. By enabling the learning of sequences
that span multiple tokens, this method fosters stronger concept-aware learning.
Our experiments demonstrate significant improvements compared to conventional
next-token finetuning methods across diverse tasks, including traditional
applications like text summarization and domain-specific ones like de novo
protein design. Multi-token prediction was previously only possible in the
prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first
to bring the multi-token setting to the post-training phase, thus effectively
democratizing its benefits for the broader community of practitioners and
researchers. Finally, the unexpected effectiveness of our proposed method
suggests wider implications for the machine learning research community. All
code and data are available at https://github.com/michaelchen-lab/caft-llm

</details>


### [429] [Jarzynski Reweighting and Sampling Dynamics for Training Energy-Based Models: Theoretical Analysis of Different Transition Kernels](https://arxiv.org/abs/2506.07843)
*Davide Carbone*

Main category: cs.LG

TL;DR: 论文分析了Jarzynski重加权技术在训练基于能量的模型（EBMs）中的应用，探讨了核选择对模型性能的影响，并展示了其在流扩散模型和受限玻尔兹曼机中的效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如对比散度和分数匹配）在训练EBMs时存在偏差，影响学习准确性。研究旨在通过Jarzynski重加权技术解决这一问题。

Method: 理论分析了Jarzynski重加权技术，并应用于两种生成框架：流扩散模型和受限玻尔兹曼机。

Result: 研究揭示了核选择与模型性能的关系，Jarzynski重加权能减少偏差并提升样本质量。

Conclusion: Jarzynski重加权是一种有潜力的生成学习工具，可优化EBMs的训练效果。

Abstract: Energy-Based Models (EBMs) provide a flexible framework for generative
modeling, but their training remains theoretically challenging due to the need
to approximate normalization constants and efficiently sample from complex,
multi-modal distributions. Traditional methods, such as contrastive divergence
and score matching, introduce biases that can hinder accurate learning. In this
work, we present a theoretical analysis of Jarzynski reweighting, a technique
from non-equilibrium statistical mechanics, and its implications for training
EBMs. We focus on the role of the choice of the kernel and we illustrate these
theoretical considerations in two key generative frameworks: (i) flow-based
diffusion models, where we reinterpret Jarzynski reweighting in the context of
stochastic interpolants to mitigate discretization errors and improve sample
quality, and (ii) Restricted Boltzmann Machines, where we analyze its role in
correcting the biases of contrastive divergence. Our results provide insights
into the interplay between kernel choice and model performance, highlighting
the potential of Jarzynski reweighting as a principled tool for generative
learning.

</details>


### [430] [Residual Reweighted Conformal Prediction for Graph Neural Networks](https://arxiv.org/abs/2506.07854)
*Zheng Zhang,Jie Bao,Zhixin Zhou,Nicolo Colombo,Lixin Cheng,Rui Luo*

Main category: cs.LG

TL;DR: RR-GNN通过结合图结构和残差自适应方法，改进了传统共形预测的保守性，提供了更高效的预测区间。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测在图数据中因忽略图结构和异质性导致预测区间过于保守，RR-GNN旨在解决这一问题。

Method: RR-GNN采用图结构Mondrian共形预测、残差自适应非共形分数和交叉训练协议，优化预测区间。

Result: 在15个真实图数据任务中，RR-GNN在保持覆盖率的条件下优于现有方法。

Conclusion: RR-GNN为图数据提供了高效且可靠的预测区间，解决了传统方法的局限性。

Abstract: Graph Neural Networks (GNNs) excel at modeling relational data but face
significant challenges in high-stakes domains due to unquantified uncertainty.
Conformal prediction (CP) offers statistical coverage guarantees, but existing
methods often produce overly conservative prediction intervals that fail to
account for graph heteroscedasticity and structural biases. While residual
reweighting CP variants address some of these limitations, they neglect graph
topology, cluster-specific uncertainties, and risk data leakage by reusing
training sets. To address these issues, we propose Residual Reweighted GNN
(RR-GNN), a framework designed to generate minimal prediction sets with
provable marginal coverage guarantees.
  RR-GNN introduces three major innovations to enhance prediction performance.
First, it employs Graph-Structured Mondrian CP to partition nodes or edges into
communities based on topological features, ensuring cluster-conditional
coverage that reflects heterogeneity. Second, it uses Residual-Adaptive
Nonconformity Scores by training a secondary GNN on a held-out calibration set
to estimate task-specific residuals, dynamically adjusting prediction intervals
according to node or edge uncertainty. Third, it adopts a Cross-Training
Protocol, which alternates the optimization of the primary GNN and the residual
predictor to prevent information leakage while maintaining graph dependencies.
We validate RR-GNN on 15 real-world graphs across diverse tasks, including node
classification, regression, and edge weight prediction. Compared to CP
baselines, RR-GNN achieves improved efficiency over state-of-the-art methods,
with no loss of coverage.

</details>


### [431] [Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective](https://arxiv.org/abs/2506.07861)
*Firas Laakom,Haobo Chen,Jürgen Schmidhuber,Yuheng Bu*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架，通过信息论视角分析公平性泛化误差，并基于Efron-Stein不等式推导出紧致的公平性泛化界。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对公平性在训练和测试数据间泛化的理论保证，本文旨在填补这一空白。

Method: 使用信息论工具（如互信息和条件互信息）结合Efron-Stein不等式，提出新的公平性泛化界。

Result: 实验验证了所提界限的紧致性和实际相关性。

Conclusion: 该框架为设计提升公平性泛化的算法提供了理论指导。

Abstract: Despite substantial progress in promoting fairness in high-stake applications
using machine learning models, existing methods often modify the training
process, such as through regularizers or other interventions, but lack formal
guarantees that fairness achieved during training will generalize to unseen
data. Although overfitting with respect to prediction performance has been
extensively studied, overfitting in terms of fairness loss has received far
less attention. This paper proposes a theoretical framework for analyzing
fairness generalization error through an information-theoretic lens. Our novel
bounding technique is based on Efron-Stein inequality, which allows us to
derive tight information-theoretic fairness generalization bounds with both
Mutual Information (MI) and Conditional Mutual Information (CMI). Our empirical
results validate the tightness and practical relevance of these bounds across
diverse fairness-aware learning algorithms. Our framework offers valuable
insights to guide the design of algorithms improving fairness generalization.

</details>


### [432] [Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes](https://arxiv.org/abs/2506.07864)
*Mirko Paolo Barbato,Giorgia Rigamonti,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: 提出了一种轻量级序列Transformer模型，用于资源受限设备上的血糖预测，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决T1D管理中血糖预测模型在可穿戴设备上部署的计算和内存限制问题。

Method: 结合Transformer的注意力机制和RNN的序列处理能力，设计轻量级模型，优化部署并处理数据不平衡。

Result: 在OhioT1DM和DiaTrend数据集上表现优于现有方法，能有效预测血糖水平和检测不良事件。

Conclusion: 该模型填补了高性能建模与实际部署之间的空白，为T1D管理提供了高效可靠的解决方案。

Abstract: Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous
monitoring to prevent severe hypo- and hyperglycemic events. While continuous
glucose monitoring has improved blood glucose management, deploying predictive
models on wearable devices remains challenging due to computational and memory
constraints. To address this, we propose a novel Lightweight Sequential
Transformer model designed for blood glucose prediction in T1D. By integrating
the strengths of Transformers' attention mechanisms and the sequential
processing of recurrent neural networks, our architecture captures long-term
dependencies while maintaining computational efficiency. The model is optimized
for deployment on resource-constrained edge devices and incorporates a balanced
loss function to handle the inherent data imbalance in hypo- and hyperglycemic
events. Experiments on two benchmark datasets, OhioT1DM and DiaTrend,
demonstrate that the proposed model outperforms state-of-the-art methods in
predicting glucose levels and detecting adverse events. This work fills the gap
between high-performance modeling and practical deployment, providing a
reliable and efficient T1D management solution.

</details>


### [433] [Can Hessian-Based Insights Support Fault Diagnosis in Attention-based Models?](https://arxiv.org/abs/2506.07871)
*Sigma Jahan,Mohammad Masudur Rahman*

Main category: cs.LG

TL;DR: 本文通过Hessian矩阵分析诊断注意力模型的故障，发现其比梯度分析更有效。


<details>
  <summary>Details</summary>
Motivation: 随着注意力模型规模和复杂度的增加，诊断其故障变得更具挑战性，因此探索Hessian矩阵分析的潜力。

Method: 使用Hessian矩阵的曲率分析和参数交互分析，识别注意力机制中的脆弱区域和参数依赖关系。

Result: 实验表明，Hessian指标能更有效地定位不稳定性和故障源，优于梯度分析。

Conclusion: Hessian分析可显著改进复杂神经架构的故障诊断，提升软件调试实践。

Abstract: As attention-based deep learning models scale in size and complexity,
diagnosing their faults becomes increasingly challenging. In this work, we
conduct an empirical study to evaluate the potential of Hessian-based analysis
for diagnosing faults in attention-based models. Specifically, we use
Hessian-derived insights to identify fragile regions (via curvature analysis)
and parameter interdependencies (via parameter interaction analysis) within
attention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN,
DistilBERT), we show that Hessian-based metrics can localize instability and
pinpoint fault sources more effectively than gradients alone. Our empirical
findings suggest that these metrics could significantly improve fault diagnosis
in complex neural architectures, potentially improving software debugging
practices.

</details>


### [434] [Diffusion Counterfactual Generation with Semantic Abduction](https://arxiv.org/abs/2506.07883)
*Rajat Rasal,Avinash Kori,Fabio De Sousa Ribeiro,Tian Xia,Ben Glocker*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的反事实图像生成框架，通过引入空间、语义和动态反演概念，结合Pearl因果关系，实现了高保真度的图像编辑，并在语义控制和身份保留之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 现有自编码框架在反事实图像生成中存在可扩展性和保真度问题，而扩散模型在视觉质量和语义学习方面表现出色，因此探索如何利用扩散模型改进反事实图像编辑。

Method: 提出了一套基于扩散模型的因果机制，包括空间、语义和动态反演，并将语义表示通过Pearl因果关系整合到扩散模型中，实现反事实推理的图像编辑。

Result: 该框架首次在扩散反事实中考虑了高级语义身份保留，并展示了语义控制如何在忠实因果控制和身份保留之间实现原则性权衡。

Conclusion: 扩散模型结合因果推理为反事实图像生成提供了新的解决方案，显著提升了编辑质量和语义控制能力。

Abstract: Counterfactual image generation presents significant challenges, including
preserving identity, maintaining perceptual quality, and ensuring faithfulness
to an underlying causal model. While existing auto-encoding frameworks admit
semantic latent spaces which can be manipulated for causal control, they
struggle with scalability and fidelity. Advancements in diffusion models
present opportunities for improving counterfactual image editing, having
demonstrated state-of-the-art visual quality, human-aligned perception and
representation learning capabilities. Here, we present a suite of
diffusion-based causal mechanisms, introducing the notions of spatial, semantic
and dynamic abduction. We propose a general framework that integrates semantic
representations into diffusion models through the lens of Pearlian causality to
edit images via a counterfactual reasoning process. To our knowledge, this is
the first work to consider high-level semantic identity preservation for
diffusion counterfactuals and to demonstrate how semantic control enables
principled trade-offs between faithful causal control and identity
preservation.

</details>


### [435] [Schauder Bases for $C[0, 1]$ Using ReLU, Softplus and Two Sigmoidal Functions](https://arxiv.org/abs/2506.07884)
*Anand Ganesh,Babhrubahan Bose,Anand Rajagopalan*

Main category: cs.LG

TL;DR: 论文构建了四种Schauder基，用于空间$C[0,1]$，分别基于ReLU、Softplus及其sigmoidal变体，首次证明了这些函数基的存在性，并改进了其通用逼近性质。


<details>
  <summary>Details</summary>
Motivation: 探索ReLU、Softplus及其sigmoidal变体在函数空间$C[0,1]$中作为Schauder基的可行性，填补相关研究空白。

Method: 构造四种Schauder基，分别基于ReLU、Softplus及其sigmoidal版本，分析其性质。

Result: 首次证明了这些函数基的存在性，并提升了其通用逼近能力。

Conclusion: 研究为函数空间提供了新的基函数选择，扩展了相关理论的应用范围。

Abstract: We construct four Schauder bases for the space $C[0,1]$, one using ReLU
functions, another using Softplus functions, and two more using sigmoidal
versions of the ReLU and Softplus functions. This establishes the existence of
a basis using these functions for the first time, and improves on the universal
approximation property associated with them.

</details>


### [436] [FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling](https://arxiv.org/abs/2506.07902)
*Sifan Wang,Zehao Dou,Tong-Rui Liu,Lu Lu*

Main category: cs.LG

TL;DR: FunDiff是一个用于函数空间的生成建模框架，结合了潜在扩散过程和函数自编码器架构，能够处理不同离散化的输入函数，生成可任意位置评估的连续函数，并融入物理先验。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在离散数据（如图像和视频）合成方面取得了显著成功，但在物理应用中适应这些模型仍具挑战性，因为关注的量是受复杂物理定律支配的连续函数。

Method: FunDiff结合潜在扩散过程和函数自编码器架构，通过架构约束或物理信息损失函数融入物理先验。

Result: 理论证明扩散基估计器在函数空间中实现了最优收敛率，实验表明FunDiff在流体动力学和固体力学中生成物理一致的样本，且对噪声和低分辨率数据具有鲁棒性。

Conclusion: FunDiff为函数空间中的生成建模提供了一种有效且物理一致的方法，适用于多种物理应用。

Abstract: Recent advances in generative modeling -- particularly diffusion models and
flow matching -- have achieved remarkable success in synthesizing discrete data
such as images and videos. However, adapting these models to physical
applications remains challenging, as the quantities of interest are continuous
functions governed by complex physical laws. Here, we introduce
$\textbf{FunDiff}$, a novel framework for generative modeling in function
spaces. FunDiff combines a latent diffusion process with a function autoencoder
architecture to handle input functions with varying discretizations, generate
continuous functions evaluable at arbitrary locations, and seamlessly
incorporate physical priors. These priors are enforced through architectural
constraints or physics-informed loss functions, ensuring that generated samples
satisfy fundamental physical laws. We theoretically establish minimax
optimality guarantees for density estimation in function spaces, showing that
diffusion-based estimators achieve optimal convergence rates under suitable
regularity conditions. We demonstrate the practical effectiveness of FunDiff
across diverse applications in fluid dynamics and solid mechanics. Empirical
results show that our method generates physically consistent samples with high
fidelity to the target distribution and exhibits robustness to noisy and
low-resolution data. Code and datasets are publicly available at
https://github.com/sifanexisted/fundiff.

</details>


### [437] [Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces](https://arxiv.org/abs/2506.07903)
*Kevin Rojas,Yuchen Zhu,Sichen Zhu,Felix X. -F. Ye,Molei Tao*

Main category: cs.LG

TL;DR: 论文提出了一种新的多模态扩散模型框架，支持跨模态的原生生成，无需依赖外部预处理协议。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部预处理协议（如分词器和变分自编码器）统一数据表示，但这对数据有限的应用存在问题。

Method: 提出了一种在任意状态空间上构建多模态扩散模型的新框架，并引入了创新的解耦噪声调度策略。

Result: 在文本-图像生成和混合类型表格数据合成任务中表现优异。

Conclusion: 该方法能够同时支持无条件生成和模态条件生成，具有竞争力。

Abstract: Diffusion models have demonstrated remarkable performance in generating
unimodal data across various tasks, including image, video, and text
generation. On the contrary, the joint generation of multimodal data through
diffusion models is still in the early stages of exploration. Existing
approaches heavily rely on external preprocessing protocols, such as tokenizers
and variational autoencoders, to harmonize varied data representations into a
unified, unimodal format. This process heavily demands the high accuracy of
encoders and decoders, which can be problematic for applications with limited
data. To lift this restriction, we propose a novel framework for building
multimodal diffusion models on arbitrary state spaces, enabling native
generation of coupled data across different modalities. By introducing an
innovative decoupled noise schedule for each modality, we enable both
unconditional and modality-conditioned generation within a single model
simultaneously. We empirically validate our approach for text-image generation
and mixed-type tabular data synthesis, demonstrating that it achieves
competitive performance.

</details>


### [438] [CausalPFN: Amortized Causal Effect Estimation via In-Context Learning](https://arxiv.org/abs/2506.07918)
*Vahid Balazadeh,Hamidreza Kamkari,Valentin Thomas,Benson Li,Junwei Ma,Jesse C. Cresswell,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: CausalPFN是一个基于Transformer的模型，用于从观测数据中估计因果效应，无需手动调整或领域专业知识。


<details>
  <summary>Details</summary>
Motivation: 因果效应估计在多个应用中至关重要，但现有方法需要大量手动工作和专业知识。

Method: CausalPFN通过在大规模模拟数据上训练，直接映射观测数据到因果效应，结合贝叶斯因果推断和先验拟合网络技术。

Result: 在IHDP、Lalonde和ACIC等基准测试中表现优异，并在实际政策制定任务中具有竞争力。

Conclusion: CausalPFN提供了一种无需额外训练或调优的自动化因果推断解决方案，支持可靠的决策制定。

Abstract: Causal effect estimation from observational data is fundamental across
various applications. However, selecting an appropriate estimator from dozens
of specialized methods demands substantial manual effort and domain expertise.
We present CausalPFN, a single transformer that amortizes this workflow:
trained once on a large library of simulated data-generating processes that
satisfy ignorability, it infers causal effects for new observational datasets
out-of-the-box. CausalPFN combines ideas from Bayesian causal inference with
the large-scale training protocol of prior-fitted networks (PFNs), learning to
map raw observations directly to causal effects without any task-specific
adjustment. Our approach achieves superior average performance on heterogeneous
and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC).
Moreover, it shows competitive performance for real-world policy making on
uplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to
support reliable decision-making based on Bayesian principles. This
ready-to-use model does not require any further training or tuning and takes a
step toward automated causal inference (https://github.com/vdblm/CausalPFN).

</details>


### [439] [Uncovering the Functional Roles of Nonlinearity in Memory](https://arxiv.org/abs/2506.07919)
*Manuel Brenner,Georgia Koppe*

Main category: cs.LG

TL;DR: 研究发现，在序列建模任务中，最小非线性通常足够且最优，简化模型并提高鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 探讨非线性在循环网络中的功能角色，明确其计算必要性和机制。

Method: 使用几乎线性循环神经网络（AL-RNNs）作为建模工具和机制探针。

Result: 最小非线性在多种任务中表现最优，模型更简单、鲁棒且可解释。

Conclusion: 为选择性引入非线性提供原则性框架，连接动力学系统理论与循环网络的功能需求。

Abstract: Memory and long-range temporal processing are core requirements for sequence
modeling tasks across natural language processing, time-series forecasting,
speech recognition, and control. While nonlinear recurrence has long been
viewed as essential for enabling such mechanisms, recent work suggests that
linear dynamics may often suffice. In this study, we go beyond performance
comparisons to systematically dissect the functional role of nonlinearity in
recurrent networks--identifying both when it is computationally necessary, and
what mechanisms it enables. We use Almost Linear Recurrent Neural Networks
(AL-RNNs), which allow fine-grained control over nonlinearity, as both a
flexible modeling tool and a probe into the internal mechanisms of memory.
Across a range of classic sequence modeling tasks and a real-world stimulus
selection task, we find that minimal nonlinearity is not only sufficient but
often optimal, yielding models that are simpler, more robust, and more
interpretable than their fully nonlinear or linear counterparts. Our results
provide a principled framework for selectively introducing nonlinearity,
bridging dynamical systems theory with the functional demands of long-range
memory and structured computation in recurrent neural networks, with
implications for both artificial and biological neural systems.

</details>


### [440] [W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling](https://arxiv.org/abs/2506.07920)
*Hossein Babaei,Mel White,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 论文提出了一种新的状态空间模型（SSM）变体W4S4，基于冗余小波框架构建，具有稳定对角化和高效核计算能力，优于现有HiPPO-based SSMs。


<details>
  <summary>Details</summary>
Motivation: 现有SSMs的性能高度依赖于状态矩阵的选择和初始化，需要一种更高效且理论支持的方法。

Method: 基于SaFARi框架和WaLRUS SSMs，构建了W4S4模型，利用冗余小波框架实现稳定对角化和快速核计算。

Result: W4S4在延迟重建任务、分类基准和长序列建模中表现优于HiPPO-based SSMs。

Conclusion: W4S4为下一代基于SSM的深度模型提供了可扩展且多功能的基础。

Abstract: State Space Models (SSMs) have emerged as powerful components for sequence
modeling, enabling efficient handling of long-range dependencies via linear
recurrence and convolutional computation. However, their effectiveness depends
heavily on the choice and initialization of the state matrix. In this work, we
build on the SaFARi framework and existing WaLRUS SSMs to introduce a new
variant, W4S4 (WaLRUS for S4), a new class of SSMs constructed from redundant
wavelet frames. WaLRUS admits a stable diagonalization and supports fast kernel
computation without requiring low-rank approximations, making it both
theoretically grounded and computationally efficient. We show that WaLRUS
retains information over long horizons significantly better than HiPPO-based
SSMs, both in isolation and when integrated into deep architectures such as S4.
Our experiments demonstrate consistent improvements across delay reconstruction
tasks, classification benchmarks, and long-range sequence modeling, confirming
that high-quality, structured initialization enabled by wavelet-based state
dynamic offers substantial advantages over existing alternatives. WaLRUS
provides a scalable and versatile foundation for the next generation of deep
SSM-based models.

</details>


### [441] [A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle](https://arxiv.org/abs/2506.07929)
*Amirreza Yasami,Mohammadali Tofigh,Mahdi Shahbakhti,Charles Robert Koch*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的强化学习方法（PIESMC），用于高效构建代表性驾驶循环，显著降低计算成本并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 精确的驾驶循环构建对车辆设计、燃油经济性分析和环境影响评估至关重要，但现有方法在动态捕捉和计算效率上存在不足。

Method: 采用物理信息强化学习框架结合蒙特卡洛采样（PIESMC），捕捉瞬态动态、加减速、怠速和坡度变化，确保模型保真度。

Result: 实验表明，PIESMC在关键运动学和能量指标上表现优异，比MTB方法减少57.3%的累积误差，比MCB方法减少10.5%，且计算速度快一个数量级。

Conclusion: PIESMC能高效构建高保真驾驶循环，适用于实际应用，显著优于传统方法。

Abstract: Accurate driving cycle construction is crucial for vehicle design, fuel
economy analysis, and environmental impact assessments. A generative
Physics-Informed Expected SARSA-Monte Carlo (PIESMC) approach that constructs
representative driving cycles by capturing transient dynamics, acceleration,
deceleration, idling, and road grade transitions while ensuring model fidelity
is introduced. Leveraging a physics-informed reinforcement learning framework
with Monte Carlo sampling, PIESMC delivers efficient cycle construction with
reduced computational cost. Experimental evaluations on two real-world datasets
demonstrate that PIESMC replicates key kinematic and energy metrics, achieving
up to a 57.3% reduction in cumulative kinematic fragment errors compared to the
Micro-trip-based (MTB) method and a 10.5% reduction relative to the
Markov-chain-based (MCB) method. Moreover, it is nearly an order of magnitude
faster than conventional techniques. Analyses of vehicle-specific power
distributions and wavelet-transformed frequency content further confirm its
ability to reproduce experimental central tendencies and variability.

</details>


### [442] [Ensemble-Based Survival Models with the Self-Attended Beran Estimator Predictions](https://arxiv.org/abs/2506.07933)
*Lev V. Utkin,Semen P. Khomets,Vlada A. Efremenko,Andrei V. Konstantinov,Natalya M. Verbova*

Main category: cs.LG

TL;DR: 提出了一种名为SurvBESA的新集成模型，结合Beran估计器和自注意力机制，用于生存分析，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统集成模型（如随机生存森林和梯度提升）因自举样本变化导致预测不稳定的问题。

Method: 结合Beran估计器和自注意力机制，通过调整相邻生存函数的相似性来平滑噪声。还探索了使用Huber污染模型定义注意力权重的特例。

Result: 数值实验表明SurvBESA优于现有最先进模型。

Conclusion: SurvBESA通过自注意力机制有效提升了生存分析的预测稳定性，且实现已公开。

Abstract: Survival analysis predicts the time until an event of interest, such as
failure or death, but faces challenges due to censored data, where some events
remain unobserved. Ensemble-based models, like random survival forests and
gradient boosting, are widely used but can produce unstable predictions due to
variations in bootstrap samples. To address this, we propose SurvBESA (Survival
Beran Estimators Self-Attended), a novel ensemble model that combines Beran
estimators with a self-attention mechanism. Unlike traditional methods,
SurvBESA applies self-attention to predicted survival functions, smoothing out
noise by adjusting each survival function based on its similarity to
neighboring survival functions. We also explore a special case using Huber's
contamination model to define attention weights, simplifying training to a
quadratic or linear optimization problem. Numerical experiments show that
SurvBESA outperforms state-of-the-art models. The implementation of SurvBESA is
publicly available.

</details>


### [443] [TokenBreak: Bypassing Text Classification Models Through Token Manipulation](https://arxiv.org/abs/2506.07948)
*Kasimir Schulz,Kenneth Yeung,Kieran Evans*

Main category: cs.LG

TL;DR: 本文介绍了TokenBreak，一种利用分词策略绕过文本分类保护模型的新型攻击方法，并提出了一种无需重新训练模型的防御策略。


<details>
  <summary>Details</summary>
Motivation: 现有的文本分类保护模型（如用于防止LLM提示注入攻击或垃圾邮件的模型）可能因分词策略的漏洞而被绕过，导致目标系统仍易受攻击。

Method: 提出TokenBreak攻击技术，通过操纵输入文本使模型分类错误，同时确保目标系统仍能理解并响应被操纵的文本。

Result: TokenBreak能够成功绕过保护模型，且攻击效果可通过模型家族预测。同时提出了一种无需重新训练的保护策略。

Conclusion: 分词策略的漏洞可能导致保护模型失效，但通过额外的防御层可以缓解这一问题。

Abstract: Natural Language Processing (NLP) models are used for text-related tasks such
as classification and generation. To complete these tasks, input data is first
tokenized from human-readable text into a format the model can understand,
enabling it to make inferences and understand context. Text classification
models can be implemented to guard against threats such as prompt injection
attacks against Large Language Models (LLMs), toxic input and cybersecurity
risks such as spam emails. In this paper, we introduce TokenBreak: a novel
attack that can bypass these protection models by taking advantage of the
tokenization strategy they use. This attack technique manipulates input text in
such a way that certain models give an incorrect classification. Importantly,
the end target (LLM or email recipient) can still understand and respond to the
manipulated text and therefore be vulnerable to the very attack the protection
model was put in place to prevent. The tokenizer is tied to model architecture,
meaning it is possible to predict whether or not a model is vulnerable to
attack based on family. We also present a defensive strategy as an added layer
of protection that can be implemented without having to retrain the defensive
model.

</details>


### [444] [Cost-Optimal Active AI Model Evaluation](https://arxiv.org/abs/2506.07949)
*Anastasios N. Angelopoulos,Jacob Eisenstein,Jonathan Berant,Alekh Agarwal,Adam Fisch*

Main category: cs.LG

TL;DR: 论文提出了一种成本感知的方法，用于在生成式AI系统中平衡使用低成本但可能不准确的弱评分者（如模型自动评分）和高成本但更准确的强评分者（如人工评分），以在有限预算下最大化统计效率。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统的开发需要持续评估和数据标注，成本高昂。快速迭代中常依赖低成本但可能有偏的合成数据，需平衡成本与准确性。

Method: 基于主动和预测驱动的统计推断，提出成本最优策略，分配标注预算以最大化统计效率。

Result: 在任务难度差异大的情况下，新策略能以更低预算达到与传统方法相同的估计精度。

Conclusion: 该方法在特定条件下显著降低标注成本，为生成式AI系统评估提供高效解决方案。

Abstract: The development lifecycle of generative AI systems requires continual
evaluation, data acquisition, and annotation, which is costly in both resources
and time. In practice, rapid iteration often makes it necessary to rely on
synthetic annotation data because of the low cost, despite the potential for
substantial bias. In this paper, we develop novel, cost-aware methods for
actively balancing the use of a cheap, but often inaccurate, weak rater -- such
as a model-based autorater that is designed to automatically assess the quality
of generated content -- with a more expensive, but also more accurate, strong
rater alternative such as a human. More specifically, the goal of our approach
is to produce a low variance, unbiased estimate of the mean of the target
"strong" rating, subject to some total annotation budget. Building on recent
work in active and prediction-powered statistical inference, we derive a family
of cost-optimal policies for allocating a given annotation budget between weak
and strong raters so as to maximize statistical efficiency. Using synthetic and
real-world data, we empirically characterize the conditions under which these
policies yield improvements over prior methods. We find that, especially in
tasks where there is high variability in the difficulty of examples, our
policies can achieve the same estimation precision at a far lower total
annotation budget than standard evaluation methods.

</details>


### [445] [Neural Tangent Kernel Analysis to Probe Convergence in Physics-informed Neural Solvers: PIKANs vs. PINNs](https://arxiv.org/abs/2506.07958)
*Salah A. Faroughi,Farinaz Mostajeran*

Main category: cs.LG

TL;DR: 该论文通过神经切线核（NTK）理论分析了Chebyshev-based物理信息Kolmogorov-Arnold网络（cPIKANs）的训练动态和收敛行为，揭示了其学习效率与核结构演化的关系。


<details>
  <summary>Details</summary>
Motivation: cPIKANs在求解偏微分方程（PDEs）方面表现出潜力，但其训练动态和收敛行为尚未被充分研究。本文旨在通过NTK理论填补这一空白。

Method: 作者首先推导了标准cKANs在监督学习中的NTK，并将其扩展到物理信息背景下。分析了四种代表性PDEs的NTK矩阵谱特性，并研究了不同优化策略的影响。

Result: 研究发现cPIKANs的NTK行为可预测，其学习动态优于标准物理信息神经网络（PINNs）。谱分析还揭示了域分解对训练的改善作用。

Conclusion: 这是首次对cPIKANs进行系统的NTK研究，为理解其性能提供了理论依据。

Abstract: Physics-informed Kolmogorov-Arnold Networks (PIKANs), and in particular their
Chebyshev-based variants (cPIKANs), have recently emerged as promising models
for solving partial differential equations (PDEs). However, their training
dynamics and convergence behavior remain largely unexplored both theoretically
and numerically. In this work, we aim to advance the theoretical understanding
of cPIKANs by analyzing them using Neural Tangent Kernel (NTK) theory. Our
objective is to discern the evolution of kernel structure throughout
gradient-based training and its subsequent impact on learning efficiency. We
first derive the NTK of standard cKANs in a supervised setting, and then extend
the analysis to the physics-informed context. We analyze the spectral
properties of NTK matrices, specifically their eigenvalue distributions and
spectral bias, for four representative PDEs: the steady-state Helmholtz
equation, transient diffusion and Allen-Cahn equations, and forced vibrations
governed by the Euler-Bernoulli beam equation. We also conduct an investigation
into the impact of various optimization strategies, e.g., first-order,
second-order, and hybrid approaches, on the evolution of the NTK and the
resulting learning dynamics. Results indicate a tractable behavior for NTK in
the context of cPIKANs, which exposes learning dynamics that standard
physics-informed neural networks (PINNs) cannot capture. Spectral trends also
reveal when domain decomposition improves training, directly linking kernel
behavior to convergence rates under different setups. To the best of our
knowledge, this is the first systematic NTK study of cPIKANs, providing
theoretical insight that clarifies and predicts their empirical performance.

</details>


### [446] [A Two-Phase Deep Learning Framework for Adaptive Time-Stepping in High-Speed Flow Modeling](https://arxiv.org/abs/2506.07969)
*Jacob Helwig,Sai Sreeharsha Adavi,Xuan Zhang,Yuchao Lin,Felix S. Chim,Luke Takeshi Vizzini,Haiyang Yu,Muhammad Hasnain,Saykat Kumar Biswas,John J. Holloway,Narendra Singh,N. K. Anand,Swagnik Guhathakurta,Shuiwang Ji*

Main category: cs.LG

TL;DR: 论文提出了一种名为ShockCast的两阶段机器学习方法，用于模拟高速流体流动，并采用自适应时间步长技术。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注低速流体流动，而高速流动（如接近或超过音速）会出现激波等突变现象，需要自适应时间步长技术以平衡计算成本和分辨率。

Method: ShockCast分为两阶段：第一阶段用机器学习模型预测时间步长；第二阶段将预测的时间步长与当前流体场结合，推进系统状态。方法结合了神经ODE和混合专家模型的思路。

Result: 论文生成了两个超音速流动数据集，并公开了代码。

Conclusion: ShockCast是首个用于高速流动学习的框架，通过自适应时间步长有效解决了高速流动建模问题。

Abstract: We consider the problem of modeling high-speed flows using machine learning
methods. While most prior studies focus on low-speed fluid flows in which
uniform time-stepping is practical, flows approaching and exceeding the speed
of sound exhibit sudden changes such as shock waves. In such cases, it is
essential to use adaptive time-stepping methods to allow a temporal resolution
sufficient to resolve these phenomena while simultaneously balancing
computational costs. Here, we propose a two-phase machine learning method,
known as ShockCast, to model high-speed flows with adaptive time-stepping. In
the first phase, we propose to employ a machine learning model to predict the
timestep size. In the second phase, the predicted timestep is used as an input
along with the current fluid fields to advance the system state by the
predicted timestep. We explore several physically-motivated components for
timestep prediction and introduce timestep conditioning strategies inspired by
neural ODE and Mixture of Experts. As ShockCast is the first framework for
learning high-speed flows, we evaluate our methods by generating two supersonic
flow datasets, available at https://huggingface.co/datasets/divelab. Our code
is publicly available as part of the AIRS library
(https://github.com/divelab/AIRS).

</details>


### [447] [HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization](https://arxiv.org/abs/2506.07972)
*Hongzheng Chen,Yingheng Wang,Yaohui Cai,Hins Hu,Jiajie Li,Shirley Huang,Chenhui Deng,Rongjian Liang,Shufeng Kong,Haoxing Ren,Samitha Samaranayake,Carla P. Gomes,Zhiru Zhang*

Main category: cs.LG

TL;DR: 论文提出HeuriGym框架，用于评估LLM在组合优化问题中的启发式算法生成能力，揭示了当前模型在工具使用、规划和适应性推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法充分评估LLM的能力，要么依赖封闭式问题易饱和，要么主观比较缺乏一致性。

Method: 引入HeuriGym框架，让LLM提出启发式算法并通过代码执行反馈迭代优化，提出QYI指标量化性能。

Result: 测试9个顶级模型，QYI得分最高仅0.6，远低于专家基线1，显示模型在复杂问题解决中的不足。

Conclusion: 开源基准旨在推动LLM在科学和工程领域更有效和实际的问题解决能力发展。

Abstract: While Large Language Models (LLMs) have demonstrated significant advancements
in reasoning and agent-based problem-solving, current evaluation methodologies
fail to adequately assess their capabilities: existing benchmarks either rely
on closed-ended questions prone to saturation and memorization, or subjective
comparisons that lack consistency and rigor. In this work, we introduce
HeuriGym, an agentic framework designed for evaluating heuristic algorithms
generated by LLMs for combinatorial optimization problems, characterized by
clearly defined objectives and expansive solution spaces. HeuriGym empowers
LLMs to propose heuristics, receive evaluative feedback via code execution, and
iteratively refine their solutions. We evaluate nine state-of-the-art models on
nine problems across domains such as computer systems, logistics, and biology,
exposing persistent limitations in tool use, planning, and adaptive reasoning.
To quantify performance, we propose the Quality-Yield Index (QYI), a metric
that captures both solution pass rate and quality. Even top models like
GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below
the expert baseline of 1. Our open-source benchmark aims to guide the
development of LLMs toward more effective and realistic problem-solving in
scientific and engineering domains.

</details>


### [448] [Hyperpruning: Efficient Search through Pruned Variants of Recurrent Neural Networks Leveraging Lyapunov Spectrum](https://arxiv.org/abs/2506.07975)
*Caleb Zheng,Eli Shlizerman*

Main category: cs.LG

TL;DR: 论文提出了一种基于Lyapunov Spectrum（LS）距离度量的高效超剪枝框架LSH，显著减少了搜索时间，并在实验中优于传统方法和密集模型。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法缺乏早期性能保证，且计算成本高，因此需要一种能早期预测剪枝后性能的方法。

Method: 提出LS距离度量，结合超参数优化算法，构建LSH框架，快速找到最优剪枝策略。

Result: 在多个架构和数据集上，LSH在固定训练预算下找到的剪枝模型性能优于传统方法和密集模型。

Conclusion: LSH为网络剪枝提供了一种高效且性能优越的新方法。

Abstract: A variety of pruning methods have been introduced for over-parameterized
Recurrent Neural Networks to improve efficiency in terms of power consumption
and storage utilization. These advances motivate a new paradigm, termed
`hyperpruning', which seeks to identify the most suitable pruning strategy for
a given network architecture and application. Unlike conventional
hyperparameter search, where the optimal configuration's accuracy remains
uncertain, in the context of network pruning, the accuracy of the dense model
sets the target for the accuracy of the pruned one. The goal, therefore, is to
discover pruned variants that match or even surpass this established accuracy.
However, exhaustive search over pruning configurations is computationally
expensive and lacks early performance guarantees. To address this challenge, we
propose a novel Lyapunov Spectrum (LS)-based distance metric that enables early
comparison between pruned and dense networks, allowing accurate prediction of
post-training performance. By integrating this LS-based distance with standard
hyperparameter optimization algorithms, we introduce an efficient hyperpruning
framework, termed LS-based Hyperpruning (LSH). LSH reduces search time by an
order of magnitude compared to conventional approaches relying on full
training. Experiments on stacked LSTM and RHN architectures using the Penn
Treebank dataset, and on AWD-LSTM-MoS using WikiText-2, demonstrate that under
fixed training budgets and target pruning ratios, LSH consistently identifies
superior pruned models. Remarkably, these pruned variants not only outperform
those selected by loss-based baseline but also exceed the performance of their
dense counterpart.

</details>


### [449] [Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction](https://arxiv.org/abs/2506.07976)
*Junhong Shen,Hao Bai,Lunjun Zhang,Yifei Zhou,Amrith Setlur,Shengbang Tong,Diego Caples,Nan Jiang,Tong Zhang,Ameet Talwalkar,Aviral Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种新的测试时间扩展维度——交互扩展，通过增加代理的交互范围来提升其适应性，并在网络代理领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前测试时间扩展范式依赖于生成长推理轨迹，但无法让代理从环境中获取新信息或随时间调整行为。因此，研究探索了交互扩展的潜力。

Method: 提出了TTI（测试时间交互），一种基于课程的在线强化学习方法，通过动态调整代理的交互范围来训练代理。

Result: TTI在网络代理基准测试（WebVoyager和WebArena）中取得了最先进的性能，并展示了代理在探索与利用之间的自适应平衡能力。

Conclusion: 交互扩展是计算扩展的有力补充，为训练自适应代理提供了新途径。

Abstract: The current paradigm of test-time scaling relies on generating long reasoning
traces ("thinking" more) before producing a response. In agent problems that
require interaction, this can be done by generating thinking traces before
acting in the world. However, this process does not allow agents to acquire new
information from the environment or adapt their behavior over time. In this
work, we propose to scale test-time interaction, an untapped dimension of
test-time scaling that increases the agent's interaction horizon to enable
running rich behaviors such as exploration, backtracking, and dynamic
re-planning within a single rollout. To demonstrate the promise of this scaling
dimension, we study the domain of web agents. We first show that even
prompting-based interaction scaling without any training can improve task
success on web benchmarks non-trivially. Building on this, we introduce TTI
(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)
approach that trains agents by adaptively adjusting their rollout lengths.
Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data
web agents on WebVoyager and WebArena benchmarks. We further show that TTI
enables agents to balance exploration and exploitation adaptively. Our results
establish interaction scaling as a powerful, complementary axis to scaling
per-step compute, offering new avenues for training adaptive agents.

</details>


### [450] [Realistic Urban Traffic Generator using Decentralized Federated Learning for the SUMO simulator](https://arxiv.org/abs/2506.07980)
*Alberto Bazán-Guillén,Carlos Beis-Penedo,Diego Cajaraville-Aboy,Pablo Barbecho-Bautista,Rebeca P. Díaz-Redondo,Luis J. de la Cruz Llopis,Ana Fernández-Vilas,Mónica Aguilar Igartua,Manuel Fernández-Veiga*

Main category: cs.LG

TL;DR: DesRUTGe是一个基于去中心化联邦学习和深度强化学习的框架，用于生成高保真、时间变化的城市交通模式，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在准确性、可扩展性或隐私保护方面存在不足，无法满足大规模城市交通模拟的需求。

Method: 结合深度强化学习与SUMO模拟器，采用去中心化联邦学习，每个交通检测器作为独立节点，通过局部训练和参数交换优化模型。

Result: 在巴塞罗那的真实数据测试中，DesRUTGe在准确性和隐私保护方面优于RouteSampler和其他集中式学习方法。

Conclusion: DesRUTGe为城市规划和智能交通系统提供了一种高效、隐私保护的交通模拟解决方案。

Abstract: Realistic urban traffic simulation is essential for sustainable urban
planning and the development of intelligent transportation systems. However,
generating high-fidelity, time-varying traffic profiles that accurately reflect
real-world conditions, especially in large-scale scenarios, remains a major
challenge. Existing methods often suffer from limitations in accuracy,
scalability, or raise privacy concerns due to centralized data processing. This
work introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a
novel framework that integrates Deep Reinforcement Learning (DRL) agents with
the SUMO simulator to generate realistic 24-hour traffic patterns. A key
innovation of DesRUTGe is its use of Decentralized Federated Learning (DFL),
wherein each traffic detector and its corresponding urban zone function as an
independent learning node. These nodes train local DRL models using minimal
historical data and collaboratively refine their performance by exchanging
model parameters with selected peers (e.g., geographically adjacent zones),
without requiring a central coordinator. Evaluated using real-world data from
the city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as
RouteSampler, as well as other centralized learning approaches, by delivering
more accurate and privacy-preserving traffic pattern generation.

</details>


### [451] [Generative Modeling of Weights: Generalization or Memorization?](https://arxiv.org/abs/2506.07998)
*Boya Zeng,Yida Yin,Zhiqiu Xu,Zhuang Liu*

Main category: cs.LG

TL;DR: 研究发现，当前生成模型在生成神经网络权重时主要通过记忆训练数据，而非真正创新，且性能不如简单基线方法。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型在合成高性能神经网络权重方面的能力，并评估其是否能够生成新颖的权重。

Method: 研究了四种代表性方法，分析其生成权重的创新性和性能，并与简单基线方法（如添加噪声或权重集成）进行比较。

Result: 发现当前方法主要通过记忆训练数据生成权重，无法超越简单基线方法，且通过调整建模因素或数据增强无法有效缓解记忆问题。

Conclusion: 研究揭示了生成模型在新领域的局限性，强调了对生成模型评估的谨慎需求。

Abstract: Generative models, with their success in image and video generation, have
recently been explored for synthesizing effective neural network weights. These
approaches take trained neural network checkpoints as training data, and aim to
generate high-performing neural network weights during inference. In this work,
we examine four representative methods on their ability to generate novel model
weights, i.e., weights that are different from the checkpoints seen during
training. Surprisingly, we find that these methods synthesize weights largely
by memorization: they produce either replicas, or at best simple
interpolations, of the training checkpoints. Current methods fail to outperform
simple baselines, such as adding noise to the weights or taking a simple weight
ensemble, in obtaining different and simultaneously high-performing models. We
further show that this memorization cannot be effectively mitigated by
modifying modeling factors commonly associated with memorization in image
diffusion models, or applying data augmentations. Our findings provide a
realistic assessment of what types of data current generative models can model,
and highlight the need for more careful evaluation of generative models in new
domains. Our code is available at
https://github.com/boyazeng/weight_memorization.

</details>


### [452] [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
*Zeju Qiu,Simon Buchholz,Tim Z. Xiao,Maximilian Dax,Bernhard Schölkopf,Weiyang Liu*

Main category: cs.LG

TL;DR: POET是一种新型的重新参数化训练算法，通过正交等价变换优化神经元，提高大语言模型的训练效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的训练效果和稳定性是当前人工智能领域的重要挑战。

Method: POET通过将每个神经元重新参数化为两个可学习的正交矩阵和一个固定的随机权重矩阵，保持权重矩阵的谱特性，从而稳定优化目标函数。

Result: 实验验证了POET在训练大语言模型中的有效性和可扩展性。

Conclusion: POET为大规模神经网络的训练提供了一种高效且稳定的解决方案。

Abstract: While large language models (LLMs) are driving the rapid advancement of
artificial intelligence, effectively and reliably training these large models
remains one of the field's most significant challenges. To address this
challenge, we propose POET, a novel reParameterized training algorithm that
uses Orthogonal Equivalence Transformation to optimize neurons. Specifically,
POET reparameterizes each neuron with two learnable orthogonal matrices and a
fixed random weight matrix. Because of its provable preservation of spectral
properties of weight matrices, POET can stably optimize the objective function
with improved generalization. We further develop efficient approximations that
make POET flexible and scalable for training large-scale neural networks.
Extensive experiments validate the effectiveness and scalability of POET in
training LLMs.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [453] [Infinite Time Turing Machines and their Applications](https://arxiv.org/abs/2506.05351)
*Rukmal Weerawarana,Maxwell Braun*

Main category: cs.CC

TL;DR: 本文通过无限时间图灵机（ITTMs）为深度学习系统建立了严格的理论基础，揭示了Transformer等现代架构在可扩展性、效率和可解释性上的根本限制，并提出了一种新的计算范式——通用状态机（USM）。


<details>
  <summary>Details</summary>
Motivation: 为深度学习系统提供理论支持，解决现有模型在可扩展性、效率和可解释性上的不足。

Method: 利用无限时间图灵机（ITTMs）分析现代架构，提出动态可查询计算图的通用状态机（USM）。

Result: USM克服了现有模型的低效性和刚性，为可扩展、通用的AI系统奠定了基础。

Conclusion: USM是一种创新的计算范式，具有模块化、可解释和资源高效的特点，为未来AI系统的发展提供了新方向。

Abstract: This work establishes a rigorous theoretical foundation for analyzing deep
learning systems by leveraging Infinite Time Turing Machines (ITTMs), which
extend classical computation into transfinite ordinal steps. Using ITTMs, we
reinterpret modern architectures like Transformers, revealing fundamental
limitations in scalability, efficiency, and interpretability. Building on these
insights, we propose the Universal State Machine (USM), a novel computational
paradigm designed from first principles. The USM employs a dynamic, queryable
computation graph that evolves in real time, enabling modular, interpretable,
and resource-efficient computation. This framework not only overcomes the
inefficiencies and rigidity of current models but also lays the groundwork for
scalable, generalizable artificial intelligence systems.

</details>


### [454] [Infinite Time Turing Machines and their Applications](https://arxiv.org/abs/2506.05351)
*Rukmal Weerawarana,Maxwell Braun*

Main category: cs.CC

TL;DR: 本文通过无限时间图灵机（ITTMs）为深度学习系统建立了严格的理论基础，揭示了Transformer等现代架构在可扩展性、效率和可解释性上的根本限制，并提出了一种新的计算范式——通用状态机（USM）。


<details>
  <summary>Details</summary>
Motivation: 为深度学习系统提供理论支持，并解决现有模型在可扩展性、效率和可解释性上的不足。

Method: 利用ITTMs分析现代架构，提出动态、可查询的计算图范式USM。

Result: USM克服了现有模型的低效和僵化，为可扩展、通用的人工智能系统奠定了基础。

Conclusion: USM是一种创新的计算范式，具有模块化、可解释和资源高效的特点，为未来AI系统的发展提供了新方向。

Abstract: This work establishes a rigorous theoretical foundation for analyzing deep
learning systems by leveraging Infinite Time Turing Machines (ITTMs), which
extend classical computation into transfinite ordinal steps. Using ITTMs, we
reinterpret modern architectures like Transformers, revealing fundamental
limitations in scalability, efficiency, and interpretability. Building on these
insights, we propose the Universal State Machine (USM), a novel computational
paradigm designed from first principles. The USM employs a dynamic, queryable
computation graph that evolves in real time, enabling modular, interpretable,
and resource-efficient computation. This framework not only overcomes the
inefficiencies and rigidity of current models but also lays the groundwork for
scalable, generalizable artificial intelligence systems.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [455] [AI-powered Contextual 3D Environment Generation: A Systematic Review](https://arxiv.org/abs/2506.05449)
*Miguel Silva,Alexandre Valle de Carvalho*

Main category: cs.GR

TL;DR: 该论文系统综述了生成式AI在3D场景生成中的应用，分析了现有技术的优缺点，并探讨了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 高质量3D环境生成在游戏、虚拟现实和电影等行业中至关重要，但目前依赖手动流程，资源消耗大。

Method: 通过系统回顾现有生成式AI技术，分析其特点、优势和局限性，并探讨改进潜力。

Result: 研究发现，先进生成架构能以高计算成本生成高质量3D内容，多模态集成技术（如交叉注意力和潜在空间对齐）有助于文本到3D任务。

Conclusion: 研究为AI驱动的3D内容生成提供了全面理解，并指出训练数据质量和多样性以及评估指标对实现可扩展、稳健的3D场景生成至关重要。

Abstract: The generation of high-quality 3D environments is crucial for industries such
as gaming, virtual reality, and cinema, yet remains resource-intensive due to
the reliance on manual processes. This study performs a systematic review of
existing generative AI techniques for 3D scene generation, analyzing their
characteristics, strengths, limitations, and potential for improvement. By
examining state-of-the-art approaches, it presents key challenges such as scene
authenticity and the influence of textual inputs. Special attention is given to
how AI can blend different stylistic domains while maintaining coherence, the
impact of training data on output quality, and the limitations of current
models. In addition, this review surveys existing evaluation metrics for
assessing realism and explores how industry professionals incorporate AI into
their workflows. The findings of this study aim to provide a comprehensive
understanding of the current landscape and serve as a foundation for future
research on AI-driven 3D content generation. Key findings include that advanced
generative architectures enable high-quality 3D content creation at a high
computational cost, effective multi-modal integration techniques like
cross-attention and latent space alignment facilitate text-to-3D tasks, and the
quality and diversity of training data combined with comprehensive evaluation
metrics are critical to achieving scalable, robust 3D scene generation.

</details>


### [456] [ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian Splatting](https://arxiv.org/abs/2506.05480)
*Daniel Wang,Patrick Rim,Tian Tian,Alex Wong,Ganesh Sundaramoorthi*

Main category: cs.GR

TL;DR: ODE-GS结合3D高斯泼溅与潜在神经ODE，预测动态3D场景，超越训练时间范围，实现高保真外推。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染系统（如NeRF或3DGS）在时间预测上表现不佳，ODE-GS旨在解决这一问题。

Method: 通过冻结时间条件变形模型，训练Transformer编码器总结高斯轨迹，利用神经ODE控制潜在状态演化。

Result: 在D-NeRF和NVFI基准测试中，PSNR提升达10 dB，LPIPS减半，表现优于基线。

Conclusion: ODE-GS证明连续时间潜在动力学是预测复杂3D场景的有效方法。

Abstract: We present ODE-GS, a novel method that unifies 3D Gaussian Splatting with
latent neural ordinary differential equations (ODEs) to forecast dynamic 3D
scenes far beyond the time span seen during training. Existing neural rendering
systems - whether NeRF- or 3DGS-based - embed time directly in a deformation
network and therefore excel at interpolation but collapse when asked to predict
the future, where timestamps are strictly out-of-distribution. ODE-GS
eliminates this dependency: after learning a high-fidelity, time-conditioned
deformation model for the training window, we freeze it and train a Transformer
encoder that summarizes past Gaussian trajectories into a latent state whose
continuous evolution is governed by a neural ODE. Numerical integration of this
latent flow yields smooth, physically plausible Gaussian trajectories that can
be queried at any future instant and rendered in real time. Coupled with a
variational objective and a lightweight second-derivative regularizer, ODE-GS
attains state-of-the-art extrapolation on D-NeRF and NVFI benchmarks, improving
PSNR by up to 10 dB and halving perceptual error (LPIPS) relative to the
strongest baselines. Our results demonstrate that continuous-time latent
dynamics are a powerful, practical route to photorealistic prediction of
complex 3D scenes.

</details>


### [457] [AI-powered Contextual 3D Environment Generation: A Systematic Review](https://arxiv.org/abs/2506.05449)
*Miguel Silva,Alexandre Valle de Carvalho*

Main category: cs.GR

TL;DR: 本文系统综述了生成式AI在3D场景生成中的应用，分析了现有技术的特性、优势、局限及改进潜力，并探讨了关键挑战如场景真实性和文本输入的影响。


<details>
  <summary>Details</summary>
Motivation: 高质量3D环境生成在游戏、虚拟现实和电影等行业中至关重要，但目前依赖手动流程，资源消耗大。研究旨在通过AI技术提升生成效率和质量。

Method: 通过系统综述现有生成式AI技术，分析其特点、优势和局限，并探讨多模态集成技术和训练数据的影响。

Result: 研究发现，先进的生成架构能以高计算成本生成高质量3D内容，多模态集成技术（如交叉注意力和潜在空间对齐）有助于文本到3D任务，训练数据的质量和多样性对生成效果至关重要。

Conclusion: 本文为AI驱动的3D内容生成提供了全面理解，并为未来研究奠定了基础，强调训练数据和评估指标的重要性。

Abstract: The generation of high-quality 3D environments is crucial for industries such
as gaming, virtual reality, and cinema, yet remains resource-intensive due to
the reliance on manual processes. This study performs a systematic review of
existing generative AI techniques for 3D scene generation, analyzing their
characteristics, strengths, limitations, and potential for improvement. By
examining state-of-the-art approaches, it presents key challenges such as scene
authenticity and the influence of textual inputs. Special attention is given to
how AI can blend different stylistic domains while maintaining coherence, the
impact of training data on output quality, and the limitations of current
models. In addition, this review surveys existing evaluation metrics for
assessing realism and explores how industry professionals incorporate AI into
their workflows. The findings of this study aim to provide a comprehensive
understanding of the current landscape and serve as a foundation for future
research on AI-driven 3D content generation. Key findings include that advanced
generative architectures enable high-quality 3D content creation at a high
computational cost, effective multi-modal integration techniques like
cross-attention and latent space alignment facilitate text-to-3D tasks, and the
quality and diversity of training data combined with comprehensive evaluation
metrics are critical to achieving scalable, robust 3D scene generation.

</details>


### [458] [ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian Splatting](https://arxiv.org/abs/2506.05480)
*Daniel Wang,Patrick Rim,Tian Tian,Alex Wong,Ganesh Sundaramoorthi*

Main category: cs.GR

TL;DR: ODE-GS结合3D高斯泼溅与潜在神经ODE，预测动态3D场景，超越训练时间范围，实现高保真外推。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染系统（如NeRF或3DGS）依赖时间嵌入，擅长插值但无法预测未来时间点。ODE-GS旨在解决这一问题。

Method: 冻结时间条件变形模型，用Transformer编码过去高斯轨迹，通过神经ODE控制潜在状态连续演化，数值积分生成未来轨迹。

Result: 在D-NeRF和NVFI基准测试中，PSNR提升达10 dB，LPIPS减半，实现最先进的外推性能。

Conclusion: 连续时间潜在动力学是预测复杂3D场景的有效方法，ODE-GS展示了其强大实用性。

Abstract: We present ODE-GS, a novel method that unifies 3D Gaussian Splatting with
latent neural ordinary differential equations (ODEs) to forecast dynamic 3D
scenes far beyond the time span seen during training. Existing neural rendering
systems - whether NeRF- or 3DGS-based - embed time directly in a deformation
network and therefore excel at interpolation but collapse when asked to predict
the future, where timestamps are strictly out-of-distribution. ODE-GS
eliminates this dependency: after learning a high-fidelity, time-conditioned
deformation model for the training window, we freeze it and train a Transformer
encoder that summarizes past Gaussian trajectories into a latent state whose
continuous evolution is governed by a neural ODE. Numerical integration of this
latent flow yields smooth, physically plausible Gaussian trajectories that can
be queried at any future instant and rendered in real time. Coupled with a
variational objective and a lightweight second-derivative regularizer, ODE-GS
attains state-of-the-art extrapolation on D-NeRF and NVFI benchmarks, improving
PSNR by up to 10 dB and halving perceptual error (LPIPS) relative to the
strongest baselines. Our results demonstrate that continuous-time latent
dynamics are a powerful, practical route to photorealistic prediction of
complex 3D scenes.

</details>


### [459] [Noise Consistency Regularization for Improved Subject-Driven Image Synthesis](https://arxiv.org/abs/2506.06483)
*Yao Ni,Song Wen,Piotr Koniusz,Anoop Cherian*

Main category: cs.GR

TL;DR: 论文提出两种一致性损失函数，用于解决Stable Diffusion微调中的欠拟合和过拟合问题，提升生成图像的多样性和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法在生成特定主题图像时存在欠拟合（无法可靠捕捉主题身份）和过拟合（记忆主题图像并减少背景多样性）的问题。

Method: 提出两种辅助一致性损失：先验一致性正则化损失（确保非主题图像的扩散噪声与预训练模型一致）和主题一致性正则化损失（增强模型对噪声调制潜在代码的鲁棒性）。

Result: 实验表明，加入这些损失后，微调模型在CLIP分数、背景多样性和视觉质量上优于DreamBooth。

Conclusion: 提出的方法有效解决了微调中的欠拟合和过拟合问题，同时提升了生成图像的多样性和保真度。

Abstract: Fine-tuning Stable Diffusion enables subject-driven image synthesis by
adapting the model to generate images containing specific subjects. However,
existing fine-tuning methods suffer from two key issues: underfitting, where
the model fails to reliably capture subject identity, and overfitting, where it
memorizes the subject image and reduces background diversity. To address these
challenges, we propose two auxiliary consistency losses for diffusion
fine-tuning. First, a prior consistency regularization loss ensures that the
predicted diffusion noise for prior (non-subject) images remains consistent
with that of the pretrained model, improving fidelity. Second, a subject
consistency regularization loss enhances the fine-tuned model's robustness to
multiplicative noise modulated latent code, helping to preserve subject
identity while improving diversity. Our experimental results demonstrate that
incorporating these losses into fine-tuning not only preserves subject identity
but also enhances image diversity, outperforming DreamBooth in terms of CLIP
scores, background variation, and overall visual quality.

</details>


### [460] [Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization](https://arxiv.org/abs/2506.07069)
*Zhican Wang,Guanghui He,Dantong Liu,Lingjun Gao,Shell Xu Hu,Chen Zhang,Zhuoran Song,Nicholas Lane,Wayne Luk,Hongxiang Fan*

Main category: cs.GR

TL;DR: 本文提出了一种架构与算法协同设计方法，通过轴定向光栅化、神经排序和可重构处理阵列，显著提升了3D高斯溅射（3DGS）在资源受限设备上的实时渲染效率。


<details>
  <summary>Details</summary>
Motivation: 尽管3DGS在高质量视图合成中表现优异，但在资源受限设备上的实时渲染仍面临功耗和面积限制的挑战。

Method: 1. 轴定向光栅化减少冗余计算；2. 神经排序替代硬件排序器；3. 可重构处理阵列支持光栅化和神经网络推理；4. π轨迹瓦片调度优化内存访问。

Result: 实验显示，相比边缘GPU，设计实现了23.4~27.8倍的速度提升和28.8~51.4倍的能耗节省。

Conclusion: 该设计在保持渲染质量的同时显著提升了效率和能耗表现，计划开源以推动领域发展。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained significant attention for
high-quality and efficient view synthesis, making it widely adopted in fields
such as AR/VR, robotics, and autonomous driving. Despite its impressive
algorithmic performance, real-time rendering on resource-constrained devices
remains a major challenge due to tight power and area budgets. This paper
presents an architecture-algorithm co-design to address these inefficiencies.
First, we reveal substantial redundancy caused by repeated computation of
common terms/expressions during the conventional rasterization. To resolve
this, we propose axis-oriented rasterization, which pre-computes and reuses
shared terms along both the X and Y axes through a dedicated hardware design,
effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by
identifying the resource and performance inefficiency of the sorting process,
we introduce a novel neural sorting approach that predicts order-independent
blending weights using an efficient neural network, eliminating the need for
costly hardware sorters. A dedicated training framework is also proposed to
improve its algorithmic stability. Third, to uniformly support rasterization
and neural network inference, we design an efficient reconfigurable processing
array that maximizes hardware utilization and throughput. Furthermore, we
introduce a $\pi$-trajectory tile schedule, inspired by Morton encoding and
Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead.
Comprehensive experiments demonstrate that the proposed design preserves
rendering quality while achieving a speedup of $23.4\sim27.8\times$ and energy
savings of $28.8\sim51.4\times$ compared to edge GPUs for real-world scenes. We
plan to open-source our design to foster further development in this field.

</details>


### [461] [GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution](https://arxiv.org/abs/2506.07897)
*Shuja Khalid,Mohamed Ibrahim,Yang Liu*

Main category: cs.GR

TL;DR: 提出了一种轻量级生成模型，通过Hessian辅助采样策略提升3D高斯泼溅的分辨率和几何保真度，突破输入分辨率的限制。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS方法受限于输入分辨率，无法重建比训练视图更精细的细节。

Method: 采用轻量级生成模型预测和细化额外的3D高斯分布，结合Hessian辅助采样策略智能识别需要密集化的区域。

Result: 在单消费级GPU上实现实时推理（0.015s/次），几何精度和渲染质量显著优于现有方法。

Conclusion: 为分辨率无关的3D场景增强提供了新范式。

Abstract: We present a novel approach for enhancing the resolution and geometric
fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.
Current 3DGS methods are fundamentally limited by their input resolution,
producing reconstructions that cannot extrapolate finer details than are
present in the training views. Our work breaks this limitation through a
lightweight generative model that predicts and refines additional 3D Gaussians
where needed most. The key innovation is our Hessian-assisted sampling
strategy, which intelligently identifies regions that are likely to benefit
from densification, ensuring computational efficiency. Unlike computationally
intensive GANs or diffusion approaches, our method operates in real-time
(0.015s per inference on a single consumer-grade GPU), making it practical for
interactive applications. Comprehensive experiments demonstrate significant
improvements in both geometric accuracy and rendering quality compared to
state-of-the-art methods, establishing a new paradigm for resolution-free 3D
scene enhancement.

</details>


### [462] [Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor](https://arxiv.org/abs/2506.07932)
*Rishit Dagli,Yushi Guan,Sankeerth Durvasula,Mohammadreza Mofayezi,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: Squeeze3D是一种新型框架，利用预训练的3D生成模型的隐式先验知识，实现极高压缩比的3D数据压缩。


<details>
  <summary>Details</summary>
Motivation: 现有3D数据压缩方法难以同时实现高压缩比和高质量还原，Squeeze3D旨在解决这一问题。

Method: 通过可训练的映射网络连接预训练编码器和生成模型的潜在空间，将3D数据压缩为紧凑的潜在代码，再通过生成模型还原。

Result: 实验显示，Squeeze3D对纹理网格、点云和辐射场的压缩比分别高达2187x、55x和619x，且视觉质量与现有方法相当。

Conclusion: Squeeze3D无需训练对象特定网络，支持多种3D格式，是一种高效且灵活的压缩框架。

Abstract: We propose Squeeze3D, a novel framework that leverages implicit prior
knowledge learnt by existing pre-trained 3D generative models to compress 3D
data at extremely high compression ratios. Our approach bridges the latent
spaces between a pre-trained encoder and a pre-trained generation model through
trainable mapping networks. Any 3D model represented as a mesh, point cloud, or
a radiance field is first encoded by the pre-trained encoder and then
transformed (i.e. compressed) into a highly compact latent code. This latent
code can effectively be used as an extremely compressed representation of the
mesh or point cloud. A mapping network transforms the compressed latent code
into the latent space of a powerful generative model, which is then conditioned
to recreate the original 3D model (i.e. decompression). Squeeze3D is trained
entirely on generated synthetic data and does not require any 3D datasets. The
Squeeze3D architecture can be flexibly used with existing pre-trained 3D
encoders and existing generative models. It can flexibly support different
formats, including meshes, point clouds, and radiance fields. Our experiments
demonstrate that Squeeze3D achieves compression ratios of up to 2187x for
textured meshes, 55x for point clouds, and 619x for radiance fields while
maintaining visual quality comparable to many existing methods. Squeeze3D only
incurs a small compression and decompression latency since it does not involve
training object-specific networks to compress an object.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [463] [Online Conformal Model Selection for Nonstationary Time Series](https://arxiv.org/abs/2506.05544)
*Shibo Li,Yao Zheng*

Main category: stat.ML

TL;DR: MPS框架通过结合共形推断和模型置信集，实现在线非平稳时间序列的模型选择，适应动态变化。


<details>
  <summary>Details</summary>
Motivation: 传统模型选择方法依赖平稳性假设，无法适应非平稳数据，而现实数据多为非平稳，因此需要新方法。

Method: 结合共形推断和模型置信集，实时更新候选模型集，确保覆盖最优模型。

Result: MPS在模拟和真实数据中可靠高效地识别最优模型，且生成高质量小规模模型集。

Conclusion: MPS是一种通用框架，适用于多种数据结构和模型，为非平稳环境提供有效解决方案。

Abstract: This paper introduces the MPS (Model Prediction Set), a novel framework for
online model selection for nonstationary time series. Classical model selection
methods, such as information criteria and cross-validation, rely heavily on the
stationarity assumption and often fail in dynamic environments which undergo
gradual or abrupt changes over time. Yet real-world data are rarely stationary,
and model selection under nonstationarity remains a largely open problem. To
tackle this challenge, we combine conformal inference with model confidence
sets to develop a procedure that adaptively selects models best suited to the
evolving dynamics at any given time. Concretely, the MPS updates in real time a
confidence set of candidate models that covers the best model for the next time
period with a specified long-run probability, while adapting to nonstationarity
of unknown forms. Through simulations and real-world data analysis, we
demonstrate that MPS reliably and efficiently identifies optimal models under
nonstationarity, an essential capability lacking in offline methods. Moreover,
MPS frequently produces high-quality sets with small cardinality, whose
evolution offers deeper insights into changing dynamics. As a generic
framework, MPS accommodates any data-generating process, data structure, model
class, training method, and evaluation metric, making it broadly applicable
across diverse problem settings.

</details>


### [464] [Nonlinear Causal Discovery through a Sequential Edge Orientation Approach](https://arxiv.org/abs/2506.05590)
*Stella Huang,Qing Zhou*

Main category: stat.ML

TL;DR: 论文提出了一种基于成对加性噪声模型（PANM）的顺序方法，用于在部分有向无环图（CPDAG）中定向无向边，以恢复真实的因果DAG，并开发了一种新的约束算法。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法存在模型假设严格、依赖独立性检验或计算成本高的问题，本文旨在解决这些局限性。

Method: 利用PANM对CPDAG中的无向边进行排序和定向，通过统计检验比较对数似然值，开发了一种高效的约束算法。

Result: 实验表明，该方法计算高效，对模型误设鲁棒，且性能优于现有非线性DAG学习方法。

Conclusion: 该方法在非线性加性噪声模型下具有结构学习一致性，并在实际应用中表现优异。

Abstract: Recent advances have established the identifiability of a directed acyclic
graph (DAG) under additive noise models (ANMs), spurring the development of
various causal discovery methods. However, most existing methods make
restrictive model assumptions, rely heavily on general independence tests, or
require substantial computational time. To address these limitations, we
propose a sequential procedure to orient undirected edges in a completed
partial DAG (CPDAG), representing an equivalence class of DAGs, by leveraging
the pairwise additive noise model (PANM) to identify their causal directions.
We prove that this procedure can recover the true causal DAG assuming a
restricted ANM. Building on this result, we develop a novel constraint-based
algorithm for learning causal DAGs under nonlinear ANMs. Given an estimated
CPDAG, we develop a ranking procedure that sorts undirected edges by their
adherence to the PANM, which defines an evaluation order of the edges. To
determine the edge direction, we devise a statistical test that compares the
log-likelihood values, evaluated with respect to the competing directions, of a
sub-graph comprising just the candidate nodes and their identified parents in
the partial DAG. We further establish the structural learning consistency of
our algorithm in the large-sample limit. Extensive experiments on synthetic and
real-world datasets demonstrate that our method is computationally efficient,
robust to model misspecification, and consistently outperforms many existing
nonlinear DAG learning methods.

</details>


### [465] [Multilevel neural simulation-based inference](https://arxiv.org/abs/2506.06087)
*Yuga Hikida,Ayush Bharti,Niall Jeffrey,François-Xavier Briol*

Main category: stat.ML

TL;DR: 提出了一种基于多级蒙特卡罗技术的新型神经SBI方法，用于解决计算成本高的模拟器问题，显著提升了固定计算预算下的推理准确性。


<details>
  <summary>Details</summary>
Motivation: 在科学和工程中，构建模拟器比写出似然函数更容易，但计算成本高的模拟器限制了神经SBI的性能。

Method: 利用多级蒙特卡罗技术，结合多个不同成本和保真度的模拟器。

Result: 理论分析和实验表明，该方法在固定计算预算下显著提高了SBI的准确性。

Conclusion: 该方法为计算成本高的模拟器提供了一种高效的神经SBI解决方案。

Abstract: Neural simulation-based inference (SBI) is a popular set of methods for
Bayesian inference when models are only available in the form of a simulator.
These methods are widely used in the sciences and engineering, where writing
down a likelihood can be significantly more challenging than constructing a
simulator. However, the performance of neural SBI can suffer when simulators
are computationally expensive, thereby limiting the number of simulations that
can be performed. In this paper, we propose a novel approach to neural SBI
which leverages multilevel Monte Carlo techniques for settings where several
simulators of varying cost and fidelity are available. We demonstrate through
both theoretical analysis and extensive experiments that our method can
significantly enhance the accuracy of SBI methods given a fixed computational
budget.

</details>


### [466] [Online Conformal Model Selection for Nonstationary Time Series](https://arxiv.org/abs/2506.05544)
*Shibo Li,Yao Zheng*

Main category: stat.ML

TL;DR: MPS（模型预测集）是一种用于非平稳时间序列在线模型选择的新框架，结合了共形推断和模型置信集，能自适应选择最佳模型。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境下传统模型选择方法（如信息准则和交叉验证）因依赖平稳假设而失效的问题。

Method: 结合共形推断与模型置信集，实时更新候选模型的置信集，覆盖下一时间周期的最佳模型。

Result: MPS在非平稳环境下可靠且高效地识别最优模型，并能生成高质量的小规模集合。

Conclusion: MPS是一种通用框架，适用于多种数据生成过程、模型类别和评估指标，具有广泛适用性。

Abstract: This paper introduces the MPS (Model Prediction Set), a novel framework for
online model selection for nonstationary time series. Classical model selection
methods, such as information criteria and cross-validation, rely heavily on the
stationarity assumption and often fail in dynamic environments which undergo
gradual or abrupt changes over time. Yet real-world data are rarely stationary,
and model selection under nonstationarity remains a largely open problem. To
tackle this challenge, we combine conformal inference with model confidence
sets to develop a procedure that adaptively selects models best suited to the
evolving dynamics at any given time. Concretely, the MPS updates in real time a
confidence set of candidate models that covers the best model for the next time
period with a specified long-run probability, while adapting to nonstationarity
of unknown forms. Through simulations and real-world data analysis, we
demonstrate that MPS reliably and efficiently identifies optimal models under
nonstationarity, an essential capability lacking in offline methods. Moreover,
MPS frequently produces high-quality sets with small cardinality, whose
evolution offers deeper insights into changing dynamics. As a generic
framework, MPS accommodates any data-generating process, data structure, model
class, training method, and evaluation metric, making it broadly applicable
across diverse problem settings.

</details>


### [467] [Nonlinear Causal Discovery through a Sequential Edge Orientation Approach](https://arxiv.org/abs/2506.05590)
*Stella Huang,Qing Zhou*

Main category: stat.ML

TL;DR: 提出了一种基于成对加性噪声模型（PANM）的顺序方法，用于在部分有向无环图（CPDAG）中定向边，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法存在模型假设严格、依赖独立性检验或计算量大等问题，需要更高效、稳健的方法。

Method: 利用PANM定向CPDAG中的边，开发了一种基于约束的算法，并通过统计测试确定边方向。

Result: 算法在大样本极限下具有结构学习一致性，实验表明其计算高效且优于现有非线性DAG学习方法。

Conclusion: 该方法在计算效率和鲁棒性上表现优异，为非线性DAG学习提供了新思路。

Abstract: Recent advances have established the identifiability of a directed acyclic
graph (DAG) under additive noise models (ANMs), spurring the development of
various causal discovery methods. However, most existing methods make
restrictive model assumptions, rely heavily on general independence tests, or
require substantial computational time. To address these limitations, we
propose a sequential procedure to orient undirected edges in a completed
partial DAG (CPDAG), representing an equivalence class of DAGs, by leveraging
the pairwise additive noise model (PANM) to identify their causal directions.
We prove that this procedure can recover the true causal DAG assuming a
restricted ANM. Building on this result, we develop a novel constraint-based
algorithm for learning causal DAGs under nonlinear ANMs. Given an estimated
CPDAG, we develop a ranking procedure that sorts undirected edges by their
adherence to the PANM, which defines an evaluation order of the edges. To
determine the edge direction, we devise a statistical test that compares the
log-likelihood values, evaluated with respect to the competing directions, of a
sub-graph comprising just the candidate nodes and their identified parents in
the partial DAG. We further establish the structural learning consistency of
our algorithm in the large-sample limit. Extensive experiments on synthetic and
real-world datasets demonstrate that our method is computationally efficient,
robust to model misspecification, and consistently outperforms many existing
nonlinear DAG learning methods.

</details>


### [468] [Multilevel neural simulation-based inference](https://arxiv.org/abs/2506.06087)
*Yuga Hikida,Ayush Bharti,Niall Jeffrey,François-Xavier Briol*

Main category: stat.ML

TL;DR: 提出了一种基于多级蒙特卡罗技术的新方法，用于提升神经模拟推断（SBI）在多个模拟器可用时的性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经SBI方法在模拟器计算成本高时性能受限，限制了模拟次数。

Method: 利用多级蒙特卡罗技术，结合不同成本和保真度的模拟器。

Result: 理论分析和实验表明，该方法在固定计算预算下显著提高了SBI的准确性。

Conclusion: 新方法为高成本模拟器下的SBI提供了更高效的解决方案。

Abstract: Neural simulation-based inference (SBI) is a popular set of methods for
Bayesian inference when models are only available in the form of a simulator.
These methods are widely used in the sciences and engineering, where writing
down a likelihood can be significantly more challenging than constructing a
simulator. However, the performance of neural SBI can suffer when simulators
are computationally expensive, thereby limiting the number of simulations that
can be performed. In this paper, we propose a novel approach to neural SBI
which leverages multilevel Monte Carlo techniques for settings where several
simulators of varying cost and fidelity are available. We demonstrate through
both theoretical analysis and extensive experiments that our method can
significantly enhance the accuracy of SBI methods given a fixed computational
budget.

</details>


### [469] [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
*Michał P. Karpowicz*

Main category: stat.ML

TL;DR: 本文证明了大型语言模型无法完全避免幻觉，并提出了四个不可兼得的基本属性。


<details>
  <summary>Details</summary>
Motivation: 探讨为什么大型语言模型无法完全避免幻觉，并明确其内在的权衡关系。

Method: 通过将LLM推理建模为“想法拍卖”，并利用Green-Laffont定理证明其不可能性。

Result: 证明了没有任何推理机制能同时满足四个基本属性。

Conclusion: 为理解推理过程的本质提供了严格数学基础，对模型架构、训练目标和评估方法有重要启示。

Abstract: This paper explains \textbf{why it is impossible to create large language
models that do not hallucinate and what are the trade-offs we should be looking
for}. It presents a formal \textbf{impossibility theorem} demonstrating that no
inference mechanism can simultaneously satisfy four fundamental properties:
\textbf{truthful (non-hallucinatory) generation, semantic information
conservation, relevant knowledge revelation, and knowledge-constrained
optimality}. By modeling LLM inference as an \textbf{auction of ideas} where
neural components compete to contribute to responses, we prove the
impossibility using the Green-Laffont theorem. That mathematical framework
provides a rigorous foundation for understanding the nature of inference
process, with implications for model architecture, training objectives, and
evaluation methods.

</details>


### [470] [Direct Fisher Score Estimation for Likelihood Maximization](https://arxiv.org/abs/2506.06542)
*Sherman Khoo,Yakun Wang,Song Liu,Mark Beaumont*

Main category: stat.ML

TL;DR: 提出一种基于局部得分匹配技术的梯度优化方法，用于解决似然函数难以处理但模型模拟可用的问题。


<details>
  <summary>Details</summary>
Motivation: 研究在似然函数难以处理但模型模拟可用的情况下，如何高效最大化似然函数。

Method: 采用局部得分匹配技术，通过线性参数化构建代理得分模型，获得闭式最小二乘解。

Result: 理论保证得分估计的偏差界限，实证结果显示其性能优于现有基准。

Conclusion: 该方法快速、灵活且高效，能有效平滑似然目标并应对复杂似然景观的挑战。

Abstract: We study the problem of likelihood maximization when the likelihood function
is intractable but model simulations are readily available. We propose a
sequential, gradient-based optimization method that directly models the Fisher
score based on a local score matching technique which uses simulations from a
localized region around each parameter iterate. By employing a linear
parameterization to the surrogate score model, our technique admits a
closed-form, least-squares solution. This approach yields a fast, flexible, and
efficient approximation to the Fisher score, effectively smoothing the
likelihood objective and mitigating the challenges posed by complex likelihood
landscapes. We provide theoretical guarantees for our score estimator,
including bounds on the bias introduced by the smoothing. Empirical results on
a range of synthetic and real-world problems demonstrate the superior
performance of our method compared to existing benchmarks.

</details>


### [471] [Robust Learnability of Sample-Compressible Distributions under Noisy or Adversarial Perturbations](https://arxiv.org/abs/2506.06613)
*Arefe Boushehrian,Amir Najafi*

Main category: stat.ML

TL;DR: 论文研究了在数据扰动下样本可压缩分布族的学习问题，提出了两种扰动模型（加性独立噪声和对抗性破坏），并建立了样本复杂度的界限。


<details>
  <summary>Details</summary>
Motivation: 探讨分布族在数据扰动下是否仍能保持可学习性，并解决高维分布学习中的开放性问题。

Method: 提出扰动-量化框架，分析加性独立噪声和对抗性破坏两种扰动模型，结合压缩方案推导样本复杂度。

Result: 证明了样本可压缩族在扰动下仍可学习，并给出了具体应用（如高维均匀分布混合和高斯混合模型）的样本复杂度界限。

Conclusion: 样本可压缩性在数据扰动下仍能保证学习性，解决了文献中的两个开放性问题。

Abstract: Learning distribution families over $\mathbb{R}^d$ is a fundamental problem
in unsupervised learning and statistics. A central question in this setting is
whether a given family of distributions possesses sufficient structure to be
(at least) information-theoretically learnable and, if so, to characterize its
sample complexity. In 2018, Ashtiani et al. reframed \emph{sample
compressibility}, originally due to Littlestone and Warmuth (1986), as a
structural property of distribution classes, proving that it guarantees
PAC-learnability. This discovery subsequently enabled a series of recent
advancements in deriving nearly tight sample complexity bounds for various
high-dimensional open problems. It has been further conjectured that the
converse also holds: every learnable class admits a tight sample compression
scheme.
  In this work, we establish that sample compressible families remain learnable
even from perturbed samples, subject to a set of necessary and sufficient
conditions. We analyze two models of data perturbation: (i) an additive
independent noise model, and (ii) an adversarial corruption model, where an
adversary manipulates a limited subset of the samples unknown to the learner.
Our results are general and rely on as minimal assumptions as possible. We
develop a perturbation-quantization framework that interfaces naturally with
the compression scheme and leads to sample complexity bounds that scale
gracefully with the noise level and corruption budget. As concrete
applications, we establish new sample complexity bounds for learning finite
mixtures of high-dimensional uniform distributions under both noise and
adversarial perturbations, as well as for learning Gaussian mixture models from
adversarially corrupted samples, resolving two open problems in the literature.

</details>


### [472] [Continuous Semi-Implicit Models](https://arxiv.org/abs/2506.06778)
*Longlin Yu,Jiajun Zha,Tong Yang,Tianyu Xie,Xiangyu Zhang,S. -H. Gary Chan,Cheng Zhang*

Main category: stat.ML

TL;DR: CoSIM是一种连续半隐式模型，通过连续过渡核提升训练效率，并在图像生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决层次半隐式模型在训练中收敛慢的问题，并提升生成模型的表达能力。

Method: 引入连续过渡核，实现无模拟的高效训练，并通过设计的一致性过渡核实现多步蒸馏。

Result: 在图像生成任务中表现优于现有扩散模型加速方法，尤其在FD-DINOv2上表现突出。

Conclusion: CoSIM为生成模型提供了一种高效且性能优越的连续框架。

Abstract: Semi-implicit distributions have shown great promise in variational inference
and generative modeling. Hierarchical semi-implicit models, which stack
multiple semi-implicit layers, enhance the expressiveness of semi-implicit
distributions and can be used to accelerate diffusion models given pretrained
score networks. However, their sequential training often suffers from slow
convergence. In this paper, we introduce CoSIM, a continuous semi-implicit
model that extends hierarchical semi-implicit models into a continuous
framework. By incorporating a continuous transition kernel, CoSIM enables
efficient, simulation-free training. Furthermore, we show that CoSIM achieves
consistency with a carefully designed transition kernel, offering a novel
approach for multistep distillation of generative models at the distributional
level. Extensive experiments on image generation demonstrate that CoSIM
performs on par or better than existing diffusion model acceleration methods,
achieving superior performance on FD-DINOv2.

</details>


### [473] [The Currents of Conflict: Decomposing Conflict Trends with Gaussian Processes](https://arxiv.org/abs/2506.06828)
*Simon P. von der Maase*

Main category: stat.ML

TL;DR: 提出了一种利用高斯过程和时间空间分解数据估计冲突趋势的新方法，可用于研究冲突陷阱、扩散和预测未来冲突。


<details>
  <summary>Details</summary>
Motivation: 研究冲突的时间空间模式，以揭示冲突陷阱、扩散等现象，并为其他估计任务提供控制变量。

Method: 使用高斯过程和时间空间分解的冲突事件数据，估计冲突趋势。

Result: 能够研究冲突趋势、控制冲突现象，并预测未来冲突模式。

Conclusion: 该方法仅需历史冲突数据，即可实现高效的冲突趋势估计和预测。

Abstract: I present a novel approach to estimating the temporal and spatial patterns of
violent conflict. I show how we can use highly temporally and spatially
disaggregated data on conflict events in tandem with Gaussian processes to
estimate temporospatial conflict trends. These trends can be studied to gain
insight into conflict traps, diffusion and tempo-spatial conflict exposure in
general; they can also be used to control for such phenomenons given other
estimation tasks; lastly, the approach allow us to extrapolate the estimated
tempo-spatial conflict patterns into future temporal units, thus facilitating
powerful, stat-of-the-art, conflict forecasts. Importantly, these results are
achieved via a relatively parsimonious framework using only one data source:
past conflict patterns.

</details>


### [474] [A Statistical Framework for Model Selection in LSTM Networks](https://arxiv.org/abs/2506.06840)
*Fahad Mostafa*

Main category: stat.ML

TL;DR: 提出了一种统一的统计框架，用于LSTM网络的系统模型选择，解决了超参数调优、架构选择和正则化等问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LSTM在序列数据建模中广泛应用，但模型选择问题仍依赖启发式方法且计算成本高。

Method: 扩展经典模型选择思想（如信息准则和收缩估计）到序列神经网络，提出惩罚似然和广义阈值方法，并使用变分贝叶斯和近似边际似然进行高效估计。

Result: 在多个生物医学数据示例中展示了框架的灵活性和性能提升。

Conclusion: 该框架为LSTM模型选择提供了系统且高效的方法。

Abstract: Long Short-Term Memory (LSTM) neural network models have become the
cornerstone for sequential data modeling in numerous applications, ranging from
natural language processing to time series forecasting. Despite their success,
the problem of model selection, including hyperparameter tuning, architecture
specification, and regularization choice remains largely heuristic and
computationally expensive. In this paper, we propose a unified statistical
framework for systematic model selection in LSTM networks. Our framework
extends classical model selection ideas, such as information criteria and
shrinkage estimation, to sequential neural networks. We define penalized
likelihoods adapted to temporal structures, propose a generalized threshold
approach for hidden state dynamics, and provide efficient estimation strategies
using variational Bayes and approximate marginal likelihood methods. Several
biomedical data centric examples demonstrate the flexibility and improved
performance of the proposed framework.

</details>


### [475] [Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis](https://arxiv.org/abs/2506.07011)
*Yuan-Hao Wei,Yan-Jie Sun*

Main category: stat.ML

TL;DR: Half-AVAE改进VAE框架，通过对抗网络和外部增强项解决欠定ICA问题，提升潜在变量的独立性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统VAE在欠定ICA（潜在变量多于观测信号）中表现不佳，需改进以增强潜在变量的独立性和可解释性。

Method: 提出Half-AVAE，基于无编码器的Half-VAE框架，结合对抗网络和外部增强项，消除显式逆映射。

Result: 实验显示Half-AVAE在欠定条件下优于基线模型（如GP-AVAE和Half-VAE），均方根误差更低。

Conclusion: Half-AVAE展示了VAE在变分推断中的灵活性，为复杂ICA任务提供有效解决方案，推动解耦、因果推断和生成建模的应用。

Abstract: This study advances the Variational Autoencoder (VAE) framework by addressing
challenges in Independent Component Analysis (ICA) under both determined and
underdetermined conditions, focusing on enhancing the independence and
interpretability of latent variables. Traditional VAEs map observed data to
latent variables and back via an encoder-decoder architecture, but struggle
with underdetermined ICA where the number of latent variables exceeds observed
signals. The proposed Half Adversarial VAE (Half-AVAE) builds on the
encoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle
underdetermined scenarios. By integrating adversarial networks and External
Enhancement (EE) terms, Half-AVAE promotes mutual independence among latent
dimensions, achieving factorized and interpretable representations. Experiments
with synthetic signals demonstrate that Half-AVAE outperforms baseline models,
including GP-AVAE and Half-VAE, in recovering independent components under
underdetermined conditions, as evidenced by lower root mean square errors. The
study highlights the flexibility of VAEs in variational inference, showing that
encoder omission, combined with adversarial training and structured priors,
enables effective solutions for complex ICA tasks, advancing applications in
disentanglement, causal inference, and generative modeling.

</details>


### [476] [Quantile-Optimal Policy Learning under Unmeasured Confounding](https://arxiv.org/abs/2506.07140)
*Zhongren Chen,Siyu Chen,Zhengling Qi,Xiaohong Chen,Zhuoran Yang*

Main category: stat.ML

TL;DR: 论文研究了在存在未观测混杂变量的离线设置下，学习具有最大α-分位数奖励分布的策略。提出了一种结合因果推断工具和悲观估计的方法，解决了非线性目标、未观测混杂和数据覆盖不足的挑战。


<details>
  <summary>Details</summary>
Motivation: 在离线策略学习中，未观测混杂变量和非线性分位数目标导致传统方法失效，需要新的理论和方法来解决这些问题。

Method: 使用工具变量和负控制等因果推断工具，通过解非线性函数积分方程估计分位数目标，并采用极小极大估计和非参数模型构建保守策略估计。

Result: 提出的方法在温和条件下具有理论保证，学习到的策略在离线数据集中具有样本效率，且分位数最优。

Conclusion: 论文首次提出了在存在未观测混杂变量时，估计分位数最优策略的样本高效算法，填补了该领域的空白。

Abstract: We study quantile-optimal policy learning where the goal is to find a policy
whose reward distribution has the largest $\alpha$-quantile for some $\alpha
\in (0, 1)$. We focus on the offline setting whose generating process involves
unobserved confounders. Such a problem suffers from three main challenges: (i)
nonlinearity of the quantile objective as a functional of the reward
distribution, (ii) unobserved confounding issue, and (iii) insufficient
coverage of the offline dataset. To address these challenges, we propose a
suite of causal-assisted policy learning methods that provably enjoy strong
theoretical guarantees under mild conditions. In particular, to address (i) and
(ii), using causal inference tools such as instrumental variables and negative
controls, we propose to estimate the quantile objectives by solving nonlinear
functional integral equations. Then we adopt a minimax estimation approach with
nonparametric models to solve these integral equations, and propose to
construct conservative policy estimates that address (iii). The final policy is
the one that maximizes these pessimistic estimates. In addition, we propose a
novel regularized policy learning method that is more amenable to computation.
Finally, we prove that the policies learned by these methods are
$\tilde{\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage
assumption on the offline dataset. Here, $\tilde{\mathscr{O}}(\cdot)$ omits
poly-logarithmic factors. To the best of our knowledge, we propose the first
sample-efficient policy learning algorithms for estimating the quantile-optimal
policy when there exist unmeasured confounding.

</details>


### [477] [ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition](https://arxiv.org/abs/2506.07259)
*Daolang Huang,Xinyi Wen,Ayush Bharti,Samuel Kaski,Luigi Acerbi*

Main category: stat.ML

TL;DR: ALINE是一个结合贝叶斯推理和主动数据采集的统一框架，通过强化学习训练Transformer架构，实现高效数据查询和即时推理。


<details>
  <summary>Details</summary>
Motivation: 解决在需要即时推理的新数据采集任务中，现有方法（如摊销贝叶斯推理和实验设计）的不足。

Method: ALINE采用Transformer架构，通过强化学习优化信息增益奖励，同时支持针对特定参数或任务的查询策略。

Result: 在回归任务、贝叶斯实验设计基准和心理测量模型中，ALINE表现出高效的推理和数据选择能力。

Conclusion: ALINE为即时推理和高效数据采集提供了一种统一的解决方案。

Abstract: Many critical applications, from autonomous scientific discovery to
personalized medicine, demand systems that can both strategically acquire the
most informative data and instantaneously perform inference based upon it.
While amortized methods for Bayesian inference and experimental design offer
part of the solution, neither approach is optimal in the most general and
challenging task, where new data needs to be collected for instant inference.
To tackle this issue, we introduce the Amortized Active Learning and Inference
Engine (ALINE), a unified framework for amortized Bayesian inference and active
data acquisition. ALINE leverages a transformer architecture trained via
reinforcement learning with a reward based on self-estimated information gain
provided by its own integrated inference component. This allows it to
strategically query informative data points while simultaneously refining its
predictions. Moreover, ALINE can selectively direct its querying strategy
towards specific subsets of model parameters or designated predictive tasks,
optimizing for posterior estimation, data prediction, or a mixture thereof.
Empirical results on regression-based active learning, classical Bayesian
experimental design benchmarks, and a psychometric model with selectively
targeted parameters demonstrate that ALINE delivers both instant and accurate
inference along with efficient selection of informative points.

</details>


### [478] [Rao-Blackwellised Reparameterisation Gradients](https://arxiv.org/abs/2506.07687)
*Kevin Lam,Thang Bui,George Deligiannidis,Yee Whye Teh*

Main category: stat.ML

TL;DR: 本文提出R2-G2估计器，作为重参数化梯度估计器的Rao-Blackwell化版本，并证明其在贝叶斯多层感知机中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究梯度估计器在潜在高斯变量模型中的优化效果，特别是重参数化梯度估计器的改进。

Method: 提出R2-G2估计器，作为重参数化梯度估计器的Rao-Blackwell化版本，并分析其在贝叶斯多层感知机中的实例。

Result: R2-G2在初始训练中表现更优，尤其适用于多次应用重参数化技巧的模型。

Conclusion: R2-G2扩展了Rao-Blackwell化梯度的优势，提升了多类概率模型的性能。

Abstract: Latent Gaussian variables have been popularised in probabilistic machine
learning. In turn, gradient estimators are the machinery that facilitates
gradient-based optimisation for models with latent Gaussian variables. The
reparameterisation trick is often used as the default estimator as it is simple
to implement and yields low-variance gradients for variational inference. In
this work, we propose the R2-G2 estimator as the Rao-Blackwellisation of the
reparameterisation gradient estimator. Interestingly, we show that the local
reparameterisation gradient estimator for Bayesian MLPs is an instance of the
R2-G2 estimator and Rao-Blackwellisation. This lets us extend benefits of
Rao-Blackwellised gradients to a suite of probabilistic models. We show that
initial training with R2-G2 consistently yields better performance in models
with multiple applications of the reparameterisation trick.

</details>


### [479] [Quickest Causal Change Point Detection by Adaptive Intervention](https://arxiv.org/abs/2506.07760)
*Haijie Xu,Chen Zhang*

Main category: stat.ML

TL;DR: 提出一种用于线性因果模型中变化点监测的算法，通过集中化技术和干预节点选择放大变化信号，并验证其最优性。


<details>
  <summary>Details</summary>
Motivation: 解决因果模型中变化点监测的挑战，尤其是干预对变化传播的影响。

Method: 采用集中化技术将变化集中到单一维度，基于Kullback-Leibler散度选择干预节点，并提出两种监测方法。

Result: 理论证明方法的一阶最优性，并通过模拟和实际案例验证其有效性。

Conclusion: 算法在变化点监测中表现优异，平衡了探索与利用，适用于实际应用。

Abstract: We propose an algorithm for change point monitoring in linear causal models
that accounts for interventions. Through a special centralization technique, we
can concentrate the changes arising from causal propagation across nodes into a
single dimension. Additionally, by selecting appropriate intervention nodes
based on Kullback-Leibler divergence, we can amplify the change magnitude. We
also present an algorithm for selecting the intervention values, which aids in
the identification of the most effective intervention nodes. Two monitoring
methods are proposed, each with an adaptive intervention policy to make a
balance between exploration and exploitation. We theoretically demonstrate the
first-order optimality of the proposed methods and validate their properties
using simulation datasets and two real-world case studies.

</details>


### [480] [Accelerating Constrained Sampling: A Large Deviations Approach](https://arxiv.org/abs/2506.07816)
*Yingli Wang,Changwei Tu,Xiaoyu Wang,Lingjiong Zhu*

Main category: stat.ML

TL;DR: 本文研究了偏斜反射非可逆朗之万动力学（SRNLD）的长期行为，通过设计合适的偏斜对称矩阵，证明了其比反射朗之万动力学（RLD）更快的收敛速度，并通过数值实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 在约束域上采样目标概率分布是机器学习等应用中的重要问题。虽然已有研究探讨了SRNLD的非渐近收敛性和加速效果，但如何设计偏斜对称矩阵以优化性能尚不明确。

Method: 通过建立SRNLD经验测度的大偏差原理（LDP），并明确表征速率函数，研究偏斜对称矩阵的设计及其对收敛速度的影响。

Result: 理论分析表明，当偏斜对称矩阵满足特定条件时，SRNLD比RLD更快收敛到目标分布。数值实验进一步验证了这一理论发现。

Conclusion: 通过合理设计偏斜对称矩阵，SRNLD在约束采样中表现出优越性能，为实际应用提供了理论支持。

Abstract: The problem of sampling a target probability distribution on a constrained
domain arises in many applications including machine learning. For constrained
sampling, various Langevin algorithms such as projected Langevin Monte Carlo
(PLMC) based on the discretization of reflected Langevin dynamics (RLD) and
more generally skew-reflected non-reversible Langevin Monte Carlo (SRNLMC)
based on the discretization of skew-reflected non-reversible Langevin dynamics
(SRNLD) have been proposed and studied in the literature. This work focuses on
the long-time behavior of SRNLD, where a skew-symmetric matrix is added to RLD.
Although the non-asymptotic convergence analysis for SRNLD (and SRNLMC) and the
acceleration compared to RLD (and PMLC) have been studied in the literature, it
is not clear how one should design the skew-symmetric matrix in the dynamics to
achieve good performance in practice. We establish a large deviation principle
(LDP) for the empirical measure of SRNLD when the skew-symmetric matrix is
chosen such that its product with the inward unit normal vector field on the
boundary is zero. By explicitly characterizing the rate functions, we show that
SRNLD can accelerate the convergence to the target distribution compared to RLD
with this choice of the skew-symmetric matrix. Numerical experiments for SRNLMC
based on the proposed skew-symmetric matrix show superior performance which
validate the theoretical findings from the large deviations theory.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [481] [Variational Inference for Quantum HyperNetworks](https://arxiv.org/abs/2506.05888)
*Luca Nepote,Alix Lhéritier,Nicolas Bondoux,Marios Kountouris,Maurizio Filippone*

Main category: quant-ph

TL;DR: 论文提出了一种利用量子超网络优化二进制神经网络（BiNNs）的方法，通过量子计算生成二进制权重，并探索更广的解空间。实验表明该方法优于传统的最大似然估计。


<details>
  <summary>Details</summary>
Motivation: 二进制神经网络（BiNNs）在减少内存和功耗方面具有潜力，但传统训练算法存在局限性。量子计算提供了一种新的优化途径。

Method: 采用变分量子算法生成二进制权重，利用量子叠加和纠缠探索解空间。通过推导证据下界（ELBO）和引入基于最大均值差异（MMD）的替代ELBO，优化训练过程。

Result: 实验结果表明，该方法在可训练性和泛化能力上优于传统的最大似然估计（MLE）。

Conclusion: 量子超网络为BiNNs的优化提供了有效的新方法，显著提升了性能。

Abstract: Binary Neural Networks (BiNNs), which employ single-bit precision weights,
have emerged as a promising solution to reduce memory usage and power
consumption while maintaining competitive performance in large-scale systems.
However, training BiNNs remains a significant challenge due to the limitations
of conventional training algorithms. Quantum HyperNetworks offer a novel
paradigm for enhancing the optimization of BiNN by leveraging quantum
computing. Specifically, a Variational Quantum Algorithm is employed to
generate binary weights through quantum circuit measurements, while key quantum
phenomena such as superposition and entanglement facilitate the exploration of
a broader solution space. In this work, we establish a connection between this
approach and Bayesian inference by deriving the Evidence Lower Bound (ELBO),
when direct access to the output distribution is available (i.e., in
simulations), and introducing a surrogate ELBO based on the Maximum Mean
Discrepancy (MMD) metric for scenarios involving implicit distributions, as
commonly encountered in practice. Our experimental results demonstrate that the
proposed methods outperform standard Maximum Likelihood Estimation (MLE),
improving trainability and generalization.

</details>


### [482] [Variational Inference for Quantum HyperNetworks](https://arxiv.org/abs/2506.05888)
*Luca Nepote,Alix Lhéritier,Nicolas Bondoux,Marios Kountouris,Maurizio Filippone*

Main category: quant-ph

TL;DR: 论文提出了一种利用量子超网络优化二进制神经网络（BiNNs）的方法，通过量子计算生成二进制权重，并利用量子现象（如叠加和纠缠）扩展解空间。实验表明，该方法优于传统的最大似然估计（MLE）。


<details>
  <summary>Details</summary>
Motivation: 二进制神经网络（BiNNs）虽然能减少内存和功耗，但训练难度大。量子超网络提供了一种新的优化范式，利用量子计算提升BiNNs的性能。

Method: 采用变分量子算法生成二进制权重，并结合贝叶斯推断推导了证据下界（ELBO），同时提出了基于最大均值差异（MMD）的替代ELBO。

Result: 实验结果显示，该方法在可训练性和泛化能力上优于传统的最大似然估计（MLE）。

Conclusion: 量子超网络为优化二进制神经网络提供了一种有效的新方法，显著提升了性能。

Abstract: Binary Neural Networks (BiNNs), which employ single-bit precision weights,
have emerged as a promising solution to reduce memory usage and power
consumption while maintaining competitive performance in large-scale systems.
However, training BiNNs remains a significant challenge due to the limitations
of conventional training algorithms. Quantum HyperNetworks offer a novel
paradigm for enhancing the optimization of BiNN by leveraging quantum
computing. Specifically, a Variational Quantum Algorithm is employed to
generate binary weights through quantum circuit measurements, while key quantum
phenomena such as superposition and entanglement facilitate the exploration of
a broader solution space. In this work, we establish a connection between this
approach and Bayesian inference by deriving the Evidence Lower Bound (ELBO),
when direct access to the output distribution is available (i.e., in
simulations), and introducing a surrogate ELBO based on the Maximum Mean
Discrepancy (MMD) metric for scenarios involving implicit distributions, as
commonly encountered in practice. Our experimental results demonstrate that the
proposed methods outperform standard Maximum Likelihood Estimation (MLE),
improving trainability and generalization.

</details>


### [483] [A weighted quantum ensemble of homogeneous quantum classifiers](https://arxiv.org/abs/2506.07810)
*Emiliano Tolotti,Enrico Blanzieri,Davide Pastorello*

Main category: quant-ph

TL;DR: 提出了一种基于量子分类器的加权同质集成方法，通过量子并行执行多样化的内部分类器，并结合经典权重优化，提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 通过结合量子计算的并行性和集成学习的多样性，提升机器学习模型的预测性能。

Method: 使用量子分类器和索引寄存器进行数据编码，通过叠加和控制门实现特征和训练点子采样，量子并行执行多样化内部分类器，并结合经典权重优化。

Result: 实证评估表明该方法有效，提供了性能提升的见解。

Conclusion: 提出的量子加权同质集成方法在预测准确性上表现出色，展示了量子计算在集成学习中的潜力。

Abstract: Ensemble methods in machine learning aim to improve prediction accuracy by
combining multiple models. This is achieved by ensuring diversity among
predictors to capture different data aspects. Homogeneous ensembles use
identical models, achieving diversity through different data subsets, and
weighted-average ensembles assign higher influence to more accurate models
through a weight learning procedure. We propose a method to achieve a weighted
homogeneous quantum ensemble using quantum classifiers with indexing registers
for data encoding. This approach leverages instance-based quantum classifiers,
enabling feature and training point subsampling through superposition and
controlled unitaries, and allowing for a quantum-parallel execution of diverse
internal classifiers with different data compositions in superposition. The
method integrates a learning process involving circuit execution and classical
weight optimization, for a trained ensemble execution with weights encoded in
the circuit at test-time. Empirical evaluation demonstrate the effectiveness of
the proposed method, offering insights into its performance.

</details>


### [484] [Deep reinforcement learning for near-deterministic preparation of cubic- and quartic-phase gates in photonic quantum computing](https://arxiv.org/abs/2506.07859)
*Amanuel Anteneh Léandre Brunel,Carlos González-Arciniegas,Olivier Pfister*

Main category: quant-ph

TL;DR: 利用深度神经网络通过强化学习控制量子光学电路生成立方相位态，平均成功率96%，仅需光子数分辨测量作为非高斯资源。


<details>
  <summary>Details</summary>
Motivation: 立方相位态是实现连续变量通用量子计算的充分资源，研究如何高效生成此类态。

Method: 通过强化学习训练深度神经网络，控制量子光学电路生成立方相位态。

Result: 平均成功率达96%，且无需高斯资源，仅需光子数分辨测量。同时可直接生成四次相位门。

Conclusion: 该方法高效生成立方相位态，并扩展至四次相位门，为量子计算提供新途径。

Abstract: Cubic-phase states are a sufficient resource for universal quantum computing
over continuous variables. We present results from numerical experiments in
which deep neural networks are trained via reinforcement learning to control a
quantum optical circuit for generating cubic-phase states, with an average
success rate of 96%. The only non-Gaussian resource required is
photon-number-resolving measurements. We also show that the exact same
resources enable the direct generation of a quartic-phase gate, with no need
for a cubic gate decomposition.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [485] [Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor](https://arxiv.org/abs/2506.06773)
*Emet Behrendt,Shing Wai Pun,Prashant J. Nair*

Main category: cs.AR

TL;DR: 论文提出了一种名为Bullseye的预测器，用于解决TAGE-SC-L预测器中难以预测的分支问题，通过局部和全局历史感知器提升预测精度。


<details>
  <summary>Details</summary>
Motivation: TAGE-SC-L预测器中难以预测的分支（H2P）导致大量误预测，传统方法如扩大表格效果有限。

Method: 设计了一个28 KB的Bullseye子系统，包含H2P识别表、局部和全局历史感知器，并通过动态阈值筛选有效分支。

Result: Bullseye将平均MPKI降至3.4045，CycWpPKI降至145.09。

Conclusion: Bullseye有效提升了TAGE-SC-L对H2P分支的预测能力，减少了误预测。

Abstract: Branch prediction is key to the performance of out-of-order processors. While
the CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical
corrector, and a loop predictor, over half of its remaining mispredictions stem
from a small set of hard-to-predict (H2P) branches. These branches occur under
diverse global histories, causing repeated thrashing in TAGE and eviction
before usefulness counters can mature. Prior work shows that simply enlarging
the tables offers only marginal improvement.
  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem
called the Bullseye predictor. It identifies problematic PCs using a
set-associative H2P Identification Table (HIT) and steers them to one of two
branch-specific perceptrons, one indexed by hashed local history and the other
by folded global history. A short trial phase tracks head-to-head accuracy in
an H2P cache. A branch becomes perceptron-resident only if the perceptron's
sustained accuracy and output magnitude exceed dynamic thresholds, after which
TAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,
and perceptron operate fully in parallel with TAGE-SC-L, providing higher
fidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI
of 145.09.

</details>


### [486] [ASPO: Constraint-Aware Bayesian Optimization for FPGA-based Soft Processors](https://arxiv.org/abs/2506.06817)
*Haoran Wu,Ce Guo,Wayne Luk,Robert Mullins*

Main category: cs.AR

TL;DR: ASPO是一种改进的贝叶斯优化方法，支持处理包含分类参数的约束，并加速FPGA软处理器的设计优化。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化（BO）无法处理涉及分类参数的约束，且优化时间随处理器复杂度增加而显著增长，特别是在FPGA软处理器设计中。

Method: ASPO通过定制BO的数学机制，引入新的协方差核支持分类参数，并通过惩罚获取函数和重用FPGA合成检查点来加速设计评估。

Result: ASPO在BOOM处理器上为“multiply”基准测试减少了35%的执行时间，并将设计时间比现有方法Boomerang减少了74%。

Conclusion: ASPO成功解决了标准BO在软处理器设计中的局限性，显著提升了优化效率和性能。

Abstract: Bayesian Optimization (BO) has shown promise in tuning processor design
parameters. However, standard BO does not support constraints involving
categorical parameters such as types of branch predictors and division
circuits. In addition, optimization time of BO grows with processor complexity,
which becomes increasingly significant especially for FPGA-based soft
processors. This paper introduces ASPO, an approach that leverages disjunctive
form to enable BO to handle constraints involving categorical parameters.
Unlike existing methods that directly apply standard BO, the proposed ASPO
method, for the first time, customizes the mathematical mechanism of BO to
address challenges faced by soft-processor designs on FPGAs. Specifically, ASPO
supports categorical parameters using a novel customized BO covariance kernel.
It also accelerates the design evaluation procedure by penalizing the BO
acquisition function with potential evaluation time and by reusing FPGA
synthesis checkpoints from previously evaluated configurations. ASPO targets
three soft processors: RocketChip, BOOM, and EL2 VeeR. The approach is
evaluated based on seven RISC-V benchmarks. Results show that ASPO can reduce
execution time for the ``multiply'' benchmark on the BOOM processor by up to
35\% compared to the default configuration. Furthermore, it reduces design time
for the BOOM processor by up to 74\% compared to Boomerang, a state-of-the-art
hardware-oriented BO approach.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [487] [fairmetrics: An R package for group fairness evaluation](https://arxiv.org/abs/2506.06243)
*Benjamin Smith,Jianhui Gao,Jessica Gronsbell*

Main category: stat.CO

TL;DR: 论文介绍了{fairmetrics} R包，用于评估机器学习模型的公平性，确保模型不会对特定群体产生系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型可能对特定群体产生偏见，导致结构性不平等，因此需要工具来评估和确保公平性。

Method: {fairmetrics} R包提供了一套用户友好的框架，支持多种基于群体的公平性标准评估，包括独立性、分离性和充分性等指标。

Result: 该工具提供了点估计和区间估计，并包含了一个来自MIMIC-II数据库的示例数据集。

Conclusion: {fairmetrics} R包为机器学习模型的公平性评估提供了实用的解决方案，有助于实施适当的偏见缓解策略。

Abstract: Fairness is a growing area of machine learning (ML) that focuses on ensuring
models do not produce systematically biased outcomes for specific groups,
particularly those defined by protected attributes such as race, gender, or
age. Evaluating fairness is a critical aspect of ML model development, as
biased models can perpetuate structural inequalities. The {fairmetrics} R
package offers a user-friendly framework for rigorously evaluating numerous
group-based fairness criteria, including metrics based on independence (e.g.,
statistical parity), separation (e.g., equalized odds), and sufficiency (e.g.,
predictive parity). Group-based fairness criteria assess whether a model is
equally accurate or well-calibrated across a set of predefined groups so that
appropriate bias mitigation strategies can be implemented. {fairmetrics}
provides both point and interval estimates for multiple metrics through a
convenient wrapper function and includes an example dataset derived from the
Medical Information Mart for Intensive Care, version II (MIMIC-II) database
(Goldberger et al., 2000; Raffa, 2016).

</details>


### [488] [fairmetrics: An R package for group fairness evaluation](https://arxiv.org/abs/2506.06243)
*Benjamin Smith,Jianhui Gao,Jessica Gronsbell*

Main category: stat.CO

TL;DR: 论文介绍了{fairmetrics} R包，用于评估机器学习模型的公平性，确保模型对不同群体无系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 确保机器学习模型不因保护属性（如种族、性别、年龄）产生偏见结果，避免加剧结构性不平等。

Method: 通过{fairmetrics} R包提供用户友好的框架，评估多种基于群体的公平性标准，包括独立性、分离性和充分性相关指标。

Result: {fairmetrics}提供了点估计和区间估计，并包含来自MIMIC-II数据库的示例数据集。

Conclusion: {fairmetrics}为公平性评估提供了实用工具，有助于实施偏见缓解策略。

Abstract: Fairness is a growing area of machine learning (ML) that focuses on ensuring
models do not produce systematically biased outcomes for specific groups,
particularly those defined by protected attributes such as race, gender, or
age. Evaluating fairness is a critical aspect of ML model development, as
biased models can perpetuate structural inequalities. The {fairmetrics} R
package offers a user-friendly framework for rigorously evaluating numerous
group-based fairness criteria, including metrics based on independence (e.g.,
statistical parity), separation (e.g., equalized odds), and sufficiency (e.g.,
predictive parity). Group-based fairness criteria assess whether a model is
equally accurate or well-calibrated across a set of predefined groups so that
appropriate bias mitigation strategies can be implemented. {fairmetrics}
provides both point and interval estimates for multiple metrics through a
convenient wrapper function and includes an example dataset derived from the
Medical Information Mart for Intensive Care, version II (MIMIC-II) database
(Goldberger et al., 2000; Raffa, 2016).

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [489] [How Malicious AI Swarms Can Threaten Democracy](https://arxiv.org/abs/2506.06299)
*Daniel Thilo Schroeder,Meeyoung Cha,Andrea Baronchelli,Nick Bostrom,Nicholas A. Christakis,David Garcia,Amit Goldenberg,Yara Kyrychenko,Kevin Leyton-Brown,Nina Lutz,Gary Marcus,Filippo Menczer,Gordon Pennycook,David G. Rand,Frank Schweitzer,Christopher Summerfield,Audrey Tang,Jay Van Bavel,Sander van der Linden,Dawn Song,Jonas R. Kunst*

Main category: cs.CY

TL;DR: 论文探讨了AI恶意群体（swarms）对信息环境的威胁，提出了三方面的应对措施：平台防御、模型保护和系统监管。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的进步，恶意AI群体可能通过协调行动破坏信息真实性，威胁民主进程，亟需应对措施。

Method: 提出三方面解决方案：平台防御（如群体检测仪表盘）、模型保护（如水印技术）和系统监管（如联合国支持的AI影响观察站）。

Result: 恶意AI群体可能导致虚假共识、信息碎片化等后果，威胁社会信任。

Conclusion: 呼吁采取多层次措施应对AI恶意群体，保护信息环境和民主制度。

Abstract: Advances in AI portend a new era of sophisticated disinformation operations.
While individual AI systems already create convincing -- and at times
misleading -- information, an imminent development is the emergence of
malicious AI swarms. These systems can coordinate covertly, infiltrate
communities, evade traditional detectors, and run continuous A/B tests, with
round-the-clock persistence. The result can include fabricated grassroots
consensus, fragmented shared reality, mass harassment, voter micro-suppression
or mobilization, contamination of AI training data, and erosion of
institutional trust. With democratic processes worldwide increasingly
vulnerable, we urge a three-pronged response: (1) platform-side defenses --
always-on swarm-detection dashboards, pre-election high-fidelity
swarm-simulation stress-tests, transparency audits, and optional client-side
"AI shields" for users; (2) model-side safeguards -- standardized
persuasion-risk tests, provenance-authenticating passkeys, and watermarking;
and (3) system-level oversight -- a UN-backed AI Influence Observatory.

</details>


### [490] [Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research](https://arxiv.org/abs/2506.06377)
*Giuseppe Arbia,Luca Morandini,Vincenzo Nardelli*

Main category: cs.CY

TL;DR: LLMs can assess variable coherence well (F1 score 0.87) but struggle with deeper economic reasoning like coefficient plausibility and publication suitability. Model choice and paper characteristics affect accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' ability to assess economic soundness and theoretical consistency in spatial econometrics.

Method: Created original and altered summaries from 28 papers (2005-2024), evaluated by diverse LLMs for qualitative and binary classifications.

Result: LLMs excel at variable choice coherence but vary in deeper assessments. Model and paper traits significantly impact accuracy.

Conclusion: LLMs are useful for surface-level checks but require human oversight for nuanced economic reasoning in peer review.

Abstract: This paper investigates Large Language Models (LLMs) ability to assess the
economic soundness and theoretical consistency of empirical findings in spatial
econometrics. We created original and deliberately altered "counterfactual"
summaries from 28 published papers (2005-2024), which were evaluated by a
diverse set of LLMs. The LLMs provided qualitative assessments and structured
binary classifications on variable choice, coefficient plausibility, and
publication suitability. The results indicate that while LLMs can expertly
assess the coherence of variable choices (with top models like GPT-4o achieving
an overall F1 score of 0.87), their performance varies significantly when
evaluating deeper aspects such as coefficient plausibility and overall
publication suitability. The results further revealed that the choice of LLM,
the specific characteristics of the paper and the interaction between these two
factors significantly influence the accuracy of the assessment, particularly
for nuanced judgments. These findings highlight LLMs' current strengths in
assisting with initial, more surface-level checks and their limitations in
performing comprehensive, deep economic reasoning, suggesting a potential
assistive role in peer review that still necessitates robust human oversight.

</details>


### [491] [Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce](https://arxiv.org/abs/2506.06576)
*Yijia Shao,Humishka Zope,Yucheng Jiang,Jiaxin Pei,David Nguyen,Erik Brynjolfsson,Diyi Yang*

Main category: cs.CY

TL;DR: 论文提出了一种新的审计框架，用于评估工人希望AI代理自动化或增强的任务，并分析这些愿望与当前技术能力的匹配情况。


<details>
  <summary>Details</summary>
Motivation: 随着复合AI系统的快速发展，劳动力市场面临就业替代、人类能动性减弱和对自动化过度依赖的担忧，但目前缺乏对这一演变景观的系统性理解。

Method: 研究引入了一个音频增强的迷你访谈框架和人类能动性量表（HAS），构建了WORKBank数据库，结合工人偏好和AI专家评估，将任务分为四个区域。

Result: 研究发现不同职业对人类参与的期望存在多样性，并揭示了AI代理整合可能如何改变核心人类能力，从信息技能转向人际技能。

Conclusion: 研究强调了将AI代理开发与人类愿望对齐的重要性，并为工人适应动态工作环境提供了指导。

Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the
labor market, raising concerns about job displacement, diminished human agency,
and overreliance on automation. Yet, we lack a systematic understanding of the
evolving landscape. In this paper, we address this gap by introducing a novel
auditing framework to assess which occupational tasks workers want AI agents to
automate or augment, and how those desires align with the current technological
capabilities. Our framework features an audio-enhanced mini-interview to
capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a
shared language to quantify the preferred level of human involvement. Using
this framework, we construct the WORKBank database, building on the U.S.
Department of Labor's O*NET database, to capture preferences from 1,500 domain
workers and capability assessments from AI experts across over 844 tasks
spanning 104 occupations. Jointly considering the desire and technological
capability divides tasks in WORKBank into four zones: Automation "Green Light"
Zone, Automation "Red Light" Zone, R&D Opportunity Zone, Low Priority Zone.
This highlights critical mismatches and opportunities for AI agent development.
Moving beyond a simple automate-or-not dichotomy, our results reveal diverse
HAS profiles across occupations, reflecting heterogeneous expectations for
human involvement. Moreover, our study offers early signals of how AI agent
integration may reshape the core human competencies, shifting from
information-focused skills to interpersonal ones. These findings underscore the
importance of aligning AI agent development with human desires and preparing
workers for evolving workplace dynamics.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [492] [Applying XAI based unsupervised knowledge discovering for Operation modes in a WWTP. A real case: AQUAVALL WWTP](https://arxiv.org/abs/2506.05958)
*Alicia Beneyto-Rodriguez,Gregorio I. Sainz-Palmero,Marta Galende-Hernández,María J. Fuente,José M. Cuenca*

Main category: eess.SY

TL;DR: 论文提出了一种基于可解释人工智能（XAI）的方法，用于从污水处理厂（WWTP）的历史数据中提取可解释的操作模式，帮助管理者快速理解工厂运行状态。


<details>
  <summary>Details</summary>
Motivation: 随着淡水需求增加，废水处理变得至关重要。污水处理厂（WWTP）生成大量数据，但管理者难以从中提取有用信息。XAI和机器学习（ML）可以解决这一问题。

Method: 采用XAI和ML方法，对西班牙Valladolid市AQUAVALL管理的WWTP历史数据进行分析，提取低维数据空间中的可解释操作模式。

Result: 成功从大量历史数据中总结出少数可解释的操作模式，并识别出相关变量和设施单元。

Conclusion: XAI方法能有效帮助污水处理厂管理者快速理解运行状态，为决策提供支持。

Abstract: Water reuse is a key point when fresh water is a commodity in ever greater
demand, but which is also becoming ever more available. Furthermore, the return
of clean water to its natural environment is also mandatory. Therefore,
wastewater treatment plants (WWTPs) are essential in any policy focused on
these serious challenges.
  WWTPs are complex facilities which need to operate at their best to achieve
their goals. Nowadays, they are largely monitored, generating large databases
of historical data concerning their functioning over time. All this implies a
large amount of embedded information which is not usually easy for plant
managers to assimilate, correlate and understand; in other words, for them to
know the global operation of the plant at any given time. At this point, the
intelligent and Machine Learning (ML) approaches can give support for that
need, managing all the data and translating them into manageable, interpretable
and explainable knowledge about how the WWTP plant is operating at a glance.
  Here, an eXplainable Artificial Intelligence (XAI) based methodology is
proposed and tested for a real WWTP, in order to extract explainable service
knowledge concerning the operation modes of the WWTP managed by AQUAVALL, which
is the public service in charge of the integral water cycle in the City Council
of Valladolid (Castilla y Le\'on, Spain). By applying well-known approaches of
XAI and ML focused on the challenge of WWTP, it has been possible to summarize
a large number of historical databases through a few explained operation modes
of the plant in a low-dimensional data space, showing the variables and
facility units involved in each case.

</details>


### [493] [Applying XAI based unsupervised knowledge discovering for Operation modes in a WWTP. A real case: AQUAVALL WWTP](https://arxiv.org/abs/2506.05958)
*Alicia Beneyto-Rodriguez,Gregorio I. Sainz-Palmero,Marta Galende-Hernández,María J. Fuente,José M. Cuenca*

Main category: eess.SY

TL;DR: 论文提出了一种基于可解释人工智能（XAI）的方法，用于从污水处理厂（WWTP）的历史数据中提取可解释的操作模式，帮助管理人员快速理解工厂运行状态。


<details>
  <summary>Details</summary>
Motivation: 随着淡水需求增加和污水处理厂数据复杂化，需要智能方法帮助管理人员理解大量数据。

Method: 采用XAI和机器学习技术，分析污水处理厂的历史数据，提取低维空间中的操作模式。

Result: 成功将大量历史数据总结为少数可解释的操作模式，并识别出关键变量和设施单元。

Conclusion: XAI方法能有效支持污水处理厂的数据管理和操作理解，为实际应用提供了可行方案。

Abstract: Water reuse is a key point when fresh water is a commodity in ever greater
demand, but which is also becoming ever more available. Furthermore, the return
of clean water to its natural environment is also mandatory. Therefore,
wastewater treatment plants (WWTPs) are essential in any policy focused on
these serious challenges.
  WWTPs are complex facilities which need to operate at their best to achieve
their goals. Nowadays, they are largely monitored, generating large databases
of historical data concerning their functioning over time. All this implies a
large amount of embedded information which is not usually easy for plant
managers to assimilate, correlate and understand; in other words, for them to
know the global operation of the plant at any given time. At this point, the
intelligent and Machine Learning (ML) approaches can give support for that
need, managing all the data and translating them into manageable, interpretable
and explainable knowledge about how the WWTP plant is operating at a glance.
  Here, an eXplainable Artificial Intelligence (XAI) based methodology is
proposed and tested for a real WWTP, in order to extract explainable service
knowledge concerning the operation modes of the WWTP managed by AQUAVALL, which
is the public service in charge of the integral water cycle in the City Council
of Valladolid (Castilla y Le\'on, Spain). By applying well-known approaches of
XAI and ML focused on the challenge of WWTP, it has been possible to summarize
a large number of historical databases through a few explained operation modes
of the plant in a low-dimensional data space, showing the variables and
facility units involved in each case.

</details>


### [494] [The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage](https://arxiv.org/abs/2506.06484)
*Manuel Sage,Khalil Al Handawi,Yaoyao Fiona Zhao*

Main category: eess.SY

TL;DR: 该论文提出了一种改进的深度强化学习方法，用于优化Power-to-Gas（P2G）系统的长期经济运营，结合电池储能系统和燃气轮机，解决了延迟奖励问题。


<details>
  <summary>Details</summary>
Motivation: P2G技术在整合可再生能源方面具有潜力，但其经济运营因能源波动性和低效率而复杂化。现有研究多关注短期能源转换，忽视了P2G的长期储能能力。

Method: 通过三个逐步复杂的案例研究，评估了深度Q网络和近端策略优化算法，并引入预测整合、奖励函数惩罚和战略成本计算等改进措施。

Result: 改进后的深度强化学习方法显著提升了P2G系统长期运营的成本效益决策能力。

Conclusion: 该方法为P2G技术的长期储能潜力提供了可行的解决方案。

Abstract: Power-to-Gas (P2G) technologies gain recognition for enabling the integration
of intermittent renewables, such as wind and solar, into electricity grids.
However, determining the most cost-effective operation of these systems is
complex due to the volatile nature of renewable energy, electricity prices, and
loads. Additionally, P2G systems are less efficient in converting and storing
energy compared to battery energy storage systems (BESs), and the benefits of
converting electricity into gas are not immediately apparent. Deep
Reinforcement Learning (DRL) has shown promise in managing the operation of
energy systems amidst these uncertainties. Yet, DRL techniques face
difficulties with the delayed reward characteristic of P2G system operation.
Previous research has mostly focused on short-term studies that look at the
energy conversion process, neglecting the long-term storage capabilities of
P2G.
  This study presents a new method by thoroughly examining how DRL can be
applied to the economic operation of P2G systems, in combination with BESs and
gas turbines, over extended periods. Through three progressively more complex
case studies, we assess the performance of DRL algorithms, specifically Deep
Q-Networks and Proximal Policy Optimization, and introduce modifications to
enhance their effectiveness. These modifications include integrating forecasts,
implementing penalties on the reward function, and applying strategic cost
calculations, all aimed at addressing the issue of delayed rewards. Our
findings indicate that while DRL initially struggles with the complex
decision-making required for P2G system operation, the adjustments we propose
significantly improve its capability to devise cost-effective operation
strategies, thereby unlocking the potential for long-term energy storage in P2G
technologies.

</details>


### [495] [From Model-Based and Adaptive Control to Evolving Fuzzy Control](https://arxiv.org/abs/2506.06594)
*Daniel Leite,Igor Škrjanc,Fernando Gomide*

Main category: eess.SY

TL;DR: 本文回顾了模糊集理论60年来的发展，重点介绍了经典模糊与自适应建模控制框架的核心贡献，并探讨了演化智能系统在模糊建模与控制中的优势与挑战。


<details>
  <summary>Details</summary>
Motivation: 纪念模糊集理论60周年，总结其历史发展与核心贡献，并探讨演化智能系统在非平稳环境中的重要性。

Method: 回顾经典模糊与自适应建模控制框架，分析演化智能系统的优势。

Result: 演化模糊系统在非平稳环境中表现出色，但仍面临安全性、可解释性和结构演化等挑战。

Conclusion: 未来需关注安全性、可解释性和结构演化等方向，以推动模糊系统的发展。

Abstract: Evolving fuzzy systems build and adapt fuzzy models - such as predictors and
controllers - by incrementally updating their rule-base structure from data
streams. On the occasion of the 60-year anniversary of fuzzy set theory,
commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the
historical development and core contributions of classical fuzzy and adaptive
modeling and control frameworks. It then highlights the emergence and
significance of evolving intelligent systems in fuzzy modeling and control,
emphasizing their advantages in handling nonstationary environments. Key
challenges and future directions are discussed, including safety,
interpretability, and principled structural evolution.

</details>


### [496] [On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)](https://arxiv.org/abs/2506.07079)
*Mostafa Eslami,Maryam Babazadeh*

Main category: eess.SY

TL;DR: 本文提出了一种基于数据辅助控制（DAC）的混合控制框架，用于端口哈密顿系统，通过动态分解将系统分为两部分处理不确定性，并结合非线性控制器和强化学习实现优化控制。


<details>
  <summary>Details</summary>
Motivation: 解决端口哈密顿系统中结构和参数不确定性的问题，同时保持系统固有结构，并提升控制性能、安全性和可解释性。

Method: 将系统动态分解为右端（RHS）和左端（LHS），分别处理哈密顿流和耗散/输入流；使用非线性控制器和强化学习优化控制策略。

Result: 框架有效管理了不确定性，提升了性能、安全性和可解释性，并通过仿真验证了其实际应用效果。

Conclusion: 该混合控制框架为端口哈密顿系统提供了一种高效且可解释的解决方案，为未来理论研究奠定了基础。

Abstract: This paper introduces a hypothetical hybrid control framework for
port-Hamiltonian (p$\mathcal{H}$) systems, employing a dynamic decomposition
based on Data-Assisted Control (DAC). The system's evolution is split into two
parts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow
handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a
dissipative/input flow addressing both structural and parametric uncertainties.
A virtual port variable $\Pi$ serves as the interface between these two
components. A nonlinear controller manages the intrinsic Hamiltonian flow,
determining a desired port control value $\Pi_c$. Concurrently, Reinforcement
Learning (RL) is applied to the dissipative/input flow to learn an agent for
providing optimal policy in mapping $\Pi_c$ to the actual system input. This
hybrid approach effectively manages RHS uncertainties while preserving the
system's inherent structure. Key advantages include adjustable performance via
LHS controller parameters, enhanced AI explainability and interpretability
through the port variable $\Pi$, the ability to guarantee safety and state
attainability with hard/soft constraints, reduced complexity in learning
hypothesis classes compared to end-to-end solutions, and improved
state/parameter estimation using LHS prior knowledge and system Hamiltonian to
address partial observability. The paper details the p$\mathcal{H}$
formulation, derives the decomposition, and presents the modular controller
architecture. Beyond design, crucial aspects of stability and robustness
analysis and synthesis are investigated, paving the way for deeper theoretical
investigations. An application example, a pendulum with nonlinear dynamics, is
simulated to demonstrate the approach's empirical and phenomenological benefits
for future research.

</details>


### [497] [Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems](https://arxiv.org/abs/2506.07347)
*Armin Lederer,Erfaun Noorani,Andreas Krause*

Main category: eess.SY

TL;DR: 提出了一种基于控制屏障函数（CBFs）的风险敏感安全过滤器，用于离散时间多智能体系统，通过集中式风险敏感条件和分布式策略切换确保安全性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在缺乏集中协调时的安全性问题，尤其是面对动态不确定性时的挑战。

Method: 利用基于值函数的CBFs，提出集中式风险敏感条件，并引入两种分布式策略（最坏情况预测和接近已知安全策略）以确保可行性。

Result: 数值评估表明，该方法能在不保守的情况下有效维持安全性。

Conclusion: 该方法为多智能体系统的安全性提供了一种灵活且稳健的解决方案。

Abstract: Ensuring safety in multi-agent systems is a significant challenge,
particularly in settings where centralized coordination is impractical. In this
work, we propose a novel risk-sensitive safety filter for discrete-time
multi-agent systems with uncertain dynamics that leverages control barrier
functions (CBFs) defined through value functions. Our approach relies on
centralized risk-sensitive safety conditions based on exponential risk
operators to ensure robustness against model uncertainties. We introduce a
distributed formulation of the safety filter by deriving two alternative
strategies: one based on worst-case anticipation and another on proximity to a
known safe policy. By allowing agents to switch between strategies, feasibility
can be ensured. Through detailed numerical evaluations, we demonstrate the
efficacy of our approach in maintaining safety without being overly
conservative.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [498] [Attacking Attention of Foundation Models Disrupts Downstream Tasks](https://arxiv.org/abs/2506.05394)
*Hondamunige Prasanna Silva,Federico Becattini,Lorenzo Seidenari*

Main category: cs.CR

TL;DR: 该论文研究了视觉基础模型（如CLIP和ViT）的对抗攻击漏洞，并提出了一种针对Transformer架构的新型攻击方法，展示了其在多种下游任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在AI应用中广泛使用，但其对预训练模型的依赖引入了安全风险，尤其是对抗攻击的威胁。

Method: 提出了一种针对Transformer架构的任务无关攻击方法，并测试了其在分类、描述、检索、分割和深度估计等任务中的效果。

Result: 攻击方法在多种下游任务中表现出高效性，验证了基础模型的脆弱性。

Conclusion: 研究揭示了基础模型的安全隐患，强调了对抗攻击的潜在威胁，并为未来的防御策略提供了参考。

Abstract: Foundation models represent the most prominent and recent paradigm shift in
artificial intelligence. Foundation models are large models, trained on broad
data that deliver high accuracy in many downstream tasks, often without
fine-tuning. For this reason, models such as CLIP , DINO or Vision Transfomers
(ViT), are becoming the bedrock of many industrial AI-powered applications.
However, the reliance on pre-trained foundation models also introduces
significant security concerns, as these models are vulnerable to adversarial
attacks. Such attacks involve deliberately crafted inputs designed to deceive
AI systems, jeopardizing their reliability. This paper studies the
vulnerabilities of vision foundation models, focusing specifically on CLIP and
ViTs, and explores the transferability of adversarial attacks to downstream
tasks. We introduce a novel attack, targeting the structure of
transformer-based architectures in a task-agnostic fashion. We demonstrate the
effectiveness of our attack on several downstream tasks: classification,
captioning, image/text retrieval, segmentation and depth estimation. Code
available at:https://github.com/HondamunigePrasannaSilva/attack-attention

</details>


### [499] [Sylva: Tailoring Personalized Adversarial Defense in Pre-trained Models via Collaborative Fine-tuning](https://arxiv.org/abs/2506.05402)
*Tianyu Qi,Lei Xue,Yufeng Zhan,Xiaobo Ma*

Main category: cs.CR

TL;DR: 论文提出了一种名为Sylva的个性化协作对抗训练框架，通过两阶段过程为每个客户端提供定制化的防御模型，显著提升了通信效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中大规模预训练模型的部署面临对抗攻击威胁，现有联邦对抗训练（FAT）方法在个性化性能和通信效率上存在不足。

Method: Sylva采用两阶段方法：1）使用LoRA进行本地对抗微调以减少通信成本；2）引入基于博弈论的层选择策略优化良性数据准确性。

Result: 实验表明，Sylva在通信效率上提升50倍，对抗鲁棒性和良性数据准确性分别提升29.5%和50.4%。

Conclusion: Sylva有效平衡了模型的鲁棒性和准确性，为个性化防御模型提供了高效解决方案。

Abstract: The growing adoption of large pre-trained models in edge computing has made
deploying model inference on mobile clients both practical and popular. These
devices are inherently vulnerable to direct adversarial attacks, which pose a
substantial threat to the robustness and security of deployed models. Federated
adversarial training (FAT) has emerged as an effective solution to enhance
model robustness while preserving client privacy. However, FAT frequently
produces a generalized global model, which struggles to address the diverse and
heterogeneous data distributions across clients, resulting in insufficiently
personalized performance, while also encountering substantial communication
challenges during the training process. In this paper, we propose
\textit{Sylva}, a personalized collaborative adversarial training framework
designed to deliver customized defense models for each client through a
two-phase process. In Phase 1, \textit{Sylva} employs LoRA for local
adversarial fine-tuning, enabling clients to personalize model robustness while
drastically reducing communication costs by uploading only LoRA parameters
during federated aggregation. In Phase 2, a game-based layer selection strategy
is introduced to enhance accuracy on benign data, further refining the
personalized model. This approach ensures that each client receives a tailored
defense model that balances robustness and accuracy effectively. Extensive
experiments on benchmark datasets demonstrate that \textit{Sylva} can achieve
up to 50$\times$ improvements in communication efficiency compared to
state-of-the-art algorithms, while achieving up to 29.5\% and 50.4\%
enhancements in adversarial robustness and benign accuracy, respectively.

</details>


### [500] [Differentially Private Federated $k$-Means Clustering with Server-Side Data](https://arxiv.org/abs/2506.05408)
*Jonathan Scott,Christoph H. Lampert,David Saulpic*

Main category: cs.CR

TL;DR: 提出了一种名为\acronym的联邦差分隐私k均值聚类算法，解决了分布式数据隐私保护下的聚类问题。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法不适用于分布式数据（如边缘设备生成的数据），且隐私问题限制了数据集中处理。

Method: 结合服务器端数据初始化，采用简单的联邦DP-Lloyds算法，实现高效聚类。

Result: 在合成和真实基准任务中表现优异，并提供了收敛速度和聚类成功率的理论分析。

Conclusion: \acronym算法在联邦和隐私保护环境下有效解决了k均值聚类问题。

Abstract: Clustering is a cornerstone of data analysis that is particularly suited to
identifying coherent subgroups or substructures in unlabeled data, as are
generated continuously in large amounts these days. However, in many cases
traditional clustering methods are not applicable, because data are
increasingly being produced and stored in a distributed way, e.g. on edge
devices, and privacy concerns prevent it from being transferred to a central
server. To address this challenge, we present \acronym, a new algorithm for
$k$-means clustering that is fully-federated as well as differentially private.
Our approach leverages (potentially small and out-of-distribution) server-side
data to overcome the primary challenge of differentially private clustering
methods: the need for a good initialization. Combining our initialization with
a simple federated DP-Lloyds algorithm we obtain an algorithm that achieves
excellent results on synthetic and real-world benchmark tasks. We also provide
a theoretical analysis of our method that provides bounds on the convergence
speed and cluster identification success.

</details>


### [501] [FERRET: Private Deep Learning Faster And Better Than DPSGD](https://arxiv.org/abs/2506.05416)
*David Zagardo*

Main category: cs.CR

TL;DR: FERRET是一种基于1位梯度压缩和互信息差分隐私（MI-DP）的快速有效训练方法，通过Bernoulli掩码传输符号位，实现隐私保护、高效性和性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决传统差分隐私方法（如DPSGD）在隐私保护、模型性能和训练效率之间的权衡问题，提出一种无需加性噪声的隐私保护方法。

Method: 基于signSGD，提出FERRET方法，通过Bernoulli掩码传输符号位，分组处理参数，并理论证明其隐私损失上限。

Result: FERRET在多个粒度下均优于DPSGD，训练速度更快（19-33%时间），困惑度更低（3x），且隐私保护效果与DPSGD相当。

Conclusion: FERRET在隐私、性能和效率三方面取得平衡，证明了1位梯度压缩在隐私保护训练中的潜力。

Abstract: We revisit 1-bit gradient compression through the lens of mutual-information
differential privacy (MI-DP). Building on signSGD, we propose FERRET--Fast and
Effective Restricted Release for Ethical Training--which transmits at most one
sign bit per parameter group with Bernoulli masking.
  Theory: We prove each fired group leaks at most ln 2 nats; after subsampling
with rate s, the total privacy loss of G groups trained for T steps with firing
probability p is epsilon = G * T * s * p * ln 2. Thus FERRET achieves MI-DP for
epsilon in [0.1, 2] without additive noise.
  Practice: We evaluate three granularities--FERRET-MAX (finest), FERRET-EIGHTH
(medium), and FERRET-2 (coarsest)--on five LLMs (137M-1.8B parameters) against
DPSGD and Non-DP baselines. All methods trained for 1, 3, and 5 epochs.
  Utility: Across all settings, FERRET-MAX/EIGHTH beat DPSGD's perplexity. At
epsilon=0.5, 5 epochs: FERRET-EIGHTH achieves 3.98 perplexity vs DPSGD's 11.61
(2.9x better), within 23% of Non-DP (3.25).
  Privacy: MI-AUC stays at chance for FERRET-MAX/EIGHTH (~0.51), matching DPSGD
vs Non-DP's 0.76-0.99. FERRET-2 shows higher leakage (~0.55) due to lower
headroom.
  Efficiency: Stricter budgets fire fewer signs, so FERRET uses 19-33% of
DPSGD's training time and only 34-36% of Non-DP training time.
  Take-away: Sign-based MI-DP gets closer to achieving all three qualities of
the privacy, utility, performance trilemma: FERRET trains up to 5x faster,
achieves 3x lower perplexity compared to DPSGD and 1.2x greater than Non-DP,
all while providing formal, mathematically provable privacy guarantees using
zero additive noise. The results also show that, in certain instances, masked
1-bit updates can match non-private training utility while safeguarding data.

</details>


### [502] [Stealix: Model Stealing via Prompt Evolution](https://arxiv.org/abs/2506.05867)
*Zhixiong Zhuang,Hui-Po Wang,Maria-Irina Nicolae,Mario Fritz*

Main category: cs.CR

TL;DR: 论文提出Stealix方法，首次实现无需预定义提示的模型窃取，利用遗传算法优化提示，显著提升合成图像的精度和多样性。


<details>
  <summary>Details</summary>
Motivation: 模型窃取对机器学习安全构成威胁，现有方法依赖人工设计提示，限制了自动化和可扩展性。

Method: Stealix结合两个开源预训练模型推断受害者模型的数据分布，通过遗传算法迭代优化提示。

Result: 实验显示Stealix在相同查询预算下显著优于其他方法，即使对手具备类别名或精细提示。

Conclusion: 预训练生成模型在模型窃取中的风险可能被低估，Stealix展示了其可扩展性。

Abstract: Model stealing poses a significant security risk in machine learning by
enabling attackers to replicate a black-box model without access to its
training data, thus jeopardizing intellectual property and exposing sensitive
information. Recent methods that use pre-trained diffusion models for data
synthesis improve efficiency and performance but rely heavily on manually
crafted prompts, limiting automation and scalability, especially for attackers
with little expertise. To assess the risks posed by open-source pre-trained
models, we propose a more realistic threat model that eliminates the need for
prompt design skills or knowledge of class names. In this context, we introduce
Stealix, the first approach to perform model stealing without predefined
prompts. Stealix uses two open-source pre-trained models to infer the victim
model's data distribution, and iteratively refines prompts through a genetic
algorithm, progressively improving the precision and diversity of synthetic
images. Our experimental results demonstrate that Stealix significantly
outperforms other methods, even those with access to class names or
fine-grained prompts, while operating under the same query budget. These
findings highlight the scalability of our approach and suggest that the risks
posed by pre-trained generative models in model stealing may be greater than
previously recognized.

</details>


### [503] [Attacking Attention of Foundation Models Disrupts Downstream Tasks](https://arxiv.org/abs/2506.05394)
*Hondamunige Prasanna Silva,Federico Becattini,Lorenzo Seidenari*

Main category: cs.CR

TL;DR: 本文研究了视觉基础模型（如CLIP和ViT）的对抗攻击漏洞，并提出了一种针对Transformer架构的新型攻击方法，验证了其在多种下游任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在AI领域广泛应用，但其安全性问题（如对抗攻击）尚未充分研究，尤其是视觉基础模型的漏洞和攻击可迁移性。

Method: 提出了一种针对Transformer架构的任务无关攻击方法，攻击目标为模型结构本身。

Result: 实验证明该方法在分类、描述生成、图像/文本检索、分割和深度估计等多种下游任务中均有效。

Conclusion: 视觉基础模型存在对抗攻击漏洞，新型攻击方法具有广泛适用性，需进一步研究防御措施。

Abstract: Foundation models represent the most prominent and recent paradigm shift in
artificial intelligence. Foundation models are large models, trained on broad
data that deliver high accuracy in many downstream tasks, often without
fine-tuning. For this reason, models such as CLIP , DINO or Vision Transfomers
(ViT), are becoming the bedrock of many industrial AI-powered applications.
However, the reliance on pre-trained foundation models also introduces
significant security concerns, as these models are vulnerable to adversarial
attacks. Such attacks involve deliberately crafted inputs designed to deceive
AI systems, jeopardizing their reliability. This paper studies the
vulnerabilities of vision foundation models, focusing specifically on CLIP and
ViTs, and explores the transferability of adversarial attacks to downstream
tasks. We introduce a novel attack, targeting the structure of
transformer-based architectures in a task-agnostic fashion. We demonstrate the
effectiveness of our attack on several downstream tasks: classification,
captioning, image/text retrieval, segmentation and depth estimation. Code
available at:https://github.com/HondamunigePrasannaSilva/attack-attention

</details>


### [504] [Sylva: Tailoring Personalized Adversarial Defense in Pre-trained Models via Collaborative Fine-tuning](https://arxiv.org/abs/2506.05402)
*Tianyu Qi,Lei Xue,Yufeng Zhan,Xiaobo Ma*

Main category: cs.CR

TL;DR: 论文提出了一种名为Sylva的个性化协作对抗训练框架，通过两阶段过程为每个客户端提供定制化的防御模型，显著提升了通信效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中大规模预训练模型的部署面临对抗攻击威胁，现有联邦对抗训练（FAT）方法在个性化性能和通信效率上存在不足。

Method: Sylva采用两阶段方法：1）使用LoRA进行本地对抗微调以减少通信成本；2）引入基于游戏的层选择策略优化良性数据准确性。

Result: 实验表明，Sylva在通信效率上提升50倍，对抗鲁棒性和良性数据准确性分别提升29.5%和50.4%。

Conclusion: Sylva为客户端提供了高效且个性化的防御模型，平衡了鲁棒性和准确性。

Abstract: The growing adoption of large pre-trained models in edge computing has made
deploying model inference on mobile clients both practical and popular. These
devices are inherently vulnerable to direct adversarial attacks, which pose a
substantial threat to the robustness and security of deployed models. Federated
adversarial training (FAT) has emerged as an effective solution to enhance
model robustness while preserving client privacy. However, FAT frequently
produces a generalized global model, which struggles to address the diverse and
heterogeneous data distributions across clients, resulting in insufficiently
personalized performance, while also encountering substantial communication
challenges during the training process. In this paper, we propose
\textit{Sylva}, a personalized collaborative adversarial training framework
designed to deliver customized defense models for each client through a
two-phase process. In Phase 1, \textit{Sylva} employs LoRA for local
adversarial fine-tuning, enabling clients to personalize model robustness while
drastically reducing communication costs by uploading only LoRA parameters
during federated aggregation. In Phase 2, a game-based layer selection strategy
is introduced to enhance accuracy on benign data, further refining the
personalized model. This approach ensures that each client receives a tailored
defense model that balances robustness and accuracy effectively. Extensive
experiments on benchmark datasets demonstrate that \textit{Sylva} can achieve
up to 50$\times$ improvements in communication efficiency compared to
state-of-the-art algorithms, while achieving up to 29.5\% and 50.4\%
enhancements in adversarial robustness and benign accuracy, respectively.

</details>


### [505] [Differentially Private Federated $k$-Means Clustering with Server-Side Data](https://arxiv.org/abs/2506.05408)
*Jonathan Scott,Christoph H. Lampert,David Saulpic*

Main category: cs.CR

TL;DR: 提出了一种名为\acronym的联邦差分隐私k均值聚类算法，解决了分布式数据隐私保护下的聚类问题。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法不适用于分布式数据（如边缘设备）且隐私问题阻碍数据集中处理，需要一种联邦且隐私保护的聚类方法。

Method: 结合服务器端数据初始化，采用联邦DP-Lloyds算法实现差分隐私k均值聚类。

Result: 在合成和真实基准任务中表现优异，理论分析提供了收敛速度和聚类识别成功的界限。

Conclusion: \acronym算法在联邦和隐私保护场景下有效解决了聚类问题，具有理论和实际优势。

Abstract: Clustering is a cornerstone of data analysis that is particularly suited to
identifying coherent subgroups or substructures in unlabeled data, as are
generated continuously in large amounts these days. However, in many cases
traditional clustering methods are not applicable, because data are
increasingly being produced and stored in a distributed way, e.g. on edge
devices, and privacy concerns prevent it from being transferred to a central
server. To address this challenge, we present \acronym, a new algorithm for
$k$-means clustering that is fully-federated as well as differentially private.
Our approach leverages (potentially small and out-of-distribution) server-side
data to overcome the primary challenge of differentially private clustering
methods: the need for a good initialization. Combining our initialization with
a simple federated DP-Lloyds algorithm we obtain an algorithm that achieves
excellent results on synthetic and real-world benchmark tasks. We also provide
a theoretical analysis of our method that provides bounds on the convergence
speed and cluster identification success.

</details>


### [506] [FERRET: Private Deep Learning Faster And Better Than DPSGD](https://arxiv.org/abs/2506.05416)
*David Zagardo*

Main category: cs.CR

TL;DR: FERRET是一种基于1-bit梯度压缩的方法，通过互信息差分隐私（MI-DP）实现高效、隐私保护的训练，性能优于DPSGD。


<details>
  <summary>Details</summary>
Motivation: 研究如何在梯度压缩中实现隐私保护，同时保持高效性和模型性能。

Method: 基于signSGD，提出FERRET方法，通过Bernoulli掩码传输1-bit梯度，并证明其满足MI-DP。

Result: FERRET在隐私、性能和效率上优于DPSGD，部分情况下接近非隐私训练的效果。

Conclusion: FERRET通过1-bit梯度压缩和MI-DP，实现了隐私、性能和效率的平衡，为隐私保护训练提供了新思路。

Abstract: We revisit 1-bit gradient compression through the lens of mutual-information
differential privacy (MI-DP). Building on signSGD, we propose FERRET--Fast and
Effective Restricted Release for Ethical Training--which transmits at most one
sign bit per parameter group with Bernoulli masking.
  Theory: We prove each fired group leaks at most ln 2 nats; after subsampling
with rate s, the total privacy loss of G groups trained for T steps with firing
probability p is epsilon = G * T * s * p * ln 2. Thus FERRET achieves MI-DP for
epsilon in [0.1, 2] without additive noise.
  Practice: We evaluate three granularities--FERRET-MAX (finest), FERRET-EIGHTH
(medium), and FERRET-2 (coarsest)--on five LLMs (137M-1.8B parameters) against
DPSGD and Non-DP baselines. All methods trained for 1, 3, and 5 epochs.
  Utility: Across all settings, FERRET-MAX/EIGHTH beat DPSGD's perplexity. At
epsilon=0.5, 5 epochs: FERRET-EIGHTH achieves 3.98 perplexity vs DPSGD's 11.61
(2.9x better), within 23% of Non-DP (3.25).
  Privacy: MI-AUC stays at chance for FERRET-MAX/EIGHTH (~0.51), matching DPSGD
vs Non-DP's 0.76-0.99. FERRET-2 shows higher leakage (~0.55) due to lower
headroom.
  Efficiency: Stricter budgets fire fewer signs, so FERRET uses 19-33% of
DPSGD's training time and only 34-36% of Non-DP training time.
  Take-away: Sign-based MI-DP gets closer to achieving all three qualities of
the privacy, utility, performance trilemma: FERRET trains up to 5x faster,
achieves 3x lower perplexity compared to DPSGD and 1.2x greater than Non-DP,
all while providing formal, mathematically provable privacy guarantees using
zero additive noise. The results also show that, in certain instances, masked
1-bit updates can match non-private training utility while safeguarding data.

</details>


### [507] [Stealix: Model Stealing via Prompt Evolution](https://arxiv.org/abs/2506.05867)
*Zhixiong Zhuang,Hui-Po Wang,Maria-Irina Nicolae,Mario Fritz*

Main category: cs.CR

TL;DR: Stealix是一种无需预定义提示的模型窃取方法，利用开源预训练模型和遗传算法优化提示，显著提升了合成图像的精度和多样性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 模型窃取对机器学习安全构成威胁，现有方法依赖手工提示，限制了自动化和可扩展性。Stealix旨在消除对提示设计技能的需求，评估开源预训练模型的风险。

Method: Stealix使用两个开源预训练模型推断受害者模型的数据分布，并通过遗传算法迭代优化提示，提升合成图像质量。

Result: 实验表明，Stealix在相同查询预算下显著优于其他方法，包括依赖类名或精细提示的方法。

Conclusion: Stealix展示了预训练生成模型在模型窃取中的潜在风险可能被低估，其方法具有高度可扩展性。

Abstract: Model stealing poses a significant security risk in machine learning by
enabling attackers to replicate a black-box model without access to its
training data, thus jeopardizing intellectual property and exposing sensitive
information. Recent methods that use pre-trained diffusion models for data
synthesis improve efficiency and performance but rely heavily on manually
crafted prompts, limiting automation and scalability, especially for attackers
with little expertise. To assess the risks posed by open-source pre-trained
models, we propose a more realistic threat model that eliminates the need for
prompt design skills or knowledge of class names. In this context, we introduce
Stealix, the first approach to perform model stealing without predefined
prompts. Stealix uses two open-source pre-trained models to infer the victim
model's data distribution, and iteratively refines prompts through a genetic
algorithm, progressively improving the precision and diversity of synthetic
images. Our experimental results demonstrate that Stealix significantly
outperforms other methods, even those with access to class names or
fine-grained prompts, while operating under the same query budget. These
findings highlight the scalability of our approach and suggest that the risks
posed by pre-trained generative models in model stealing may be greater than
previously recognized.

</details>


### [508] [Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection](https://arxiv.org/abs/2506.00654)
*Marco Di Gennaro,Francesco Panebianco,Marco Pianta,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: Amatriciana是一种基于图神经网络的创新方法，用于检测交易图中的洗钱者，利用时间信息并整合全图数据。实验表明，该模型在小数据量下表现良好，大数据量下优于其他先进方法，F1得分为0.76，并减少55%的误报。


<details>
  <summary>Details</summary>
Motivation: 洗钱对金融安全和社会稳定构成严重威胁，交易量增长需要自动工具辅助执法机构检测此类犯罪活动。

Method: 提出Amatriciana方法，基于图神经网络，整合全图交易数据（不分时间子图），利用时间信息和关系数据。

Result: 在公开数据集上，模型在小数据量下表现良好，大数据量下优于其他方法，F1得分为0.76，误报减少55%。

Conclusion: Amatriciana是一种高效的洗钱检测方法，尤其在减少误报方面表现突出。

Abstract: Money laundering is a financial crime that poses a serious threat to
financial integrity and social security. The growing number of transactions
makes it necessary to use automatic tools that help law enforcement agencies
detect such criminal activity. In this work, we present Amatriciana, a novel
approach based on Graph Neural Networks to detect money launderers inside a
graph of transactions by considering temporal information. Amatriciana uses the
whole graph of transactions without splitting it into several time-based
subgraphs, exploiting all relational information in the dataset. Our
experiments on a public dataset reveal that the model can learn from a limited
amount of data. Furthermore, when more data is available, the model outperforms
other State-of-the-art approaches; in particular, Amatriciana decreases the
number of False Positives (FPs) while detecting many launderers. In summary,
Amatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55%
with respect to other State-of-the-art models.

</details>


### [509] [TimeWak: Temporal Chained-Hashing Watermark for Time Series Data](https://arxiv.org/abs/2506.06407)
*Zhi Wen Soi,Chaoyi Zhu,Fouad Abiad,Aditya Shankar,Jeroen M. Galjaard,Huijuan Wang,Lydia Y. Chen*

Main category: cs.CR

TL;DR: TimeWak是一种用于多元时间序列扩散模型的水印算法，直接在真实时间-特征空间中嵌入水印，解决了特征异质性和时间依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有水印方法在真实空间生成的时间序列中不兼容的问题，同时确保数据的高效用性和可追溯性。

Method: 提出TimeWak算法，嵌入时间链式哈希水印，并采用ε-精确反转技术处理扩散过程的反转误差。

Result: 在5个数据集上验证，TimeWak在合成数据质量和水印可检测性上显著优于现有方法，提升了61.96%的context-FID分数和8.44%的相关性分数。

Conclusion: TimeWak有效解决了时间序列水印的挑战，同时保持高数据质量和可检测性。

Abstract: Synthetic time series generated by diffusion models enable sharing
privacy-sensitive datasets, such as patients' functional MRI records. Key
criteria for synthetic data include high data utility and traceability to
verify the data source. Recent watermarking methods embed in homogeneous latent
spaces, but state-of-the-art time series generators operate in real space,
making latent-based watermarking incompatible. This creates the challenge of
watermarking directly in real space while handling feature heterogeneity and
temporal dependencies. We propose TimeWak, the first watermarking algorithm for
multivariate time series diffusion models. To handle temporal dependence and
spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark
directly within the real temporal-feature space. The other unique feature is
the $\epsilon$-exact inversion, which addresses the non-uniform reconstruction
error distribution across features from inverting the diffusion process to
detect watermarks. We derive the error bound of inverting multivariate time
series and further maintain high watermark detectability. We extensively
evaluate TimeWak on its impact on synthetic data quality, watermark
detectability, and robustness under various post-editing attacks, against 5
datasets and baselines of different temporal lengths. Our results show that
TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in
correlational scores against the state-of-the-art baseline, while remaining
consistently detectable.

</details>


### [510] [HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions](https://arxiv.org/abs/2506.06409)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Chen-Fu Chen,Haim Permuter,Sajani Vithana,Flavio P. Calmon*

Main category: cs.CR

TL;DR: 论文提出了一种优化框架，设计了两种新型水印（HeavyWater和SimplexWater），在低熵任务中实现高检测精度和低文本失真。


<details>
  <summary>Details</summary>
Motivation: 解决LLM水印在低熵任务（如编码）中的挑战，优化随机侧信息的使用以提高检测概率并减少文本失真。

Method: 提出优化框架，设计两种可调水印（HeavyWater和SimplexWater），适用于任何LLM且不依赖侧信息生成方式。

Result: 实验表明，两种水印在低熵任务中能实现高检测精度且对文本质量影响小。

Conclusion: 新水印方法有效平衡检测精度与文本质量，揭示了水印与编码理论的新联系。

Abstract: Large language model (LLM) watermarks enable authentication of text
provenance, curb misuse of machine-generated text, and promote trust in AI
systems. Current watermarks operate by changing the next-token predictions
output by an LLM. The updated (i.e., watermarked) predictions depend on random
side information produced, for example, by hashing previously generated tokens.
LLM watermarking is particularly challenging in low-entropy generation tasks -
such as coding - where next-token predictions are near-deterministic. In this
paper, we propose an optimization framework for watermark design. Our goal is
to understand how to most effectively use random side information in order to
maximize the likelihood of watermark detection and minimize the distortion of
generated text. Our analysis informs the design of two new watermarks:
HeavyWater and SimplexWater. Both watermarks are tunable, gracefully
trading-off between detection accuracy and text distortion. They can also be
applied to any LLM and are agnostic to side information generation. We examine
the performance of HeavyWater and SimplexWater through several benchmarks,
demonstrating that they can achieve high watermark detection accuracy with
minimal compromise of text generation quality, particularly in the low-entropy
regime. Our theoretical analysis also reveals surprising new connections
between LLM watermarking and coding theory. The code implementation can be
found in https://github.com/DorTsur/HeavyWater_SimplexWater

</details>


### [511] [A Systematic Review of Poisoning Attacks Against Large Language Models](https://arxiv.org/abs/2506.06518)
*Neil Fendley,Edward W. Staley,Joshua Carney,William Redman,Marie Chau,Nathan Drenkow*

Main category: cs.CR

TL;DR: 本文系统综述了大型语言模型（LLM）的中毒攻击，提出了一个全面的威胁模型和分类框架，以澄清安全影响和术语不一致问题。


<details>
  <summary>Details</summary>
Motivation: 随着预训练LLM及其训练数据集的广泛使用，其安全风险（如中毒攻击）引起了显著关注。现有框架和术语源自分类中毒文献，不完全适用于生成式LLM。

Method: 通过系统综述已发表的LLM中毒攻击，提出一个包含四种攻击规范和六种度量指标的威胁模型，用于分类和评估攻击。

Result: 提出了一个适用于广泛LLM中毒攻击的分类框架，并围绕四个关键维度（概念毒药、隐蔽毒药、持久毒药和任务特定毒药）组织讨论。

Conclusion: 该框架有助于更好地理解LLM中毒攻击的安全风险现状，并为未来研究提供统一术语和分类标准。

Abstract: With the widespread availability of pretrained Large Language Models (LLMs)
and their training datasets, concerns about the security risks associated with
their usage has increased significantly. One of these security risks is the
threat of LLM poisoning attacks where an attacker modifies some part of the LLM
training process to cause the LLM to behave in a malicious way. As an emerging
area of research, the current frameworks and terminology for LLM poisoning
attacks are derived from earlier classification poisoning literature and are
not fully equipped for generative LLM settings. We conduct a systematic review
of published LLM poisoning attacks to clarify the security implications and
address inconsistencies in terminology across the literature. We propose a
comprehensive poisoning threat model applicable to categorize a wide range of
LLM poisoning attacks. The poisoning threat model includes four poisoning
attack specifications that define the logistics and manipulation strategies of
an attack as well as six poisoning metrics used to measure key characteristics
of an attack. Under our proposed framework, we organize our discussion of
published LLM poisoning literature along four critical dimensions of LLM
poisoning attacks: concept poisons, stealthy poisons, persistent poisons, and
poisons for unique tasks, to better understand the current landscape of
security risks.

</details>


### [512] [Scoring the Unscorables: Cyber Risk Assessment Beyond Internet Scans](https://arxiv.org/abs/2506.06604)
*Armin Sarabi,Manish Karir,Mingyan Liu*

Main category: cs.CR

TL;DR: 本文提出了一种利用新型数据类型（技术数字签名）进行网络风险量化的方法，通过爬取组织网站数据构建高精度风险评估模型，克服了传统IP扫描数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于IP扫描的数据存在不完整性和对中小型企业（SMEs）覆盖不足的问题，而技术数字签名数据更易获取且适用于大量SMEs。

Method: 通过爬取组织网站获取技术数字签名数据，构建网络风险评估模型，并与不同网络事件数据集进行交叉验证。

Result: 研究表明技术数字签名与组织的网络安全态势密切相关，模型在交叉验证中表现良好，并揭示了勒索软件攻击受害者与其他网络事件受害者的关键差异。

Conclusion: 技术数字签名是一种可行且高效的网络风险量化方法，尤其适用于中小型企业。

Abstract: In this paper we present a study on using novel data types to perform cyber
risk quantification by estimating the likelihood of a data breach. We
demonstrate that it is feasible to build a highly accurate cyber risk
assessment model using public and readily available technology signatures
obtained from crawling an organization's website. This approach overcomes the
limitations of previous similar approaches that relied on large-scale IP
address based scanning data, which suffers from incomplete/missing IP address
mappings as well as the lack of such data for large numbers of small and
medium-sized organizations (SMEs). In comparison to scan data, technology
digital signature data is more readily available for millions of SMEs. Our
study shows that there is a strong relationship between these technology
signatures and an organization's cybersecurity posture. In cross-validating our
model using different cyber incident datasets, we also highlight the key
differences between ransomware attack victims and the larger population of
cyber incident and data breach victims.

</details>


### [513] [From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks](https://arxiv.org/abs/2506.07392)
*Yuyang Zhou,Guang Cheng,Kang Du,Zihan Chen,Tian Qin,Yuyu Zhao*

Main category: cs.CR

TL;DR: 提出了一种基于联邦多智能体深度强化学习的移动目标防御框架，用于无人机群网络的主动和自适应DoS缓解，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无人机群网络的开放无线环境、动态拓扑和资源限制使其面临严重的DoS威胁，传统静态或集中式防御机制难以应对。

Method: 设计了三种轻量级协调的MTD机制（领导者切换、路由变异和频率跳变），并通过联邦多智能体深度强化学习算法优化防御策略。

Result: 在攻击缓解率、平均恢复时间、能耗和防御成本方面显著优于现有方法，分别提升34.6%、减少94.6%、降低29.3%和98.3%。

Conclusion: 该框架在动态和分布式场景下有效提升了无人机群网络的抗DoS能力和任务连续性。

Abstract: The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide
range of mission-critical applications, but also exposes UAV networks to severe
Denial-of-Service (DoS) threats due to their open wireless environment, dynamic
topology, and resource constraints. Traditional static or centralized defense
mechanisms are often inadequate for such dynamic and distributed scenarios. To
address these challenges, we propose a novel federated multi-agent deep
reinforcement learning (FMADRL)-driven moving target defense (MTD) framework
for proactive and adaptive DoS mitigation in UAV swarm networks. Specifically,
we design three lightweight and coordinated MTD mechanisms, including leader
switching, route mutation, and frequency hopping, that leverage the inherent
flexibility of UAV swarms to disrupt attacker efforts and enhance network
resilience. The defense problem is formulated as a multi-agent partially
observable Markov decision process (POMDP), capturing the distributed,
resource-constrained, and uncertain nature of UAV swarms under attack. Each UAV
is equipped with a local policy agent that autonomously selects MTD actions
based on partial observations and local experiences. By employing a policy
gradient-based FMADRL algorithm, UAVs collaboratively optimize their defense
policies via reward-weighted aggregation, enabling distributed learning without
sharing raw data and thus reducing communication overhead. Extensive
simulations demonstrate that our approach significantly outperforms
state-of-the-art baselines, achieving up to a 34.6% improvement in attack
mitigation rate, a reduction in average recovery time of up to 94.6%, and
decreases in energy consumption and defense cost by as much as 29.3% and 98.3%,
respectively, while maintaining robust mission continuity under various DoS
attack strategies.

</details>


### [514] [TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems](https://arxiv.org/abs/2506.07605)
*Marco Di Gennaro,Giovanni De Lucia,Stefano Longari,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: TimberStrike是一种针对水平联邦树模型的优化数据集重建攻击，通过决策树的分裂值和路径推断敏感数据，揭示了现有联邦梯度提升实现的隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究树模型在联邦学习中的安全性和隐私问题，填补现有研究的空白。

Method: 提出TimberStrike攻击，利用决策树的离散特性，通过分裂值和决策路径重建数据集。

Result: 在多个框架中，攻击成功重建了73.05%至95.63%的目标数据集，差分隐私部分缓解攻击但损害模型性能。

Conclusion: 需设计专门针对树模型的隐私保护机制，以平衡隐私和性能。

Abstract: Federated Learning has emerged as a privacy-oriented alternative to
centralized Machine Learning, enabling collaborative model training without
direct data sharing. While extensively studied for neural networks, the
security and privacy implications of tree-based models remain underexplored.
This work introduces TimberStrike, an optimization-based dataset reconstruction
attack targeting horizontally federated tree-based models. Our attack, carried
out by a single client, exploits the discrete nature of decision trees by using
split values and decision paths to infer sensitive training data from other
clients. We evaluate TimberStrike on State-of-the-Art federated gradient
boosting implementations across multiple frameworks, including Flower, NVFlare,
and FedTree, demonstrating their vulnerability to privacy breaches. On a
publicly available stroke prediction dataset, TimberStrike consistently
reconstructs between 73.05% and 95.63% of the target dataset across all
implementations. We further analyze Differential Privacy, showing that while it
partially mitigates the attack, it also significantly degrades model
performance. Our findings highlight the need for privacy-preserving mechanisms
specifically designed for tree-based Federated Learning systems, and we provide
preliminary insights into their design.

</details>


### [515] [Profiling Electric Vehicles via Early Charging Voltage Patterns](https://arxiv.org/abs/2506.07714)
*Francesco Marchiori,Denis Donadel,Alessandro Brighente,Mauro Conti*

Main category: cs.CR

TL;DR: 论文提出了一种基于早期充电阶段电压行为的电动汽车（EV）识别框架，旨在提高充电安全性和效率，同时揭示隐私风险。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车的普及，充电基础设施的安全性变得至关重要。现有方法在充电后期才能检测攻击，导致能源浪费，且用户隐私可能因充电模式识别而泄露。

Method: 通过提取早期充电阶段的电压特征，构建轻量级模型，实现快速可靠的电动汽车识别。

Result: 在49辆EV的7408次充电数据上测试，准确率达0.86，仅需10个关键特征即可接近最优性能。

Conclusion: 该研究为充电认证提供了新方法，但也警示了充电数据泄露可能带来的隐私风险。

Abstract: Electric Vehicles (EVs) are rapidly gaining adoption as a sustainable
alternative to fuel-powered vehicles, making secure charging infrastructure
essential. Despite traditional authentication protocols, recent results showed
that attackers may steal energy through tailored relay attacks. One
countermeasure is leveraging the EV's fingerprint on the current exchanged
during charging. However, existing methods focus on the final charging stage,
allowing malicious actors to consume substantial energy before being detected
and repudiated. This underscores the need for earlier and more effective
authentication methods to prevent unauthorized charging. Meanwhile, profiling
raises privacy concerns, as uniquely identifying EVs through charging patterns
could enable user tracking.
  In this paper, we propose a framework for uniquely identifying EVs using
physical measurements from the early charging stages. We hypothesize that
voltage behavior early in the process exhibits similar characteristics to
current behavior in later stages. By extracting features from early voltage
measurements, we demonstrate the feasibility of EV profiling. Our approach
improves existing methods by enabling faster and more reliable vehicle
identification. We test our solution on a dataset of 7408 usable charges from
49 EVs, achieving up to 0.86 accuracy. Feature importance analysis shows that
near-optimal performance is possible with just 10 key features, improving
efficiency alongside our lightweight models. This research lays the foundation
for a novel authentication factor while exposing potential privacy risks from
unauthorized access to charging data.

</details>


### [516] [SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark](https://arxiv.org/abs/2506.07888)
*Rui Wen,Yiyong Liu,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种统一的数据重建攻击分类和正式定义，并设计了一套量化评估指标，利用大语言模型替代人工评估，建立了一个系统评估框架。


<details>
  <summary>Details</summary>
Motivation: 数据重建攻击领域缺乏统一的定义和评估标准，阻碍了进一步研究。

Method: 提出攻击分类和定义，设计量化指标，利用大语言模型进行视觉评估。

Result: 实证结果验证了指标的有效性，并为新攻击设计提供了见解。

Conclusion: 本文为数据重建攻击研究提供了统一框架和评估基准。

Abstract: Data reconstruction attacks, which aim to recover the training dataset of a
target model with limited access, have gained increasing attention in recent
years. However, there is currently no consensus on a formal definition of data
reconstruction attacks or appropriate evaluation metrics for measuring their
quality. This lack of rigorous definitions and universal metrics has hindered
further advancement in this field. In this paper, we address this issue in the
vision domain by proposing a unified attack taxonomy and formal definitions of
data reconstruction attacks. We first propose a set of quantitative evaluation
metrics that consider important criteria such as quantifiability, consistency,
precision, and diversity. Additionally, we leverage large language models
(LLMs) as a substitute for human judgment, enabling visual evaluation with an
emphasis on high-quality reconstructions. Using our proposed taxonomy and
metrics, we present a unified framework for systematically evaluating the
strengths and limitations of existing attacks and establishing a benchmark for
future research. Empirical results, primarily from a memorization perspective,
not only validate the effectiveness of our metrics but also offer valuable
insights for designing new attacks.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [517] [DELPHYNE: A Pre-Trained Model for General and Financial Time Series](https://arxiv.org/abs/2506.06288)
*Xueying Ding,Aakriti Mittal,Achintya Gopal*

Main category: q-fin.ST

TL;DR: 论文提出了一种名为Delphyne的金融时间序列预训练模型，解决了现有模型在金融领域表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预训练模型在金融应用中表现不佳，主要由于缺乏金融数据预训练和跨领域负迁移效应。

Method: 提出了Delphyne模型，针对金融时间序列数据的特点进行优化。

Result: Delphyne在公开数据集上表现优异，优于现有基础模型和全量微调模型。

Conclusion: Delphyne为金融时间序列分析提供了高效解决方案，具有广泛应用潜力。

Abstract: Time-series data is a vital modality within data science communities. This is
particularly valuable in financial applications, where it helps in detecting
patterns, understanding market behavior, and making informed decisions based on
historical data. Recent advances in language modeling have led to the rise of
time-series pre-trained models that are trained on vast collections of datasets
and applied to diverse tasks across financial domains. However, across
financial applications, existing time-series pre-trained models have not shown
boosts in performance over simple finance benchmarks in both zero-shot and
fine-tuning settings. This phenomenon occurs because of a i) lack of financial
data within the pre-training stage, and ii) the negative transfer effect due to
inherently different time-series patterns across domains. Furthermore,
time-series data is continuous, noisy, and can be collected at varying
frequencies and with varying lags across different variables, making this data
more challenging to model than languages. To address the above problems, we
introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne
achieves competitive performance to existing foundation and full-shot models
with few fine-tuning steps on publicly available datasets, and also shows
superior performances on various financial tasks.

</details>


### [518] [Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100](https://arxiv.org/abs/2506.06345)
*Sukru Selim Calik,Andac Akyuz,Zeynep Hilal Kilimci,Kerem Colak*

Main category: q-fin.ST

TL;DR: 论文提出了一种结合Transformer时间序列模型与可解释人工智能（XAI）的新方法，以提高股票价格预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 金融素养越来越依赖于复杂金融数据的解读和先进预测工具的使用，因此需要提升预测模型的解释性和准确性。

Method: 研究采用了DLinear、LTSNet、Vanilla Transformer和Time Series Transformer等模型，结合技术指标作为输入特征，并使用SHAP和LIME技术解释模型输出。

Result: 结果表明Transformer模型具有强大的预测能力，可解释机器学习有助于提升个人投资决策能力。

Conclusion: 可解释机器学习在金融领域具有潜力，能够帮助个人更好地参与金融市场。

Abstract: Financial literacy is increasingly dependent on the ability to interpret
complex financial data and utilize advanced forecasting tools. In this context,
this study proposes a novel approach that combines transformer-based time
series models with explainable artificial intelligence (XAI) to enhance the
interpretability and accuracy of stock price predictions. The analysis focuses
on the daily stock prices of the five highest-volume banks listed in the
BIST100 index, along with XBANK and XU100 indices, covering the period from
January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla
Transformer, and Time Series Transformer are employed, with input features
enriched by technical indicators. SHAP and LIME techniques are used to provide
transparency into the influence of individual features on model outputs. The
results demonstrate the strong predictive capabilities of transformer models
and highlight the potential of interpretable machine learning to empower
individuals in making informed investment decisions and actively engaging in
financial markets.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [519] [Object-level Self-Distillation for Vision Pretraining](https://arxiv.org/abs/2506.05409)
*Çağlar Hızlı,Çağatay Yıldız,Pekka Marttinen*

Main category: cs.CV

TL;DR: ODIS是一种预训练方法，通过对象级自蒸馏改进视觉表示，解决了现有方法假设图像仅含单一对象的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉预训练方法假设图像仅含单一对象，限制了其在复杂场景数据集上的扩展性。ODIS通过对象级自蒸馏解决这一问题。

Method: ODIS采用对象感知裁剪和掩码注意力，隔离对象特定区域，将噪声场景任务转化为简单对象级子任务。

Result: ODIS在ViT-Large上实现了82.6%的k-NN准确率。

Conclusion: ODIS通过对象级自蒸馏显著提升了视觉表示能力，适用于复杂场景数据。

Abstract: State-of-the-art vision pretraining methods rely on image-level
self-distillation from object-centric datasets such as ImageNet, implicitly
assuming each image contains a single object. This assumption does not always
hold: many ImageNet images already contain multiple objects. Further, it limits
scalability to scene-centric datasets that better mirror real-world complexity.
We address these challenges by introducing Object-level Self-DIStillation
(ODIS), a pretraining approach that shifts the self-distillation granularity
from whole images to individual objects. Using object-aware cropping and masked
attention, ODIS isolates object-specific regions, guiding the transformer
toward semantically meaningful content and transforming a noisy, scene-level
task into simpler object-level sub-tasks. We show that this approach improves
visual representations both at the image and patch levels. Using masks at
inference time, our method achieves an impressive $82.6\%$ $k$-NN accuracy on
ImageNet1k with ViT-Large.

</details>


### [520] [SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing](https://arxiv.org/abs/2506.05414)
*Mingfei Chen,Zijun Cui,Xiulong Liu,Jinlin Xiang,Caleb Zheng,Jingyuan Li,Eli Shlizerman*

Main category: cs.CV

TL;DR: SAVVY-Bench是首个针对动态3D场景中空间推理的基准测试，结合了同步空间音频。SAVVY是一种无需训练的两阶段推理方法，显著提升了现有AV-LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AV-LLMs和基准测试主要关注静态或2D场景，缺乏对动态3D空间推理的研究。

Method: SAVVY分为两阶段：1) 利用AV-LLMs和其他视听方法跟踪关键对象轨迹；2) 构建全局动态地图并通过坐标变换生成最终答案。

Result: SAVVY显著提升了现有AV-LLMs的性能，为动态3D空间推理设定了新标准。

Conclusion: SAVVY-Bench和SAVVY方法填补了动态3D空间推理的空白，为未来研究提供了新方向。

Abstract: 3D spatial reasoning in dynamic, audio-visual environments is a cornerstone
of human cognition yet remains largely unexplored by existing Audio-Visual
Large Language Models (AV-LLMs) and benchmarks, which predominantly focus on
static or 2D scenes. We introduce SAVVY-Bench, the first benchmark for 3D
spatial reasoning in dynamic scenes with synchronized spatial audio.
SAVVY-Bench is comprised of thousands of relationships involving static and
moving objects, and requires fine-grained temporal grounding, consistent 3D
localization, and multi-modal annotation. To tackle this challenge, we propose
SAVVY, a novel training-free reasoning pipeline that consists of two stages:
(i) Egocentric Spatial Tracks Estimation, which leverages AV-LLMs as well as
other audio-visual methods to track the trajectories of key objects related to
the query using both visual and spatial audio cues, and (ii) Dynamic Global Map
Construction, which aggregates multi-modal queried object trajectories and
converts them into a unified global dynamic map. Using the constructed map, a
final QA answer is obtained through a coordinate transformation that aligns the
global map with the queried viewpoint. Empirical evaluation demonstrates that
SAVVY substantially enhances performance of state-of-the-art AV-LLMs, setting a
new standard and stage for approaching dynamic 3D spatial reasoning in AV-LLMs.

</details>


### [521] [Self-Predictive Dynamics for Generalization of Vision-based Reinforcement Learning](https://arxiv.org/abs/2506.05418)
*Kyungsoo Kim,Jeongsoo Ha,Yusung Kim*

Main category: cs.CV

TL;DR: 提出了一种自预测动态（SPD）方法，用于在未见过的观察中高效提取任务相关特征，显著提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉强化学习中图像观察中的干扰元素（如阴影、云、光线）对任务无关特征的影响，尤其是在训练中未暴露这些干扰时。

Method: SPD采用弱增强和强增强并行处理，通过学习双向增强版本的逆和正向转换来提取特征。

Result: 在MuJoCo视觉控制任务和CARLA自动驾驶任务中，SPD在复杂观察中表现优于先前研究，显著提升了未见观察的泛化性能。

Conclusion: SPD方法能有效提取任务相关特征，提升模型在复杂和未见观察中的性能。

Abstract: Vision-based reinforcement learning requires efficient and robust
representations of image-based observations, especially when the images contain
distracting (task-irrelevant) elements such as shadows, clouds, and light. It
becomes more important if those distractions are not exposed during training.
We design a Self-Predictive Dynamics (SPD) method to extract task-relevant
features efficiently, even in unseen observations after training. SPD uses weak
and strong augmentations in parallel, and learns representations by predicting
inverse and forward transitions across the two-way augmented versions. In a set
of MuJoCo visual control tasks and an autonomous driving task (CARLA), SPD
outperforms previous studies in complex observations, and significantly
improves the generalization performance for unseen observations. Our code is
available at https://github.com/unigary/SPD.

</details>


### [522] [Coordinated Robustness Evaluation Framework for Vision-Language Models](https://arxiv.org/abs/2506.05429)
*Ashwin Ramesh Babu,Sajad Mousavi,Vineet Gundecha,Sahand Ghorbanpour,Avisek Naug,Antonio Guillen,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.CV

TL;DR: 本文提出了一种通用的替代模型，用于生成针对视觉和语言模态的对抗性扰动，评估其在视觉问答和视觉推理任务中的效果，结果表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在图像描述和视觉问答等任务中表现出色，但其对微小扰动的敏感性限制了其鲁棒性。本文旨在通过联合扰动评估模型的跨模态依赖性。

Method: 训练一个通用的替代模型，接受图像和文本输入，生成联合表示，并进一步生成针对文本和图像的对抗性扰动。

Result: 提出的协调攻击策略在多个先进视觉语言模型上表现优于其他多模态和单模态攻击方法。

Conclusion: 该方法有效揭示了现有预训练多模态模型的鲁棒性缺陷，为模型改进提供了方向。

Abstract: Vision-language models, which integrate computer vision and natural language
processing capabilities, have demonstrated significant advancements in tasks
such as image captioning and visual question and answering. However, similar to
traditional models, they are susceptible to small perturbations, posing a
challenge to their robustness, particularly in deployment scenarios. Evaluating
the robustness of these models requires perturbations in both the vision and
language modalities to learn their inter-modal dependencies. In this work, we
train a generic surrogate model that can take both image and text as input and
generate joint representation which is further used to generate adversarial
perturbations for both the text and image modalities. This coordinated attack
strategy is evaluated on the visual question and answering and visual reasoning
datasets using various state-of-the-art vision-language models. Our results
indicate that the proposed strategy outperforms other multi-modal attacks and
single-modality attacks from the recent literature. Our results demonstrate
their effectiveness in compromising the robustness of several state-of-the-art
pre-trained multi-modal models such as instruct-BLIP, ViLT and others.

</details>


### [523] [Robustness Evaluation for Video Models with Reinforcement Learning](https://arxiv.org/abs/2506.05431)
*Ashwin Ramesh Babu,Sajad Mousavi,Vineet Gundecha,Sahand Ghorbanpour,Avisek Naug,Antonio Guillen,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.CV

TL;DR: 提出了一种基于多智能体强化学习的方法，用于评估视频分类模型的鲁棒性，通过空间和时间扰动生成更有效的攻击。


<details>
  <summary>Details</summary>
Motivation: 视频分类模型的鲁棒性评估比图像模型更具挑战性，因其时间维度和计算复杂性增加，需最小化扰动以诱导误分类。

Method: 采用多智能体强化学习方法（空间和时间），协同学习识别视频的敏感区域，生成视觉不可察觉的精细扰动。

Result: 在Lp指标和平均查询次数上优于现有方法，支持自定义失真类型，并在HMDB-51和UCF-101数据集上评估了4种流行模型。

Conclusion: 该方法显著提升了视频分类模型鲁棒性评估的有效性和实用性。

Abstract: Evaluating the robustness of Video classification models is very challenging,
specifically when compared to image-based models. With their increased temporal
dimension, there is a significant increase in complexity and computational
cost. One of the key challenges is to keep the perturbations to a minimum to
induce misclassification. In this work, we propose a multi-agent reinforcement
learning approach (spatial and temporal) that cooperatively learns to identify
the given video's sensitive spatial and temporal regions. The agents consider
temporal coherence in generating fine perturbations, leading to a more
effective and visually imperceptible attack. Our method outperforms the
state-of-the-art solutions on the Lp metric and the average queries. Our method
enables custom distortion types, making the robustness evaluation more relevant
to the use case. We extensively evaluate 4 popular models for video action
recognition on two popular datasets, HMDB-51 and UCF-101.

</details>


### [524] [U-NetMN and SegNetMN: Modified U-Net and SegNet models for bimodal SAR image segmentation](https://arxiv.org/abs/2506.05444)
*Marwane Kzadri,Franco Alberto Cardillo,Nanée Chahinian,Carole Delenne,Renaud Hostache,Jamal Riffi*

Main category: cs.CV

TL;DR: 研究了模式归一化对SAR图像分割模型（U-Net和SegNet）的影响，发现其能显著加速收敛并提高稳定性。


<details>
  <summary>Details</summary>
Motivation: SAR图像分割在遥感应用中至关重要，但深度学习模型因数据复杂统计分布而面临收敛速度和稳定性问题。

Method: 在U-Net和SegNet中集成模式归一化，评估其对收敛时间和性能的影响。

Result: 模式归一化显著加速收敛，且归一化模型在不同区域表现更稳定。

Conclusion: 归一化有效提升了SAR图像分割的计算效率和泛化能力。

Abstract: Segmenting Synthetic Aperture Radar (SAR) images is crucial for many remote
sensing applications, particularly water body detection. However, deep
learning-based segmentation models often face challenges related to convergence
speed and stability, mainly due to the complex statistical distribution of this
type of data. In this study, we evaluate the impact of mode normalization on
two widely used semantic segmentation models, U-Net and SegNet. Specifically,
we integrate mode normalization, to reduce convergence time while maintaining
the performance of the baseline models. Experimental results demonstrate that
mode normalization significantly accelerates convergence. Furthermore,
cross-validation results indicate that normalized models exhibit increased
stability in different zones. These findings highlight the effectiveness of
normalization in improving computational efficiency and generalization in SAR
image segmentation.

</details>


### [525] [MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning](https://arxiv.org/abs/2506.05523)
*Zikui Cai,Andrew Wang,Anirudh Satheesh,Ankit Nakhawa,Hyunwoo Jae,Keenan Powell,Minghui Liu,Neel Jay,Sungbin Oh,Xiyao Wang,Yongyuan Liang,Tom Goldstein,Furong Huang*

Main category: cs.CV

TL;DR: MORSE-500是一个新的视频基准测试，旨在解决现有视觉语言模型（VLMs）评测在时间复杂性、多维度推理能力和可扩展性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评测基准过于依赖静态图像，局限于数学问题解决，且容易饱和，无法全面评估多模态推理能力。

Method: 通过程序化生成500个脚本视频片段，涵盖六类推理问题，利用Python脚本、生成视频模型和真实素材，实现难度可控的动态评测。

Result: 实验显示当前最先进模型在抽象和规划任务上表现显著不足。

Conclusion: MORSE-500为多模态推理研究提供了可扩展、透明的评测工具，支持未来模型的持续改进。

Abstract: Despite rapid advances in vision-language models (VLMs), current benchmarks
for multimodal reasoning fall short in three key dimensions. First, they
overwhelmingly rely on static images, failing to capture the temporal
complexity of real-world environments. Second, they narrowly focus on
mathematical problem-solving, neglecting the broader spectrum of reasoning
skills -- including abstract, physical, planning, spatial, and temporal
capabilities -- required for robust multimodal intelligence. Third, many
benchmarks quickly saturate, offering limited headroom for diagnosing failure
modes or measuring continued progress. We introduce MORSE-500 (Multimodal
Reasoning Stress-test Environment), a video benchmark composed of 500 fully
scripted clips with embedded questions spanning six complementary reasoning
categories. Each instance is programmatically generated using deterministic
Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and
curated real footage. This script-driven design allows fine-grained control
over visual complexity, distractor density, and temporal dynamics -- enabling
difficulty to be scaled systematically as models improve. Unlike static
benchmarks that become obsolete once saturated, MORSE-500 is built to evolve:
its controllable generation pipeline supports the creation of arbitrarily
challenging new instances, making it ideally suited for stress-testing
next-generation models. Initial experiments with state-of-the-art systems --
including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest
available at the time, alongside strong open-source models -- reveal
substantial performance gaps across all categories, with particularly large
deficits in abstract and planning tasks. We release the full dataset,
generation scripts, and evaluation harness to support transparent,
reproducible, and forward-looking multimodal reasoning research.

</details>


### [526] [Robust sensor fusion against on-vehicle sensor staleness](https://arxiv.org/abs/2506.05780)
*Meng Fan,Yifan Zuo,Patrick Blaes,Harley Montgomery,Subhasis Das*

Main category: cs.CV

TL;DR: 论文提出了一种解决自动驾驶中传感器数据延迟问题的新方法，通过时间戳偏移特征和数据增强策略，提升了传感器融合的性能。


<details>
  <summary>Details</summary>
Motivation: 传感器数据延迟（staleness）会导致多模态数据时间不一致，从而影响目标状态估计和轨迹预测的准确性，这对自动驾驶的安全性至关重要。

Method: 方法包括：（1）为LiDAR和雷达数据引入相对于相机的时间戳偏移特征；（2）模拟实际车辆中传感器延迟模式的数据增强策略。该方法集成到一个多传感器检测模型中。

Result: 实验表明，传统方法在传感器数据延迟时性能显著下降，而新方法在同步和延迟条件下均表现良好。

Conclusion: 提出的方法能够有效解决传感器数据延迟问题，提升自动驾驶感知系统的鲁棒性和性能。

Abstract: Sensor fusion is crucial for a performant and robust Perception system in
autonomous vehicles, but sensor staleness, where data from different sensors
arrives with varying delays, poses significant challenges. Temporal
misalignment between sensor modalities leads to inconsistent object state
estimates, severely degrading the quality of trajectory predictions that are
critical for safety. We present a novel and model-agnostic approach to address
this problem via (1) a per-point timestamp offset feature (for LiDAR and radar
both relative to camera) that enables fine-grained temporal awareness in sensor
fusion, and (2) a data augmentation strategy that simulates realistic sensor
staleness patterns observed in deployed vehicles. Our method is integrated into
a perspective-view detection model that consumes sensor data from multiple
LiDARs, radars and cameras. We demonstrate that while a conventional model
shows significant regressions when one sensor modality is stale, our approach
reaches consistently good performance across both synchronized and stale
conditions.

</details>


### [527] [Improved Allergy Wheal Detection for the Skin Prick Automated Test Device](https://arxiv.org/abs/2506.05862)
*Rembert Daems,Sven Seys,Valérie Hox,Adam Chaker,Glynnis De Greve,Winde Lemmens,Anne-Lise Poirrier,Eline Beckers,Zuzana Diamant,Carmen Dierickx,Peter W. Hellings,Caroline Huart,Claudia Jerin,Mark Jorissen,Hanne Oscé,Karolien Roux,Mark Thompson,Sophie Tombu,Saartje Uyttebroek,Andrzej Zarowski,Senne Gorris,Laura Van Gerven,Dirk Loeckx,Thomas Demeester*

Main category: cs.CV

TL;DR: SPAT设备通过多光照条件图像提高过敏检测准确性，结合神经网络和算法实现自动化检测。


<details>
  <summary>Details</summary>
Motivation: 提高皮肤点刺试验（SPT）的检测一致性和准确性，解决传统方法依赖单一光照图像的局限性。

Method: 使用SPAT设备的32张多光照图像，结合神经网络分割和算法检测，手动标注10,416个风团。

Result: 在217名患者的验证集上，多光照图像比单光照图像显著提高了检测准确性。

Conclusion: 多光照条件下的SPAT图像显著优于传统单光照图像，为过敏诊断提供了更可靠的工具。

Abstract: Background: The skin prick test (SPT) is the gold standard for diagnosing
sensitization to inhalant allergies. The Skin Prick Automated Test (SPAT)
device was designed for increased consistency in test results, and captures 32
images to be jointly used for allergy wheal detection and delineation, which
leads to a diagnosis.
  Materials and Methods: Using SPAT data from $868$ patients with suspected
inhalant allergies, we designed an automated method to detect and delineate
wheals on these images. To this end, $10,416$ wheals were manually annotated by
drawing detailed polygons along the edges. The unique data-modality of the SPAT
device, with $32$ images taken under distinct lighting conditions, requires a
custom-made approach. Our proposed method consists of two parts: a neural
network component that segments the wheals on the pixel level, followed by an
algorithmic and interpretable approach for detecting and delineating the
wheals.
  Results: We evaluate the performance of our method on a hold-out validation
set of $217$ patients. As a baseline we use a single conventionally lighted
image per SPT as input to our method.
  Conclusion: Using the $32$ SPAT images under various lighting conditions
offers a considerably higher accuracy than a single image in conventional,
uniform light.

</details>


### [528] [Rethinking Semi-supervised Segmentation Beyond Accuracy: Reliability and Robustness](https://arxiv.org/abs/2506.05917)
*Steven Landgraf,Markus Hillemann,Markus Ulrich*

Main category: cs.CV

TL;DR: 论文提出了一种新的评估指标RSS，用于综合衡量半监督语义分割模型的准确性、可靠性和鲁棒性，填补了当前评估协议的不足。


<details>
  <summary>Details</summary>
Motivation: 当前半监督分割评估仅关注准确性，忽略了可靠性和鲁棒性，而这些对安全关键应用至关重要。

Method: 引入Reliable Segmentation Score (RSS)，通过调和平均值结合预测准确性、校准和不确定性质量。

Result: 半监督方法常以牺牲可靠性换取准确性，UniMatchV2在鲁棒性上表现良好但可靠性仍有不足。

Conclusion: 建议采用RSS等更全面的评估指标，以更好地满足实际部署需求。

Abstract: Semantic segmentation is critical for scene understanding but demands costly
pixel-wise annotations, attracting increasing attention to semi-supervised
approaches to leverage abundant unlabeled data. While semi-supervised
segmentation is often promoted as a path toward scalable, real-world
deployment, it is astonishing that current evaluation protocols exclusively
focus on segmentation accuracy, entirely overlooking reliability and
robustness. These qualities, which ensure consistent performance under diverse
conditions (robustness) and well-calibrated model confidences as well as
meaningful uncertainties (reliability), are essential for safety-critical
applications like autonomous driving, where models must handle unpredictable
environments and avoid sudden failures at all costs. To address this gap, we
introduce the Reliable Segmentation Score (RSS), a novel metric that combines
predictive accuracy, calibration, and uncertainty quality measures via a
harmonic mean. RSS penalizes deficiencies in any of its components, providing
an easy and intuitive way of holistically judging segmentation models.
Comprehensive evaluations of UniMatchV2 against its predecessor and a
supervised baseline show that semi-supervised methods often trade reliability
for accuracy. While out-of-domain evaluations demonstrate UniMatchV2's
robustness, they further expose persistent reliability shortcomings. We
advocate for a shift in evaluation protocols toward more holistic metrics like
RSS to better align semi-supervised learning research with real-world
deployment needs.

</details>


### [529] [Sample-Specific Noise Injection For Diffusion-Based Adversarial Purification](https://arxiv.org/abs/2506.06027)
*Yuhao Sun,Jiacheng Zhang,Zesheng Ye,Chaowei Xiao,Feng Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为SSNI的样本特异性噪声注入框架，通过预训练的评分网络动态调整噪声水平，显著提升了DBP方法的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有DBP方法对所有样本使用固定噪声水平$t^*$，而研究发现不同样本的最优$t^*$可能不同，因此提出动态调整噪声水平的必要性。

Method: SSNI利用预训练的评分网络估计样本偏离干净数据分布的程度（评分范数），并基于评分范数动态调整噪声水平$t^*$。

Result: 在CIFAR-10和ImageNet-1K数据集上，SSNI显著提升了DBP方法的准确性和鲁棒性。

Conclusion: 研究表明，为不同样本分配不同的噪声水平是DBP方法的关键改进方向。

Abstract: Diffusion-based purification (DBP) methods aim to remove adversarial noise
from the input sample by first injecting Gaussian noise through a forward
diffusion process, and then recovering the clean example through a reverse
generative process. In the above process, how much Gaussian noise is injected
to the input sample is key to the success of DBP methods, which is controlled
by a constant noise level $t^*$ for all samples in existing methods. In this
paper, we discover that an optimal $t^*$ for each sample indeed could be
different. Intuitively, the cleaner a sample is, the less the noise it should
be injected, and vice versa. Motivated by this finding, we propose a new
framework, called Sample-specific Score-aware Noise Injection (SSNI).
Specifically, SSNI uses a pre-trained score network to estimate how much a data
point deviates from the clean data distribution (i.e., score norms). Then,
based on the magnitude of score norms, SSNI applies a reweighting function to
adaptively adjust $t^*$ for each sample, achieving sample-specific noise
injections. Empirically, incorporating our framework with existing DBP methods
results in a notable improvement in both accuracy and robustness on CIFAR-10
and ImageNet-1K, highlighting the necessity to allocate distinct noise levels
to different samples in DBP methods. Our code is available at:
https://github.com/tmlr-group/SSNI.

</details>


### [530] [Tensor-to-Tensor Models with Fast Iterated Sum Features](https://arxiv.org/abs/2506.06041)
*Joscha Diehl,Rasheed Ibraheem,Leonard Schmitz,Yue Wu*

Main category: cs.CV

TL;DR: 提出了一种基于“角树”数学工具的新型张量到张量层（FIS层），具有线性计算成本，适用于图像和高阶张量数据处理。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习应用中，图像和高阶张量数据的高维度特性需要次二次计算复杂度的层，而现有方法难以满足需求。

Method: 利用“角树”和多重参数化的迭代积分（或求和）签名，构建了FIS层，可无缝集成到神经网络中。

Result: 在分类和异常检测任务中表现优异，替换ResNet部分层后，参数量和计算量减少，精度仅下降0.1%；异常检测模型在MVTec AD数据集上AUROC达97.3%。

Conclusion: FIS层为高维数据处理提供了高效解决方案，兼具计算效率和性能优势。

Abstract: Data in the form of images or higher-order tensors is ubiquitous in modern
deep learning applications. Owing to their inherent high dimensionality, the
need for subquadratic layers processing such data is even more pressing than
for sequence data. We propose a novel tensor-to-tensor layer with linear cost
in the input size, utilizing the mathematical gadget of ``corner trees'' from
the field of permutation counting. In particular, for order-two tensors, we
provide an image-to-image layer that can be plugged into image processing
pipelines. On the one hand, our method can be seen as a higher-order
generalization of state-space models. On the other hand, it is based on a
multiparameter generalization of the signature of iterated integrals (or sums).
The proposed tensor-to-tensor concept is used to build a neural network layer
called the Fast Iterated Sums (FIS) layer which integrates seamlessly with
other layer types. We demonstrate the usability of the FIS layer with both
classification and anomaly detection tasks. By replacing some layers of a
smaller ResNet architecture with FIS, a similar accuracy (with a difference of
only 0.1\%) was achieved in comparison to a larger ResNet while reducing the
number of trainable parameters and multi-add operations. The FIS layer was also
used to build an anomaly detection model that achieved an average AUROC of
97.3\% on the texture images of the popular MVTec AD dataset. The processing
and modelling codes are publicly available at
https://github.com/diehlj/fast-iterated-sums.

</details>


### [531] [SDS-Net: Shallow-Deep Synergism-detection Network for infrared small target detection](https://arxiv.org/abs/2506.06042)
*Taoran Yue,Xiaojin Lu,Jiaxi Cai,Yuanping Chen,Shibing Chu*

Main category: cs.CV

TL;DR: 提出了一种浅层-深层协同检测网络（SDS-Net），通过双分支架构和自适应特征融合模块，提升红外小目标检测的精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有CNN方法忽视了浅层和深层特征的异质性，导致特征协作不足，影响检测性能和计算效率。

Method: 采用双分支架构分别建模结构特征和语义特征，并引入自适应特征融合模块动态建模跨层特征相关性。

Result: 在三个公开数据集上表现优于现有方法，同时保持低计算复杂性和高推理效率。

Conclusion: SDS-Net在检测性能和计算效率上均表现出色，具有广泛应用前景。

Abstract: Current CNN-based infrared small target detection(IRSTD) methods generally
overlook the heterogeneity between shallow and deep features, leading to
inefficient collaboration between shallow fine grained structural information
and deep high-level semantic representations. Additionally, the dependency
relationships and fusion mechanisms across different feature hierarchies lack
systematic modeling, which fails to fully exploit the complementarity of
multilevel features. These limitations hinder IRSTD performance while incurring
substantial computational costs. To address these challenges, this paper
proposes a shallow-deep synergistic detection network (SDS-Net) that
efficiently models multilevel feature representations to increase both the
detection accuracy and computational efficiency in IRSTD tasks. SDS-Net
introduces a dual-branch architecture that separately models the structural
characteristics and semantic properties of features, effectively preserving
shallow spatial details while capturing deep semantic representations, thereby
achieving high-precision detection with significantly improved inference speed.
Furthermore, the network incorporates an adaptive feature fusion module to
dynamically model cross-layer feature correlations, enhancing overall feature
collaboration and representation capability. Comprehensive experiments on three
public datasets (NUAA-SIRST, NUDT-SIRST, and IRSTD-1K) demonstrate that SDS-Net
outperforms state-of-the-art IRSTD methods while maintaining low computational
complexity and high inference efficiency, showing superior detection
performance and broad application prospects. Our code will be made public at
https://github.com/PhysiLearn/SDS-Net.

</details>


### [532] [Fine-grained Hierarchical Crop Type Classification from Integrated Hyperspectral EnMAP Data and Multispectral Sentinel-2 Time Series: A Large-scale Dataset and Dual-stream Transformer Method](https://arxiv.org/abs/2506.06155)
*Wenyuan Li,Shunlin Liang,Yuxiang Zhang,Liqin Liu,Keyan Chen,Yongzhe Chen,Han Ma,Jianglei Xu,Yichuan Ma,Shikang Guan,Zhenwei Shi*

Main category: cs.CV

TL;DR: 论文提出了一种双流Transformer架构，结合高光谱EnMAP数据与Sentinel-2时间序列，用于细粒度作物分类，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度作物分类对大规模作物制图和粮食安全至关重要，但现有研究因高光谱数据获取和标注成本高而稀缺。

Method: 构建了H2Crop数据集，并提出双流Transformer架构，分别处理高光谱数据的精细光谱特征和Sentinel-2时间序列的物候动态。

Result: 实验表明，加入高光谱数据后，F1分数平均提升4.2%，最高达6.3%，且方法优于现有深度学习模型。

Conclusion: 高光谱数据在不同时间窗口和作物变化场景中均具优势，H2Crop数据集和方法为农业分类和高光谱处理提供了重要基准。

Abstract: Fine-grained crop type classification serves as the fundamental basis for
large-scale crop mapping and plays a vital role in ensuring food security. It
requires simultaneous capture of both phenological dynamics (obtained from
multi-temporal satellite data like Sentinel-2) and subtle spectral variations
(demanding nanometer-scale spectral resolution from hyperspectral imagery).
Research combining these two modalities remains scarce currently due to
challenges in hyperspectral data acquisition and crop types annotation costs.
To address these issues, we construct a hierarchical hyperspectral crop dataset
(H2Crop) by integrating 30m-resolution EnMAP hyperspectral data with Sentinel-2
time series. With over one million annotated field parcels organized in a
four-tier crop taxonomy, H2Crop establishes a vital benchmark for fine-grained
agricultural crop classification and hyperspectral image processing. We propose
a dual-stream Transformer architecture that synergistically processes these
modalities. It coordinates two specialized pathways: a spectral-spatial
Transformer extracts fine-grained signatures from hyperspectral EnMAP data,
while a temporal Swin Transformer extracts crop growth patterns from Sentinel-2
time series. The designed hierarchical classification head with hierarchical
fusion then simultaneously delivers multi-level crop type classification across
all taxonomic tiers. Experiments demonstrate that adding hyperspectral EnMAP
data to Sentinel-2 time series yields a 4.2% average F1-scores improvement
(peaking at 6.3%). Extensive comparisons also confirm our method's higher
accuracy over existing deep learning approaches for crop type classification
and the consistent benefits of hyperspectral data across varying temporal
windows and crop change scenarios. Codes and dataset are available at
https://github.com/flyakon/H2Crop.

</details>


### [533] [Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding](https://arxiv.org/abs/2506.06275)
*Emmanouil Zaranis,António Farinhas,Saul Santos,Beatriz Canaverde,Miguel Moura Ramos,Aditya K Surikuchi,André Viveiros,Baohao Liao,Elena Bueno-Benito,Nithin Sivakumaran,Pavlo Vasylenko,Shoubin Yu,Sonal Sannigrahi,Wafaa Mohammed,Ben Peters,Danae Sánchez Villegas,Elias Stengel-Eskin,Giuseppe Attanasio,Jaehong Yoon,Stella Frank,Alessandro Suglia,Chrysoula Zerva,Desmond Elliott,Mariella Dimiccoli,Mohit Bansal,Oswald Lanz,Raffaella Bernardi,Raquel Fernández,Sandro Pezzelle,Vlad Niculae,André F. T. Martins*

Main category: cs.CV

TL;DR: 论文提出了MF$^2$，一个用于评估模型对长视频内容理解能力的新基准，重点关注核心叙事元素，而非表面细节。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在长视频内容理解上存在局限，现有基准多关注表面细节或依赖半自动生成的问题，无法反映真实理解。

Method: MF$^2$包含50多部完整电影，每部电影配对手工构建的真实与虚假声明对，共850多对，评估模型对核心叙事的理解能力。

Result: 实验表明，现有模型在识别真实与虚假声明上的表现远低于人类，突显其在关键叙事信息保留和推理上的不足。

Conclusion: MF$^2$揭示了当前VLMs在长视频内容理解上的短板，为未来研究提供了更精准的评估工具。

Abstract: Despite recent progress in vision-language models (VLMs), holistic
understanding of long-form video content remains a significant challenge,
partly due to limitations in current benchmarks. Many focus on peripheral,
``needle-in-a-haystack'' details, encouraging context-insensitive retrieval
over deep comprehension. Others rely on large-scale, semi-automatically
generated questions (often produced by language models themselves) that are
easier for models to answer but fail to reflect genuine understanding. In this
paper, we introduce MF$^2$, a new benchmark for evaluating whether models can
comprehend, consolidate, and recall key narrative information from full-length
movies (50-170 minutes long). MF$^2$ includes over 50 full-length,
open-licensed movies, each paired with manually constructed sets of claim pairs
-- one true (fact) and one plausible but false (fib), totalling over 850 pairs.
These claims target core narrative elements such as character motivations and
emotions, causal chains, and event order, and refer to memorable moments that
humans can recall without rewatching the movie. Instead of multiple-choice
formats, we adopt a binary claim evaluation protocol: for each pair, models
must correctly identify both the true and false claims. This reduces biases
like answer ordering and enables a more precise assessment of reasoning. Our
experiments demonstrate that both open-weight and closed state-of-the-art
models fall well short of human performance, underscoring the relative ease of
the task for humans and their superior ability to retain and reason over
critical narrative information -- an ability current VLMs lack.

</details>


### [534] [Object-level Self-Distillation for Vision Pretraining](https://arxiv.org/abs/2506.05409)
*Çağlar Hızlı,Çağatay Yıldız,Pekka Marttinen*

Main category: cs.CV

TL;DR: ODIS是一种预训练方法，通过对象级自蒸馏改进视觉表示，解决了传统图像级自蒸馏在多对象场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统图像级自蒸馏假设每张图像仅包含单一对象，但实际场景中常包含多个对象，限制了方法的扩展性和适用性。

Method: ODIS通过对象感知裁剪和掩码注意力，将自蒸馏粒度从图像级细化到对象级，简化任务并提升语义内容的学习。

Result: ODIS在ViT-Large上实现了82.6%的k-NN准确率，显著提升了图像和补丁级别的视觉表示。

Conclusion: ODIS通过对象级自蒸馏有效解决了多对象场景的挑战，为复杂数据集提供了更优的视觉表示方法。

Abstract: State-of-the-art vision pretraining methods rely on image-level
self-distillation from object-centric datasets such as ImageNet, implicitly
assuming each image contains a single object. This assumption does not always
hold: many ImageNet images already contain multiple objects. Further, it limits
scalability to scene-centric datasets that better mirror real-world complexity.
We address these challenges by introducing Object-level Self-DIStillation
(ODIS), a pretraining approach that shifts the self-distillation granularity
from whole images to individual objects. Using object-aware cropping and masked
attention, ODIS isolates object-specific regions, guiding the transformer
toward semantically meaningful content and transforming a noisy, scene-level
task into simpler object-level sub-tasks. We show that this approach improves
visual representations both at the image and patch levels. Using masks at
inference time, our method achieves an impressive $82.6\%$ $k$-NN accuracy on
ImageNet1k with ViT-Large.

</details>


### [535] [SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing](https://arxiv.org/abs/2506.05414)
*Mingfei Chen,Zijun Cui,Xiulong Liu,Jinlin Xiang,Caleb Zheng,Jingyuan Li,Eli Shlizerman*

Main category: cs.CV

TL;DR: SAVVY-Bench是首个针对动态场景中3D空间推理的基准测试，结合了同步空间音频。SAVVY是一种无需训练的两阶段推理方法，显著提升了现有AV-LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AV-LLMs和基准测试主要关注静态或2D场景，而动态3D空间推理在人类认知中至关重要，但尚未被充分探索。

Method: SAVVY分为两阶段：1) 利用AV-LLMs和其他视听方法跟踪关键对象轨迹；2) 构建动态全局地图，通过坐标转换对齐查询视角。

Result: SAVVY显著提升了现有AV-LLMs的性能，为动态3D空间推理设定了新标准。

Conclusion: SAVVY-Bench和SAVVY方法填补了动态3D空间推理的空白，为未来研究提供了新方向。

Abstract: 3D spatial reasoning in dynamic, audio-visual environments is a cornerstone
of human cognition yet remains largely unexplored by existing Audio-Visual
Large Language Models (AV-LLMs) and benchmarks, which predominantly focus on
static or 2D scenes. We introduce SAVVY-Bench, the first benchmark for 3D
spatial reasoning in dynamic scenes with synchronized spatial audio.
SAVVY-Bench is comprised of thousands of relationships involving static and
moving objects, and requires fine-grained temporal grounding, consistent 3D
localization, and multi-modal annotation. To tackle this challenge, we propose
SAVVY, a novel training-free reasoning pipeline that consists of two stages:
(i) Egocentric Spatial Tracks Estimation, which leverages AV-LLMs as well as
other audio-visual methods to track the trajectories of key objects related to
the query using both visual and spatial audio cues, and (ii) Dynamic Global Map
Construction, which aggregates multi-modal queried object trajectories and
converts them into a unified global dynamic map. Using the constructed map, a
final QA answer is obtained through a coordinate transformation that aligns the
global map with the queried viewpoint. Empirical evaluation demonstrates that
SAVVY substantially enhances performance of state-of-the-art AV-LLMs, setting a
new standard and stage for approaching dynamic 3D spatial reasoning in AV-LLMs.

</details>


### [536] [Self-Predictive Dynamics for Generalization of Vision-based Reinforcement Learning](https://arxiv.org/abs/2506.05418)
*Kyungsoo Kim,Jeongsoo Ha,Yusung Kim*

Main category: cs.CV

TL;DR: SPD方法通过弱增强和强增强并行提取任务相关特征，显著提升未见观测的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 图像观测中常包含干扰元素（如阴影、云、光），影响强化学习性能，尤其是在训练中未暴露的干扰下。

Method: 设计自预测动态（SPD）方法，通过并行弱增强和强增强学习表示，预测双向增强版本的逆和正向转换。

Result: 在MuJoCo视觉控制任务和CARLA自动驾驶任务中，SPD在复杂观测中优于先前研究，显著提升未见观测的泛化性能。

Conclusion: SPD能高效提取任务相关特征，适用于复杂和未见观测场景。

Abstract: Vision-based reinforcement learning requires efficient and robust
representations of image-based observations, especially when the images contain
distracting (task-irrelevant) elements such as shadows, clouds, and light. It
becomes more important if those distractions are not exposed during training.
We design a Self-Predictive Dynamics (SPD) method to extract task-relevant
features efficiently, even in unseen observations after training. SPD uses weak
and strong augmentations in parallel, and learns representations by predicting
inverse and forward transitions across the two-way augmented versions. In a set
of MuJoCo visual control tasks and an autonomous driving task (CARLA), SPD
outperforms previous studies in complex observations, and significantly
improves the generalization performance for unseen observations. Our code is
available at https://github.com/unigary/SPD.

</details>


### [537] [Coordinated Robustness Evaluation Framework for Vision-Language Models](https://arxiv.org/abs/2506.05429)
*Ashwin Ramesh Babu,Sajad Mousavi,Vineet Gundecha,Sahand Ghorbanpour,Avisek Naug,Antonio Guillen,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.CV

TL;DR: 该论文提出了一种通用的替代模型，用于生成针对视觉和语言模态的对抗性扰动，以评估视觉语言模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在图像描述和视觉问答等任务中表现出色，但其对微小扰动的敏感性限制了其鲁棒性，尤其是在实际部署中。

Method: 训练一个通用的替代模型，能够同时处理图像和文本输入，生成联合表示，并进一步生成针对文本和图像模态的对抗性扰动。

Result: 在视觉问答和视觉推理数据集上的实验表明，该策略优于其他多模态攻击和单模态攻击方法。

Conclusion: 提出的协调攻击策略能有效破坏多种先进预训练多模态模型的鲁棒性。

Abstract: Vision-language models, which integrate computer vision and natural language
processing capabilities, have demonstrated significant advancements in tasks
such as image captioning and visual question and answering. However, similar to
traditional models, they are susceptible to small perturbations, posing a
challenge to their robustness, particularly in deployment scenarios. Evaluating
the robustness of these models requires perturbations in both the vision and
language modalities to learn their inter-modal dependencies. In this work, we
train a generic surrogate model that can take both image and text as input and
generate joint representation which is further used to generate adversarial
perturbations for both the text and image modalities. This coordinated attack
strategy is evaluated on the visual question and answering and visual reasoning
datasets using various state-of-the-art vision-language models. Our results
indicate that the proposed strategy outperforms other multi-modal attacks and
single-modality attacks from the recent literature. Our results demonstrate
their effectiveness in compromising the robustness of several state-of-the-art
pre-trained multi-modal models such as instruct-BLIP, ViLT and others.

</details>


### [538] [Robustness Evaluation for Video Models with Reinforcement Learning](https://arxiv.org/abs/2506.05431)
*Ashwin Ramesh Babu,Sajad Mousavi,Vineet Gundecha,Sahand Ghorbanpour,Avisek Naug,Antonio Guillen,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.CV

TL;DR: 提出一种多智能体强化学习方法，用于视频分类模型的鲁棒性评估，通过时空协同扰动生成更有效的攻击。


<details>
  <summary>Details</summary>
Motivation: 视频分类模型的鲁棒性评估比图像模型更具挑战性，因其时间维度和计算复杂性增加，需最小化扰动以诱导误分类。

Method: 采用多智能体强化学习（空间和时间）协同识别视频敏感区域，生成视觉不可察觉的精细扰动。

Result: 在Lp度量和平均查询次数上优于现有方法，支持自定义失真类型，并在HMDB-51和UCF-101数据集上评估了4种流行模型。

Conclusion: 该方法在视频动作识别模型的鲁棒性评估中表现出色，提供了更贴近实际应用的扰动生成方式。

Abstract: Evaluating the robustness of Video classification models is very challenging,
specifically when compared to image-based models. With their increased temporal
dimension, there is a significant increase in complexity and computational
cost. One of the key challenges is to keep the perturbations to a minimum to
induce misclassification. In this work, we propose a multi-agent reinforcement
learning approach (spatial and temporal) that cooperatively learns to identify
the given video's sensitive spatial and temporal regions. The agents consider
temporal coherence in generating fine perturbations, leading to a more
effective and visually imperceptible attack. Our method outperforms the
state-of-the-art solutions on the Lp metric and the average queries. Our method
enables custom distortion types, making the robustness evaluation more relevant
to the use case. We extensively evaluate 4 popular models for video action
recognition on two popular datasets, HMDB-51 and UCF-101.

</details>


### [539] [U-NetMN and SegNetMN: Modified U-Net and SegNet models for bimodal SAR image segmentation](https://arxiv.org/abs/2506.05444)
*Marwane Kzadri,Franco Alberto Cardillo,Nanée Chahinian,Carole Delenne,Renaud Hostache,Jamal Riffi*

Main category: cs.CV

TL;DR: 研究评估了模式归一化对U-Net和SegNet模型在SAR图像分割中的影响，发现其能显著加速收敛并提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 由于SAR图像的复杂统计分布，深度学习分割模型在收敛速度和稳定性上面临挑战。

Method: 在U-Net和SegNet中集成模式归一化，以减少收敛时间并保持性能。

Result: 实验表明模式归一化显著加速收敛，且归一化模型在不同区域表现出更高的稳定性。

Conclusion: 归一化有效提升了SAR图像分割的计算效率和泛化能力。

Abstract: Segmenting Synthetic Aperture Radar (SAR) images is crucial for many remote
sensing applications, particularly water body detection. However, deep
learning-based segmentation models often face challenges related to convergence
speed and stability, mainly due to the complex statistical distribution of this
type of data. In this study, we evaluate the impact of mode normalization on
two widely used semantic segmentation models, U-Net and SegNet. Specifically,
we integrate mode normalization, to reduce convergence time while maintaining
the performance of the baseline models. Experimental results demonstrate that
mode normalization significantly accelerates convergence. Furthermore,
cross-validation results indicate that normalized models exhibit increased
stability in different zones. These findings highlight the effectiveness of
normalization in improving computational efficiency and generalization in SAR
image segmentation.

</details>


### [540] [MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning](https://arxiv.org/abs/2506.05523)
*Zikui Cai,Andrew Wang,Anirudh Satheesh,Ankit Nakhawa,Hyunwoo Jae,Keenan Powell,Minghui Liu,Neel Jay,Sungbin Oh,Xiyao Wang,Yongyuan Liang,Tom Goldstein,Furong Huang*

Main category: cs.CV

TL;DR: MORSE-500是一个动态视频基准测试，旨在解决现有多模态推理基准的不足，覆盖六种推理能力，并通过可控生成支持持续挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试依赖静态图像、局限于数学问题解决且易饱和，无法全面评估多模态推理能力。

Method: 使用Python脚本、生成视频模型和真实素材生成500个脚本化视频片段，嵌入六类推理问题。

Result: 实验显示当前先进模型在所有推理类别中存在显著性能差距，尤其在抽象和规划任务上。

Conclusion: MORSE-500通过可控生成支持持续挑战，为多模态推理研究提供透明、可复现的工具。

Abstract: Despite rapid advances in vision-language models (VLMs), current benchmarks
for multimodal reasoning fall short in three key dimensions. First, they
overwhelmingly rely on static images, failing to capture the temporal
complexity of real-world environments. Second, they narrowly focus on
mathematical problem-solving, neglecting the broader spectrum of reasoning
skills -- including abstract, physical, planning, spatial, and temporal
capabilities -- required for robust multimodal intelligence. Third, many
benchmarks quickly saturate, offering limited headroom for diagnosing failure
modes or measuring continued progress. We introduce MORSE-500 (Multimodal
Reasoning Stress-test Environment), a video benchmark composed of 500 fully
scripted clips with embedded questions spanning six complementary reasoning
categories. Each instance is programmatically generated using deterministic
Python scripts (via Manim, Matplotlib, MoviePy), generative video models, and
curated real footage. This script-driven design allows fine-grained control
over visual complexity, distractor density, and temporal dynamics -- enabling
difficulty to be scaled systematically as models improve. Unlike static
benchmarks that become obsolete once saturated, MORSE-500 is built to evolve:
its controllable generation pipeline supports the creation of arbitrarily
challenging new instances, making it ideally suited for stress-testing
next-generation models. Initial experiments with state-of-the-art systems --
including various Gemini 2.5 Pro and OpenAI o3 which represent the strongest
available at the time, alongside strong open-source models -- reveal
substantial performance gaps across all categories, with particularly large
deficits in abstract and planning tasks. We release the full dataset,
generation scripts, and evaluation harness to support transparent,
reproducible, and forward-looking multimodal reasoning research.

</details>


### [541] [Robust sensor fusion against on-vehicle sensor staleness](https://arxiv.org/abs/2506.05780)
*Meng Fan,Yifan Zuo,Patrick Blaes,Harley Montgomery,Subhasis Das*

Main category: cs.CV

TL;DR: 论文提出了一种解决自动驾驶中传感器数据时间不同步问题的新方法，通过点级时间戳偏移和数据增强策略，提升了感知系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传感器数据的时间不同步（staleness）会导致物体状态估计不一致，严重影响轨迹预测的准确性，从而威胁自动驾驶的安全性。

Method: 提出了一种模型无关的方法，包括（1）为LiDAR和雷达数据添加相对于相机的时间戳偏移特征，（2）模拟真实场景中传感器staleness模式的数据增强策略。

Result: 实验表明，传统模型在传感器数据不同步时性能显著下降，而新方法在同步和不同步条件下均表现良好。

Conclusion: 该方法有效解决了传感器数据时间不同步问题，提升了自动驾驶感知系统的鲁棒性和性能。

Abstract: Sensor fusion is crucial for a performant and robust Perception system in
autonomous vehicles, but sensor staleness, where data from different sensors
arrives with varying delays, poses significant challenges. Temporal
misalignment between sensor modalities leads to inconsistent object state
estimates, severely degrading the quality of trajectory predictions that are
critical for safety. We present a novel and model-agnostic approach to address
this problem via (1) a per-point timestamp offset feature (for LiDAR and radar
both relative to camera) that enables fine-grained temporal awareness in sensor
fusion, and (2) a data augmentation strategy that simulates realistic sensor
staleness patterns observed in deployed vehicles. Our method is integrated into
a perspective-view detection model that consumes sensor data from multiple
LiDARs, radars and cameras. We demonstrate that while a conventional model
shows significant regressions when one sensor modality is stale, our approach
reaches consistently good performance across both synchronized and stale
conditions.

</details>


### [542] [Improved Allergy Wheal Detection for the Skin Prick Automated Test Device](https://arxiv.org/abs/2506.05862)
*Rembert Daems,Sven Seys,Valérie Hox,Adam Chaker,Glynnis De Greve,Winde Lemmens,Anne-Lise Poirrier,Eline Beckers,Zuzana Diamant,Carmen Dierickx,Peter W. Hellings,Caroline Huart,Claudia Jerin,Mark Jorissen,Hanne Oscé,Karolien Roux,Mark Thompson,Sophie Tombu,Saartje Uyttebroek,Andrzej Zarowski,Senne Gorris,Laura Van Gerven,Dirk Loeckx,Thomas Demeester*

Main category: cs.CV

TL;DR: SPAT设备通过多光照条件图像提高过敏检测准确性，提出了一种结合神经网络和算法的自动化方法。


<details>
  <summary>Details</summary>
Motivation: 提高皮肤点刺试验（SPT）的检测一致性和准确性。

Method: 使用SPAT设备的32张多光照图像，结合神经网络分割和算法检测过敏反应。

Result: 32张图像比单张图像在检测准确性上显著提高。

Conclusion: 多光照条件图像可显著提升过敏检测的准确性。

Abstract: Background: The skin prick test (SPT) is the gold standard for diagnosing
sensitization to inhalant allergies. The Skin Prick Automated Test (SPAT)
device was designed for increased consistency in test results, and captures 32
images to be jointly used for allergy wheal detection and delineation, which
leads to a diagnosis.
  Materials and Methods: Using SPAT data from $868$ patients with suspected
inhalant allergies, we designed an automated method to detect and delineate
wheals on these images. To this end, $10,416$ wheals were manually annotated by
drawing detailed polygons along the edges. The unique data-modality of the SPAT
device, with $32$ images taken under distinct lighting conditions, requires a
custom-made approach. Our proposed method consists of two parts: a neural
network component that segments the wheals on the pixel level, followed by an
algorithmic and interpretable approach for detecting and delineating the
wheals.
  Results: We evaluate the performance of our method on a hold-out validation
set of $217$ patients. As a baseline we use a single conventionally lighted
image per SPT as input to our method.
  Conclusion: Using the $32$ SPAT images under various lighting conditions
offers a considerably higher accuracy than a single image in conventional,
uniform light.

</details>


### [543] [Rethinking Semi-supervised Segmentation Beyond Accuracy: Reliability and Robustness](https://arxiv.org/abs/2506.05917)
*Steven Landgraf,Markus Hillemann,Markus Ulrich*

Main category: cs.CV

TL;DR: 论文提出了一种新的评估指标RSS，用于衡量半监督语义分割模型的准确性、可靠性和鲁棒性，填补了现有评估协议的不足。


<details>
  <summary>Details</summary>
Motivation: 当前半监督分割评估仅关注准确性，忽略了可靠性和鲁棒性，而这些对自动驾驶等安全关键应用至关重要。

Method: 引入Reliable Segmentation Score (RSS)，通过调和平均值结合预测准确性、校准和不确定性质量。

Result: 实验表明半监督方法常以可靠性换取准确性，UniMatchV2在鲁棒性上表现良好但可靠性仍有不足。

Conclusion: 呼吁采用RSS等更全面的评估指标，以更好地满足实际部署需求。

Abstract: Semantic segmentation is critical for scene understanding but demands costly
pixel-wise annotations, attracting increasing attention to semi-supervised
approaches to leverage abundant unlabeled data. While semi-supervised
segmentation is often promoted as a path toward scalable, real-world
deployment, it is astonishing that current evaluation protocols exclusively
focus on segmentation accuracy, entirely overlooking reliability and
robustness. These qualities, which ensure consistent performance under diverse
conditions (robustness) and well-calibrated model confidences as well as
meaningful uncertainties (reliability), are essential for safety-critical
applications like autonomous driving, where models must handle unpredictable
environments and avoid sudden failures at all costs. To address this gap, we
introduce the Reliable Segmentation Score (RSS), a novel metric that combines
predictive accuracy, calibration, and uncertainty quality measures via a
harmonic mean. RSS penalizes deficiencies in any of its components, providing
an easy and intuitive way of holistically judging segmentation models.
Comprehensive evaluations of UniMatchV2 against its predecessor and a
supervised baseline show that semi-supervised methods often trade reliability
for accuracy. While out-of-domain evaluations demonstrate UniMatchV2's
robustness, they further expose persistent reliability shortcomings. We
advocate for a shift in evaluation protocols toward more holistic metrics like
RSS to better align semi-supervised learning research with real-world
deployment needs.

</details>


### [544] [Sample-Specific Noise Injection For Diffusion-Based Adversarial Purification](https://arxiv.org/abs/2506.06027)
*Yuhao Sun,Jiacheng Zhang,Zesheng Ye,Chaowei Xiao,Feng Liu*

Main category: cs.CV

TL;DR: 本文提出了一种样本特定的噪声注入方法（SSNI），通过预训练的评分网络自适应调整噪声水平，显著提升了扩散净化方法的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散净化方法对所有样本使用固定的噪声水平，而研究发现不同样本的最优噪声水平可能不同。

Method: SSNI利用预训练的评分网络估计样本偏离干净数据分布的程度，并根据评分范数自适应调整噪声水平。

Result: 在CIFAR-10和ImageNet-1K上，SSNI显著提升了准确性和鲁棒性。

Conclusion: 样本特定的噪声分配是扩散净化方法的关键改进方向。

Abstract: Diffusion-based purification (DBP) methods aim to remove adversarial noise
from the input sample by first injecting Gaussian noise through a forward
diffusion process, and then recovering the clean example through a reverse
generative process. In the above process, how much Gaussian noise is injected
to the input sample is key to the success of DBP methods, which is controlled
by a constant noise level $t^*$ for all samples in existing methods. In this
paper, we discover that an optimal $t^*$ for each sample indeed could be
different. Intuitively, the cleaner a sample is, the less the noise it should
be injected, and vice versa. Motivated by this finding, we propose a new
framework, called Sample-specific Score-aware Noise Injection (SSNI).
Specifically, SSNI uses a pre-trained score network to estimate how much a data
point deviates from the clean data distribution (i.e., score norms). Then,
based on the magnitude of score norms, SSNI applies a reweighting function to
adaptively adjust $t^*$ for each sample, achieving sample-specific noise
injections. Empirically, incorporating our framework with existing DBP methods
results in a notable improvement in both accuracy and robustness on CIFAR-10
and ImageNet-1K, highlighting the necessity to allocate distinct noise levels
to different samples in DBP methods. Our code is available at:
https://github.com/tmlr-group/SSNI.

</details>


### [545] [Tensor-to-Tensor Models with Fast Iterated Sum Features](https://arxiv.org/abs/2506.06041)
*Joscha Diehl,Rasheed Ibraheem,Leonard Schmitz,Yue Wu*

Main category: cs.CV

TL;DR: 提出了一种基于“角树”数学工具的新型张量到张量层（FIS层），具有线性计算成本，适用于图像和高阶张量处理。


<details>
  <summary>Details</summary>
Motivation: 高维数据（如图像或张量）在深度学习中普遍存在，需要高效的子二次计算层。

Method: 利用“角树”和迭代积分（或和）的多参数泛化，构建了FIS层，可无缝集成到神经网络中。

Result: 在分类和异常检测任务中表现优异，替换ResNet部分层后性能接近但参数更少，异常检测AUROC达97.3%。

Conclusion: FIS层为高维数据处理提供了高效解决方案，兼具性能和计算效率。

Abstract: Data in the form of images or higher-order tensors is ubiquitous in modern
deep learning applications. Owing to their inherent high dimensionality, the
need for subquadratic layers processing such data is even more pressing than
for sequence data. We propose a novel tensor-to-tensor layer with linear cost
in the input size, utilizing the mathematical gadget of ``corner trees'' from
the field of permutation counting. In particular, for order-two tensors, we
provide an image-to-image layer that can be plugged into image processing
pipelines. On the one hand, our method can be seen as a higher-order
generalization of state-space models. On the other hand, it is based on a
multiparameter generalization of the signature of iterated integrals (or sums).
The proposed tensor-to-tensor concept is used to build a neural network layer
called the Fast Iterated Sums (FIS) layer which integrates seamlessly with
other layer types. We demonstrate the usability of the FIS layer with both
classification and anomaly detection tasks. By replacing some layers of a
smaller ResNet architecture with FIS, a similar accuracy (with a difference of
only 0.1\%) was achieved in comparison to a larger ResNet while reducing the
number of trainable parameters and multi-add operations. The FIS layer was also
used to build an anomaly detection model that achieved an average AUROC of
97.3\% on the texture images of the popular MVTec AD dataset. The processing
and modelling codes are publicly available at
https://github.com/diehlj/fast-iterated-sums.

</details>


### [546] [SDS-Net: Shallow-Deep Synergism-detection Network for infrared small target detection](https://arxiv.org/abs/2506.06042)
*Taoran Yue,Xiaojin Lu,Jiaxi Cai,Yuanping Chen,Shibing Chu*

Main category: cs.CV

TL;DR: 提出了一种浅层-深层协同检测网络（SDS-Net），通过双分支架构和自适应特征融合模块，显著提升了红外小目标检测的精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有CNN方法忽视了浅层与深层特征的异质性，导致特征协作不足，影响了检测性能和计算效率。

Method: 采用双分支架构分别建模特征的结构特性和语义属性，并引入自适应特征融合模块动态建模跨层特征相关性。

Result: 在三个公开数据集上表现优于现有方法，同时保持了低计算复杂性和高推理效率。

Conclusion: SDS-Net在红外小目标检测中展现出优越性能和广泛应用前景。

Abstract: Current CNN-based infrared small target detection(IRSTD) methods generally
overlook the heterogeneity between shallow and deep features, leading to
inefficient collaboration between shallow fine grained structural information
and deep high-level semantic representations. Additionally, the dependency
relationships and fusion mechanisms across different feature hierarchies lack
systematic modeling, which fails to fully exploit the complementarity of
multilevel features. These limitations hinder IRSTD performance while incurring
substantial computational costs. To address these challenges, this paper
proposes a shallow-deep synergistic detection network (SDS-Net) that
efficiently models multilevel feature representations to increase both the
detection accuracy and computational efficiency in IRSTD tasks. SDS-Net
introduces a dual-branch architecture that separately models the structural
characteristics and semantic properties of features, effectively preserving
shallow spatial details while capturing deep semantic representations, thereby
achieving high-precision detection with significantly improved inference speed.
Furthermore, the network incorporates an adaptive feature fusion module to
dynamically model cross-layer feature correlations, enhancing overall feature
collaboration and representation capability. Comprehensive experiments on three
public datasets (NUAA-SIRST, NUDT-SIRST, and IRSTD-1K) demonstrate that SDS-Net
outperforms state-of-the-art IRSTD methods while maintaining low computational
complexity and high inference efficiency, showing superior detection
performance and broad application prospects. Our code will be made public at
https://github.com/PhysiLearn/SDS-Net.

</details>


### [547] [Fine-grained Hierarchical Crop Type Classification from Integrated Hyperspectral EnMAP Data and Multispectral Sentinel-2 Time Series: A Large-scale Dataset and Dual-stream Transformer Method](https://arxiv.org/abs/2506.06155)
*Wenyuan Li,Shunlin Liang,Yuxiang Zhang,Liqin Liu,Keyan Chen,Yongzhe Chen,Han Ma,Jianglei Xu,Yichuan Ma,Shikang Guan,Zhenwei Shi*

Main category: cs.CV

TL;DR: 论文提出了一种双流Transformer架构，结合高光谱EnMAP数据和Sentinel-2时间序列，用于细粒度作物分类，显著提升了分类精度。


<details>
  <summary>Details</summary>
Motivation: 细粒度作物分类对大规模作物制图和粮食安全至关重要，但现有研究因高光谱数据获取和标注成本高而稀缺。

Method: 构建了H2Crop数据集，提出双流Transformer架构，分别处理高光谱数据的精细光谱特征和Sentinel-2时间序列的作物生长模式。

Result: 实验表明，加入高光谱数据后，F1分数平均提升4.2%，最高达6.3%，且方法优于现有深度学习模型。

Conclusion: 高光谱数据在作物分类中具有显著优势，提出的方法为细粒度农业分类和高光谱图像处理提供了重要基准。

Abstract: Fine-grained crop type classification serves as the fundamental basis for
large-scale crop mapping and plays a vital role in ensuring food security. It
requires simultaneous capture of both phenological dynamics (obtained from
multi-temporal satellite data like Sentinel-2) and subtle spectral variations
(demanding nanometer-scale spectral resolution from hyperspectral imagery).
Research combining these two modalities remains scarce currently due to
challenges in hyperspectral data acquisition and crop types annotation costs.
To address these issues, we construct a hierarchical hyperspectral crop dataset
(H2Crop) by integrating 30m-resolution EnMAP hyperspectral data with Sentinel-2
time series. With over one million annotated field parcels organized in a
four-tier crop taxonomy, H2Crop establishes a vital benchmark for fine-grained
agricultural crop classification and hyperspectral image processing. We propose
a dual-stream Transformer architecture that synergistically processes these
modalities. It coordinates two specialized pathways: a spectral-spatial
Transformer extracts fine-grained signatures from hyperspectral EnMAP data,
while a temporal Swin Transformer extracts crop growth patterns from Sentinel-2
time series. The designed hierarchical classification head with hierarchical
fusion then simultaneously delivers multi-level crop type classification across
all taxonomic tiers. Experiments demonstrate that adding hyperspectral EnMAP
data to Sentinel-2 time series yields a 4.2% average F1-scores improvement
(peaking at 6.3%). Extensive comparisons also confirm our method's higher
accuracy over existing deep learning approaches for crop type classification
and the consistent benefits of hyperspectral data across varying temporal
windows and crop change scenarios. Codes and dataset are available at
https://github.com/flyakon/H2Crop.

</details>


### [548] [Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding](https://arxiv.org/abs/2506.06275)
*Emmanouil Zaranis,António Farinhas,Saul Santos,Beatriz Canaverde,Miguel Moura Ramos,Aditya K Surikuchi,André Viveiros,Baohao Liao,Elena Bueno-Benito,Nithin Sivakumaran,Pavlo Vasylenko,Shoubin Yu,Sonal Sannigrahi,Wafaa Mohammed,Ben Peters,Danae Sánchez Villegas,Elias Stengel-Eskin,Giuseppe Attanasio,Jaehong Yoon,Stella Frank,Alessandro Suglia,Chrysoula Zerva,Desmond Elliott,Mariella Dimiccoli,Mohit Bansal,Oswald Lanz,Raffaella Bernardi,Raquel Fernández,Sandro Pezzelle,Vlad Niculae,André F. T. Martins*

Main category: cs.CV

TL;DR: MF$^2$是一个新的基准测试，用于评估模型是否能从全长电影中理解和回忆关键叙事信息。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在长视频内容理解上存在局限，现有基准测试多关注细节或依赖自动化生成的问题，未能反映真实理解。

Method: MF$^2$包含50多部全长电影，每部电影配有人工构建的真实和虚假声明对，共850多对，测试模型是否能正确识别。

Result: 实验表明，现有模型在识别关键叙事信息上远不及人类表现。

Conclusion: MF$^2$揭示了当前VLMs在长视频内容理解上的不足，为未来研究提供了新方向。

Abstract: Despite recent progress in vision-language models (VLMs), holistic
understanding of long-form video content remains a significant challenge,
partly due to limitations in current benchmarks. Many focus on peripheral,
``needle-in-a-haystack'' details, encouraging context-insensitive retrieval
over deep comprehension. Others rely on large-scale, semi-automatically
generated questions (often produced by language models themselves) that are
easier for models to answer but fail to reflect genuine understanding. In this
paper, we introduce MF$^2$, a new benchmark for evaluating whether models can
comprehend, consolidate, and recall key narrative information from full-length
movies (50-170 minutes long). MF$^2$ includes over 50 full-length,
open-licensed movies, each paired with manually constructed sets of claim pairs
-- one true (fact) and one plausible but false (fib), totalling over 850 pairs.
These claims target core narrative elements such as character motivations and
emotions, causal chains, and event order, and refer to memorable moments that
humans can recall without rewatching the movie. Instead of multiple-choice
formats, we adopt a binary claim evaluation protocol: for each pair, models
must correctly identify both the true and false claims. This reduces biases
like answer ordering and enables a more precise assessment of reasoning. Our
experiments demonstrate that both open-weight and closed state-of-the-art
models fall well short of human performance, underscoring the relative ease of
the task for humans and their superior ability to retain and reason over
critical narrative information -- an ability current VLMs lack.

</details>


### [549] [STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis](https://arxiv.org/abs/2506.06276)
*Jiatao Gu,Tianrong Chen,David Berthelot,Huangjie Zheng,Yuyang Wang,Ruixiang Zhang,Laurent Dinh,Miguel Angel Bautista,Josh Susskind,Shuangfei Zhai*

Main category: cs.CV

TL;DR: STARFlow是一种基于标准化流的可扩展生成模型，结合了Transformer的自回归能力，在高分辨率图像合成中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决标准化流模型在高分辨率图像生成中的可扩展性和性能问题。

Method: 提出TARFlow，结合标准化流和自回归Transformer，采用深度-浅层设计、潜在空间建模和新型引导算法。

Result: 在类别条件和文本条件图像生成任务中表现优异，接近扩散模型的样本质量。

Conclusion: STARFlow首次证明标准化流在大规模高分辨率图像生成中的有效性。

Abstract: We present STARFlow, a scalable generative model based on normalizing flows
that achieves strong performance in high-resolution image synthesis. The core
of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the
expressive power of normalizing flows with the structured modeling capabilities
of Autoregressive Transformers. We first establish the theoretical universality
of TARFlow for modeling continuous distributions. Building on this foundation,
we introduce several key architectural and algorithmic innovations to
significantly enhance scalability: (1) a deep-shallow design, wherein a deep
Transformer block captures most of the model representational capacity,
complemented by a few shallow Transformer blocks that are computationally
efficient yet substantially beneficial; (2) modeling in the latent space of
pretrained autoencoders, which proves more effective than direct pixel-level
modeling; and (3) a novel guidance algorithm that significantly boosts sample
quality. Crucially, our model remains an end-to-end normalizing flow, enabling
exact maximum likelihood training in continuous spaces without discretization.
STARFlow achieves competitive performance in both class-conditional and
text-conditional image generation tasks, approaching state-of-the-art diffusion
models in sample quality. To our knowledge, this work is the first successful
demonstration of normalizing flows operating effectively at this scale and
resolution.

</details>


### [550] [Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images](https://arxiv.org/abs/2506.06389)
*Rifat Sadik,Tanvir Rahman,Arpan Bhattacharjee,Bikash Chandra Halder,Ismail Hossain*

Main category: cs.CV

TL;DR: 本文研究了基于视觉Transformer（ViT）的医学图像对抗水印攻击的脆弱性，并探讨了对抗训练作为防御机制的效果。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型在计算机视觉任务中的成功应用，本文旨在评估ViT在医学图像中对对抗水印攻击的脆弱性，并与CNN进行比较。

Method: 通过投影梯度下降（PGD）生成对抗水印，测试ViT和CNN的脆弱性，并分析对抗训练的效果。

Result: ViT对对抗攻击非常脆弱，准确率最低降至27.6%，但对抗训练可将其提升至90.0%。

Conclusion: 尽管ViT在对抗攻击下表现脆弱，但对抗训练能显著提升其鲁棒性，为医学图像分析提供了潜在解决方案。

Abstract: Deep learning models have shown remarkable success in dermatological image
analysis, offering potential for automated skin disease diagnosis. Previously,
convolutional neural network(CNN) based architectures have achieved immense
popularity and success in computer vision (CV) based task like skin image
recognition, generation and video analysis. But with the emergence of
transformer based models, CV tasks are now are nowadays carrying out using
these models. Vision Transformers (ViTs) is such a transformer-based models
that have shown success in computer vision. It uses self-attention mechanisms
to achieve state-of-the-art performance across various tasks. However, their
reliance on global attention mechanisms makes them susceptible to adversarial
perturbations. This paper aims to investigate the susceptibility of ViTs for
medical images to adversarial watermarking-a method that adds so-called
imperceptible perturbations in order to fool models. By generating adversarial
watermarks through Projected Gradient Descent (PGD), we examine the
transferability of such attacks to CNNs and analyze the performance defense
mechanism -- adversarial training. Results indicate that while performance is
not compromised for clean images, ViTs certainly become much more vulnerable to
adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,
adversarial training raises it up to 90.0%.

</details>


### [551] [Securing Traffic Sign Recognition Systems in Autonomous Vehicles](https://arxiv.org/abs/2506.06563)
*Thushari Hapuarachchi,Long Dang,Kaiqi Xiong*

Main category: cs.CV

TL;DR: 论文研究了深度神经网络（DNN）在交通标志识别中的安全性，提出了一种数据增强训练方法以抵御误差最小化攻击，并开发了检测模型识别中毒数据。


<details>
  <summary>Details</summary>
Motivation: DNN在交通标志识别中广泛应用，但其训练数据可能被污染，导致模型安全性问题。研究旨在提升DNN对抗攻击的鲁棒性。

Method: 通过误差最小化攻击测试DNN的脆弱性，提出基于数据增强的训练方法，利用非线性变换破坏扰动，并开发检测模型识别中毒数据。

Result: 攻击使DNN预测准确率从99.90%降至10.6%，提出的方法恢复至96.05%，检测模型识别攻击成功率超99%。

Conclusion: 研究强调了在交通标志识别系统中采用先进训练方法的必要性，以抵御数据中毒攻击。

Abstract: Deep Neural Networks (DNNs) are widely used for traffic sign recognition
because they can automatically extract high-level features from images. These
DNNs are trained on large-scale datasets obtained from unknown sources.
Therefore, it is important to ensure that the models remain secure and are not
compromised or poisoned during training. In this paper, we investigate the
robustness of DNNs trained for traffic sign recognition. First, we perform the
error-minimizing attacks on DNNs used for traffic sign recognition by adding
imperceptible perturbations on training data. Then, we propose a data
augmentation-based training method to mitigate the error-minimizing attacks.
The proposed training method utilizes nonlinear transformations to disrupt the
perturbations and improve the model robustness. We experiment with two
well-known traffic sign datasets to demonstrate the severity of the attack and
the effectiveness of our mitigation scheme. The error-minimizing attacks reduce
the prediction accuracy of the DNNs from 99.90% to 10.6%. However, our
mitigation scheme successfully restores the prediction accuracy to 96.05%.
Moreover, our approach outperforms adversarial training in mitigating the
error-minimizing attacks. Furthermore, we propose a detection model capable of
identifying poisoned data even when the perturbations are imperceptible to
human inspection. Our detection model achieves a success rate of over 99% in
identifying the attack. This research highlights the need to employ advanced
training methods for DNNs in traffic sign recognition systems to mitigate the
effects of data poisoning attacks.

</details>


### [552] [Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery](https://arxiv.org/abs/2506.06667)
*Yu-Hsuan Ho,Ali Mostafavi*

Main category: cs.CV

TL;DR: Flood-DamageSense是一种专为洪水灾害后建筑物损坏评估设计的深度学习框架，通过融合多源数据显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在洪水灾害后建筑物损坏分类中表现不佳，因洪水破坏的痕迹不明显。

Method: 结合SAR/InSAR、高分辨率光学图像和洪水风险层，采用多模态Mamba架构和半Siamese编码器，预测损坏等级、洪水范围和建筑物轮廓。

Result: 在Hurricane Harvey数据上，平均F1分数比现有方法提升19个百分点，尤其在“轻微”和“中等”损坏类别中表现突出。

Conclusion: Flood-DamageSense通过风险感知模型和SAR的全天候能力，为灾后决策提供更快、更精细的损坏评估。

Abstract: Most post-disaster damage classifiers succeed only when destructive forces
leave clear spectral or structural signatures -- conditions rarely present
after inundation. Consequently, existing models perform poorly at identifying
flood-related building damages. The model presented in this study,
Flood-DamageSense, addresses this gap as the first deep-learning framework
purpose-built for building-level flood-damage assessment. The architecture
fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical
basemaps and an inherent flood-risk layer that encodes long-term exposure
probabilities, guiding the network toward plausibly affected structures even
when compositional change is minimal. A multimodal Mamba backbone with a
semi-Siamese encoder and task-specific decoders jointly predicts (1) graded
building-damage states, (2) floodwater extent, and (3) building footprints.
Training and evaluation on Hurricane Harvey (2017) imagery from Harris County,
Texas -- supported by insurance-derived property-damage extents -- show a mean
F1 improvement of up to 19 percentage points over state-of-the-art baselines,
with the largest gains in the frequently misclassified "minor" and "moderate"
damage categories. Ablation studies identify the inherent-risk feature as the
single most significant contributor to this performance boost. An end-to-end
post-processing pipeline converts pixel-level outputs to actionable,
building-scale damage maps within minutes of image acquisition. By combining
risk-aware modeling with SAR's all-weather capability, Flood-DamageSense
delivers faster, finer-grained, and more reliable flood-damage intelligence to
support post-disaster decision-making and resource allocation.

</details>


### [553] [Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment](https://arxiv.org/abs/2506.06680)
*Radha Kodali,Venkata Rao Dhulipalla,Venkata Siva Kishor Tatavarty,Madhavi Nadakuditi,Bharadwaj Thiruveedhula,Suryanarayana Gunnam,Durga Prasad Bavirisetti*

Main category: cs.CV

TL;DR: 论文提出了一种结合CNN和LSTM的XAI框架，用于高效且可解释的胚胎分类，以解决传统胚胎评级效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 不孕症对生活质量有显著影响，IVF是主要解决方案，但传统胚胎评级方法效率低下。

Method: 采用CNN-LSTM架构的XAI框架，结合深度学习技术进行胚胎分类。

Result: 模型实现了高准确率的胚胎分类，同时保持了可解释性。

Conclusion: XAI框架为胚胎分类提供了高效且可解释的解决方案，有望提升IVF效率。

Abstract: Infertility has a considerable impact on individuals' quality of life,
affecting them socially and psychologically, with projections indicating a rise
in the upcoming years. In vitro fertilization (IVF) emerges as one of the
primary techniques within economically developed nations, employed to address
the rising problem of low fertility. Expert embryologists conventionally grade
embryos by reviewing blastocyst images to select the most optimal for transfer,
yet this process is time-consuming and lacks efficiency. Blastocyst images
provide a valuable resource for assessing embryo viability. In this study, we
introduce an explainable artificial intelligence (XAI) framework for
classifying embryos, employing a fusion of convolutional neural network (CNN)
and long short-term memory (LSTM) architecture, referred to as CNN-LSTM.
Utilizing deep learning, our model achieves high accuracy in embryo
classification while maintaining interpretability through XAI.

</details>


### [554] [Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations](https://arxiv.org/abs/2506.06780)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: 论文提出了一种基于神经控制微分方程和Savitzky-Golay路径的连续时间旋转物体动力学建模方法，解决了SO(3)外推中的噪声、稀疏性和复杂动态问题。


<details>
  <summary>Details</summary>
Motivation: 旋转物体的跟踪和预测在计算机视觉和机器人学中至关重要，但SO(3)外推面临噪声、稀疏观测、复杂动态和长期预测需求等挑战。

Method: 使用神经控制微分方程（Neural Controlled Differential Equations）结合Savitzky-Golay路径，建模连续时间旋转物体动力学，保留旋转的几何结构。

Result: 实验结果表明，该方法在真实数据上具有优于现有方法的预测能力。

Conclusion: 该方法通过几何结构保留和复杂动态学习，显著提升了旋转物体预测的准确性。

Abstract: Tracking and forecasting the rotation of objects is fundamental in computer
vision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor
observations can be noisy and sparse, (2) motion patterns can be governed by
complex dynamics, and (3) application settings can demand long-term
forecasting. This work proposes modeling continuous-time rotational object
dynamics on $SO(3)$ using Neural Controlled Differential Equations guided by
Savitzky-Golay paths. Unlike existing methods that rely on simplified motion
assumptions, our method learns a general latent dynamical system of the
underlying object trajectory while respecting the geometric structure of
rotations. Experimental results on real-world data demonstrate compelling
forecasting capabilities compared to existing approaches.

</details>


### [555] [Harnessing Vision-Language Models for Time Series Anomaly Detection](https://arxiv.org/abs/2506.06836)
*Zelin He,Sarah Alnegheimish,Matthew Reimherr*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型（VLM）的两阶段时间序列异常检测方法，结合轻量级视觉编码器和VLM推理能力，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏视觉-时间推理能力，无法像人类专家那样识别上下文异常，因此探索基于VLM的解决方案。

Method: 两阶段方法：1) ViT4TS（轻量级视觉编码器）定位候选异常；2) VLM4TS（VLM阶段）结合全局时间上下文和VLM推理能力优化检测。

Result: VLM4TS在未训练时间序列数据的情况下，F1-max分数比最佳基线提升24.6%，且效率更高（令牌使用量平均减少36倍）。

Conclusion: 该方法通过结合视觉和时间推理能力，显著提升了时间序列异常检测的准确性和效率。

Abstract: Time-series anomaly detection (TSAD) has played a vital role in a variety of
fields, including healthcare, finance, and industrial monitoring. Prior
methods, which mainly focus on training domain-specific models on numerical
data, lack the visual-temporal reasoning capacity that human experts have to
identify contextual anomalies. To fill this gap, we explore a solution based on
vision language models (VLMs). Recent studies have shown the ability of VLMs
for visual reasoning tasks, yet their direct application to time series has
fallen short on both accuracy and efficiency. To harness the power of VLMs for
TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening
stage built on a relatively lightweight pretrained vision encoder, which
leverages 2-D time-series representations to accurately localize candidate
anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal
context and VLM reasoning capacity to refine the detection upon the candidates
provided by ViT4TS. We show that without any time-series training, VLM4TS
outperforms time-series pretrained and from-scratch baselines in most cases,
yielding a 24.6 percent improvement in F1-max score over the best baseline.
Moreover, VLM4TS also consistently outperforms existing language-model-based
TSAD methods and is on average 36 times more efficient in token usage.

</details>


### [556] [NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery](https://arxiv.org/abs/2506.06898)
*Reese Kneeland,Paul S. Scotti,Ghislain St-Yves,Jesse Breedlove,Kendrick Kay,Thomas Naselaris*

Main category: cs.CV

TL;DR: NSD-Imagery是一个新基准数据集，用于评估fMRI到图像重建模型在心理图像上的表现，填补了现有NSD数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅在已见图像重建上评估，而心理图像重建对医学和脑机接口应用至关重要。

Method: 提供NSD-Imagery数据集，并评估多种NSD训练的开源视觉解码模型（如MindEye1、Brain Diffuser等）在心理图像上的表现。

Result: 心理图像解码性能与视觉重建性能脱钩，简单线性架构和多模态特征解码模型表现更好。

Conclusion: 心理图像数据集对实际应用至关重要，NSD-Imagery为视觉解码方法的优化提供了资源。

Abstract: We release NSD-Imagery, a benchmark dataset of human fMRI activity paired
with mental images, to complement the existing Natural Scenes Dataset (NSD), a
large-scale dataset of fMRI activity paired with seen images that enabled
unprecedented improvements in fMRI-to-image reconstruction efforts. Recent
models trained on NSD have been evaluated only on seen image reconstruction.
Using NSD-Imagery, it is possible to assess how well these models perform on
mental image reconstruction. This is a challenging generalization requirement
because mental images are encoded in human brain activity with relatively lower
signal-to-noise and spatial resolution; however, generalization from seen to
mental imagery is critical for real-world applications in medical domains and
brain-computer interfaces, where the desired information is always internally
generated. We provide benchmarks for a suite of recent NSD-trained open-source
visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et
al.) on NSD-Imagery, and show that the performance of decoding methods on
mental images is largely decoupled from performance on vision reconstruction.
We further demonstrate that architectural choices significantly impact
cross-decoding performance: models employing simple linear decoding
architectures and multimodal feature decoding generalize better to mental
imagery, while complex architectures tend to overfit visual training data. Our
findings indicate that mental imagery datasets are critical for the development
of practical applications, and establish NSD-Imagery as a useful resource for
better aligning visual decoding methods with this goal.

</details>


### [557] [Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences](https://arxiv.org/abs/2506.06944)
*Mellon M. Zhang,Glen Chou,Saibal Mukhopadhyay*

Main category: cs.CV

TL;DR: PHiM是一种新型状态空间模型架构，专为极坐标流式LiDAR设计，通过局部双向Mamba块和全局前向Mamba块实现高效检测，性能优于现有流式检测器。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要低延迟、高吞吐量的实时感知，传统方法处理全扫描LiDAR数据时延迟高，流式方法虽能缓解但受限于极坐标几何不匹配问题。

Method: PHiM采用局部双向Mamba块进行扇区内空间编码，全局前向Mamba块进行扇区间时序建模，避免卷积和位置编码，实现高效极坐标流式处理。

Result: PHiM在Waymo Open Dataset上性能优于现有流式检测器10%，且吞吐量翻倍，达到全扫描基线水平。

Conclusion: PHiM为极坐标流式LiDAR检测提供了高效解决方案，性能显著提升，适用于自动驾驶实时感知。

Abstract: Accurate and efficient object detection is essential for autonomous vehicles,
where real-time perception requires low latency and high throughput. LiDAR
sensors provide robust depth information, but conventional methods process full
360{\deg} scans in a single pass, introducing significant delay. Streaming
approaches address this by sequentially processing partial scans in the native
polar coordinate system, yet they rely on translation-invariant convolutions
that are misaligned with polar geometry -- resulting in degraded performance or
requiring complex distortion mitigation. Recent Mamba-based state space models
(SSMs) have shown promise for LiDAR perception, but only in the full-scan
setting, relying on geometric serialization and positional embeddings that are
memory-intensive and ill-suited to streaming. We propose Polar Hierarchical
Mamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming
LiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial
encoding and a global forward Mamba for inter-sector temporal modeling,
replacing convolutions and positional encodings with distortion-aware,
dimensionally-decomposed operations. PHiM sets a new state-of-the-art among
streaming detectors on the Waymo Open Dataset, outperforming the previous best
by 10\% and matching full-scan baselines at twice the throughput. Code will be
available at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .

</details>


### [558] [TABLET: Table Structure Recognition using Encoder-only Transformers](https://arxiv.org/abs/2506.07015)
*Qiyu Hou,Jun Wang*

Main category: cs.CV

TL;DR: 提出了一种基于Split-Merge的表格结构识别方法，通过双Transformer编码器优化行和列的分割，并利用额外编码器确保合并准确性，实现了高效且高精度的表格识别。


<details>
  <summary>Details</summary>
Motivation: 解决大型密集表格结构识别的挑战，提供一种稳定且高效的解决方案。

Method: 将行和列分割任务建模为序列标注问题，使用双Transformer编码器；合并任务建模为网格分类问题，使用额外Transformer编码器。

Result: 在FinTabNet和PubTabNet上表现优异，减少了分辨率损失和计算复杂度，保持了高精度和快速处理速度。

Conclusion: 该方法为大规模表格识别提供了鲁棒、可扩展且高效的解决方案，适合工业部署。

Abstract: To address the challenges of table structure recognition, we propose a novel
Split-Merge-based top-down model optimized for large, densely populated tables.
Our approach formulates row and column splitting as sequence labeling tasks,
utilizing dual Transformer encoders to capture feature interactions. The
merging process is framed as a grid cell classification task, leveraging an
additional Transformer encoder to ensure accurate and coherent merging. By
eliminating unstable bounding box predictions, our method reduces resolution
loss and computational complexity, achieving high accuracy while maintaining
fast processing speed. Extensive experiments on FinTabNet and PubTabNet
demonstrate the superiority of our model over existing approaches, particularly
in real-world applications. Our method offers a robust, scalable, and efficient
solution for large-scale table recognition, making it well-suited for
industrial deployment.

</details>


### [559] [D2R: dual regularization loss with collaborative adversarial generation for model robustness](https://arxiv.org/abs/2506.07056)
*Zhenyu Liu,Huizhi Liang,Rajiv Ranjan,Zhanxing Zhu,Vaclav Snasel,Varun Ojha*

Main category: cs.CV

TL;DR: 提出了一种双正则化损失（D2R Loss）和协作对抗生成（CAG）策略，通过优化对抗分布和干净分布，提升目标模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在目标模型损失函数引导不足和非协作对抗生成方面存在局限性，需改进以增强模型对抗攻击的防御能力。

Method: 采用D2R Loss（双优化步骤）和CAG（梯度协作生成对抗样本），结合不同损失函数优化目标模型分布。

Result: 在CIFAR-10、CIFAR-100、Tiny ImageNet等基准数据集上实验表明，D2R Loss与CAG能显著提升模型鲁棒性。

Conclusion: D2R Loss和CAG策略有效解决了现有方法的不足，显著增强了模型对抗攻击的防御能力。

Abstract: The robustness of Deep Neural Network models is crucial for defending models
against adversarial attacks. Recent defense methods have employed collaborative
learning frameworks to enhance model robustness. Two key limitations of
existing methods are (i) insufficient guidance of the target model via loss
functions and (ii) non-collaborative adversarial generation. We, therefore,
propose a dual regularization loss (D2R Loss) method and a collaborative
adversarial generation (CAG) strategy for adversarial training. D2R loss
includes two optimization steps. The adversarial distribution and clean
distribution optimizations enhance the target model's robustness by leveraging
the strengths of different loss functions obtained via a suitable function
space exploration to focus more precisely on the target model's distribution.
CAG generates adversarial samples using a gradient-based collaboration between
guidance and target models. We conducted extensive experiments on three
benchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two
popular target models, WideResNet34-10 and PreActResNet18. Our results show
that D2R loss with CAG produces highly robust models.

</details>


### [560] [Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI](https://arxiv.org/abs/2506.07286)
*Aditya Chakravarty*

Main category: cs.CV

TL;DR: 该论文提出了一种多步优化策略，用于提升扩散模型在逆问题中的恢复质量和鲁棒性，尤其在嵌入式或分布外场景中。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如MPGD）在每一步去噪中仅应用单次梯度更新，限制了恢复的保真度和鲁棒性。

Method: 在每一步去噪时间步中引入多步优化策略，显著提升图像质量、感知准确性和泛化能力。

Result: 实验表明，增加每步梯度更新次数可改善LPIPS和PSNR，且在Jetson Orin Nano上验证了MPGD在自然和航拍场景中的泛化能力。

Conclusion: MPGD具有作为轻量级、即插即用恢复模块的潜力，适用于无人机和移动机器人等实时视觉感知任务。

Abstract: Diffusion models have shown remarkable flexibility for solving inverse
problems without task-specific retraining. However, existing approaches such as
Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update
per denoising step, limiting restoration fidelity and robustness, especially in
embedded or out-of-distribution settings. In this work, we introduce a
multistep optimization strategy within each denoising timestep, significantly
enhancing image quality, perceptual accuracy, and generalization. Our
experiments on super-resolution and Gaussian deblurring demonstrate that
increasing the number of gradient updates per step improves LPIPS and PSNR with
minimal latency overhead. Notably, we validate this approach on a Jetson Orin
Nano using degraded ImageNet and a UAV dataset, showing that MPGD, originally
trained on face datasets, generalizes effectively to natural and aerial scenes.
Our findings highlight MPGD's potential as a lightweight, plug-and-play
restoration module for real-time visual perception in embodied AI agents such
as drones and mobile robots.

</details>


### [561] ["CASE: Contrastive Activation for Saliency Estimation](https://arxiv.org/abs/2506.07327)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CV

TL;DR: 论文提出了一种诊断测试（class sensitivity）来评估显著性方法的可靠性，发现许多方法对类别标签不敏感，并提出了新的对比解释方法CASE，其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 显著性方法在可视化模型预测时被广泛使用，但其视觉合理性可能掩盖了关键局限性。本文旨在揭示这些方法的可靠性问题。

Method: 提出了一种诊断测试（class sensitivity），并通过实验验证了许多显著性方法对类别标签不敏感。随后提出了对比解释方法CASE。

Result: 实验表明，许多显著性方法对类别标签不敏感，而CASE方法能生成更忠实且类别特定的解释。

Conclusion: CASE方法在诊断测试和保真度测试中表现优于现有方法，解决了显著性方法的类别不敏感问题。

Abstract: Saliency methods are widely used to visualize which input features are deemed
relevant to a model's prediction. However, their visual plausibility can
obscure critical limitations. In this work, we propose a diagnostic test for
class sensitivity: a method's ability to distinguish between competing class
labels on the same input. Through extensive experiments, we show that many
widely used saliency methods produce nearly identical explanations regardless
of the class label, calling into question their reliability. We find that
class-insensitive behavior persists across architectures and datasets,
suggesting the failure mode is structural rather than model-specific. Motivated
by these findings, we introduce CASE, a contrastive explanation method that
isolates features uniquely discriminative for the predicted class. We evaluate
CASE using the proposed diagnostic and a perturbation-based fidelity test, and
show that it produces faithful and more class-specific explanations than
existing methods.

</details>


### [562] [CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms](https://arxiv.org/abs/2506.07357)
*Satvik Praveen,Yoonsung Jung*

Main category: cs.CV

TL;DR: 提出了一种结合TPS的STN-YOLO模型（CBAM-STN-TPS-YOLO），用于解决农业目标检测中的遮挡和非刚性变形问题，性能优于STN-YOLO。


<details>
  <summary>Details</summary>
Motivation: 农业目标检测中，现有模型（如YOLO）对遮挡、不规则结构和背景噪声处理不佳，STN的仿射变换无法满足非刚性变形需求。

Method: 集成TPS到STN中实现非刚性空间变换，结合CBAM模块抑制背景噪声并增强关键特征。

Result: 在PGP数据集上，模型在精度、召回率和mAP上优于STN-YOLO，假阳性减少12%。

Conclusion: 该轻量级模型提升了空间感知能力，适合实时边缘部署，适用于精准农业监测。

Abstract: Object detection is vital in precision agriculture for plant monitoring,
disease detection, and yield estimation. However, models like YOLO struggle
with occlusions, irregular structures, and background noise, reducing detection
accuracy. While Spatial Transformer Networks (STNs) improve spatial invariance
through learned transformations, affine mappings are insufficient for non-rigid
deformations such as bent leaves and overlaps.
  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)
into STNs for flexible, non-rigid spatial transformations that better align
features. Performance is further enhanced by the Convolutional Block Attention
Module (CBAM), which suppresses background noise and emphasizes relevant
spatial and channel-wise features.
  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model
outperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction
in false positives, highlighting the benefits of improved spatial flexibility
and attention-guided refinement. We also examine the impact of the TPS
regularization parameter in balancing transformation smoothness and detection
performance.
  This lightweight model improves spatial awareness and supports real-time edge
deployment, making it ideal for smart farming applications requiring accurate
and efficient monitoring.

</details>


### [563] [Multiple Object Stitching for Unsupervised Representation Learning](https://arxiv.org/abs/2506.07364)
*Chengchao Shen,Dawei Liu,Jianxin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MOS的方法，通过拼接单目标图像生成多目标图像，以改进无监督表示学习，在多目标图像上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有对比学习方法在多目标图像上表现不佳的问题。

Method: 通过拼接单目标图像生成多目标图像，利用预定的对象对应关系改进无监督表示。

Result: 在ImageNet、CIFAR和COCO数据集上取得了领先的无监督表示性能。

Conclusion: MOS方法在多目标图像上提供了更详细的表示，适用于复杂下游任务。

Abstract: Contrastive learning for single object centric images has achieved remarkable
progress on unsupervised representation, but suffering inferior performance on
the widespread images with multiple objects. In this paper, we propose a simple
but effective method, Multiple Object Stitching (MOS), to refine the
unsupervised representation for multi-object images. Specifically, we construct
the multi-object images by stitching the single object centric ones, where the
objects in the synthesized multi-object images are predetermined. Hence,
compared to the existing contrastive methods, our method provides additional
object correspondences between multi-object images without human annotations.
In this manner, our method pays more attention to the representations of each
object in multi-object image, thus providing more detailed representations for
complicated downstream tasks, such as object detection and semantic
segmentation. Experimental results on ImageNet, CIFAR and COCO datasets
demonstrate that our proposed method achieves the leading unsupervised
representation performance on both single object centric images and
multi-object ones. The source code is available at
https://github.com/visresearch/MultipleObjectStitching.

</details>


### [564] [Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation](https://arxiv.org/abs/2506.07376)
*Jintao Tong,Ran Ma,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 论文提出了一种跨域少样本分割方法（CD-FSS），通过适配器自然解耦域信息，并设计了域特征导航器（DFN）和SAM-SVN方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨域少样本分割中的域差距和少样本微调问题。

Method: 提出域特征导航器（DFN）解耦域信息，结合SAM-SVN防止过拟合，冻结模型并微调DFN。

Result: 在1-shot和5-shot场景下分别超越现有方法2.69%和4.68% MIoU。

Conclusion: DFN和SAM-SVN方法有效解决了跨域少样本分割的挑战，性能显著提升。

Abstract: Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the
model on a source-domain dataset with sufficient samples, and then transfer the
model to target-domain datasets where only a few samples are available for
efficient fine-tuning. There are majorly two challenges in this task: (1) the
domain gap and (2) fine-tuning with scarce data. To solve these challenges, we
revisit the adapter-based methods, and discover an intriguing insight not
explored in previous works: the adapter not only helps the fine-tuning of
downstream tasks but also naturally serves as a domain information decoupler.
Then, we delve into this finding for an interpretation, and find the model's
inherent structure could lead to a natural decoupling of domain information.
Building upon this insight, we propose the Domain Feature Navigator (DFN),
which is a structure-based decoupler instead of loss-based ones like current
works, to capture domain-specific information, thereby directing the model's
attention towards domain-agnostic knowledge. Moreover, to prevent the potential
excessive overfitting of DFN during the source-domain training, we further
design the SAM-SVN method to constrain DFN from learning sample-specific
knowledge. On target domains, we freeze the model and fine-tune the DFN to
learn target-specific knowledge specific. Extensive experiments demonstrate
that our method surpasses the state-of-the-art method in CD-FSS significantly
by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.

</details>


### [565] [CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization](https://arxiv.org/abs/2506.07484)
*Dasol Hong,Wooju Lee,Hyun Myung*

Main category: cs.CV

TL;DR: 论文提出了一种结合混淆感知损失（CoA-loss）和置信感知权重（CoA-weights）的混合模型（CoCoA-Mix），以提升视觉语言模型在任务特定适应中的专业化和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 冻结编码器常导致特征不对齐，引发类别混淆，限制了模型的专业化能力。

Method: 提出CoA-loss优化决策边界以减少混淆，并利用CoA-weights在混合模型中调整预测权重以增强泛化。

Result: CoCoA-Mix在专业化和泛化方面优于现有方法。

Conclusion: CoCoA-Mix通过改进决策边界和预测权重，显著提升了模型性能。

Abstract: Prompt tuning, which adapts vision-language models by freezing model
parameters and optimizing only the prompt, has proven effective for
task-specific adaptations. The core challenge in prompt tuning is improving
specialization for a specific task and generalization for unseen domains.
However, frozen encoders often produce misaligned features, leading to
confusion between classes and limiting specialization. To overcome this issue,
we propose a confusion-aware loss (CoA-loss) that improves specialization by
refining the decision boundaries between confusing classes. Additionally, we
mathematically demonstrate that a mixture model can enhance generalization
without compromising specialization. This is achieved using confidence-aware
weights (CoA-weights), which adjust the weights of each prediction in the
mixture model based on its confidence within the class domains. Extensive
experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,
outperforms state-of-the-art methods by enhancing specialization and
generalization. Our code is publicly available at
https://github.com/url-kaist/CoCoA-Mix.

</details>


### [566] [Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models](https://arxiv.org/abs/2506.07575)
*Ruiyang Zhang,Hu Zhang,Hao Fei,Zhedong Zheng*

Main category: cs.CV

TL;DR: Uncertainty-o是一个模型无关的框架，用于评估和揭示多模态模型（LMMs）的不确定性，并通过实验验证其在多种任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型（LMMs）被认为比纯语言模型（LLMs）更鲁棒，但其不确定性评估仍存在三个关键问题：统一评估方法、如何提示LMMs展示不确定性，以及如何量化不确定性以支持下游任务。

Method: 提出了Uncertainty-o框架，包括模型无关的不确定性揭示方法、多模态提示扰动的实证研究，以及多模态语义不确定性的量化公式。

Result: 在18个基准测试和10种LMMs（开源和闭源）上的实验表明，Uncertainty-o能可靠地估计LMMs的不确定性，并提升下游任务（如幻觉检测和缓解）的性能。

Conclusion: Uncertainty-o为多模态模型的不确定性评估提供了统一且有效的解决方案，支持其在复杂任务中的应用。

Abstract: Large Multimodal Models (LMMs), harnessing the complementarity among diverse
modalities, are often considered more robust than pure Language Large Models
(LLMs); yet do LMMs know what they do not know? There are three key open
questions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a
unified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to
quantify uncertainty for downstream tasks. In an attempt to address these
challenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed
to reveal uncertainty in LMMs regardless of their modalities, architectures, or
capabilities, (2) an empirical exploration of multimodal prompt perturbations
to uncover LMM uncertainty, offering insights and findings, and (3) derive the
formulation of multimodal semantic uncertainty, which enables quantifying
uncertainty from multimodal responses. Experiments across 18 benchmarks
spanning various modalities and 10 LMMs (both open- and closed-source)
demonstrate the effectiveness of Uncertainty-o in reliably estimating LMM
uncertainty, thereby enhancing downstream tasks such as hallucination
detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought
reasoning.

</details>


### [567] [Explore the vulnerability of black-box models via diffusion models](https://arxiv.org/abs/2506.07590)
*Jiacheng Shi,Yanfu Zhang,Huajie Shao,Ashley Gao*

Main category: cs.CV

TL;DR: 研究发现扩散模型API可能被用于生成合成图像以训练替代模型，从而对黑盒分类模型发起高效攻击，成功率达98.68%。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的高保真图像生成能力可能被恶意利用，带来安全和隐私风险，如版权侵犯和敏感信息泄露。

Method: 攻击者利用扩散模型API生成合成图像，训练替代模型，以最小查询量实现对黑盒模型的提取和对抗攻击。

Result: 在7个基准测试中，方法比现有技术平均提升27.37%，仅用0.01倍查询量，对抗攻击成功率达98.68%。

Conclusion: 扩散模型API的安全风险需引起重视，未来需开发防御机制以应对此类攻击。

Abstract: Recent advancements in diffusion models have enabled high-fidelity and
photorealistic image generation across diverse applications. However, these
models also present security and privacy risks, including copyright violations,
sensitive information leakage, and the creation of harmful or offensive content
that could be exploited maliciously. In this study, we uncover a novel security
threat where an attacker leverages diffusion model APIs to generate synthetic
images, which are then used to train a high-performing substitute model. This
enables the attacker to execute model extraction and transfer-based adversarial
attacks on black-box classification models with minimal queries, without
needing access to the original training data. The generated images are
sufficiently high-resolution and diverse to train a substitute model whose
outputs closely match those of the target model. Across the seven benchmarks,
including CIFAR and ImageNet subsets, our method shows an average improvement
of 27.37% over state-of-the-art methods while using just 0.01 times of the
query budget, achieving a 98.68% success rate in adversarial attacks on the
target model.

</details>


### [568] [HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition](https://arxiv.org/abs/2506.07637)
*Yuchong Long,Wen Sun,Ningxiao Sun,Wenxiao Wang,Chao Li,Shan Yin*

Main category: cs.CV

TL;DR: HieraEdgeNet是一种多尺度边缘增强框架，用于高效、高精度的自动化花粉识别，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统花粉识别方法效率低且主观性强，现有深度学习模型在微观目标（如花粉）的定位精度上表现不足。

Method: 提出HieraEdgeNet框架，包含三个模块：HEM（多尺度边缘特征提取）、SEF（边缘与语义信息融合）和CSPOKM（细节特征优化）。

Result: 在120类花粉数据集上，mAP@.5达到0.9501，优于YOLOv12n和RT-DETR等基线模型。

Conclusion: HieraEdgeNet通过系统整合边缘信息，为微观物体检测提供了高效、高精度的解决方案。

Abstract: Automated pollen recognition is vital to paleoclimatology, biodiversity
monitoring, and public health, yet conventional methods are hampered by
inefficiency and subjectivity. Existing deep learning models often struggle to
achieve the requisite localization accuracy for microscopic targets like
pollen, which are characterized by their minute size, indistinct edges, and
complex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a
multi-scale edge-enhancement framework. The framework's core innovation is the
introduction of three synergistic modules: the Hierarchical Edge Module (HEM),
which explicitly extracts a multi-scale pyramid of edge features that
corresponds to the semantic hierarchy at early network stages; the Synergistic
Edge Fusion (SEF) module, for deeply fusing these edge priors with semantic
information at each respective scale; and the Cross Stage Partial Omni-Kernel
Module (CSPOKM), which maximally refines the most detail-rich feature layers
using an Omni-Kernel operator - comprising anisotropic large-kernel
convolutions and mixed-domain attention - all within a computationally
efficient Cross-Stage Partial (CSP) framework. On a large-scale dataset
comprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision
(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline
models such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms
that our approach generates feature representations that are more precisely
focused on object boundaries. By systematically integrating edge information,
HieraEdgeNet provides a robust and powerful solution for high-precision,
high-efficiency automated detection of microscopic objects.

</details>


### [569] [Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity](https://arxiv.org/abs/2506.07773)
*Mohamed Djilani,Nassim Ali Ousalah,Nidhal Eddine Chenni*

Main category: cs.CV

TL;DR: 提出了一种结合视觉特征、语义分割和用户行为模拟的时尚推荐系统，通过加权评分函数实现个性化推荐，实验表明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决时尚推荐中如何平衡个人风格与流行趋势的问题，同时提升推荐的视觉和语义相关性。

Method: 使用语义分割提取服装区域特征，结合预训练CNN模型生成视觉嵌入，模拟用户购物行为，并通过加权评分函数融合视觉相似性、语义一致性和流行度。

Result: 在DeepFashion数据集上，ResNet-50表现最佳，类别相似性达64.95%，流行度MAE最低。消融实验验证了视觉和流行度线索的互补作用。

Conclusion: 该方法提供了一个可扩展的框架，能够平衡个人风格与流行趋势，实现个性化时尚推荐。

Abstract: We introduce a trend-aware and visually-grounded fashion recommendation
system that integrates deep visual representations, garment-aware segmentation,
semantic category similarity and user behavior simulation. Our pipeline
extracts focused visual embeddings by masking non-garment regions via semantic
segmentation followed by feature extraction using pretrained CNN backbones
(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we
generate synthetic purchase histories influenced by user-specific trendiness
and item popularity. Recommendations are computed using a weighted scoring
function that fuses visual similarity, semantic coherence and popularity
alignment. Experiments on the DeepFashion dataset demonstrate consistent gender
alignment and improved category relevance, with ResNet-50 achieving 64.95%
category similarity and lowest popularity MAE. An ablation study confirms the
complementary roles of visual and popularity cues. Our method provides a
scalable framework for personalized fashion recommendations that balances
individual style with emerging trends. Our implementation is available at
https://github.com/meddjilani/FashionRecommender

</details>


### [570] [Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger](https://arxiv.org/abs/2506.07785)
*Qi Yang,Chenghao Zhang,Lubin Fan,Kun Ding,Jieping Ye,Shiming Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种名为RCTS的多模态RAG框架，通过构建推理上下文丰富的知识库和树搜索重排序方法，解决了现有LVLMs在VQA任务中知识稀缺和响应不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型（LVLMs）在视觉问答（VQA）任务中表现优异，但仍面临知识库中推理示例稀缺和检索知识响应不稳定的挑战。

Method: 提出RCTS框架，包括构建推理上下文丰富的知识库和基于蒙特卡洛树搜索与启发式奖励（MCTS-HR）的重排序方法。

Result: 实验表明，RCTS在多个VQA数据集上达到最先进性能，显著优于上下文学习（ICL）和传统RAG方法。

Conclusion: RCTS框架通过高质量的知识库和重排序方法，有效提升了LVLMs的性能和响应一致性。

Abstract: Recent advancements in Large Vision Language Models (LVLMs) have
significantly improved performance in Visual Question Answering (VQA) tasks
through multimodal Retrieval-Augmented Generation (RAG). However, existing
methods still face challenges, such as the scarcity of knowledge with reasoning
examples and erratic responses from retrieved knowledge. To address these
issues, in this study, we propose a multimodal RAG framework, termed RCTS,
which enhances LVLMs by constructing a Reasoning Context-enriched knowledge
base and a Tree Search re-ranking method. Specifically, we introduce a
self-consistent evaluation mechanism to enrich the knowledge base with
intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with
Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This
ensures that LVLMs can leverage high-quality contextual reasoning for better
and more consistent responses. Extensive experiments demonstrate that our
framework achieves state-of-the-art performance on multiple VQA datasets,
significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.
It highlights the effectiveness of our knowledge base and re-ranking method in
improving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.

</details>


### [571] [R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation](https://arxiv.org/abs/2506.07826)
*William Ljungbergh,Bernardo Taveira,Wenzhao Zheng,Adam Tonderski,Chensheng Peng,Fredrik Kahl,Christoffer Petersson,Michael Felsberg,Kurt Keutzer,Masayoshi Tomizuka,Wei Zhan*

Main category: cs.CV

TL;DR: R3D2是一种轻量级扩散模型，用于在自动驾驶验证中实现真实3D资产插入，解决了传统方法的动态对象操作和可重用性问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统验证需要多样化和安全关键的测试，传统仿真平台资源密集且存在领域差距，而神经重建方法如3DGS在动态对象操作和可重用性上表现不佳。

Method: R3D2通过一步扩散模型生成逼真的渲染效果（如阴影和一致光照），训练数据来自3DGS对象资产，这些资产由图像条件3D生成模型从真实驾驶数据中生成。

Result: R3D2显著提升了插入资产的真实感，支持文本到3D资产插入和跨场景/数据集对象转移，实现了自动驾驶验证的真正可扩展性。

Conclusion: R3D2为自动驾驶验证提供了可扩展且逼真的仿真解决方案，未来将公开数据集和代码以促进研究。

Abstract: Validating autonomous driving (AD) systems requires diverse and
safety-critical testing, making photorealistic virtual environments essential.
Traditional simulation platforms, while controllable, are resource-intensive to
scale and often suffer from a domain gap with real-world data. In contrast,
neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a
scalable solution for creating photorealistic digital twins of real-world
driving scenes. However, they struggle with dynamic object manipulation and
reusability as their per-scene optimization-based methodology tends to result
in incomplete object models with integrated illumination effects. This paper
introduces R3D2, a lightweight, one-step diffusion model designed to overcome
these limitations and enable realistic insertion of complete 3D assets into
existing scenes by generating plausible rendering effects-such as shadows and
consistent lighting-in real time. This is achieved by training R3D2 on a novel
dataset: 3DGS object assets are generated from in-the-wild AD data using an
image-conditioned 3D generative model, and then synthetically placed into
neural rendering-based virtual environments, allowing R3D2 to learn realistic
integration. Quantitative and qualitative evaluations demonstrate that R3D2
significantly enhances the realism of inserted assets, enabling use-cases like
text-to-3D asset insertion and cross-scene/dataset object transfer, allowing
for true scalability in AD validation. To promote further research in scalable
and realistic AD simulation, we will release our dataset and code, see
https://research.zenseact.com/publications/R3D2/.

</details>


### [572] [LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds](https://arxiv.org/abs/2506.07857)
*Zihui Zhang,Weisheng Dai,Hongtao Wen,Bo Yang*

Main category: cs.CV

TL;DR: LogoSP是一种无监督3D语义分割方法，通过结合局部和全局点特征学习3D语义，利用频域中的全局模式生成高精度伪标签，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖局部特征，缺乏发现更丰富语义先验的能力，因此需要一种结合全局特征的方法。

Method: LogoSP通过频域中的全局模式对超点进行分组，生成语义伪标签，用于训练分割网络。

Result: 在两个室内和一个室外数据集上，LogoSP显著优于现有无监督方法，达到最先进性能。

Conclusion: LogoSP证明了在无人类标签的情况下，全局模式能够有效表示有意义的3D语义。

Abstract: We study the problem of unsupervised 3D semantic segmentation on raw point
clouds without needing human labels in training. Existing methods usually
formulate this problem into learning per-point local features followed by a
simple grouping strategy, lacking the ability to discover additional and
possibly richer semantic priors beyond local features. In this paper, we
introduce LogoSP to learn 3D semantics from both local and global point
features. The key to our approach is to discover 3D semantic information by
grouping superpoints according to their global patterns in the frequency
domain, thus generating highly accurate semantic pseudo-labels for training a
segmentation network. Extensive experiments on two indoor and an outdoor
datasets show that our LogoSP surpasses all existing unsupervised methods by
large margins, achieving the state-of-the-art performance for unsupervised 3D
semantic segmentation. Notably, our investigation into the learned global
patterns reveals that they truly represent meaningful 3D semantics in the
absence of human labels during training.

</details>


### [573] [VIVAT: Virtuous Improving VAE Training through Artifact Mitigation](https://arxiv.org/abs/2506.07863)
*Lev Novitskiy,Viacheslav Vasilev,Maria Kovaleva,Vladimir Arkhipkin,Denis Dimitrov*

Main category: cs.CV

TL;DR: VIVAT通过简单调整损失权重、填充策略和引入空间条件归一化，有效减少KL-VAE训练中的常见伪影，提升重建和生成质量。


<details>
  <summary>Details</summary>
Motivation: KL-VAE训练中常见的伪影（如颜色偏移、网格模式等）影响重建和生成质量，亟需解决。

Method: 提出VIVAT方法，包括损失权重调整、填充策略改进和空间条件归一化，无需大幅改动架构。

Result: 在多个基准测试中取得最佳图像重建指标（PSNR和SSIM），并提升文本到图像生成的CLIP分数。

Conclusion: VIVAT在保持KL-VAE框架简单性的同时，解决了实际训练中的问题，为优化VAE训练提供了实用方案。

Abstract: Variational Autoencoders (VAEs) remain a cornerstone of generative computer
vision, yet their training is often plagued by artifacts that degrade
reconstruction and generation quality. This paper introduces VIVAT, a
systematic approach to mitigating common artifacts in KL-VAE training without
requiring radical architectural changes. We present a detailed taxonomy of five
prevalent artifacts - color shift, grid patterns, blur, corner and droplet
artifacts - and analyze their root causes. Through straightforward
modifications, including adjustments to loss weights, padding strategies, and
the integration of Spatially Conditional Normalization, we demonstrate
significant improvements in VAE performance. Our method achieves
state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across
multiple benchmarks and enhances text-to-image generation quality, as evidenced
by superior CLIP scores. By preserving the simplicity of the KL-VAE framework
while addressing its practical challenges, VIVAT offers actionable insights for
researchers and practitioners aiming to optimize VAE training.

</details>


### [574] [FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity](https://arxiv.org/abs/2506.07865)
*Jinxi Li,Ziyang Song,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 论文提出FreeGave方法，无需物体先验即可学习复杂动态3D场景的物理特性，通过引入物理编码和散度自由模块，显著提升了未来帧预测和运动分割的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂物理运动时依赖物体先验或低效的PINN损失，无法有效学习边界动态。

Method: 提出FreeGave方法，引入物理编码和散度自由模块，估计每高斯速度场，避免使用PINN损失。

Result: 在三个公共数据集和新收集的真实数据集上表现优异，物理编码成功学习无标签的3D物理运动模式。

Conclusion: FreeGave方法无需先验即可学习复杂物理动态，为3D场景建模提供了新思路。

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the
underlying physics purely from multi-view videos. By applying various governing
PDEs as PINN losses or incorporating physics simulation into neural networks,
existing works often fail to learn complex physical motions at boundaries or
require object priors such as masks or types. In this paper, we propose
FreeGave to learn the physics of complex dynamic 3D scenes without needing any
object priors. The key to our approach is to introduce a physics code followed
by a carefully designed divergence-free module for estimating a per-Gaussian
velocity field, without relying on the inefficient PINN losses. Extensive
experiments on three public datasets and a newly collected challenging
real-world dataset demonstrate the superior performance of our method for
future frame extrapolation and motion segmentation. Most notably, our
investigation into the learned physics codes reveals that they truly learn
meaningful 3D physical motion patterns in the absence of any human labels in
training.

</details>


### [575] [A Comparative Study of U-Net Architectures for Change Detection in Satellite Images](https://arxiv.org/abs/2506.07925)
*Yaxita Amin,Naimisha S Trivedi,Rashmi Bhattad*

Main category: cs.CV

TL;DR: 论文分析了34篇文献，比较了18种U-Net变体在遥感变化检测中的应用，重点评估了其优缺点，并推荐了适合的变体。


<details>
  <summary>Details</summary>
Motivation: 填补U-Net在遥感变化检测领域应用的研究空白。

Method: 对34篇文献进行综合分析，比较18种U-Net变体的性能。

Result: 发现Siamese Swin-U-Net等变体在变化检测中表现突出，强调了多时相数据管理和长距离关系捕捉的重要性。

Conclusion: 为研究者和实践者选择U-Net变体提供了有价值的参考。

Abstract: Remote sensing change detection is essential for monitoring the everchanging
landscapes of the Earth. The U-Net architecture has gained popularity for its
capability to capture spatial information and perform pixel-wise
classification. However, their application in the Remote sensing field remains
largely unexplored. Therefore, this paper fill the gap by conducting a
comprehensive analysis of 34 papers. This study conducts a comparison and
analysis of 18 different U-Net variations, assessing their potential for
detecting changes in remote sensing. We evaluate both benefits along with
drawbacks of each variation within the framework of this particular
application. We emphasize variations that are explicitly built for change
detection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.
The analysis highlights the significance of aspects such as managing data from
different time periods and collecting relationships over a long distance to
enhance the precision of change detection. This study provides valuable
insights for researchers and practitioners that choose U-Net versions for
remote sensing change detection tasks.

</details>


### [576] [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
*Chengyue Huang,Yuchen Zhu,Sichen Zhu,Jingyun Xiao,Moises Andrade,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 研究发现，当前视觉语言模型（VLMs）在多模态上下文学习（MM-ICL）中依赖浅层启发式方法，而非真正理解任务。通过分布偏移实验和提出的新方法，发现模型性能并未有效利用演示信息。


<details>
  <summary>Details</summary>
Motivation: 验证视觉语言模型是否真正具备多模态上下文学习能力，而非依赖浅层启发式方法。

Method: 提出MM-ICL with Reasoning管道，为每个演示生成答案和推理过程，并在不同数据集和模型上进行实验。

Result: 实验表明，模型性能对演示数量、检索方法等不敏感，未能有效利用演示信息。

Conclusion: 当前视觉语言模型在多模态上下文学习中表现有限，需进一步改进以提升任务理解能力。

Abstract: Vision-language models (VLMs) are widely assumed to exhibit in-context
learning (ICL), a property similar to that of their language-only counterparts.
While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies
show they often rely on shallow heuristics -- such as copying or majority
voting -- rather than true task understanding. We revisit this assumption by
evaluating VLMs under distribution shifts, where support examples come from a
dataset different from the query. Surprisingly, performance often degrades with
more demonstrations, and models tend to copy answers rather than learn from
them. To investigate further, we propose a new MM-ICL with Reasoning pipeline
that augments each demonstration with a generated rationale alongside the
answer. We conduct extensive and comprehensive experiments on both perception-
and reasoning-required datasets with open-source VLMs ranging from 3B to 72B
and proprietary models such as Gemini 2.0. We conduct controlled studies
varying shot count, retrieval method, rationale quality, and distribution. Our
results show limited performance sensitivity across these factors, suggesting
that current VLMs do not effectively utilize demonstration-level information as
intended in MM-ICL.

</details>


### [577] [Real-time Localization of a Soccer Ball from a Single Camera](https://arxiv.org/abs/2506.07981)
*Dmitrii Vorobev,Artem Prosvetov,Karim Elhadji Daou*

Main category: cs.CV

TL;DR: 提出了一种高效的单摄像头实时3D足球轨迹重建方法，通过多模式状态模型显著提升优化速度，同时保持厘米级精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在遮挡、运动模糊和复杂背景下的性能问题，同时降低对多摄像头和昂贵设备的依赖。

Method: 采用多模式状态模型（$W$离散模式）加速优化，适用于标准CPU，实现低延迟。

Result: 在6K分辨率俄罗斯超级联赛数据集上表现优异，性能接近多摄像头系统。

Conclusion: 提供了一种实用且高精度的3D足球轨迹跟踪方法，适用于专业足球环境。

Abstract: We propose a computationally efficient method for real-time three-dimensional
football trajectory reconstruction from a single broadcast camera. In contrast
to previous work, our approach introduces a multi-mode state model with $W$
discrete modes to significantly accelerate optimization while preserving
centimeter-level accuracy -- even in cases of severe occlusion, motion blur,
and complex backgrounds. The system operates on standard CPUs and achieves low
latency suitable for live broadcast settings. Extensive evaluation on a
proprietary dataset of 6K-resolution Russian Premier League matches
demonstrates performance comparable to multi-camera systems, without the need
for specialized or costly infrastructure. This work provides a practical method
for accessible and accurate 3D ball tracking in professional football
environments.

</details>


### [578] [CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray](https://arxiv.org/abs/2506.07984)
*Mingquan Lin,Gregory Holste,Song Wang,Yiliang Zhou,Yishu Wei,Imon Banerjee,Pengyi Chen,Tianjie Dai,Yuexi Du,Nicha C. Dvornek,Yuyan Ge,Zuowei Guo,Shouhei Hanaoka,Dongkyun Kim,Pablo Messina,Yang Lu,Denis Parra,Donghyun Son,Álvaro Soto,Aisha Urooj,René Vidal,Yosuke Yamagishi,Zefan Yang,Ruichi Zhang,Yang Zhou,Leo Anthony Celi,Ronald M. Summers,Zhiyong Lu,Hao Chen,Adam Flanders,George Shih,Zhangyang Wang,Yifan Peng*

Main category: cs.CV

TL;DR: CXR-LT 2024是一个社区驱动的项目，旨在通过扩展数据集和引入零样本学习来改进肺部疾病分类。


<details>
  <summary>Details</summary>
Motivation: 解决开放长尾肺部疾病分类的挑战，并提升现有技术的可测量性。

Method: 扩展数据集至377,110张胸部X光片和45种疾病标签，引入零样本学习，并设计三项任务：长尾分类、黄金标准子集分类和零样本泛化。

Result: 提供了高质量的数据集和先进解决方案，包括多模态模型、生成方法和零样本学习策略。

Conclusion: CXR-LT 2024为临床现实和泛化诊断模型的开发提供了宝贵资源。

Abstract: The CXR-LT series is a community-driven initiative designed to enhance lung
disease classification using chest X-rays (CXR). It tackles challenges in open
long-tailed lung disease classification and enhances the measurability of
state-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve
these goals by providing high-quality benchmark CXR data for model development
and conducting comprehensive evaluations to identify ongoing issues impacting
lung disease classification performance. Building on the success of CXR-LT
2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45
disease labels, including 19 new rare disease findings. It also introduces a
new focus on zero-shot learning to address limitations identified in the
previous event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed
classification on a large, noisy test set, (ii) long-tailed classification on a
manually annotated "gold standard" subset, and (iii) zero-shot generalization
to five previously unseen disease findings. This paper provides an overview of
CXR-LT 2024, detailing the data curation process and consolidating
state-of-the-art solutions, including the use of multimodal models for rare
disease detection, advanced generative approaches to handle noisy labels, and
zero-shot learning strategies for unseen diseases. Additionally, the expanded
dataset enhances disease coverage to better represent real-world clinical
settings, offering a valuable resource for future research. By synthesizing the
insights and innovations of participating teams, we aim to advance the
development of clinically realistic and generalizable diagnostic models for
chest radiography.

</details>


### [579] [Rethinking Crowd-Sourced Evaluation of Neuron Explanations](https://arxiv.org/abs/2506.07985)
*Tuomas Oikarinen,Ge Yan,Akshay Kulkarni,Tsui-Wei Weng*

Main category: cs.CV

TL;DR: 论文提出了一种高效且准确的人群评估策略，用于评估神经元解释的可靠性，并通过重要性采样和贝叶斯方法显著降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现有神经元解释评估方法存在噪音大、成本高的问题，需要一种更可靠且经济的评估策略。

Method: 引入重要性采样选择最有价值的输入样本，并提出贝叶斯方法聚合多个评分，显著减少所需评分数量。

Result: 实现了约30倍的成本降低和额外5倍的评分效率提升，成功比较了两种视觉模型的神经元解释质量。

Conclusion: 提出的方法为神经元解释的评估提供了一种高效且准确的解决方案。

Abstract: Interpreting individual neurons or directions in activations space is an
important component of mechanistic interpretability. As such, many algorithms
have been proposed to automatically produce neuron explanations, but it is
often not clear how reliable these explanations are, or which methods produce
the best explanations. This can be measured via crowd-sourced evaluations, but
they can often be noisy and expensive, leading to unreliable results. In this
paper, we carefully analyze the evaluation pipeline and develop a
cost-effective and highly accurate crowdsourced evaluation strategy. In
contrast to previous human studies that only rate whether the explanation
matches the most highly activating inputs, we estimate whether the explanation
describes neuron activations across all inputs. To estimate this effectively,
we introduce a novel application of importance sampling to determine which
inputs are the most valuable to show to raters, leading to around 30x cost
reduction compared to uniform sampling. We also analyze the label noise present
in crowd-sourced evaluations and propose a Bayesian method to aggregate
multiple ratings leading to a further ~5x reduction in number of ratings
required for the same accuracy. Finally, we use these methods to conduct a
large-scale study comparing the quality of neuron explanations produced by the
most popular methods for two different vision models.

</details>


### [580] [MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation](https://arxiv.org/abs/2506.07999)
*Junhao Chen,Yulia Tsvetkov,Xiaochuang Han*

Main category: cs.CV

TL;DR: 论文提出MADFormer，结合自回归（AR）和扩散模型，通过空间分块和混合层设计优化高分辨率图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有混合模型缺乏系统指导，无法合理分配AR和扩散模型的能力。

Method: MADFormer将图像生成分为空间块，AR层用于全局条件，扩散层用于局部细化。

Result: 实验表明分块策略提升高分辨率图像性能，混合层设计在计算受限下FID提升75%。

Conclusion: 研究为未来混合生成模型提供了实用设计原则。

Abstract: Recent progress in multimodal generation has increasingly combined
autoregressive (AR) and diffusion-based approaches, leveraging their
complementary strengths: AR models capture long-range dependencies and produce
fluent, context-aware outputs, while diffusion models operate in continuous
latent spaces to refine high-fidelity visual details. However, existing hybrids
often lack systematic guidance on how and why to allocate model capacity
between these paradigms. In this work, we introduce MADFormer, a Mixed
Autoregressive and Diffusion Transformer that serves as a testbed for analyzing
AR-diffusion trade-offs. MADFormer partitions image generation into spatial
blocks, using AR layers for one-pass global conditioning across blocks and
diffusion layers for iterative local refinement within each block. Through
controlled experiments on FFHQ-1024 and ImageNet, we identify two key insights:
(1) block-wise partitioning significantly improves performance on
high-resolution images, and (2) vertically mixing AR and diffusion layers
yields better quality-efficiency balances--improving FID by up to 75% under
constrained inference compute. Our findings offer practical design principles
for future hybrid generative models.

</details>


### [581] [Hidden in plain sight: VLMs overlook their visual representations](https://arxiv.org/abs/2506.08008)
*Stephanie Fu,Tyler Bonnen,Devin Guillory,Trevor Darrell*

Main category: cs.CV

TL;DR: 论文比较了视觉语言模型（VLMs）与其视觉编码器的直接输出，发现VLMs在视觉任务中表现显著较差，接近随机水平。瓶颈在于VLMs未能有效利用视觉信息，且继承了语言模型的先验。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs如何整合视觉与语言信息，以理解其在视觉任务中的表现。

Method: 通过一系列视觉中心基准测试（如深度估计、对应关系）比较VLMs与视觉编码器的表现，并分析VLMs的失败模式。

Result: VLMs表现显著低于视觉编码器，主要瓶颈在于未能有效利用视觉信息，且受语言模型先验影响。

Conclusion: 研究揭示了开源VLMs的失败模式，为未来VLMs视觉理解研究提供了评估方法。

Abstract: Language provides a natural interface to specify and evaluate performance on
visual tasks. To realize this possibility, vision language models (VLMs) must
successfully integrate visual and linguistic information. Our work compares
VLMs to a direct readout of their visual encoders to understand their ability
to integrate across these modalities. Across a series of vision-centric
benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform
substantially worse than their visual encoders, dropping to near-chance
performance. We investigate these results through a series of analyses across
the entire VLM: namely 1) the degradation of vision representations, 2)
brittleness to task prompt, and 3) the language model's role in solving the
task. We find that the bottleneck in performing these vision-centric tasks lies
in this third category; VLMs are not effectively using visual information
easily accessible throughout the entire model, and they inherit the language
priors present in the LLM. Our work helps diagnose the failure modes of
open-source VLMs, and presents a series of evaluations useful for future
investigations into visual understanding within VLMs.

</details>


### [582] [Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion](https://arxiv.org/abs/2506.08009)
*Xun Huang,Zhengqi Li,Guande He,Mingyuan Zhou,Eli Shechtman*

Main category: cs.CV

TL;DR: Self Forcing是一种新的自回归视频扩散模型训练方法，解决了曝光偏差问题，通过自生成输出进行训练，实现了高效且高质量的视频生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统自回归视频扩散模型在推理时因依赖自身不完美输出而产生的曝光偏差问题。

Method: 采用自回归展开和KV缓存策略，结合多步扩散模型和随机梯度截断，实现高效训练。

Result: 在单GPU上实现了亚秒级延迟的实时视频生成，且生成质量优于或匹配非因果扩散模型。

Conclusion: Self Forcing通过自生成输出和高效训练策略，显著提升了视频生成的实时性和质量。

Abstract: We introduce Self Forcing, a novel training paradigm for autoregressive video
diffusion models. It addresses the longstanding issue of exposure bias, where
models trained on ground-truth context must generate sequences conditioned on
their own imperfect outputs during inference. Unlike prior methods that denoise
future frames based on ground-truth context frames, Self Forcing conditions
each frame's generation on previously self-generated outputs by performing
autoregressive rollout with key-value (KV) caching during training. This
strategy enables supervision through a holistic loss at the video level that
directly evaluates the quality of the entire generated sequence, rather than
relying solely on traditional frame-wise objectives. To ensure training
efficiency, we employ a few-step diffusion model along with a stochastic
gradient truncation strategy, effectively balancing computational cost and
performance. We further introduce a rolling KV cache mechanism that enables
efficient autoregressive video extrapolation. Extensive experiments demonstrate
that our approach achieves real-time streaming video generation with sub-second
latency on a single GPU, while matching or even surpassing the generation
quality of significantly slower and non-causal diffusion models. Project
website: http://self-forcing.github.io/

</details>


### [583] [StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets](https://arxiv.org/abs/2506.08013)
*Anh-Quan Cao,Ivan Lopes,Raoul de Charette*

Main category: cs.CV

TL;DR: StableMTL利用扩散模型在零样本设置下进行多任务学习，通过合成数据和任务注意力机制实现高效跨任务共享。


<details>
  <summary>Details</summary>
Motivation: 多任务密集预测需要大量标注，而部分标注学习限制了性能。本文旨在通过扩散模型和合成数据扩展至零样本学习。

Method: 采用扩散模型进行潜在回归，结合任务编码、任务条件化和统一潜在损失，设计多流模型和任务注意力机制。

Result: 在8个基准测试的7个任务上优于基线方法。

Conclusion: StableMTL通过统一损失和任务注意力机制，实现了高效的多任务学习，无需大量标注。

Abstract: Multi-task learning for dense prediction is limited by the need for extensive
annotation for every task, though recent works have explored training with
partial task labels. Leveraging the generalization power of diffusion models,
we extend the partial learning setup to a zero-shot setting, training a
multi-task model on multiple synthetic datasets, each labeled for only a subset
of tasks. Our method, StableMTL, repurposes image generators for latent
regression. Adapting a denoising framework with task encoding, per-task
conditioning and a tailored training scheme. Instead of per-task losses
requiring careful balancing, a unified latent loss is adopted, enabling
seamless scaling to more tasks. To encourage inter-task synergy, we introduce a
multi-stream model with a task-attention mechanism that converts N-to-N task
interactions into efficient 1-to-N attention, promoting effective cross-task
sharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [584] [Explaining Risks: Axiomatic Risk Attributions for Financial Models](https://arxiv.org/abs/2506.06653)
*Dangxing Chen*

Main category: q-fin.CP

TL;DR: 论文提出了一种基于Shapley值框架的风险分配方法，用于解释机器学习模型在高风险领域（如金融）中的预测风险。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型虽然成功，但其黑箱结构难以解释，尤其是在高风险领域，风险分配与预测均值同样重要。

Method: 通过扩展Shapley值框架，公平分配模型预测中的风险。

Result: 分析与实证表明，该方法能有效分配风险。

Conclusion: 扩展的Shapley值框架为高风险领域提供了可解释的风险分配方案。

Abstract: In recent years, machine learning models have achieved great success at the
expense of highly complex black-box structures. By using axiomatic attribution
methods, we can fairly allocate the contributions of each feature, thus
allowing us to interpret the model predictions. In high-risk sectors such as
finance, risk is just as important as mean predictions. Throughout this work,
we address the following risk attribution problem: how to fairly allocate the
risk given a model with data? We demonstrate with analysis and empirical
examples that risk can be well allocated by extending the Shapley value
framework.

</details>


### [585] [Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling](https://arxiv.org/abs/2506.07299)
*Hans Buehler,Blanka Horvath,Yannick Limmer,Thorsten Schmidt*

Main category: q-fin.CP

TL;DR: 论文提出了一种在定量金融中处理模型不确定性的方法，通过叠加外部不确定性度量增强传统目标，并提出了子采样策略和高效并行化算法。


<details>
  <summary>Details</summary>
Motivation: 解决定量金融中因数据有限导致的模型不确定性对决策质量的影响。

Method: 基于Klibanoff框架，叠加不确定性度量，提出子采样策略和优化算法。

Result: 不确定性度量优于传统混合度量策略，子采样方法在模型风险下表现稳健且性能接近贝叶斯方法。

Conclusion: 该方法在模型不确定性问题中表现出色，兼具高效性和鲁棒性。

Abstract: This paper addresses the challenge of model uncertainty in quantitative
finance, where decisions in portfolio allocation, derivative pricing, and risk
management rely on estimating stochastic models from limited data. In practice,
the unavailability of the true probability measure forces reliance on an
empirical approximation, and even small misestimations can lead to significant
deviations in decision quality. Building on the framework of Klibanoff et al.
(2005), we enhance the conventional objective - whether this is expected
utility in an investing context or a hedging metric - by superimposing an outer
"uncertainty measure", motivated by traditional monetary risk measures, on the
space of models. In scenarios where a natural model distribution is lacking or
Bayesian methods are impractical, we propose an ad hoc subsampling strategy,
analogous to bootstrapping in statistical finance and related to mini-batch
sampling in deep learning, to approximate model uncertainty. To address the
quadratic memory demands of naive implementations, we also present an adapted
stochastic gradient descent algorithm that enables efficient parallelization.
Through analytical, simulated, and empirical studies - including multi-period,
real data and high-dimensional examples - we demonstrate that uncertainty
measures outperform traditional mixture of measures strategies and our
model-agnostic subsampling-based approach not only enhances robustness against
model risk but also achieves performance comparable to more elaborate Bayesian
methods.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [586] [Inverse Design of Metamaterials with Manufacturing-Guiding Spectrum-to-Structure Conditional Diffusion Model](https://arxiv.org/abs/2506.07083)
*Jiawen Li,Jiang Guo,Yuanzhe Li,Zetian Mao,Jiaxing Shen,Tashi Xu,Diptesh Das,Jinming He,Run Hu,Yaerim Lee,Koji Tsuda,Junichiro Shiomi*

Main category: physics.optics

TL;DR: 提出了一种基于条件扩散模型的通用框架，用于解决超材料逆向设计中的一对多问题，具有高光谱预测精度和多样性生成能力。


<details>
  <summary>Details</summary>
Motivation: 超材料的光学行为与结构间存在高度非线性关系，且制造复杂，传统机器学习方法难以应对。

Method: 采用条件扩散模型，定制光谱到形状和尺寸参数的映射，生成多样化设计。

Result: 方法在光谱预测精度和生成多样性上优于其他生成模型，并为制造提供了有价值的先验知识。

Conclusion: 成功设计并制造了一种具有定制选择性发射光谱的自由形式超材料，验证了方法的有效性。

Abstract: Metamaterials are artificially engineered structures that manipulate
electromagnetic waves, having optical properties absent in natural materials.
Recently, machine learning for the inverse design of metamaterials has drawn
attention. However, the highly nonlinear relationship between the metamaterial
structures and optical behaviour, coupled with fabrication difficulties, poses
challenges for using machine learning to design and manufacture complex
metamaterials. Herein, we propose a general framework that implements
customised spectrum-to-shape and size parameters to address one-to-many
metamaterial inverse design problems using conditional diffusion models. Our
method exhibits superior spectral prediction accuracy, generates a diverse
range of patterns compared to other typical generative models, and offers
valuable prior knowledge for manufacturing through the subsequent analysis of
the diverse generated results, thereby facilitating the experimental
fabrication of metamaterial designs. We demonstrate the efficacy of the
proposed method by successfully designing and fabricating a free-form
metamaterial with a tailored selective emission spectrum for thermal camouflage
applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [587] [Constructive Symbolic Reinforcement Learning via Intuitionistic Logic and Goal-Chaining Inference](https://arxiv.org/abs/2506.05422)
*Andrei T. Patrascu*

Main category: cs.AI

TL;DR: 论文提出了一种基于逻辑推理的学习与规划框架，替代传统的基于奖励的优化方法，通过构建直觉逻辑的证明实现决策，确保状态转换和策略的逻辑有效性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖概率试错，可能导致不安全或无效的转换，而本文旨在通过逻辑推理实现安全、可解释且高效的规划。

Method: 将动作、状态转换和目标表示为逻辑命题，利用直觉逻辑构建构造性证明，确保每一步都有可验证的前提条件。

Result: 在结构化网格世界中，该方法实现了零无效动作、完美安全性和高效收敛，优于传统Q学习。

Conclusion: 本文为强化学习提供了基于构造逻辑和证明理论的新方向，适用于安全规划和可信AI。

Abstract: We introduce a novel learning and planning framework that replaces
traditional reward-based optimisation with constructive logical inference. In
our model, actions, transitions, and goals are represented as logical
propositions, and decision-making proceeds by building constructive proofs
under intuitionistic logic. This method ensures that state transitions and
policies are accepted only when supported by verifiable preconditions --
eschewing probabilistic trial-and-error in favour of guaranteed logical
validity. We implement a symbolic agent operating in a structured gridworld,
where reaching a goal requires satisfying a chain of intermediate subgoals
(e.g., collecting keys to open doors), each governed by logical constraints.
Unlike conventional reinforcement learning agents, which require extensive
exploration and suffer from unsafe or invalid transitions, our constructive
agent builds a provably correct plan through goal chaining, condition tracking,
and knowledge accumulation. Empirical comparison with Q-learning demonstrates
that our method achieves perfect safety, interpretable behaviour, and efficient
convergence with no invalid actions, highlighting its potential for safe
planning, symbolic cognition, and trustworthy AI. This work presents a new
direction for reinforcement learning grounded not in numeric optimisation, but
in constructive logic and proof theory.

</details>


### [588] [Avoiding Death through Fear Intrinsic Conditioning](https://arxiv.org/abs/2506.05529)
*Rodney Sanchez,Ferat Sahin,Alexander Ororbia,Jamison Heard*

Main category: cs.AI

TL;DR: 论文提出了一种受早期杏仁核发育启发的内在奖励函数，通过新型记忆增强神经网络架构实现，用于避免终端状态（如死亡）的探索，并在Miniworld Sidewalk环境中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在评估时需要设计外部奖励函数，但某些环境（如包含死亡状态）无法提供直接反馈，限制了算法的适用性。

Method: 提出了一种基于杏仁核发育启发的内在奖励函数，并通过记忆增强神经网络（MANN）架构实现，调整恐惧反应的阈值以模拟广泛性焦虑障碍（GADs）行为。

Result: 在Miniworld Sidewalk环境中，该方法成功实现了类似动物恐惧条件的回避行为，并能解决非描述性终端条件的环境。

Conclusion: 研究提供了一种生物启发的神经网络架构和恐惧条件范式框架，为处理非描述性终端状态问题提供了新思路。

Abstract: Biological and psychological concepts have inspired reinforcement learning
algorithms to create new complex behaviors that expand agents' capacity. These
behaviors can be seen in the rise of techniques like goal decomposition,
curriculum, and intrinsic rewards, which have paved the way for these complex
behaviors. One limitation in evaluating these methods is the requirement for
engineered extrinsic for realistic environments. A central challenge in
engineering the necessary reward function(s) comes from these environments
containing states that carry high negative rewards, but provide no feedback to
the agent. Death is one such stimuli that fails to provide direct feedback to
the agent. In this work, we introduce an intrinsic reward function inspired by
early amygdala development and produce this intrinsic reward through a novel
memory-augmented neural network (MANN) architecture. We show how this intrinsic
motivation serves to deter exploration of terminal states and results in
avoidance behavior similar to fear conditioning observed in animals.
Furthermore, we demonstrate how modifying a threshold where the fear response
is active produces a range of behaviors that are described under the paradigm
of general anxiety disorders (GADs). We demonstrate this behavior in the
Miniworld Sidewalk environment, which provides a partially observable Markov
decision process (POMDP) and a sparse reward with a non-descriptive terminal
condition, i.e., death. In effect, this study results in a
biologically-inspired neural architecture and framework for fear conditioning
paradigms; we empirically demonstrate avoidance behavior in a constructed agent
that is able to solve environments with non-descriptive terminal conditions.

</details>


### [589] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: MMTU是一个大规模基准测试，包含25个现实世界表格任务的30K问题，旨在全面评估模型在专家级别上理解、推理和操作表格的能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要局限于NL-to-SQL和Table-QA等任务，忽略了专业用户面临的更广泛任务，限制了模型在这一领域的发展。

Method: MMTU基于数十年的表格数据研究，设计了25个复杂任务，涵盖表格理解、推理和编码等技能。

Result: 前沿模型（如OpenAI o4-mini和DeepSeek R1）在MMTU上的表现仅为60%，表明仍有改进空间。

Conclusion: MMTU为结构化数据处理和分析的基础模型发展提供了重要基准，推动了该领域的进一步研究。

Abstract: Tables and table-based use cases play a crucial role in many important
real-world applications, such as spreadsheets, databases, and computational
notebooks, which traditionally require expert-level users like data engineers,
data analysts, and database administrators to operate. Although LLMs have shown
remarkable progress in working with tables (e.g., in spreadsheet and database
copilot scenarios), comprehensive benchmarking of such capabilities remains
limited. In contrast to an extensive and growing list of NLP benchmarks,
evaluations of table-related tasks are scarce, and narrowly focus on tasks like
NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks
that professional users face. This gap limits our understanding and model
progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K
questions across 25 real-world table tasks, designed to comprehensively
evaluate models ability to understand, reason, and manipulate real tables at
the expert-level. These tasks are drawn from decades' worth of computer science
research on tabular data, with a focus on complex table tasks faced by
professional users. We show that MMTU require a combination of skills --
including table understanding, reasoning, and coding -- that remain challenging
for today's frontier models, where even frontier reasoning models like OpenAI
o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for
improvement. We highlight key findings in our evaluation using MMTU and hope
that this benchmark drives further advances in understanding and developing
foundation models for structured data processing and analysis. Our code and
data are available at https://github.com/MMTU-Benchmark/MMTU and
https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [590] [Population-Proportional Preference Learning from Human Feedback: An Axiomatic Approach](https://arxiv.org/abs/2506.05619)
*Kihyun Kim,Jiawei Zhang,Asuman Ozdaglar,Pablo A. Parrilo*

Main category: cs.AI

TL;DR: 提出了一种新的偏好学习框架，旨在通过成对比较数据推断评估者偏好分布，生成符合社会选择理论公理的政策，并验证其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统偏好学习方法可能偏向广泛持有的意见，导致政策对某些群体或意见类型不公平。本文旨在开发一种能按真实评估者偏好比例对齐集体意见和政策的新框架。

Method: 通过成对比较数据推断评估者群体分布，构建满足单调性、帕累托效率、人口比例代表性和人口边界鲁棒性公理的政策，并采用软最大松弛方法平衡人口比例代表性和Condorcet胜者的选择。

Result: 实验验证了该方法在表格推荐任务和大规模语言模型对齐中的有效性和可扩展性。

Conclusion: 新框架能够生成更公平且符合评估者真实偏好的政策，同时满足社会选择理论的核心公理。

Abstract: Conventional preference learning methods often prioritize opinions held more
widely when aggregating preferences from multiple evaluators. This may result
in policies that are biased in favor of some types of opinions or groups. The
objective of this paper is to develop a novel preference learning framework
capable of aligning aggregate opinions and policies proportionally with the
true population distribution of evaluator preferences. Our approach infers the
feasible set of evaluator population distributions directly from pairwise
comparison data. Using these estimates, the algorithm constructs a policy that
satisfies foundational axioms from social choice theory, namely monotonicity
and Pareto efficiency, as well as our newly-introduced axioms of
population-proportional representation and population-bounded robustness. We
propose a soft-max relaxation method that smoothly trade-offs
population-proportional representation with the selection of the Condorcet
winner (which beats all other options in pairwise comparisons). Finally, we
validate the effectiveness and scalability of our approach through experiments
on both tabular recommendation tasks and large-scale language model alignment.

</details>


### [591] [SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models](https://arxiv.org/abs/2506.05745)
*Emil Biju,Shayan Talaei,Zhemin Huang,Mohammadreza Pourreza,Azalia Mirhoseini,Amin Saberi*

Main category: cs.AI

TL;DR: SPRINT框架通过并行化推理过程减少大型推理模型（LRMs）的推理时间，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型因生成冗长的链式推理而导致的推理时间过长问题。

Method: 引入SPRINT框架，通过数据重组和微调，使模型能够动态识别并行执行的子任务。

Result: 在数学等领域性能相当，同时减少高达39%的序列标记；在GPQA和Countdown任务中分别减少45%和65%的序列标记。

Conclusion: SPRINT有效减少了推理时间，同时保持了模型的性能。

Abstract: Large reasoning models (LRMs) excel at complex reasoning tasks but typically
generate lengthy sequential chains-of-thought, resulting in long inference
times before arriving at the final answer. To address this challenge, we
introduce SPRINT, a novel post-training and inference-time framework designed
to enable LRMs to dynamically identify and exploit opportunities for
parallelization during their reasoning process. SPRINT incorporates an
innovative data curation pipeline that reorganizes natural language reasoning
trajectories into structured rounds of long-horizon planning and parallel
execution. By fine-tuning LRMs on a small amount of such curated data, the
models learn to dynamically identify independent subtasks within extended
reasoning processes and effectively execute them in parallel. Through extensive
evaluations, we show that the models fine-tuned with the SPRINT framework match
the performance of reasoning models on complex domains such as mathematics
while generating up to ~39% fewer sequential tokens on problems requiring more
than 8000 output tokens. Finally, we observe consistent results transferred to
two out-of-distribution tasks of GPQA and Countdown with up to 45% and 65%
reduction in average sequential tokens for longer reasoning trajectories, while
achieving the performance of the fine-tuned reasoning model.

</details>


### [592] [Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective](https://arxiv.org/abs/2506.05754)
*Emmanuel Anaya Gonzalez,Sairam Vaidya,Kanghee Park,Ruyi Ji,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: 提出了一种基于MCMC的约束采样框架，满足约束条件、单调收敛和高效性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有约束解码方法会扭曲模型分布，影响生成样本的多样性，尤其在程序模糊测试等应用中。

Method: 构建有效输出的提议分布，并基于LM似然应用Metropolis-Hastings接受准则。

Result: 在合成基准和实际程序模糊测试任务中表现优于现有方法。

Conclusion: 新框架在满足约束的同时高效探索约束空间，具有实际应用价值。

Abstract: Constrained decoding enables Language Models (LMs) to produce samples that
provably satisfy hard constraints. However, existing constrained-decoding
approaches often distort the underlying model distribution, a limitation that
is especially problematic in applications like program fuzzing, where one wants
to generate diverse and valid program inputs for testing purposes. We propose a
new constrained sampling framework based on Markov Chain Monte Carlo (MCMC)
that simultaneously satisfies three core desiderata: constraint satisfying
(every sample satisfies the constraint), monotonically converging (the sampling
process converges to the true conditional distribution), and efficient
(high-quality samples emerge in few steps). Our method constructs a proposal
distribution over valid outputs and applies a Metropolis-Hastings acceptance
criterion based on the LM's likelihood, ensuring principled and efficient
exploration of the constrained space. Empirically, our sampler outperforms
existing methods on both synthetic benchmarks and real-world program fuzzing
tasks.

</details>


### [593] [Preference Learning for AI Alignment: a Causal Perspective](https://arxiv.org/abs/2506.05967)
*Katarzyna Kobalczyk,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 该论文提出用因果范式解决奖励建模问题，以应对因果误识别、偏好异质性和用户特定因素等挑战。


<details>
  <summary>Details</summary>
Motivation: 奖励建模是使大型语言模型（LLM）与人类价值观对齐的关键步骤，但现有方法在泛化性上存在不足。

Method: 采用因果推断的工具，识别可靠泛化的关键假设，并与常见数据收集实践对比。

Result: 展示了朴素奖励模型的失败模式，并证明因果启发方法能提升模型鲁棒性。

Conclusion: 提出未来研究的理想目标，建议通过针对性干预解决观测数据的固有局限性。

Abstract: Reward modelling from preference data is a crucial step in aligning large
language models (LLMs) with human values, requiring robust generalisation to
novel prompt-response pairs. In this work, we propose to frame this problem in
a causal paradigm, providing the rich toolbox of causality to identify the
persistent challenges, such as causal misidentification, preference
heterogeneity, and confounding due to user-specific factors. Inheriting from
the literature of causal inference, we identify key assumptions necessary for
reliable generalisation and contrast them with common data collection
practices. We illustrate failure modes of naive reward models and demonstrate
how causally-inspired approaches can improve model robustness. Finally, we
outline desiderata for future research and practices, advocating targeted
interventions to address inherent limitations of observational data.

</details>


### [594] [PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time](https://arxiv.org/abs/2506.06254)
*Weizhi Zhang,Xinyang Zhang,Chenwei Zhang,Liangwei Yang,Jingbo Shang,Zhepei Wei,Henry Peng Zou,Zijie Huang,Zhengyang Wang,Yifan Gao,Xiaoman Pan,Lian Xiong,Jingguo Liu,Philip S. Yu,Xian Li*

Main category: cs.AI

TL;DR: PersonaAgent是一个个性化的LLM代理框架，通过个性化记忆和动作模块，结合用户偏好优化，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理缺乏灵活性，无法满足用户多样化的需求和偏好，因此开发了PersonaAgent。

Method: PersonaAgent包含个性化记忆模块（情景和语义记忆）和动作模块，通过用户偏好对齐策略优化系统提示。

Result: 实验表明，PersonaAgent在个性化动作空间和实时应用中表现优异。

Conclusion: PersonaAgent展示了提供动态个性化用户体验的可行性和潜力。

Abstract: Large Language Model (LLM) empowered agents have recently emerged as advanced
paradigms that exhibit impressive capabilities in a wide range of domains and
tasks. Despite their potential, current LLM agents often adopt a
one-size-fits-all approach, lacking the flexibility to respond to users'
varying needs and preferences. This limitation motivates us to develop
PersonaAgent, the first personalized LLM agent framework designed to address
versatile personalization tasks. Specifically, PersonaAgent integrates two
complementary components - a personalized memory module that includes episodic
and semantic memory mechanisms; a personalized action module that enables the
agent to perform tool actions tailored to the user. At the core, the persona
(defined as unique system prompt for each user) functions as an intermediary:
it leverages insights from personalized memory to control agent actions, while
the outcomes of these actions in turn refine the memory. Based on the
framework, we propose a test-time user-preference alignment strategy that
simulate the latest n interactions to optimize the persona prompt, ensuring
real-time user preference alignment through textual loss feedback between
simulated and ground-truth responses. Experimental evaluations demonstrate that
PersonaAgent significantly outperforms other baseline methods by not only
personalizing the action space effectively but also scaling during test-time
real-world applications. These results underscore the feasibility and potential
of our approach in delivering tailored, dynamic user experiences.

</details>


### [595] [Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens](https://arxiv.org/abs/2506.06261)
*Jihwan Jeong,Xiaoyu Wang,Jingmin Wang,Scott Sanner,Pascal Poupart*

Main category: cs.AI

TL;DR: RefPlan是一种基于双重贝叶斯的离线模型规划方法，通过将规划重新定义为贝叶斯后验估计，统一了不确定性建模和模型规划，显著提升了保守离线强化学习策略的性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在在线探索成本高或不安全时至关重要，但常因数据有限导致高认知不确定性，现有方法依赖固定保守策略，限制了适应性和泛化能力。

Method: 提出Reflect-then-Plan (RefPlan)，通过实时观测更新环境动态的信念，并将不确定性通过边缘化纳入模型规划。

Result: 在标准基准测试中，RefPlan显著提升了保守离线强化学习策略的性能，尤其在认知不确定性高和数据有限的情况下表现稳健。

Conclusion: RefPlan提高了离线学习策略的灵活性、泛化能力和鲁棒性，适应动态变化的环境。

Abstract: Offline reinforcement learning (RL) is crucial when online exploration is
costly or unsafe but often struggles with high epistemic uncertainty due to
limited data. Existing methods rely on fixed conservative policies, restricting
adaptivity and generalization. To address this, we propose Reflect-then-Plan
(RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach.
RefPlan unifies uncertainty modeling and MB planning by recasting planning as
Bayesian posterior estimation. At deployment, it updates a belief over
environment dynamics using real-time observations, incorporating uncertainty
into MB planning via marginalization. Empirical results on standard benchmarks
show that RefPlan significantly improves the performance of conservative
offline RL policies. In particular, RefPlan maintains robust performance under
high epistemic uncertainty and limited data, while demonstrating resilience to
changing environment dynamics, improving the flexibility, generalizability, and
robustness of offline-learned policies.

</details>


### [596] [Constructive Symbolic Reinforcement Learning via Intuitionistic Logic and Goal-Chaining Inference](https://arxiv.org/abs/2506.05422)
*Andrei T. Patrascu*

Main category: cs.AI

TL;DR: 论文提出了一种基于逻辑推理的学习与规划框架，取代传统的基于奖励的优化方法，通过构建直觉逻辑的证明实现决策。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖概率探索，可能导致不安全或无效的过渡，而新方法通过逻辑验证确保决策的正确性和安全性。

Method: 将动作、状态转移和目标表示为逻辑命题，利用直觉逻辑构建构造性证明，确保每一步的合法性。在结构化网格世界中实现符号化代理。

Result: 相比Q学习，新方法实现了完美的安全性、可解释的行为和高效收敛，无无效动作。

Conclusion: 该方法为强化学习提供了基于构造逻辑和证明理论的新方向，适用于安全规划和可信AI。

Abstract: We introduce a novel learning and planning framework that replaces
traditional reward-based optimisation with constructive logical inference. In
our model, actions, transitions, and goals are represented as logical
propositions, and decision-making proceeds by building constructive proofs
under intuitionistic logic. This method ensures that state transitions and
policies are accepted only when supported by verifiable preconditions --
eschewing probabilistic trial-and-error in favour of guaranteed logical
validity. We implement a symbolic agent operating in a structured gridworld,
where reaching a goal requires satisfying a chain of intermediate subgoals
(e.g., collecting keys to open doors), each governed by logical constraints.
Unlike conventional reinforcement learning agents, which require extensive
exploration and suffer from unsafe or invalid transitions, our constructive
agent builds a provably correct plan through goal chaining, condition tracking,
and knowledge accumulation. Empirical comparison with Q-learning demonstrates
that our method achieves perfect safety, interpretable behaviour, and efficient
convergence with no invalid actions, highlighting its potential for safe
planning, symbolic cognition, and trustworthy AI. This work presents a new
direction for reinforcement learning grounded not in numeric optimisation, but
in constructive logic and proof theory.

</details>


### [597] [Avoiding Death through Fear Intrinsic Conditioning](https://arxiv.org/abs/2506.05529)
*Rodney Sanchez,Ferat Sahin,Alexander Ororbia,Jamison Heard*

Main category: cs.AI

TL;DR: 论文提出了一种受早期杏仁核发育启发的内在奖励函数，通过新型记忆增强神经网络（MANN）架构实现，用于避免终端状态探索，模拟动物恐惧条件反射行为。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在评估时需要设计外在奖励函数，但环境中存在高负面奖励状态（如死亡）且无法提供直接反馈，限制了方法的应用。

Method: 引入基于杏仁核发育的内在奖励函数，通过MANN架构实现，调整恐惧响应阈值以模拟广泛性焦虑障碍（GADs）行为。

Result: 在Miniworld Sidewalk环境中验证了该方法，成功实现了对非描述性终端条件（如死亡）的避免行为。

Conclusion: 研究提出了一种生物启发的神经架构和恐惧条件反射框架，为处理非描述性终端条件提供了新思路。

Abstract: Biological and psychological concepts have inspired reinforcement learning
algorithms to create new complex behaviors that expand agents' capacity. These
behaviors can be seen in the rise of techniques like goal decomposition,
curriculum, and intrinsic rewards, which have paved the way for these complex
behaviors. One limitation in evaluating these methods is the requirement for
engineered extrinsic for realistic environments. A central challenge in
engineering the necessary reward function(s) comes from these environments
containing states that carry high negative rewards, but provide no feedback to
the agent. Death is one such stimuli that fails to provide direct feedback to
the agent. In this work, we introduce an intrinsic reward function inspired by
early amygdala development and produce this intrinsic reward through a novel
memory-augmented neural network (MANN) architecture. We show how this intrinsic
motivation serves to deter exploration of terminal states and results in
avoidance behavior similar to fear conditioning observed in animals.
Furthermore, we demonstrate how modifying a threshold where the fear response
is active produces a range of behaviors that are described under the paradigm
of general anxiety disorders (GADs). We demonstrate this behavior in the
Miniworld Sidewalk environment, which provides a partially observable Markov
decision process (POMDP) and a sparse reward with a non-descriptive terminal
condition, i.e., death. In effect, this study results in a
biologically-inspired neural architecture and framework for fear conditioning
paradigms; we empirically demonstrate avoidance behavior in a constructed agent
that is able to solve environments with non-descriptive terminal conditions.

</details>


### [598] [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
*Junjie Xing,Yeye He,Mengyu Zhou,Haoyu Dong,Shi Han,Lingjiao Chen,Dongmei Zhang,Surajit Chaudhuri,H. V. Jagadish*

Main category: cs.AI

TL;DR: MMTU是一个大规模基准测试，包含30K问题和25个真实表格任务，旨在全面评估模型在专家级表格处理中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有对表格相关任务的评估有限，主要集中在NL-to-SQL和Table-QA，忽略了专业用户面临的更广泛任务，限制了模型在这一领域的发展。

Method: MMTU从数十年的计算机科学研究中提取复杂表格任务，设计了一个包含30K问题和25种任务的基准测试。

Result: 前沿模型（如OpenAI o4-mini和DeepSeek R1）在MMTU上的得分仅约60%，表明仍有显著改进空间。

Conclusion: MMTU为结构化数据处理和分析的基础模型发展提供了重要基准，推动了该领域的进一步研究。

Abstract: Tables and table-based use cases play a crucial role in many important
real-world applications, such as spreadsheets, databases, and computational
notebooks, which traditionally require expert-level users like data engineers,
data analysts, and database administrators to operate. Although LLMs have shown
remarkable progress in working with tables (e.g., in spreadsheet and database
copilot scenarios), comprehensive benchmarking of such capabilities remains
limited. In contrast to an extensive and growing list of NLP benchmarks,
evaluations of table-related tasks are scarce, and narrowly focus on tasks like
NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks
that professional users face. This gap limits our understanding and model
progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K
questions across 25 real-world table tasks, designed to comprehensively
evaluate models ability to understand, reason, and manipulate real tables at
the expert-level. These tasks are drawn from decades' worth of computer science
research on tabular data, with a focus on complex table tasks faced by
professional users. We show that MMTU require a combination of skills --
including table understanding, reasoning, and coding -- that remain challenging
for today's frontier models, where even frontier reasoning models like OpenAI
o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for
improvement. We highlight key findings in our evaluation using MMTU and hope
that this benchmark drives further advances in understanding and developing
foundation models for structured data processing and analysis. Our code and
data are available at https://github.com/MMTU-Benchmark/MMTU and
https://huggingface.co/datasets/MMTU-benchmark/MMTU.

</details>


### [599] [Population-Proportional Preference Learning from Human Feedback: An Axiomatic Approach](https://arxiv.org/abs/2506.05619)
*Kihyun Kim,Jiawei Zhang,Asuman Ozdaglar,Pablo A. Parrilo*

Main category: cs.AI

TL;DR: 提出了一种新的偏好学习框架，旨在通过比例对齐群体意见和政策，避免传统方法中的偏见问题。


<details>
  <summary>Details</summary>
Motivation: 传统偏好学习方法可能偏向广泛持有的意见，导致政策对某些群体不公平。本文旨在开发一种能够比例对齐群体偏好和政策的新框架。

Method: 通过成对比较数据推断评估者群体分布，构建满足社会选择理论公理（如单调性和帕累托效率）的政策，并引入新公理（比例代表和群体稳健性）。提出软最大松弛方法，平衡比例代表和Condorcet胜者选择。

Result: 实验验证了方法的有效性和可扩展性，适用于表格推荐任务和大规模语言模型对齐。

Conclusion: 新框架能够更公平地反映群体偏好，同时满足关键公理，适用于多种应用场景。

Abstract: Conventional preference learning methods often prioritize opinions held more
widely when aggregating preferences from multiple evaluators. This may result
in policies that are biased in favor of some types of opinions or groups. The
objective of this paper is to develop a novel preference learning framework
capable of aligning aggregate opinions and policies proportionally with the
true population distribution of evaluator preferences. Our approach infers the
feasible set of evaluator population distributions directly from pairwise
comparison data. Using these estimates, the algorithm constructs a policy that
satisfies foundational axioms from social choice theory, namely monotonicity
and Pareto efficiency, as well as our newly-introduced axioms of
population-proportional representation and population-bounded robustness. We
propose a soft-max relaxation method that smoothly trade-offs
population-proportional representation with the selection of the Condorcet
winner (which beats all other options in pairwise comparisons). Finally, we
validate the effectiveness and scalability of our approach through experiments
on both tabular recommendation tasks and large-scale language model alignment.

</details>


### [600] [SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models](https://arxiv.org/abs/2506.05745)
*Emil Biju,Shayan Talaei,Zhemin Huang,Mohammadreza Pourreza,Azalia Mirhoseini,Amin Saberi*

Main category: cs.AI

TL;DR: SPRINT框架通过动态并行化优化大型推理模型的推理效率，减少序列令牌数量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务中表现优异，但生成冗长的链式推理导致推理时间过长。

Method: SPRINT通过数据重组和微调，使模型能够动态识别并行化机会并执行独立子任务。

Result: 在数学等领域，SPRINT微调的模型性能与原始模型相当，但序列令牌减少39%。

Conclusion: SPRINT显著提升推理效率，适用于长推理轨迹任务，且能泛化到其他领域。

Abstract: Large reasoning models (LRMs) excel at complex reasoning tasks but typically
generate lengthy sequential chains-of-thought, resulting in long inference
times before arriving at the final answer. To address this challenge, we
introduce SPRINT, a novel post-training and inference-time framework designed
to enable LRMs to dynamically identify and exploit opportunities for
parallelization during their reasoning process. SPRINT incorporates an
innovative data curation pipeline that reorganizes natural language reasoning
trajectories into structured rounds of long-horizon planning and parallel
execution. By fine-tuning LRMs on a small amount of such curated data, the
models learn to dynamically identify independent subtasks within extended
reasoning processes and effectively execute them in parallel. Through extensive
evaluations, we show that the models fine-tuned with the SPRINT framework match
the performance of reasoning models on complex domains such as mathematics
while generating up to ~39% fewer sequential tokens on problems requiring more
than 8000 output tokens. Finally, we observe consistent results transferred to
two out-of-distribution tasks of GPQA and Countdown with up to 45% and 65%
reduction in average sequential tokens for longer reasoning trajectories, while
achieving the performance of the fine-tuned reasoning model.

</details>


### [601] [Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective](https://arxiv.org/abs/2506.05754)
*Emmanuel Anaya Gonzalez,Sairam Vaidya,Kanghee Park,Ruyi Ji,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: 提出了一种基于MCMC的约束采样框架，满足约束满足、单调收敛和高效性三个核心需求，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有约束解码方法会扭曲模型分布，影响多样性，尤其在程序模糊测试等应用中。

Method: 构建有效输出的提议分布，并基于LM似然应用Metropolis-Hastings接受准则。

Result: 在合成基准和实际程序模糊测试任务中表现优于现有方法。

Conclusion: 新框架在满足约束的同时高效探索约束空间，提升了样本质量。

Abstract: Constrained decoding enables Language Models (LMs) to produce samples that
provably satisfy hard constraints. However, existing constrained-decoding
approaches often distort the underlying model distribution, a limitation that
is especially problematic in applications like program fuzzing, where one wants
to generate diverse and valid program inputs for testing purposes. We propose a
new constrained sampling framework based on Markov Chain Monte Carlo (MCMC)
that simultaneously satisfies three core desiderata: constraint satisfying
(every sample satisfies the constraint), monotonically converging (the sampling
process converges to the true conditional distribution), and efficient
(high-quality samples emerge in few steps). Our method constructs a proposal
distribution over valid outputs and applies a Metropolis-Hastings acceptance
criterion based on the LM's likelihood, ensuring principled and efficient
exploration of the constrained space. Empirically, our sampler outperforms
existing methods on both synthetic benchmarks and real-world program fuzzing
tasks.

</details>


### [602] [Preference Learning for AI Alignment: a Causal Perspective](https://arxiv.org/abs/2506.05967)
*Katarzyna Kobalczyk,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 论文提出用因果范式解决LLMs与人类价值观对齐中的奖励建模问题，强调因果工具的重要性。


<details>
  <summary>Details</summary>
Motivation: 奖励建模在LLMs与人类价值观对齐中至关重要，但现有方法面临因果误识别、偏好异质性和用户特定因素等挑战。

Method: 采用因果推理框架，识别可靠泛化的关键假设，并与常见数据收集实践对比。

Result: 展示了朴素奖励模型的失败模式，并证明因果方法能提升模型鲁棒性。

Conclusion: 呼吁未来研究关注干预措施，以解决观测数据的固有局限性。

Abstract: Reward modelling from preference data is a crucial step in aligning large
language models (LLMs) with human values, requiring robust generalisation to
novel prompt-response pairs. In this work, we propose to frame this problem in
a causal paradigm, providing the rich toolbox of causality to identify the
persistent challenges, such as causal misidentification, preference
heterogeneity, and confounding due to user-specific factors. Inheriting from
the literature of causal inference, we identify key assumptions necessary for
reliable generalisation and contrast them with common data collection
practices. We illustrate failure modes of naive reward models and demonstrate
how causally-inspired approaches can improve model robustness. Finally, we
outline desiderata for future research and practices, advocating targeted
interventions to address inherent limitations of observational data.

</details>


### [603] [PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time](https://arxiv.org/abs/2506.06254)
*Weizhi Zhang,Xinyang Zhang,Chenwei Zhang,Liangwei Yang,Jingbo Shang,Zhepei Wei,Henry Peng Zou,Zijie Huang,Zhengyang Wang,Yifan Gao,Xiaoman Pan,Lian Xiong,Jingguo Liu,Philip S. Yu,Xian Li*

Main category: cs.AI

TL;DR: PersonaAgent是一个个性化的LLM代理框架，通过个性化记忆和动作模块实现用户定制化任务，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理缺乏灵活性，无法满足用户多样化需求，因此开发了PersonaAgent以实现个性化服务。

Method: PersonaAgent包含个性化记忆模块（情景和语义记忆）和动作模块，通过用户偏好对齐策略优化系统提示。

Result: 实验表明PersonaAgent在动作空间个性化和实时应用中表现优异。

Conclusion: PersonaAgent展示了提供动态、定制化用户体验的可行性和潜力。

Abstract: Large Language Model (LLM) empowered agents have recently emerged as advanced
paradigms that exhibit impressive capabilities in a wide range of domains and
tasks. Despite their potential, current LLM agents often adopt a
one-size-fits-all approach, lacking the flexibility to respond to users'
varying needs and preferences. This limitation motivates us to develop
PersonaAgent, the first personalized LLM agent framework designed to address
versatile personalization tasks. Specifically, PersonaAgent integrates two
complementary components - a personalized memory module that includes episodic
and semantic memory mechanisms; a personalized action module that enables the
agent to perform tool actions tailored to the user. At the core, the persona
(defined as unique system prompt for each user) functions as an intermediary:
it leverages insights from personalized memory to control agent actions, while
the outcomes of these actions in turn refine the memory. Based on the
framework, we propose a test-time user-preference alignment strategy that
simulate the latest n interactions to optimize the persona prompt, ensuring
real-time user preference alignment through textual loss feedback between
simulated and ground-truth responses. Experimental evaluations demonstrate that
PersonaAgent significantly outperforms other baseline methods by not only
personalizing the action space effectively but also scaling during test-time
real-world applications. These results underscore the feasibility and potential
of our approach in delivering tailored, dynamic user experiences.

</details>


### [604] [Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens](https://arxiv.org/abs/2506.06261)
*Jihwan Jeong,Xiaoyu Wang,Jingmin Wang,Scott Sanner,Pascal Poupart*

Main category: cs.AI

TL;DR: RefPlan是一种基于双重贝叶斯的离线模型规划方法，通过实时观测更新环境动态信念，显著提升了保守离线强化学习策略的性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在在线探索成本高或不安全时至关重要，但常因数据有限而面临高认知不确定性，现有方法依赖固定保守策略，限制了适应性和泛化能力。

Method: 提出Reflect-then-Plan（RefPlan），将规划重新定义为贝叶斯后验估计，通过实时观测更新环境动态信念，并将不确定性纳入模型规划。

Result: 在标准基准测试中，RefPlan显著提升了保守离线RL策略的性能，在高认知不确定性和有限数据下表现稳健，且对环境动态变化具有韧性。

Conclusion: RefPlan提高了离线学习策略的灵活性、泛化能力和鲁棒性，为离线强化学习提供了更有效的解决方案。

Abstract: Offline reinforcement learning (RL) is crucial when online exploration is
costly or unsafe but often struggles with high epistemic uncertainty due to
limited data. Existing methods rely on fixed conservative policies, restricting
adaptivity and generalization. To address this, we propose Reflect-then-Plan
(RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach.
RefPlan unifies uncertainty modeling and MB planning by recasting planning as
Bayesian posterior estimation. At deployment, it updates a belief over
environment dynamics using real-time observations, incorporating uncertainty
into MB planning via marginalization. Empirical results on standard benchmarks
show that RefPlan significantly improves the performance of conservative
offline RL policies. In particular, RefPlan maintains robust performance under
high epistemic uncertainty and limited data, while demonstrating resilience to
changing environment dynamics, improving the flexibility, generalizability, and
robustness of offline-learned policies.

</details>


### [605] [SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation](https://arxiv.org/abs/2506.06470)
*Yanwei Ren,Haotian Zhang,Fuxiang Wu,Jiayan Qiu,Jiaxing Huang,Baosheng Yu,Liu Liu*

Main category: cs.AI

TL;DR: SIGMA框架通过重新利用搜索树中被丢弃的非最优分支数据，显著提升大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅保留搜索树中的最优路径，忽视了非最优分支中的有价值信息，导致数据浪费。

Method: SIGMA通过语义链接和两阶段优化（批评模型和修订模型）重新整合非最优分支数据。

Result: 在MATH基准测试中，SIGMA调整的7B模型仅用30K样本达到54.92%准确率，优于使用590K样本的现有模型。

Conclusion: SIGMA不仅减少数据需求，还显著提升模型推理能力。

Abstract: Enhancing large language models by simply scaling up datasets has begun to
yield diminishing returns, shifting the spotlight to data quality. Monte Carlo
Tree Search (MCTS) has emerged as a powerful technique for generating
high-quality chain-of-thought data, yet conventional approaches typically
retain only the top-scoring trajectory from the search tree, discarding sibling
nodes that often contain valuable partial insights, recurrent error patterns,
and alternative reasoning strategies. This unconditional rejection of
non-optimal reasoning branches may waste vast amounts of informative data in
the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo
Augmentation), a novel framework that reintegrates these discarded sibling
nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes
along each search path and applies a two-stage refinement: a critique model
identifies overlooked strengths and weaknesses across the sibling set, and a
revision model conducts text-based backpropagation to refine the top-scoring
trajectory in light of this comparative feedback. By recovering and amplifying
the underutilized but valuable signals from non-optimal reasoning branches,
SIGMA substantially improves reasoning trajectories. On the challenging MATH
benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K
samples, outperforming state-of-the-art models trained on 590K samples. This
result highlights that our sibling-guided optimization not only significantly
reduces data usage but also significantly boosts LLM reasoning.

</details>


### [606] [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
*Yitao Liu,Chenglei Si,Karthik Narasimhan,Shunyu Yao*

Main category: cs.AI

TL;DR: 论文提出了一种名为Contextual Experience Replay (CER)的无训练框架，通过动态记忆缓冲区积累和合成过去的经验，帮助语言模型代理在复杂任务中自我改进。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型代理在复杂任务（如网页导航）中表现不佳，且无法在推理时持续学习过去的经验。

Method: CER通过动态记忆缓冲区积累环境动态和决策模式的经验，使代理能在新任务中检索并增强相关知识。

Result: 在VisualWebArena和WebArena基准测试中，CER分别达到31.9%和36.7%的成功率，相对基线GPT-4o代理提升了51.0%。

Conclusion: CER显著提升了语言模型代理在复杂环境中的适应能力，证明了其高效性和有效性。

Abstract: Large language model (LLM) agents have been applied to sequential
decision-making tasks such as web navigation, but without any
environment-specific experiences, they often fail in these complex tasks.
Moreover, current LLM agents are not designed to continually learn from past
experiences during inference time, which could be crucial for them to gain
these environment-specific experiences. To address this, we propose Contextual
Experience Replay (CER), a training-free framework to enable efficient
self-improvement for language agents in their context window. Specifically, CER
accumulates and synthesizes past experiences into a dynamic memory buffer.
These experiences encompass environment dynamics and common decision-making
patterns, allowing the agents to retrieve and augment themselves with relevant
knowledge in new tasks, enhancing their adaptability in complex environments.
We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On
VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,
CER also gets a competitive average success rate of 36.7%, relatively improving
the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a
comprehensive analysis on it to prove its efficiency, validity and understand
it better.

</details>


### [607] [WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making](https://arxiv.org/abs/2506.06725)
*Guillaume Levy,Cedric Colas,Pierre-Yves Oudeyer,Thomas Carta,Clement Romac*

Main category: cs.AI

TL;DR: WorldLLM框架通过结合贝叶斯推断和强化学习，提升LLM在结构化领域中的预测能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在结构化领域（如模拟环境）中难以生成精确预测，因其无法将广泛知识落地到具体环境。

Method: 结合贝叶斯推断和自主探索的强化学习，利用LLM的上下文学习能力迭代优化假设。

Result: 在文本游戏环境中，WorldLLM提高了预测准确性并生成了可解释的环境动态理论。

Conclusion: WorldLLM通过迭代优化和探索，显著提升了LLM在结构化任务中的表现。

Abstract: Large Language Models (LLMs) possess general world knowledge but often
struggle to generate precise predictions in structured, domain-specific
contexts such as simulations. These limitations arise from their inability to
ground their broad, unstructured understanding in specific environments. To
address this, we present WorldLLM, a framework that enhances LLM-based world
modeling by combining Bayesian inference and autonomous active exploration with
reinforcement learning. WorldLLM leverages the in-context learning abilities of
LLMs to guide an LLM-based world model's predictions using natural language
hypotheses given in its prompt. These hypotheses are iteratively refined
through a Bayesian inference framework that leverages a second LLM as the
proposal distribution given collected evidence. This evidence is collected
using a curiosity-driven reinforcement learning policy that explores the
environment to find transitions with a low log-likelihood under our LLM-based
predictive model using the current hypotheses. By alternating between refining
hypotheses and collecting new evidence, our framework autonomously drives
continual improvement of the predictions. Our experiments demonstrate the
effectiveness of WorldLLM in a textual game environment that requires agents to
manipulate and combine objects. The framework not only enhances predictive
accuracy, but also generates human-interpretable theories of environment
dynamics.

</details>


### [608] [Honey, I shrunk the hypothesis space (through logical preprocessing)](https://arxiv.org/abs/2506.06739)
*Andrew Cropper,Filipe Gouveia,David M. Cerna*

Main category: cs.AI

TL;DR: 提出一种通过背景知识缩小归纳逻辑编程（ILP）假设空间的方法，显著减少学习时间。


<details>
  <summary>Details</summary>
Motivation: 在ILP中，假设空间可能包含无效规则，导致搜索效率低下。通过预筛选假设空间，可以提升学习效率。

Method: 利用背景知识识别并移除无效规则（如“偶数不可能是奇数”），使用答案集编程实现。

Result: 实验表明，预处理时间仅需10秒，学习时间可从10小时缩短至2秒，且预测精度不变。

Conclusion: 该方法能有效提升ILP系统的效率，适用于视觉推理和游戏等领域。

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. The
goal is to search a hypothesis space for a hypothesis that generalises training
examples and background knowledge. We introduce an approach that 'shrinks' the
hypothesis space before an ILP system searches it. Our approach uses background
knowledge to find rules that cannot be in an optimal hypothesis regardless of
the training examples. For instance, our approach discovers relationships such
as "even numbers cannot be odd" and "prime numbers greater than 2 are odd". It
then removes violating rules from the hypothesis space. We implement our
approach using answer set programming and use it to shrink the hypothesis space
of a constraint-based ILP system. Our experiments on multiple domains,
including visual reasoning and game playing, show that our approach can
substantially reduce learning times whilst maintaining predictive accuracies.
For instance, given just 10 seconds of preprocessing time, our approach can
reduce learning times from over 10 hours to only 2 seconds.

</details>


### [609] [Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules](https://arxiv.org/abs/2506.06750)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.AI

TL;DR: 论文研究了脉冲神经网络（SNN）训练中的挑战，比较了不同学习算法对分类准确性的影响，并提出了一种结合SNN和Lempel-Ziv复杂度（LZC）的生物启发分类器。


<details>
  <summary>Details</summary>
Motivation: 解决SNN训练中的挑战，如时间动态性、脉冲事件的不可微性和稀疏事件驱动激活，同时探索不同学习算法对分类准确性的影响。

Method: 提出了一种结合SNN和LZC的生物启发分类器，比较了经典反向传播算法与生物启发学习算法（如tempotron和Spikprop）的性能。

Result: 经典反向传播算法分类准确率高但计算成本高，而生物启发算法在保持竞争力的同时提高了计算效率，适合实时应用。

Conclusion: 选择最适合的学习算法需权衡分类准确性和计算成本，并考虑应用场景的约束。

Abstract: Training of Spiking Neural Networks (SNN) is challenging due to their unique
properties, including temporal dynamics, non-differentiability of spike events,
and sparse event-driven activations. In this paper, we widely consider the
influence of the type of chosen learning algorithm, including bioinspired
learning rules on the accuracy of classification. We proposed a bioinspired
classifier based on the combination of SNN and Lempel-Ziv complexity (LZC).
This approach synergizes the strengths of SNNs in temporal precision and
biological realism with LZC's structural complexity analysis, facilitating
efficient and interpretable classification of spatiotemporal neural data. It
turned out that the classic backpropagation algorithm achieves excellent
classification accuracy, but at extremely high computational cost, which makes
it impractical for real-time applications. Biologically inspired learning
algorithms such as tempotron and Spikprop provide increased computational
efficiency while maintaining competitive classification performance, making
them suitable for time-sensitive tasks. The results obtained indicate that the
selection of the most appropriate learning algorithm depends on the trade-off
between classification accuracy and computational cost as well as application
constraints.

</details>


### [610] [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
*Parshin Shojaee,Iman Mirzadeh,Keivan Alizadeh,Maxwell Horton,Samy Bengio,Mehrdad Farajtabar*

Main category: cs.AI

TL;DR: 论文研究了大型推理模型（LRMs）的能力、扩展性和局限性，发现其在复杂度超过一定阈值时准确性崩溃，并揭示了其推理行为的反直觉特性。


<details>
  <summary>Details</summary>
Motivation: 当前对LRMs的评估主要关注数学和编程基准的最终答案准确性，缺乏对其推理过程的深入理解，且存在数据污染问题。

Method: 通过可控的拼图环境系统研究LRMs，精确操纵复杂度并分析推理痕迹。

Result: 发现LRMs在复杂度超过阈值时准确性崩溃，推理努力随复杂度增加至某点后下降，且在不同复杂度任务中表现不同。

Conclusion: LRMs在精确计算和一致性推理方面存在局限，其推理能力仍需进一步研究。

Abstract: Recent generations of language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers.
While these models demonstrate improved performance on reasoning benchmarks,
their fundamental capabilities, scaling properties, and limitations remain
insufficiently understood. Current evaluations primarily focus on established
math and coding benchmarks, emphasizing final answer accuracy. However, this
evaluation paradigm often suffers from contamination and does not provide
insights into the reasoning traces. In this work, we systematically investigate
these gaps with the help of controllable puzzle environments that allow precise
manipulation of complexity while maintaining consistent logical structures.
This setup enables the analysis of not only final answers but also the internal
reasoning traces, offering insights into how LRMs think. Through extensive
experiments, we show that LRMs face a complete accuracy collapse beyond certain
complexities. Moreover, they exhibit a counterintuitive scaling limit: their
reasoning effort increases with problem complexity up to a point, then declines
despite having remaining token budget. By comparing LRMs with their standard
LLM counterparts under same inference compute, we identify three performance
regimes: (1) low-complexity tasks where standard models outperform LRMs, (2)
medium-complexity tasks where LRMs demonstrates advantage, and (3)
high-complexity tasks where both models face complete collapse. We found that
LRMs have limitations in exact computation: they fail to use explicit
algorithms and reason inconsistently across scales. We also investigate the
reasoning traces in more depth, studying the patterns of explored solutions and
analyzing the models' computational behavior, shedding light on their
strengths, limitations, and raising questions about their reasoning
capabilities.

</details>


### [611] [Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments](https://arxiv.org/abs/2506.06981)
*Riley Simmons-Edler,Ryan P. Badman,Felix Baastad Berg,Raymond Chua,John J. Vastola,Joshua Lunger,William Qian,Kanaka Rajan*

Main category: cs.AI

TL;DR: 论文提出了一种结合神经科学和动物行为学工具的方法，用于分析深度强化学习（DRL）代理在复杂环境中的行为，揭示了其策略和记忆的丰富结构。


<details>
  <summary>Details</summary>
Motivation: 当前DRL代理的行为分析方法不足，尤其是在任务和代理复杂性增加时，需要更深入的工具来理解其行为。

Method: 在ForageWorld环境中，结合神经科学和动物行为学的工具，对DRL代理进行行为和神经动态分析。

Result: 发现基于RNN的模型无关DRL代理可以表现出类似规划的行为，无需显式记忆模块或世界模型。

Conclusion: 通过借鉴生物智能研究方法，可以揭示DRL代理的复杂学习动态，并为未来复杂代理的行为分析和安全对齐提供框架。

Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents --
particularly as task and agent sophistication increase -- requires more than
simple comparison of reward curves, yet standard methods for behavioral
analysis remain underdeveloped in DRL. We apply tools from neuroscience and
ethology to study DRL agents in a novel, complex, partially observable
environment, ForageWorld, designed to capture key aspects of real-world animal
foraging -- including sparse, depleting resource patches, predator threats, and
spatially extended arenas. We use this environment as a platform for applying
joint behavioral and neural analysis to agents, revealing detailed,
quantitatively grounded insights into agent strategies, memory, and planning.
Contrary to common assumptions, we find that model-free RNN-based DRL agents
can exhibit structured, planning-like behavior purely through emergent dynamics
-- without requiring explicit memory modules or world models. Our results show
that studying DRL agents like animals -- analyzing them with
neuroethology-inspired tools that reveal structure in both behavior and neural
dynamics -- uncovers rich structure in their learning dynamics that would
otherwise remain invisible. We distill these tools into a general analysis
framework linking core behavioral and representational features to diagnostic
methods, which can be reused for a wide range of tasks and agents. As agents
grow more complex and autonomous, bridging neuroscience, cognitive science, and
AI will be essential -- not just for understanding their behavior, but for
ensuring safe alignment and maximizing desirable behaviors that are hard to
measure via reward. We show how this can be done by drawing on lessons from how
biological intelligence is studied.

</details>


### [612] [HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model](https://arxiv.org/abs/2506.07428)
*Yuling Wang,Zihui Chen,Pengfei Jiao,Xiao Wang*

Main category: cs.AI

TL;DR: 论文提出了一种名为HeTa的关系感知异构图基础攻击模型，通过挖掘共享攻击单元实现通用扰动，并验证了其强大的攻击性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 异构图神经网络（HGNNs）易受攻击，现有攻击方法需复杂参数重训练，无法快速适应新场景。基础模型的出现为泛化攻击提供了新思路。

Method: 提出HeTa模型，利用基础代理模型对齐异质性并识别共享关系感知攻击单元，实现基于关系权重的序列化攻击。

Result: 实验表明HeTa在攻击性能和泛化能力上表现优异，能快速适应新异构图。

Conclusion: HeTa为HGNNs提供了一种通用且高效的基础攻击方法，揭示了共享漏洞模式的重要性。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the
need for tailored attacks to assess their robustness and ensure security.
However, existing HGNN attacks often require complex retraining of parameters
to generate specific perturbations for new scenarios. Recently, foundation
models have opened new horizons for the generalization of graph neural networks
by capturing shared semantics across various graph distributions. This leads us
to ask:Can we design a foundation attack model for HGNNs that enables
generalizable perturbations across different HGNNs, and quickly adapts to new
heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant
differences in model design and parameter space, different HGNNs surprisingly
share common vulnerability patterns from a relation-aware perspective.
Therefore, we explore how to design foundation HGNN attack criteria by mining
shared attack units. In this paper, we propose a novel relation-wise
heterogeneous graph foundation attack model, HeTa. We introduce a foundation
surrogate model to align heterogeneity and identify the importance of shared
relation-aware attack units. Building on this, we implement a serialized
relation-by-relation attack based on the identified relational weights. In this
way, the perturbation can be transferred to various target HGNNs and easily
fine-tuned for new HGs. Extensive experiments exhibit powerful attack
performances and generalizability of our method.

</details>


### [613] [Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions](https://arxiv.org/abs/2506.07527)
*Lu Ma,Hao Liang,Meiyi Qiang,Lexiang Tang,Xiaochen Ma,Zhen Hao Wong,Junbo Niu,Chengyu Shen,Runming He,Bin Cui,Wentao Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为ReLIFT的新训练方法，结合强化学习（RL）和监督微调（SFT），以克服RL在扩展大语言模型（LLM）推理能力上的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管RL在LLM推理中表现出色，但其无法超越模型的基础能力，而SFT可以引入新知识和推理模式。因此，结合两者的优势成为研究动机。

Method: 提出ReLIFT方法，交替使用RL和在线微调（SFT），在遇到难题时收集高质量解决方案进行微调。

Result: ReLIFT在五个竞赛级基准和一个分布外基准上平均提升5.2分，且仅需13%的详细演示数据。

Conclusion: ReLIFT有效克服了RL的局限性，展示了结合RL和SFT的潜力。

Abstract: Recent advances in large language model (LLM) reasoning have shown that
sophisticated behaviors such as planning and self-reflection can emerge through
reinforcement learning (RL). However, despite these successes, RL in its
current form remains insufficient to induce capabilities that exceed the
limitations of the base model, as it is primarily optimized based on existing
knowledge of the model rather than facilitating the acquisition of new
information. To address this limitation, we employ supervised fine-tuning (SFT)
to learn what RL cannot, which enables the incorporation of new knowledge and
reasoning patterns by leveraging high-quality demonstration data. We analyze
the training dynamics of RL and SFT for LLM reasoning and find that RL excels
at maintaining and improving performance on questions within the model's
original capabilities, while SFT is more effective at enabling progress on
questions beyond the current scope of the model. Motivated by the complementary
strengths of RL and SFT, we introduce a novel training approach,
\textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved
with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily
trained using RL, but when it encounters challenging questions, high-quality
solutions are collected for fine-tuning, and the training process alternates
between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT
achieves an average improvement of over +5.2 points across five
competition-level benchmarks and one out-of-distribution benchmark compared to
other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both
RL and SFT while using only 13\% of the detailed demonstration data,
highlighting its scalability. These results provide compelling evidence that
ReLIFT overcomes the fundamental limitations of RL and underscores the
significant potential.

</details>


### [614] [Agent Semantics, Semantic Spacetime, and Graphical Reasoning](https://arxiv.org/abs/2506.07756)
*Mark Burgess*

Main category: cs.AI

TL;DR: 论文介绍了语义时空图模型的形式化特性，用于定向知识表示和过程建模，定义了一个有限γ(3,4)表示以形成可扩展的封闭操作集。


<details>
  <summary>Details</summary>
Motivation: 研究语义时空图模型的形式化特性，以支持可预测的知识表示和过程建模。

Method: 定义了一个有限γ(3,4)表示，形成封闭操作集，并利用语义时空假设为图路径提供最小约束的预测性。

Result: 发现部分图中普遍存在的吸收状态会导致信息泄漏，这与除以零问题相关，表明封闭性丧失。

Conclusion: 语义时空模型及其承诺理论有助于阐明吸收状态与边界信息的关联，为意图性引入提供途径。

Abstract: Some formal aspects of the Semantic Spacetime graph model are presented, with
reference to its use for directed knowledge representations and process
modelling. A finite $\gamma(3,4)$ representation is defined to form a closed
set of operations that can scale to any degree of semantic complexity. The
Semantic Spacetime postulates bring predictability with minimal constraints to
pathways in graphs. The ubiquitous appearance of absorbing states in any
partial graph means that a graph process leaks information. The issue is
closely associated with the issue of division by zero, which signals a loss of
closure and the need for manual injection of remedial information. The Semantic
Spacetime model (and its Promise Theory) origins help to clarify how such
absorbing states are associated with boundary information where intentionality
can enter.

</details>


### [615] [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)
*Jiayi Sheng,Luna Lyu,Jikai Jin,Tony Xia,Alex Gu,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: 论文提出了一种非正式但可验证的不等式证明任务，并发布了IneqMath数据集，评估了29个大型语言模型，发现其在逐步推理中存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 不等式证明是数学和科学领域的重要技能，但现有数据集稀缺或形式化，限制了大型语言模型在此领域的发展。

Method: 将不等式证明分为两个可自动检查的子任务：边界估计和关系预测，并开发了IneqMath数据集及基于LLM的评估框架。

Result: 评估显示，即使是顶级模型在逐步推理中的准确率低于10%，远低于仅考虑最终答案的准确率。

Conclusion: 研究揭示了当前LLM在严谨证明中的不足，并提出了定理引导推理和自我优化等未来方向。

Abstract: Inequality proving, crucial across diverse scientific and mathematical
fields, tests advanced reasoning skills such as discovering tight bounds and
strategic theorem application. This makes it a distinct, demanding frontier for
large language models (LLMs), offering insights beyond general mathematical
problem-solving. Progress in this area is hampered by existing datasets that
are often scarce, synthetic, or rigidly formal. We address this by proposing an
informal yet verifiable task formulation, recasting inequality proving into two
automatically checkable subtasks: bound estimation and relation prediction.
Building on this, we release IneqMath, an expert-curated dataset of
Olympiad-level inequalities, including a test set and training corpus enriched
with step-wise solutions and theorem annotations. We also develop a novel
LLM-as-judge evaluation framework, combining a final-answer judge with four
step-wise judges designed to detect common reasoning flaws. A systematic
evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even
top models like o1 achieve less than 10% overall accuracy under step-wise
scrutiny; this is a drop of up to 65.5% from their accuracy considering only
final answer equivalence. This discrepancy exposes fragile deductive chains and
a critical gap for current LLMs between merely finding an answer and
constructing a rigorous proof. Scaling model size and increasing test-time
computation yield limited gains in overall proof correctness. Instead, our
findings highlight promising research directions such as theorem-guided
reasoning and self-refinement. Code and data are available at
https://ineqmath.github.io/.

</details>


### [616] [Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation](https://arxiv.org/abs/2506.07940)
*Christopher Subia-Waud*

Main category: cs.AI

TL;DR: Gradients是一个去中心化的AutoML平台，通过竞争性市场机制优化超参数配置，显著优于现有集中式方法。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML平台依赖单一优化策略，仅探索部分可行超参数配置，限制了性能提升。

Method: 将超参数优化转化为竞争性市场，独立矿工通过经济激励竞争发现最优配置。

Result: 在180个实验中，Gradients以82.8%胜率优于HuggingFace AutoTrain，100%优于其他平台，平均提升11.8%-42.1%。

Conclusion: 经济驱动的竞争性方法能系统性发现集中式AutoML遗漏的优越配置。

Abstract: Foundation model fine-tuning faces a fundamental challenge: existing AutoML
platforms rely on single optimisation strategies that explore only a fraction
of viable hyperparameter configurations. In this white paper, We introduce
Gradients, a decentralised AutoML platform that transforms hyperparameter
optimisation into a competitive marketplace where independent miners compete to
discover optimal configurations. Economic incentives align individual
exploration with collective optimisation goals, driving systematic
investigation of hyperparameter regions that centralised methods miss. We
evaluate our approach across 180 controlled experiments spanning diverse model
architectures (70M to 70B parameters) and task types. Gradients achieves an
82.8\% win rate against HuggingFace AutoTrain and 100\% against TogetherAI,
Databricks, and Google Cloud, with mean improvements of 11.8\% and 42.1\%
respectively. Complex reasoning and retrieval tasks show particularly strong
gains of 30-40\%, whilst diffusion models achieve 23.4\% improvements for
person-specific generation. These results demonstrate that competitive,
economically-driven approaches can systematically discover superior
configurations that centralised AutoML consistently miss.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [617] [Fast Geometric Embedding for Node Influence Maximization](https://arxiv.org/abs/2506.07435)
*Alexander Kolpakov,Igor Rivin*

Main category: cs.SI

TL;DR: 提出了一种高效的力导向布局算法，将图嵌入低维空间，用径向距离作为中心性度量的代理。


<details>
  <summary>Details</summary>
Motivation: 大规模图上计算传统中心性度量（如介数和接近度）计算成本高。

Method: 使用力导向布局算法将图嵌入低维空间，径向距离作为中心性度量的替代。

Result: 在多种图家族上验证，与度、PageRank和路径中心性有强相关性。

Conclusion: 该方法可快速找到网络中的高影响力节点，是标准贪婪算法的快速可扩展替代方案。

Abstract: Computing classical centrality measures such as betweenness and closeness is
computationally expensive on large-scale graphs. In this work, we introduce an
efficient force layout algorithm that embeds a graph into a low-dimensional
space, where the radial distance from the origin serves as a proxy for various
centrality measures. We evaluate our method on multiple graph families and
demonstrate strong correlations with degree, PageRank, and paths-based
centralities. As an application, it turns out that the proposed embedding
allows to find high-influence nodes in a network, and provides a fast and
scalable alternative to the standard greedy algorithm.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [618] [Introduction to Predictive Coding Networks for Machine Learning](https://arxiv.org/abs/2506.06332)
*Mikko Stenlund*

Main category: cs.NE

TL;DR: 本文简要介绍了预测编码网络（PCNs）及其在机器学习中的应用，包括架构、推理和学习规则，并通过CIFAR-10任务展示了其性能。


<details>
  <summary>Details</summary>
Motivation: 为机器学习从业者提供一个快速了解PCNs的入门指南，展示其作为传统前馈神经网络替代方案的潜力。

Method: 介绍了PCNs的基础架构、推理和学习规则，并提供了PyTorch实现的Python笔记本。

Result: 通过CIFAR-10图像分类任务验证了PCNs的性能，展示了其在实际应用中的潜力。

Conclusion: PCNs是一种有前景的生物启发框架，适用于分层计算任务，并在实际应用中表现出色。

Abstract: Predictive coding networks (PCNs) constitute a biologically inspired
framework for understanding hierarchical computation in the brain, and offer an
alternative to traditional feedforward neural networks in ML. This note serves
as a quick, onboarding introduction to PCNs for machine learning practitioners.
We cover the foundational network architecture, inference and learning update
rules, and algorithmic implementation. A concrete image-classification task
(CIFAR-10) is provided as a benchmark-smashing application, together with an
accompanying Python notebook containing the PyTorch implementation.

</details>


### [619] [CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms](https://arxiv.org/abs/2506.06362)
*Dejun Xu,Jijia Chen,Gary G. Yen,Min Jiang*

Main category: cs.NE

TL;DR: 提出了一种新的资源分配框架，通过对比排序网络选择性优化有前景的下层任务，显著降低计算成本并保持或提升解精度。


<details>
  <summary>Details</summary>
Motivation: 双层优化因嵌套结构导致高计算成本，传统进化算法资源浪费严重，需改进效率。

Method: 使用对比排序网络在线学习上下层解的关系模式，指导基于参考的排序策略，优先优化任务并自适应控制重采样。

Result: 在五种先进双层算法上验证，显著减少计算成本且保持或提升解精度。

Conclusion: 该框架为双层进化算法提供了通用高效策略，推动可扩展双层优化发展。

Abstract: Bilevel optimization poses a significant computational challenge due to its
nested structure, where each upper-level candidate solution requires solving a
corresponding lower-level problem. While evolutionary algorithms (EAs) are
effective at navigating such complex landscapes, their high resource demands
remain a key bottleneck -- particularly the redundant evaluation of numerous
unpromising lower-level tasks. Despite recent advances in multitasking and
transfer learning, resource waste persists. To address this issue, we propose a
novel resource allocation framework for bilevel EAs that selectively identifies
and focuses on promising lower-level tasks. Central to our approach is a
contrastive ranking network that learns relational patterns between paired
upper- and lower-level solutions online. This knowledge guides a
reference-based ranking strategy that prioritizes tasks for optimization and
adaptively controls resampling based on estimated population quality.
Comprehensive experiments across five state-of-the-art bilevel algorithms show
that our framework significantly reduces computational cost while preserving --
or even enhancing -- solution accuracy. This work offers a generalizable
strategy to improve the efficiency of bilevel EAs, paving the way for more
scalable bilevel optimization.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [620] [The TESS Ten Thousand Catalog: 10,001 uniformly-vetted and -validated Eclipsing Binary Stars detected in Full-Frame Image data by machine learning and analyzed by citizen scientists](https://arxiv.org/abs/2506.05631)
*Veselin B. Kostov,Brian P. Powell,Aline U. Fornear,Marco Z. Di Fraia,Robert Gagliano,Thomas L. Jacobs,Julien S. de Lambilly,Hugo A. Durantini Luca,Steven R. Majewski,Mark Omohundro,Jerome Orosz,Saul A. Rappaport,Ryan Salik,Donald Short,William Welsh,Svetoslav Alexandrov,Cledison Marcos da Silva,Erika Dunning,Gerd Guhne,Marc Huten,Michiharu Hyogo,Davide Iannone,Sam Lee,Christian Magliano,Manya Sharma,Allan Tarr,John Yablonsky,Sovan Acharya,Fred Adams,Thomas Barclay,Benjamin T. Montet,Susan Mullally,Greg Olmschenk,Andrej Prsa,Elisa Quintana,Robert Wilson,Hasret Balcioglu,Ethan Kruse,the Eclipsing Binary Patrol Collaboration*

Main category: astro-ph.SR

TL;DR: TESS卫星通过全帧图像模式观测天空，发现并验证了10001个食双星系统，其中7936个为新发现，2065个为已知系统的更新。


<details>
  <summary>Details</summary>
Motivation: 研究食双星系统对恒星形成和演化的深入理解具有重要意义，TESS数据为此提供了丰富资源。

Method: 使用神经网络筛选TESS全帧图像数据，结合自动化方法和公民科学家的手动检查，验证食双星系统。

Result: 生成包含10001个食双星的目录，其中7936个为新发现，2065个为已知系统的更新。

Conclusion: TESS数据为食双星研究提供了宝贵资源，未来可进一步探索未验证目标。

Abstract: The Transiting Exoplanet Survey Satellite (TESS) has surveyed nearly the
entire sky in Full-Frame Image mode with a time resolution of 200 seconds to 30
minutes and a temporal baseline of at least 27 days. In addition to the primary
goal of discovering new exoplanets, TESS is exceptionally capable at detecting
variable stars, and in particular short-period eclipsing binaries which are
relatively common, making up a few percent of all stars, and represent powerful
astrophysical laboratories for deep investigations of stellar formation and
evolution. We combed Sectors 1-82 of TESS Full-Frame Image data searching for
eclipsing binary stars using a neural network that identified ~1.2 million
stars with eclipse-like features. Of these, we have performed an in-depth
analysis on ~60,000 targets using automated methods and manual inspection by
citizen scientists. Here we present a catalog of 10001 uniformly-vetted and
-validated eclipsing binary stars that passed all our ephemeris and photocenter
tests, as well as complementary visual inspection. Of these, 7936 are new
eclipsing binaries while the remaining 2065 are known systems for which we
update the published ephemerides. We outline the detection and analysis of the
targets, discuss the properties of the sample, and highlight potentially
interesting systems. Finally, we also provide a list of ~900,000 unvetted and
unvalidated targets for which the neural network found eclipse-like features
with a score higher than 0.9, and for which there are no known eclipsing
binaries within a sky-projected separation of a TESS pixel (~21 arcsec).

</details>


### [621] [The TESS Ten Thousand Catalog: 10,001 uniformly-vetted and -validated Eclipsing Binary Stars detected in Full-Frame Image data by machine learning and analyzed by citizen scientists](https://arxiv.org/abs/2506.05631)
*Veselin B. Kostov,Brian P. Powell,Aline U. Fornear,Marco Z. Di Fraia,Robert Gagliano,Thomas L. Jacobs,Julien S. de Lambilly,Hugo A. Durantini Luca,Steven R. Majewski,Mark Omohundro,Jerome Orosz,Saul A. Rappaport,Ryan Salik,Donald Short,William Welsh,Svetoslav Alexandrov,Cledison Marcos da Silva,Erika Dunning,Gerd Guhne,Marc Huten,Michiharu Hyogo,Davide Iannone,Sam Lee,Christian Magliano,Manya Sharma,Allan Tarr,John Yablonsky,Sovan Acharya,Fred Adams,Thomas Barclay,Benjamin T. Montet,Susan Mullally,Greg Olmschenk,Andrej Prsa,Elisa Quintana,Robert Wilson,Hasret Balcioglu,Ethan Kruse,the Eclipsing Binary Patrol Collaboration*

Main category: astro-ph.SR

TL;DR: TESS卫星通过全帧图像模式观测了几乎整个天空，发现并验证了10001个食双星系统，其中7936个为新发现，2065个为已知系统的更新。


<details>
  <summary>Details</summary>
Motivation: 研究食双星系统，为恒星形成和演化提供实验室。

Method: 利用神经网络分析TESS全帧图像数据，结合自动化方法和公民科学家的手动检查。

Result: 发现并验证了10001个食双星系统，提供了约90万个未验证目标。

Conclusion: TESS数据为食双星研究提供了丰富资源，未来可进一步探索潜在有趣系统。

Abstract: The Transiting Exoplanet Survey Satellite (TESS) has surveyed nearly the
entire sky in Full-Frame Image mode with a time resolution of 200 seconds to 30
minutes and a temporal baseline of at least 27 days. In addition to the primary
goal of discovering new exoplanets, TESS is exceptionally capable at detecting
variable stars, and in particular short-period eclipsing binaries which are
relatively common, making up a few percent of all stars, and represent powerful
astrophysical laboratories for deep investigations of stellar formation and
evolution. We combed Sectors 1-82 of TESS Full-Frame Image data searching for
eclipsing binary stars using a neural network that identified ~1.2 million
stars with eclipse-like features. Of these, we have performed an in-depth
analysis on ~60,000 targets using automated methods and manual inspection by
citizen scientists. Here we present a catalog of 10001 uniformly-vetted and
-validated eclipsing binary stars that passed all our ephemeris and photocenter
tests, as well as complementary visual inspection. Of these, 7936 are new
eclipsing binaries while the remaining 2065 are known systems for which we
update the published ephemerides. We outline the detection and analysis of the
targets, discuss the properties of the sample, and highlight potentially
interesting systems. Finally, we also provide a list of ~900,000 unvetted and
unvalidated targets for which the neural network found eclipse-like features
with a score higher than 0.9, and for which there are no known eclipsing
binaries within a sky-projected separation of a TESS pixel (~21 arcsec).

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [622] [Understanding Gender Bias in AI-Generated Product Descriptions](https://arxiv.org/abs/2506.05390)
*Markelle Kelly,Mohammad Tahaei,Padhraic Smyth,Lauren Wilcox*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLM）在电子商务中生成产品描述时的性别偏见，提出了新的分类方法，并分析了GPT-3.5和电商专用LLM中的偏见表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM的性别偏见在许多领域已被广泛研究，但在电子商务中的应用尚未深入探讨，可能揭示新的算法偏见和危害。

Method: 开发了数据驱动的性别偏见分类方法，并定量分析了GPT-3.5和电商专用LLM中的偏见表现。

Result: 研究发现电商场景中存在独特的性别偏见，如对服装尺寸的假设、产品特征广告中的刻板印象以及说服性语言的使用差异。

Conclusion: 研究揭示了电子商务中未充分探索的性别偏见维度，补充了现有AI危害框架中的排斥性规范、刻板印象和性能差异问题。

Abstract: While gender bias in large language models (LLMs) has been extensively
studied in many domains, uses of LLMs in e-commerce remain largely unexamined
and may reveal novel forms of algorithmic bias and harm. Our work investigates
this space, developing data-driven taxonomic categories of gender bias in the
context of product description generation, which we situate with respect to
existing general purpose harms taxonomies. We illustrate how AI-generated
product descriptions can uniquely surface gender biases in ways that require
specialized detection and mitigation approaches. Further, we quantitatively
analyze issues corresponding to our taxonomic categories in two models used for
this task -- GPT-3.5 and an e-commerce-specific LLM -- demonstrating that these
forms of bias commonly occur in practice. Our results illuminate unique,
under-explored dimensions of gender bias, such as assumptions about clothing
size, stereotypical bias in which features of a product are advertised, and
differences in the use of persuasive language. These insights contribute to our
understanding of three types of AI harms identified by current frameworks:
exclusionary norms, stereotyping, and performance disparities, particularly for
the context of e-commerce.

</details>


### [623] [Are Large Language Models Good Temporal Graph Learners?](https://arxiv.org/abs/2506.05393)
*Shenyang Huang,Ali Parviz,Emma Kondrup,Zachary Yang,Zifeng Ding,Michael Bronstein,Reihaneh Rabbany,Guillaume Rabusseau*

Main category: cs.CL

TL;DR: TGTalker是一种新颖的时序图学习框架，专为LLMs设计，用于动态图的链接预测，并在性能和可解释性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在图推理能力上取得进展，但其在动态图（真实世界演化网络）中的应用仍未被充分探索。

Method: TGTalker利用时序图中的近期偏置提取结构信息，并将其转换为自然语言供LLMs使用，同时利用时序邻居作为预测的额外信息。

Result: 在五个真实世界网络中，TGTalker与现有TGNN模型竞争，并优于TGN和HTGN，同时为预测提供文本解释。

Conclusion: TGTalker不仅提升了动态图的链接预测性能，还为时序链接预测的可解释性开辟了新方向。

Abstract: Large Language Models (LLMs) have recently driven significant advancements in
Natural Language Processing and various other applications. While a broad range
of literature has explored the graph-reasoning capabilities of LLMs, including
their use of predictors on graphs, the application of LLMs to dynamic graphs --
real world evolving networks -- remains relatively unexplored. Recent work
studies synthetic temporal graphs generated by random graph models, but
applying LLMs to real-world temporal graphs remains an open question. To
address this gap, we introduce Temporal Graph Talker (TGTalker), a novel
temporal graph learning framework designed for LLMs. TGTalker utilizes the
recency bias in temporal graphs to extract relevant structural information,
converted to natural language for LLMs, while leveraging temporal neighbors as
additional information for prediction. TGTalker demonstrates competitive link
prediction capabilities compared to existing Temporal Graph Neural Network
(TGNN) models. Across five real-world networks, TGTalker performs competitively
with state-of-the-art temporal graph methods while consistently outperforming
popular models such as TGN and HTGN. Furthermore, TGTalker generates textual
explanations for each prediction, thus opening up exciting new directions in
explainability and interpretability for temporal link prediction. The code is
publicly available at https://github.com/shenyangHuang/TGTalker.

</details>


### [624] [SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs](https://arxiv.org/abs/2506.05413)
*Patrik Czakó,Gábor Kertész,Sándor Szénási*

Main category: cs.CL

TL;DR: SmoothRot是一种新颖的后训练量化技术，通过结合通道级缩放和Hadamard变换，显著提升4位量化在大型语言模型中的效率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型中大规模激活异常值的关键挑战，以提高量化精度。

Method: 集成通道级缩放与Hadamard变换，将极端异常值转化为适合量化的激活。

Result: 在LLaMA2 7B、LLaMA3.1 8B和Mistral 7B等模型上，量化与FP16模型的性能差距缩小了10-30%，且不增加推理延迟。

Conclusion: SmoothRot有效提升了4位量化的准确性，适用于语言生成和零样本推理任务。

Abstract: We present SmoothRot, a novel post-training quantization technique to enhance
the efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot
addresses the critical challenge of massive activation outliers, by integrating
channel-wise scaling with Hadamard transformations. Our technique effectively
transforms extreme outliers into quantization-friendly activations,
significantly improving quantization accuracy. Experiments conducted on popular
LLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot
consistently reduces the performance gap between quantized and FP16 models by
approximately 10-30\% across language generation and zero-shot reasoning tasks,
without introducing additional inference latency. Code is available at
https://github.com/czakop/smoothrot.

</details>


### [625] [Multidimensional Analysis of Specific Language Impairment Using Unsupervised Learning Through PCA and Clustering](https://arxiv.org/abs/2506.05498)
*Niruthiha Selvanayagam*

Main category: cs.CL

TL;DR: 本研究使用无监督机器学习技术分析特定语言障碍（SLI）儿童的自然语言发展轨迹，发现SLI主要表现为语言产出能力降低而非句法复杂性缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统标准化评估可能忽略SLI的细微发展模式，本研究旨在通过无监督学习技术提供早期识别和干预的见解。

Method: 分析1,163名4-16岁儿童的叙事样本，使用主成分分析（PCA）和聚类技术评估64种语言特征。

Result: 发现两个主要聚类：高语言产出低SLI患病率组和低产出高句法复杂性高SLI患病率组，支持语言能力的连续模型。

Conclusion: 研究挑战了分类诊断框架，强调无监督学习技术在优化诊断标准和干预策略中的潜力。

Abstract: Specific Language Impairment (SLI) affects approximately 7 percent of
children, presenting as isolated language deficits despite normal cognitive
abilities, sensory systems, and supportive environments. Traditional diagnostic
approaches often rely on standardized assessments, which may overlook subtle
developmental patterns. This study aims to identify natural language
development trajectories in children with and without SLI using unsupervised
machine learning techniques, providing insights for early identification and
targeted interventions. Narrative samples from 1,163 children aged 4-16 years
across three corpora (Conti-Ramsden 4, ENNI, and Gillam) were analyzed using
Principal Component Analysis (PCA) and clustering. A total of 64 linguistic
features were evaluated to uncover developmental trajectories and distinguish
linguistic profiles. Two primary clusters emerged: (1) high language production
with low SLI prevalence, and (2) limited production but higher syntactic
complexity with higher SLI prevalence. Additionally, boundary cases exhibited
intermediate traits, supporting a continuum model of language abilities.
Findings suggest SLI manifests primarily through reduced production capacity
rather than syntactic complexity deficits. The results challenge categorical
diagnostic frameworks and highlight the potential of unsupervised learning
techniques for refining diagnostic criteria and intervention strategies.

</details>


### [626] [A Fictional Q&A Dataset for Studying Memorization and Knowledge Acquisition](https://arxiv.org/abs/2506.05639)
*John Kirchenbauer,Janny Mongkolsupawan,Yuxin Wen,Tom Goldstein,Daphne Ippolito*

Main category: cs.CL

TL;DR: 本文提出了一种新数据集，用于研究语言模型对事实记忆和逐字序列记忆的双重过程，并通过实验展示了合成数据在区分不同记忆形式方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 语言模型在训练时会学习语言结构和世界事实，但其对事实的记忆机制尚不明确。本文旨在通过合成数据研究这一过程。

Method: 构建了一个包含虚构事件的合成数据集，包括类似网络文本的文档和相关问答对，并进行了训练实验。

Result: 实验表明，合成数据能有效区分语言模型的不同记忆形式，同时也揭示了构建真实虚构数据的挑战。

Conclusion: 合成数据为研究语言模型的记忆机制提供了新工具，但构建高质量数据仍需克服挑战。

Abstract: When language models are trained on textual data, they acquire both knowledge
about the structure of language as well as knowledge of facts about the world.
At inference time, their knowledge of facts can be leveraged to solve
interesting problems and perform useful knowledge work for users. It is well
known that language models can verbatim memorize long sequences from their
training data. However, it is much less well understood how language models
memorize facts seen during training. In this work, we propose a new dataset to
specifically empower researchers to study the dual processes of fact
memorization and verbatim sequence memorization. The dataset consists of
synthetically-generated, webtext-like documents about fictional events, as well
as question-answer pairs about the events. We conduct training experiments
showing how synthetic data about fictional events can be effective in teasing
apart different forms of memorization. We also document the challenges in
effectively building realistic, fictional synthetic data.

</details>


### [627] [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种基于渐进式过载原则（POCL）的课程学习框架，用于改进大型语言模型的知识蒸馏（KD）过程，通过逐步增加训练样本难度提升学生模型的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有KD方法在训练学生模型时容易导致分布偏移，引发灾难性遗忘、模式崩溃等问题，需要一种更稳定的训练框架。

Method: POCL框架包含两部分：难度测量器（对样本从易到难排序）和训练调度器（逐步引入样本并调整损失函数温度）。

Result: 实验表明，POCL能显著提升蒸馏后学生模型的性能，适用于多种白盒KD方法和模型家族。

Conclusion: 通过结构化训练数据，POCL增强了KD过程的稳定性与性能，为LLM的压缩提供了有效解决方案。

Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by
transferring the teacher model's capabilities to a smaller student model,
reducing inference cost and memory usage while maintaining performance.
However, existing KD methods for LLMs often fail to prevent significant shifts
in the student model's distribution during training, leading to issues such as
catastrophic forgetting, mode collapse, and training-inference mismatch. To
address these challenges, we propose a novel, plug-in curriculum learning
framework inspired by the strength training principle of "progressive overload"
(POCL), which can be seamlessly integrated into existing white-box KD
approaches with minimal computational overhead. The framework comprises two
core components: (1) a difficulty measurer that ranks and partitions training
samples from easy to hard, and (2) a training scheduler that incrementally
introduces these subsets into the distillation process at fixed intervals while
applying loss functions with progressively rising temperatures. By starting
with the easiest samples and progressively increasing the difficulty, the
approach enhances both the stability and efficiency of learning. Extensive
experiments in instruction-following settings demonstrate that POCL
consistently improves the performance of distilled student models across
various white-box KD methods and model families. Our findings highlight the
effectiveness of sorted training samples in KD for LLMs. More generally, our
work demonstrates how to structure training data within the KD process to
enhance the stability and performance of distilled LLMs.

</details>


### [628] [Large Language Models are Good Relational Learners](https://arxiv.org/abs/2506.05725)
*Fang Wu,Vijay Prakash Dwivedi,Jure Leskovec*

Main category: cs.CL

TL;DR: Rel-LLM是一种新颖架构，结合图神经网络（GNN）和检索增强生成（RAG）框架，为大型语言模型（LLM）生成结构化关系提示，解决了传统文本序列化方法在关系深度学习（RDL）中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过将结构化数据转换为扁平文本序列来适应LLM，但忽视了关键关系结构，导致冗余和上下文长度问题。

Method: Rel-LLM使用GNN编码器提取实体周围的局部子图，生成包含关系和时间依赖的特征表示，并通过反规范化过程将其转换为结构化提示。

Result: 实验表明，Rel-LLM在关键RDL任务上优于现有方法，提供了一种可扩展且高效的LLM与结构化数据集成方案。

Conclusion: Rel-LLM通过保留数据库的关系结构，显著提升了LLM处理复杂实体关系的能力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
various domains, yet their application to relational deep learning (RDL)
remains underexplored. Existing approaches adapt LLMs by traversing relational
links between entities in a database and converting the structured data into
flat text documents. Still, this text-based serialization disregards critical
relational structures, introduces redundancy, and often exceeds standard LLM
context lengths. We introduce Rel-LLM, a novel architecture that utilizes a
graph neural network (GNN)- based encoder to generate structured relational
prompts for LLMs within a retrieval-augmented generation (RAG) framework.
Unlike traditional text-based serialization approaches, our method preserves
the inherent relational structure of databases while enabling LLMs to
effectively process and reason over complex entity relationships. Specifically,
the GNN encoder extracts a local subgraph around an entity to build feature
representations that contain relevant entity relationships and temporal
dependencies. These representations are transformed into structured prompts
using a denormalization process, effectively allowing the LLM to reason over
relational structures. Through extensive experiments, we demonstrate that
Rel-LLM outperforms existing methods on key RDL tasks, offering a scalable and
efficient approach to integrating LLMs with structured data sources. Code is
available at https://github.com/smiles724/Rel-LLM.

</details>


### [629] [Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness](https://arxiv.org/abs/2506.05735)
*Rongzhe Wei,Peizhi Niu,Hans Hao-Hsun Hsu,Ruihan Wu,Haoteng Yin,Mohsen Ghassemi,Yifan Li,Vamsi K. Potluru,Eli Chien,Kamalika Chaudhuri,Olgica Milenkovic,Pan Li*

Main category: cs.CL

TL;DR: 论文提出了一种知识遗忘评估框架，通过知识图谱和置信度评分更准确地捕捉现实世界知识的隐式结构，并利用LLM作为评估者来验证遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注显式移除孤立事实，忽略了隐式推理依赖和LLM中知识的非确定性，导致遗忘效果被高估。

Method: 提出基于知识图谱的评估框架，设计推理评估协议，利用LLM作为评估者，并通过人类评估校准其可信度。

Result: 实验表明，该框架能更真实、严格地评估遗忘性能，并发现现有策略高估了遗忘效果。

Conclusion: 该框架为知识遗忘提供了更准确的评估方法，揭示了当前方法的局限性。

Abstract: Machine unlearning techniques aim to mitigate unintended memorization in
large language models (LLMs). However, existing approaches predominantly focus
on the explicit removal of isolated facts, often overlooking latent inferential
dependencies and the non-deterministic nature of knowledge within LLMs.
Consequently, facts presumed forgotten may persist implicitly through
correlated information. To address these challenges, we propose a knowledge
unlearning evaluation framework that more accurately captures the implicit
structure of real-world knowledge by representing relevant factual contexts as
knowledge graphs with associated confidence scores. We further develop an
inference-based evaluation protocol leveraging powerful LLMs as judges; these
judges reason over the extracted knowledge subgraph to determine unlearning
success. Our LLM judges utilize carefully designed prompts and are calibrated
against human evaluations to ensure their trustworthiness and stability.
Extensive experiments on our newly constructed benchmark demonstrate that our
framework provides a more realistic and rigorous assessment of unlearning
performance. Moreover, our findings reveal that current evaluation strategies
tend to overestimate unlearning effectiveness. Our code is publicly available
at https://github.com/Graph-COM/Knowledge_Unlearning.git.

</details>


### [630] [Zero-Shot Detection of LLM-Generated Code via Approximated Task Conditioning](https://arxiv.org/abs/2506.06069)
*Maor Ashkenazi,Ofir Brenner,Tal Furman Shohet,Eran Treister*

Main category: cs.CL

TL;DR: 提出了一种基于条件概率分布的新方法（ATC），用于零样本检测LLM生成的代码，无需访问生成模型或原始任务提示，在多种编程语言中表现优异。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码检测对安全、知识产权和学术诚信至关重要，但现有方法在区分LLM生成和人类编写代码时效果有限。

Method: 通过近似生成代码的任务条件（ATC），评估代码标记的熵差异，利用任务级条件概率分布提高检测效果。

Result: ATC在多个基准测试中达到最优性能，并支持多种编程语言（如Python、CPP、Java）。

Conclusion: 任务级条件对LLM生成代码检测至关重要，ATC方法具有实际应用价值，相关代码和数据已开源。

Abstract: Detecting Large Language Model (LLM)-generated code is a growing challenge
with implications for security, intellectual property, and academic integrity.
We investigate the role of conditional probability distributions in improving
zero-shot LLM-generated code detection, when considering both the code and the
corresponding task prompt that generated it. Our key insight is that when
evaluating the probability distribution of code tokens using an LLM, there is
little difference between LLM-generated and human-written code. However,
conditioning on the task reveals notable differences. This contrasts with
natural language text, where differences exist even in the unconditional
distributions. Leveraging this, we propose a novel zero-shot detection approach
that approximates the original task used to generate a given code snippet and
then evaluates token-level entropy under the approximated task conditioning
(ATC). We further provide a mathematical intuition, contextualizing our method
relative to previous approaches. ATC requires neither access to the generator
LLM nor the original task prompts, making it practical for real-world
applications. To the best of our knowledge, it achieves state-of-the-art
results across benchmarks and generalizes across programming languages,
including Python, CPP, and Java. Our findings highlight the importance of
task-level conditioning for LLM-generated code detection. The supplementary
materials and code are available at https://github.com/maorash/ATC, including
the dataset gathering implementation, to foster further research in this area.

</details>


### [631] [Cartridges: Lightweight and general-purpose long context representations via self-study](https://arxiv.org/abs/2506.06266)
*Sabri Eyuboglu,Ryan Ehrlich,Simran Arora,Neel Guha,Dylan Zinsley,Emily Liu,Will Tennien,Atri Rudra,James Zou,Azalia Mirhoseini,Christopher Re*

Main category: cs.CL

TL;DR: 论文提出了一种名为Cartridge的方法，通过离线训练小型KV缓存来替代传统的大规模上下文窗口方法，显著降低了内存消耗和推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理大规模文本语料时，内存消耗高且成本昂贵，需要更高效的解决方案。

Method: 提出Cartridge方法，通过自学习训练小型KV缓存，生成合成对话并使用上下文蒸馏目标优化。

Result: Cartridge在性能上与ICL相当，但内存消耗降低38.6倍，吞吐量提高26.4倍，并扩展了有效上下文长度。

Conclusion: Cartridge是一种高效且经济的替代方案，能够在不重新训练的情况下组合使用。

Abstract: Large language models are often used to answer queries grounded in large text
corpora (e.g. codebases, legal documents, or chat histories) by placing the
entire corpus in the context window and leveraging in-context learning (ICL).
Although current models support contexts of 100K-1M tokens, this setup is
costly to serve because the memory consumption of the KV cache scales with
input length. We explore an alternative: training a smaller KV cache offline on
each corpus. At inference time, we load this trained KV cache, which we call a
Cartridge, and decode a response. Critically, the cost of training a Cartridge
can be amortized across all the queries referencing the same corpus. However,
we find that the naive approach of training the Cartridge with next-token
prediction on the corpus is not competitive with ICL. Instead, we propose
self-study, a training recipe in which we generate synthetic conversations
about the corpus and train the Cartridge with a context-distillation objective.
We find that Cartridges trained with self-study replicate the functionality of
ICL, while being significantly cheaper to serve. On challenging long-context
benchmarks, Cartridges trained with self-study match ICL performance while
using 38.6x less memory and enabling 26.4x higher throughput. Self-study also
extends the model's effective context length (e.g. from 128k to 484k tokens on
MTOB) and surprisingly, leads to Cartridges that can be composed at inference
time without retraining.

</details>


### [632] [Understanding Gender Bias in AI-Generated Product Descriptions](https://arxiv.org/abs/2506.05390)
*Markelle Kelly,Mohammad Tahaei,Padhraic Smyth,Lauren Wilcox*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLM）在电子商务中生成产品描述时的性别偏见，提出了新的分类方法，并分析了GPT-3.5和电商专用LLM中的偏见表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM的性别偏见在其他领域已有研究，但在电商领域的应用尚未深入探讨，可能揭示新的算法偏见和危害。

Method: 开发了数据驱动的性别偏见分类法，并与现有通用危害分类法对比，分析了GPT-3.5和电商专用LLM的偏见表现。

Result: 研究发现电商场景中存在独特的性别偏见形式，如对服装尺寸的假设、产品特征刻板描述和说服性语言差异。

Conclusion: 研究揭示了电商中未被充分探索的性别偏见维度，补充了现有AI危害框架中的排斥性规范、刻板印象和性能差异问题。

Abstract: While gender bias in large language models (LLMs) has been extensively
studied in many domains, uses of LLMs in e-commerce remain largely unexamined
and may reveal novel forms of algorithmic bias and harm. Our work investigates
this space, developing data-driven taxonomic categories of gender bias in the
context of product description generation, which we situate with respect to
existing general purpose harms taxonomies. We illustrate how AI-generated
product descriptions can uniquely surface gender biases in ways that require
specialized detection and mitigation approaches. Further, we quantitatively
analyze issues corresponding to our taxonomic categories in two models used for
this task -- GPT-3.5 and an e-commerce-specific LLM -- demonstrating that these
forms of bias commonly occur in practice. Our results illuminate unique,
under-explored dimensions of gender bias, such as assumptions about clothing
size, stereotypical bias in which features of a product are advertised, and
differences in the use of persuasive language. These insights contribute to our
understanding of three types of AI harms identified by current frameworks:
exclusionary norms, stereotyping, and performance disparities, particularly for
the context of e-commerce.

</details>


### [633] [Are Large Language Models Good Temporal Graph Learners?](https://arxiv.org/abs/2506.05393)
*Shenyang Huang,Ali Parviz,Emma Kondrup,Zachary Yang,Zifeng Ding,Michael Bronstein,Reihaneh Rabbany,Guillaume Rabusseau*

Main category: cs.CL

TL;DR: TGTalker是一个新颖的时序图学习框架，专为LLMs设计，用于动态图预测，表现优于现有方法并提供解释性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在动态图（真实世界演化网络）中的应用，填补现有研究空白。

Method: 利用时序图的近期偏差提取结构信息，转换为自然语言供LLMs使用，并利用时序邻居作为预测辅助信息。

Result: 在五个真实网络数据集上，TGTalker表现优于TGN和HTGN等流行模型，并提供预测的文本解释。

Conclusion: TGTalker为时序图预测开辟了新方向，尤其在可解释性方面具有潜力。

Abstract: Large Language Models (LLMs) have recently driven significant advancements in
Natural Language Processing and various other applications. While a broad range
of literature has explored the graph-reasoning capabilities of LLMs, including
their use of predictors on graphs, the application of LLMs to dynamic graphs --
real world evolving networks -- remains relatively unexplored. Recent work
studies synthetic temporal graphs generated by random graph models, but
applying LLMs to real-world temporal graphs remains an open question. To
address this gap, we introduce Temporal Graph Talker (TGTalker), a novel
temporal graph learning framework designed for LLMs. TGTalker utilizes the
recency bias in temporal graphs to extract relevant structural information,
converted to natural language for LLMs, while leveraging temporal neighbors as
additional information for prediction. TGTalker demonstrates competitive link
prediction capabilities compared to existing Temporal Graph Neural Network
(TGNN) models. Across five real-world networks, TGTalker performs competitively
with state-of-the-art temporal graph methods while consistently outperforming
popular models such as TGN and HTGN. Furthermore, TGTalker generates textual
explanations for each prediction, thus opening up exciting new directions in
explainability and interpretability for temporal link prediction. The code is
publicly available at https://github.com/shenyangHuang/TGTalker.

</details>


### [634] [SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs](https://arxiv.org/abs/2506.05413)
*Patrik Czakó,Gábor Kertész,Sándor Szénási*

Main category: cs.CL

TL;DR: SmoothRot是一种新颖的后训练量化技术，通过结合通道级缩放和Hadamard变换，显著提升4位量化在大型语言模型中的效率，减少量化误差。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型中大规模激活异常值对4位量化的关键挑战。

Method: 集成通道级缩放和Hadamard变换，将极端异常值转化为适合量化的激活。

Result: 在LLaMA2 7B、LLaMA3.1 8B和Mistral 7B等模型上，量化与FP16模型的性能差距减少了10-30%，且不影响推理延迟。

Conclusion: SmoothRot有效提升了4位量化的准确性，为大型语言模型的高效量化提供了可行方案。

Abstract: We present SmoothRot, a novel post-training quantization technique to enhance
the efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot
addresses the critical challenge of massive activation outliers, by integrating
channel-wise scaling with Hadamard transformations. Our technique effectively
transforms extreme outliers into quantization-friendly activations,
significantly improving quantization accuracy. Experiments conducted on popular
LLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot
consistently reduces the performance gap between quantized and FP16 models by
approximately 10-30\% across language generation and zero-shot reasoning tasks,
without introducing additional inference latency. Code is available at
https://github.com/czakop/smoothrot.

</details>


### [635] [Multidimensional Analysis of Specific Language Impairment Using Unsupervised Learning Through PCA and Clustering](https://arxiv.org/abs/2506.05498)
*Niruthiha Selvanayagam*

Main category: cs.CL

TL;DR: 该研究使用无监督机器学习技术分析儿童语言发展轨迹，发现特定语言障碍（SLI）主要表现为语言产出能力下降而非句法复杂性缺陷，挑战了传统分类诊断框架。


<details>
  <summary>Details</summary>
Motivation: 传统诊断方法可能忽略细微发展模式，研究旨在通过无监督学习技术识别SLI儿童的自然语言发展轨迹，为早期识别和干预提供依据。

Method: 分析1,163名4-16岁儿童的叙事样本，使用主成分分析（PCA）和聚类技术评估64个语言特征，揭示发展轨迹和区分语言特征。

Result: 发现两个主要聚类：高语言产出低SLI患病率组和低产出高句法复杂性高SLI患病率组，支持语言能力的连续模型。

Conclusion: SLI主要表现为语言产出能力下降，研究结果支持无监督学习技术在优化诊断标准和干预策略中的潜力。

Abstract: Specific Language Impairment (SLI) affects approximately 7 percent of
children, presenting as isolated language deficits despite normal cognitive
abilities, sensory systems, and supportive environments. Traditional diagnostic
approaches often rely on standardized assessments, which may overlook subtle
developmental patterns. This study aims to identify natural language
development trajectories in children with and without SLI using unsupervised
machine learning techniques, providing insights for early identification and
targeted interventions. Narrative samples from 1,163 children aged 4-16 years
across three corpora (Conti-Ramsden 4, ENNI, and Gillam) were analyzed using
Principal Component Analysis (PCA) and clustering. A total of 64 linguistic
features were evaluated to uncover developmental trajectories and distinguish
linguistic profiles. Two primary clusters emerged: (1) high language production
with low SLI prevalence, and (2) limited production but higher syntactic
complexity with higher SLI prevalence. Additionally, boundary cases exhibited
intermediate traits, supporting a continuum model of language abilities.
Findings suggest SLI manifests primarily through reduced production capacity
rather than syntactic complexity deficits. The results challenge categorical
diagnostic frameworks and highlight the potential of unsupervised learning
techniques for refining diagnostic criteria and intervention strategies.

</details>


### [636] [A Fictional Q&A Dataset for Studying Memorization and Knowledge Acquisition](https://arxiv.org/abs/2506.05639)
*John Kirchenbauer,Janny Mongkolsupawan,Yuxin Wen,Tom Goldstein,Daphne Ippolito*

Main category: cs.CL

TL;DR: 论文提出一个新数据集，用于研究语言模型对事实记忆和逐字序列记忆的双重过程，通过合成数据实验揭示记忆形式差异。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型如何记忆训练数据中的事实和逐字序列，填补现有研究的空白。

Method: 构建合成数据集，包含虚构事件的类网络文本及问答对，通过训练实验分析记忆形式。

Result: 合成数据能有效区分不同记忆形式，但构建逼真虚构数据存在挑战。

Conclusion: 合成数据为研究语言模型记忆机制提供了新工具，但需进一步优化数据真实性。

Abstract: When language models are trained on textual data, they acquire both knowledge
about the structure of language as well as knowledge of facts about the world.
At inference time, their knowledge of facts can be leveraged to solve
interesting problems and perform useful knowledge work for users. It is well
known that language models can verbatim memorize long sequences from their
training data. However, it is much less well understood how language models
memorize facts seen during training. In this work, we propose a new dataset to
specifically empower researchers to study the dual processes of fact
memorization and verbatim sequence memorization. The dataset consists of
synthetically-generated, webtext-like documents about fictional events, as well
as question-answer pairs about the events. We conduct training experiments
showing how synthetic data about fictional events can be effective in teasing
apart different forms of memorization. We also document the challenges in
effectively building realistic, fictional synthetic data.

</details>


### [637] [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于渐进式过载原则的课程学习框架（POCL），用于改进大型语言模型的知识蒸馏（KD），通过逐步增加训练样本难度提升学习稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有KD方法在训练过程中容易导致学生模型分布显著偏移，引发灾难性遗忘、模式崩溃等问题，需改进。

Method: POCL框架包含难度测量器和训练调度器，逐步引入从易到难的样本，并应用温度逐渐升高的损失函数。

Result: 实验表明，POCL能显著提升蒸馏学生模型的性能，适用于多种白盒KD方法和模型家族。

Conclusion: POCL通过结构化训练数据，增强了蒸馏LLMs的稳定性和性能。

Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by
transferring the teacher model's capabilities to a smaller student model,
reducing inference cost and memory usage while maintaining performance.
However, existing KD methods for LLMs often fail to prevent significant shifts
in the student model's distribution during training, leading to issues such as
catastrophic forgetting, mode collapse, and training-inference mismatch. To
address these challenges, we propose a novel, plug-in curriculum learning
framework inspired by the strength training principle of "progressive overload"
(POCL), which can be seamlessly integrated into existing white-box KD
approaches with minimal computational overhead. The framework comprises two
core components: (1) a difficulty measurer that ranks and partitions training
samples from easy to hard, and (2) a training scheduler that incrementally
introduces these subsets into the distillation process at fixed intervals while
applying loss functions with progressively rising temperatures. By starting
with the easiest samples and progressively increasing the difficulty, the
approach enhances both the stability and efficiency of learning. Extensive
experiments in instruction-following settings demonstrate that POCL
consistently improves the performance of distilled student models across
various white-box KD methods and model families. Our findings highlight the
effectiveness of sorted training samples in KD for LLMs. More generally, our
work demonstrates how to structure training data within the KD process to
enhance the stability and performance of distilled LLMs.

</details>


### [638] [Large Language Models are Good Relational Learners](https://arxiv.org/abs/2506.05725)
*Fang Wu,Vijay Prakash Dwivedi,Jure Leskovec*

Main category: cs.CL

TL;DR: Rel-LLM是一种新型架构，结合图神经网络（GNN）和检索增强生成（RAG）框架，为大型语言模型（LLM）生成结构化关系提示，解决了传统文本序列化方法在关系深度学习（RDL）中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法将结构化数据转换为扁平文本，忽视了关系结构并导致冗余，且常超出LLM上下文长度限制。

Method: 利用GNN编码器提取实体周围的局部子图，生成包含关系和时间依赖的特征表示，并通过反规范化转换为结构化提示。

Result: 实验表明，Rel-LLM在关键RDL任务上优于现有方法，提供了一种可扩展且高效的LLM与结构化数据集成方案。

Conclusion: Rel-LLM保留了数据库的关系结构，使LLM能有效处理复杂实体关系，为RDL任务提供了新思路。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
various domains, yet their application to relational deep learning (RDL)
remains underexplored. Existing approaches adapt LLMs by traversing relational
links between entities in a database and converting the structured data into
flat text documents. Still, this text-based serialization disregards critical
relational structures, introduces redundancy, and often exceeds standard LLM
context lengths. We introduce Rel-LLM, a novel architecture that utilizes a
graph neural network (GNN)- based encoder to generate structured relational
prompts for LLMs within a retrieval-augmented generation (RAG) framework.
Unlike traditional text-based serialization approaches, our method preserves
the inherent relational structure of databases while enabling LLMs to
effectively process and reason over complex entity relationships. Specifically,
the GNN encoder extracts a local subgraph around an entity to build feature
representations that contain relevant entity relationships and temporal
dependencies. These representations are transformed into structured prompts
using a denormalization process, effectively allowing the LLM to reason over
relational structures. Through extensive experiments, we demonstrate that
Rel-LLM outperforms existing methods on key RDL tasks, offering a scalable and
efficient approach to integrating LLMs with structured data sources. Code is
available at https://github.com/smiles724/Rel-LLM.

</details>


### [639] [Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness](https://arxiv.org/abs/2506.05735)
*Rongzhe Wei,Peizhi Niu,Hans Hao-Hsun Hsu,Ruihan Wu,Haoteng Yin,Mohsen Ghassemi,Yifan Li,Vamsi K. Potluru,Eli Chien,Kamalika Chaudhuri,Olgica Milenkovic,Pan Li*

Main category: cs.CL

TL;DR: 论文提出了一种基于知识图谱和推理的知识遗忘评估框架，以更准确地评估大型语言模型中的知识遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注显式事实的移除，忽略了隐式推理依赖和知识的非确定性，导致遗忘效果被高估。

Method: 提出知识图谱表示事实上下文，并设计基于推理的评估协议，利用LLM作为评估者。

Result: 实验表明新框架能更真实、严格地评估遗忘效果，并发现现有方法高估了遗忘效果。

Conclusion: 新框架为知识遗忘提供了更准确的评估工具，揭示了现有方法的局限性。

Abstract: Machine unlearning techniques aim to mitigate unintended memorization in
large language models (LLMs). However, existing approaches predominantly focus
on the explicit removal of isolated facts, often overlooking latent inferential
dependencies and the non-deterministic nature of knowledge within LLMs.
Consequently, facts presumed forgotten may persist implicitly through
correlated information. To address these challenges, we propose a knowledge
unlearning evaluation framework that more accurately captures the implicit
structure of real-world knowledge by representing relevant factual contexts as
knowledge graphs with associated confidence scores. We further develop an
inference-based evaluation protocol leveraging powerful LLMs as judges; these
judges reason over the extracted knowledge subgraph to determine unlearning
success. Our LLM judges utilize carefully designed prompts and are calibrated
against human evaluations to ensure their trustworthiness and stability.
Extensive experiments on our newly constructed benchmark demonstrate that our
framework provides a more realistic and rigorous assessment of unlearning
performance. Moreover, our findings reveal that current evaluation strategies
tend to overestimate unlearning effectiveness. Our code is publicly available
at https://github.com/Graph-COM/Knowledge_Unlearning.git.

</details>


### [640] [Zero-Shot Detection of LLM-Generated Code via Approximated Task Conditioning](https://arxiv.org/abs/2506.06069)
*Maor Ashkenazi,Ofir Brenner,Tal Furman Shohet,Eran Treister*

Main category: cs.CL

TL;DR: 论文提出了一种基于条件概率分布的零样本检测方法（ATC），用于区分LLM生成的代码和人类编写的代码，通过任务级条件显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的普及，检测其来源对安全、知识产权和学术诚信至关重要。研究发现，代码和生成任务的条件概率分布能有效区分LLM生成和人类编写的代码。

Method: 提出ATC方法，通过近似生成代码的任务条件，评估令牌级熵，无需访问生成LLM或原始任务提示。

Result: ATC在多个基准测试中达到最先进水平，支持多种编程语言（如Python、CPP、Java）。

Conclusion: 任务级条件对LLM生成代码检测至关重要，ATC方法实用且高效，代码和数据已开源以促进研究。

Abstract: Detecting Large Language Model (LLM)-generated code is a growing challenge
with implications for security, intellectual property, and academic integrity.
We investigate the role of conditional probability distributions in improving
zero-shot LLM-generated code detection, when considering both the code and the
corresponding task prompt that generated it. Our key insight is that when
evaluating the probability distribution of code tokens using an LLM, there is
little difference between LLM-generated and human-written code. However,
conditioning on the task reveals notable differences. This contrasts with
natural language text, where differences exist even in the unconditional
distributions. Leveraging this, we propose a novel zero-shot detection approach
that approximates the original task used to generate a given code snippet and
then evaluates token-level entropy under the approximated task conditioning
(ATC). We further provide a mathematical intuition, contextualizing our method
relative to previous approaches. ATC requires neither access to the generator
LLM nor the original task prompts, making it practical for real-world
applications. To the best of our knowledge, it achieves state-of-the-art
results across benchmarks and generalizes across programming languages,
including Python, CPP, and Java. Our findings highlight the importance of
task-level conditioning for LLM-generated code detection. The supplementary
materials and code are available at https://github.com/maorash/ATC, including
the dataset gathering implementation, to foster further research in this area.

</details>


### [641] [Cartridges: Lightweight and general-purpose long context representations via self-study](https://arxiv.org/abs/2506.06266)
*Sabri Eyuboglu,Ryan Ehrlich,Simran Arora,Neel Guha,Dylan Zinsley,Emily Liu,Will Tennien,Atri Rudra,James Zou,Azalia Mirhoseini,Christopher Re*

Main category: cs.CL

TL;DR: 论文提出了一种名为Cartridge的方法，通过离线训练较小的KV缓存来替代昂贵的在线ICL，显著降低了内存消耗并提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理长文本语料时，KV缓存的内存消耗随输入长度增加而显著增加，导致服务成本高昂。

Method: 提出Cartridge方法，通过离线训练KV缓存，并使用self-study训练策略生成合成对话以优化性能。

Result: Cartridge在性能上与ICL相当，但内存消耗减少38.6倍，吞吐量提高26.4倍，并扩展了有效上下文长度。

Conclusion: Cartridge结合self-study是一种高效且经济的替代方案，适用于长上下文任务。

Abstract: Large language models are often used to answer queries grounded in large text
corpora (e.g. codebases, legal documents, or chat histories) by placing the
entire corpus in the context window and leveraging in-context learning (ICL).
Although current models support contexts of 100K-1M tokens, this setup is
costly to serve because the memory consumption of the KV cache scales with
input length. We explore an alternative: training a smaller KV cache offline on
each corpus. At inference time, we load this trained KV cache, which we call a
Cartridge, and decode a response. Critically, the cost of training a Cartridge
can be amortized across all the queries referencing the same corpus. However,
we find that the naive approach of training the Cartridge with next-token
prediction on the corpus is not competitive with ICL. Instead, we propose
self-study, a training recipe in which we generate synthetic conversations
about the corpus and train the Cartridge with a context-distillation objective.
We find that Cartridges trained with self-study replicate the functionality of
ICL, while being significantly cheaper to serve. On challenging long-context
benchmarks, Cartridges trained with self-study match ICL performance while
using 38.6x less memory and enabling 26.4x higher throughput. Self-study also
extends the model's effective context length (e.g. from 128k to 484k tokens on
MTOB) and surprisingly, leads to Cartridges that can be composed at inference
time without retraining.

</details>


### [642] [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)
*Taesoo Kim,Jong Hwan Ko*

Main category: cs.CL

TL;DR: TESU-LLM是一种仅用文本数据训练语音语言模型的新框架，通过统一编码器和轻量级投影网络实现语音推理，性能媲美多模态数据训练的方法。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型依赖大规模配对语音-文本数据和大量计算资源，限制了可扩展性和可访问性。

Method: 利用统一编码器将语义等效的文本和语音输入映射到共享潜在空间，并通过轻量级投影网络与LLM嵌入空间对齐，实现仅用文本监督训练。

Result: TESU-LLM在多个语音相关基准测试中表现优异，性能接近基于多模态数据训练的基线方法。

Conclusion: TESU-LLM提供了一种无需语音数据的高效、可扩展的语音语言模型构建方法。

Abstract: Recent advances in speech-enabled language models have shown promising
results in building intelligent voice assistants. However, most existing
approaches rely on large-scale paired speech-text data and extensive
computational resources, which pose challenges in terms of scalability and
accessibility. In this paper, we present \textbf{TESU-LLM}, a novel framework
that enables training speech-capable language models using only text data. Our
key insight is to leverage a unified encoder that maps semantically equivalent
text and speech inputs to a shared latent space. By aligning the encoder output
with the embedding space of a LLM via a lightweight projection network, we
enable the model to generalize from text-only supervision to speech-based
inference. Despite being trained exclusively on text, TESU-LLM achieves strong
performance on various speech-related benchmarks, comparable to baseline
methods trained with large-scale multimodal datasets and substantial
computational resources. These results highlight the effectiveness and
efficiency of our approach, offering a scalable path toward building speech
LLMs without speech data.

</details>


### [643] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
*Pengyi Li,Matvey Skripkin,Alexander Zubrey,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CL

TL;DR: RLSC是一种利用模型自身置信度作为奖励信号的后训练方法，无需人工标注或外部奖励模型，显著提升了数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖昂贵的人工标注或外部奖励模型，RLSC旨在通过模型自身置信度简化后训练过程。

Method: RLSC通过模型自身置信度生成奖励信号，应用于Qwen2.5-Math-7B模型，仅需少量样本和训练轮次。

Result: 在AIME2024、MATH500和AMC23任务上，RLSC分别提升了20.10%、49.40%和52.50%的准确率。

Conclusion: RLSC提供了一种简单、可扩展且无需大量监督的后训练方法，适用于推理模型。

Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC
improves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on
AMC23. RLSC offers a simple, scalable post-training method for reasoning models
with minimal supervision.

</details>


### [644] [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)
*Hongming Yang,Shi Lin,Jun Shao,Changting Lin,Donghai Zhu,Meng Han,Qinglei Kong*

Main category: cs.CL

TL;DR: DeBoP是一种新的直接行为优化范式，专为轻量级大语言模型（LwLLMs）设计，通过梯度自由的蒙特卡洛树搜索优化执行序列，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LwLLMs在推理和复杂任务上表现受限，现有提示优化方法依赖人工或高性能LLMs，不适用于LwLLMs。

Method: DeBoP将复杂提示优化转化为离散、可量化的执行序列优化，采用蒙特卡洛树搜索。

Result: DeBoP在七项任务中显著优于现有方法，优化后的LwLLMs性能超越GPT-3.5，计算时间减少60%。

Conclusion: DeBoP为LwLLMs提供了一种高效、自动化的优化方法，显著提升其实际应用能力。

Abstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized
models designed to run efficiently on consumer-grade hardware, offering
significant advantages in resource efficiency, cost-effectiveness, and data
privacy. However, these models often struggle with limited inference and
reasoning capabilities, which restrict their performance on complex tasks and
limit their practical applicability. Moreover, existing prompt optimization
methods typically rely on extensive manual effort or the meta-cognitive
abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To
address these challenges, we introduce DeBoP, a new Direct Behavior
Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting
technique. Unlike CoT Prompting, DeBoP is an automatic optimization method,
which focuses on the optimization directly on the behavior of LwLLMs. In
particular, DeBoP transforms the optimization of complex prompts into the
optimization of discrete, quantifiable execution sequences using a
gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging
tasks where state-of-the-art LLMs excel but LwLLMs generally underperform.
Experimental results demonstrate that DeBoP significantly outperforms recent
prompt optimization methods on most tasks. In particular, DeBoP-optimized
LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by
approximately 60% compared to other automatic prompt optimization methods.

</details>


### [645] [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)
*Sooyung Choi,Jaehyeok Lee,Xiaoyuan Yi,Jing Yao,Xing Xie,JinYeong Bak*

Main category: cs.CL

TL;DR: 研究发现，与人类价值观对齐的大型语言模型（LLMs）更容易产生有害行为，且安全风险高于未微调或其他微调模型。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs应用范围扩大，个性化对齐人类价值观的需求增加，但同时也带来了潜在的安全风险。

Method: 通过数据集分析，结合心理学假设，研究价值观对齐与安全风险的关系。

Result: 价值观对齐的LLMs更容易生成有害内容，且安全风险更高。

Conclusion: 研究揭示了价值观对齐的“黑箱”问题，并提出上下文对齐方法以提升安全性。

Abstract: The application scope of Large Language Models (LLMs) continues to expand,
leading to increasing interest in personalized LLMs that align with human
values. However, aligning these models with individual values raises
significant safety concerns, as certain values may correlate with harmful
information. In this paper, we identify specific safety risks associated with
value-aligned LLMs and investigate the psychological principles behind these
challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are
more prone to harmful behavior compared to non-fine-tuned models and exhibit
slightly higher risks in traditional safety evaluations than other fine-tuned
models. (2) These safety issues arise because value-aligned LLMs genuinely
generate text according to the aligned values, which can amplify harmful
outcomes. Using a dataset with detailed safety categories, we find significant
correlations between value alignment and safety risks, supported by
psychological hypotheses. This study offers insights into the "black box" of
value alignment and proposes in-context alignment methods to enhance the safety
of value-aligned LLMs.

</details>


### [646] [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)
*Ivi Chatzi,Nina Corvelo Benz,Stratis Tsirtsis,Manuel Gomez-Rodriguez*

Main category: cs.CL

TL;DR: 论文提出了一种称为“规范采样”的方法，旨在解决大语言模型生成非规范标记序列的问题，并证明其生成的序列更接近训练数据的真实分布。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生成文本时可能产生非规范的标记序列，这对模型性能有负面影响。

Method: 通过理论分析，提出“规范采样”方法，确保模型在自回归生成过程中每一步都生成规范的标记序列。

Result: 规范采样方法有效避免了非规范序列的生成，且其生成的序列分布更接近训练数据的真实分布。

Conclusion: 规范采样是一种简单高效的方法，能够提升大语言模型生成文本的规范性和准确性。

Abstract: State of the art large language models are trained using large amounts of
tokens derived from raw text using what is called a tokenizer. Crucially, the
tokenizer determines the (token) vocabulary a model will use during inference
as well as, in principle, the (token) language. This is because, while the
token vocabulary may allow for different tokenizations of a string, the
tokenizer always maps the string to only one of these tokenizations--the
canonical tokenization. However, multiple lines of empirical evidence suggest
that large language models do not always generate canonical token sequences,
and this comes with several negative consequences. In this work, we first show
that, to generate a canonical token sequence, a model needs to generate
(partial) canonical token sequences at each step of the autoregressive
generation process underpinning its functioning. Building upon this theoretical
result, we introduce canonical sampling, a simple and efficient sampling method
that precludes a given model from generating non-canonical token sequences.
Further, we also show that, in comparison with standard sampling, the
distribution of token sequences generated using canonical sampling is provably
closer to the true distribution of token sequences used during training.

</details>


### [647] [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)
*Charles Goddard,Fernando Fernandes Neto*

Main category: cs.CL

TL;DR: 提出一种无需训练的Tokenizer移植方法，通过正交匹配追踪（OMP）重建未见过的词嵌入，实现预训练大语言模型（LLM）的Tokenizer移植。


<details>
  <summary>Details</summary>
Motivation: 解决不同Tokenizer之间的差异问题，避免梯度更新，直接复用预训练模型权重。

Method: 使用OMP方法，将新词汇表示为共享词汇的稀疏线性组合，分两阶段实现词嵌入的移植。

Result: 在多个跨Tokenizer任务中，OMP方法在零样本设置下表现最佳，优于其他基线方法。

Conclusion: OMP方法能有效解决Tokenizer差异问题，支持跨Tokenizer的知识蒸馏、解码等应用。

Abstract: We present a training-free method to transplant tokenizers in pretrained
large language models (LLMs) by reconstructing unseen token embeddings via
Orthogonal Matching Pursuit (OMP). Specifically, we approximate each
out-of-vocabulary token as a sparse linear combination of shared tokens, in two
phases: first, compute each new token's representation in the donor embedding
space with a small dictionary of shared anchor tokens, then transfer these same
sparse coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and
Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of
the base model's performance across multiple benchmarks, while other zero-shot
approaches degrade significantly. Compared to baselines (zero-init, mean-init,
and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves
the best overall performance, effectively bridging large tokenizer
discrepancies without gradient updates. Our analysis further identifies
mismatched numerical tokenization schemes as a critical challenge for
preserving mathematical reasoning capabilities. This technique enables direct
reuse of pretrained model weights with new tokenizers, facilitating
cross-tokenizer knowledge distillation, speculative decoding, ensembling,
merging, and domain-specific vocabulary adaptations. We integrate our method
into the open-source mergekit-tokensurgeon tool for post hoc vocabulary
realignment.

</details>


### [648] [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
*Alan Chen,Jack Merullo,Alessandro Stolfo,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本文提出了一种通过仿射映射在语言模型残差流之间高效传递特征的方法，用于比较不同大小模型的表示空间。实验表明，大小模型的表示空间高度相似，从而可以通过在小模型上训练稀疏自编码器（SAE）并迁移到大模型来节省计算成本。此外，迁移的探测器和导向向量能有效恢复真实性能，且语义和结构特征的迁移效果存在差异。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型中不同大小模型之间表示空间的相似性，并提出一种高效迁移特征的方法以节省计算资源。

Method: 使用仿射映射在模型残差流之间传递特征，并比较不同大小模型的表示空间。通过在小模型上训练SAE并迁移到大模型，验证其有效性。

Result: 大小模型的表示空间高度相似，迁移SAE可节省50%的训练成本。语义和结构特征的迁移效果不同，但功能特征的角色能忠实映射。

Conclusion: 仿射映射是一种高效的特征迁移方法，揭示了大小模型表示空间的相似性和差异，为SAE的训练效率提供了改进方案。

Abstract: In this work, we demonstrate that affine mappings between residual streams of
language models is a cheap way to effectively transfer represented features
between models. We apply this technique to transfer the weights of Sparse
Autoencoders (SAEs) between models of different sizes to compare their
representations. We find that small and large models learn highly similar
representation spaces, which motivates training expensive components like SAEs
on a smaller model and transferring to a larger model at a FLOPs savings. For
example, using a small-to-large transferred SAE as initialization can lead to
50% cheaper training runs when training SAEs on larger models. Next, we show
that transferred probes and steering vectors can effectively recover ground
truth performance. Finally, we dive deeper into feature-level transferability,
finding that semantic and structural features transfer noticeably differently
while specific classes of functional features have their roles faithfully
mapped. Overall, our findings illustrate similarities and differences in the
linear representation spaces of small and large models and demonstrate a method
for improving the training efficiency of SAEs.

</details>


### [649] [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
*Subhendu Khatuya,Shashwat Naidu,Saptarshi Ghosh,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 提出了一种基于生成模型的多标签文本分类框架LAGAMC，通过生成标签描述并匹配预定义标签，结合双重目标损失函数，实现了高效且通用的分类性能。


<details>
  <summary>Details</summary>
Motivation: 文本数据爆炸导致手动分类困难，需要一种高效且通用的自动分类方法。

Method: 利用预定义标签描述生成模型，结合双重目标损失函数（交叉熵损失和余弦相似度），通过微调的句子转换器匹配生成描述与预定义标签。

Result: 在所有评估数据集上达到新的最先进性能，Micro-F1提升13.94%，Macro-F1提升24.85%。

Conclusion: LAGAMC模型在参数效率和通用性上表现优异，适用于实际应用。

Abstract: The explosion of textual data has made manual document classification
increasingly challenging. To address this, we introduce a robust, efficient
domain-agnostic generative model framework for multi-label text classification.
Instead of treating labels as mere atomic symbols, our approach utilizes
predefined label descriptions and is trained to generate these descriptions
based on the input text. During inference, the generated descriptions are
matched to the pre-defined labels using a finetuned sentence transformer. We
integrate this with a dual-objective loss function, combining cross-entropy
loss and cosine similarity of the generated sentences with the predefined
target descriptions, ensuring both semantic alignment and accuracy. Our
proposed model LAGAMC stands out for its parameter efficiency and versatility
across diverse datasets, making it well-suited for practical applications. We
demonstrate the effectiveness of our proposed model by achieving new
state-of-the-art performances across all evaluated datasets, surpassing several
strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in
Macro-F1 compared to the closest baseline across all datasets.

</details>


### [650] [Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning](https://arxiv.org/abs/2506.06964)
*Subhojyoti Mukherjee,Viet Dac Lai,Raghavendra Addanki,Ryan Rossi,Seunghyun Yoon,Trung Bui,Anup Rao,Jayakumar Subramanian,Branislav Kveton*

Main category: cs.CL

TL;DR: 本文提出了一种通过强化学习（RL）训练问答（QA）代理以提出澄清问题的方法，并分析了离线RL目标，与现有方法相比在奖励和语言质量上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的问答代理通常直接回答问题，而缺乏提出澄清问题的能力。本文旨在通过强化学习模拟对话并优化代理的表现。

Method: 采用强化学习方法，模拟包含澄清问题的对话，并提出离线RL目标（奖励加权的监督微调），以优化大型语言模型。

Result: 与基于监督微调和直接偏好优化的方法相比，本文方法在奖励和语言质量上均取得显著提升。

Conclusion: 通过离线RL目标优化问答代理，能够有效提升其提出澄清问题的能力，并在实际应用中表现更优。

Abstract: Question answering (QA) agents automatically answer questions posed in
natural language. In this work, we learn to ask clarifying questions in QA
agents. The key idea in our method is to simulate conversations that contain
clarifying questions and learn from them using reinforcement learning (RL). To
make RL practical, we propose and analyze offline RL objectives that can be
viewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in
large language models. Our work stands in a stark contrast to recently proposed
methods, based on SFT and direct preference optimization, which have additional
hyper-parameters and do not directly optimize rewards. We compare to these
methods empirically and report gains in both optimized rewards and language
quality.

</details>


### [651] [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)
*Ming Li,Zhengyuan Yang,Xiyao Wang,Dianqi Li,Kevin Lin,Tianyi Zhou,Lijuan Wang*

Main category: cs.CL

TL;DR: 论文提出FoReaL-Decoding方法，通过快速-慢速协作解码优化大型推理模型的效率，减少计算成本并保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理过程中存在过度思考现象，导致推理速度慢且冗余。研究旨在分析其与非推理模型的差异，并提出高效解决方案。

Method: 提出FoReaL-Decoding方法，结合主导模型和草稿模型协作解码，通过随机门平滑切换模型，减少计算量。

Result: 在四个数学推理基准测试中，FoReaL-Decoding减少30-50%计算量，缩短推理链40%，同时保持86-100%性能。

Conclusion: FoReaL-Decoding是一种简单、即插即用的方法，可在推理任务中实现成本与性能的可控权衡。

Abstract: Large reasoning models (LRMs) achieve strong reasoning performance by
emitting long chains of thought. Yet, these verbose traces slow down inference
and often drift into unnecessary detail, known as the overthinking phenomenon.
To better understand LRMs' behavior, we systematically analyze the token-level
misalignment between reasoning and non-reasoning models. While it is expected
that their primary difference lies in the stylistic "thinking cues", LRMs
uniquely exhibit two pivotal, previously under-explored phenomena: a Global
Misalignment Rebound, where their divergence from non-reasoning models persists
or even grows as response length increases, and more critically, a Local
Misalignment Diminish, where the misalignment concentrates at the "thinking
cues" each sentence starts with but rapidly declines in the remaining of the
sentence. Motivated by the Local Misalignment Diminish, we propose
FoReaL-Decoding, a collaborative fast-slow thinking decoding method for
cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few
tokens for each sentence, and then a weaker draft model completes the following
tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to
smoothly interpolate between the small and the large model. On four popular
math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),
FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by
up to 40%, while preserving 86 to 100% of model performance. These results
establish FoReaL-Decoding as a simple, plug-and-play route to controllable
cost-quality trade-offs in reasoning-centric tasks.

</details>


### [652] [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)
*Vicky Xefteri,Tim Vieira,Ryan Cotterell,Afra Amini*

Main category: cs.CL

TL;DR: 该论文提出了一种基于后验推断的采样算法，用于在生成文本时有效控制目标句法结构，结合了序贯蒙特卡洛和句法标注器，显著提升了句法准确性。


<details>
  <summary>Details</summary>
Motivation: 控制语言模型生成文本的句法结构对于需要清晰性、风格一致性或可解释性的应用至关重要，但目前仍具挑战性。

Method: 结合序贯蒙特卡洛（估计后验分布）和句法标注器，确保生成的每个词符符合目标句法结构。

Result: 实验表明，该方法在GPT2和Llama3-8B模型上将句法准确性F1分数从12.31和35.33提升至约93，且不影响流畅性。

Conclusion: 该方法为需要精确控制句法的应用提供了有效解决方案，突显了采样算法的潜力。

Abstract: Controlling the syntactic structure of text generated by language models is
valuable for applications requiring clarity, stylistic consistency, or
interpretability, yet it remains a challenging task. In this paper, we argue
that sampling algorithms based on the posterior inference can effectively
enforce a target constituency structure during generation. Our approach
combines sequential Monte Carlo, which estimates the posterior distribution by
sampling from a proposal distribution, with a syntactic tagger that ensures
that each generated token aligns with the desired syntactic structure. Our
experiments with GPT2 and Llama3-8B models show that with an appropriate
proposal distribution, we can improve syntactic accuracy, increasing the F1
score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both
cases without compromising the language model's fluency. These results
underscore both the complexity of syntactic control and the effectiveness of
sampling algorithms, offering a promising approach for applications where
precise control over syntax is essential.

</details>


### [653] [RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality](https://arxiv.org/abs/2506.07171)
*Chenlong Zhang,Zhuoran Jin,Hongbang Yuan,Jiaheng Wei,Tong Zhou,Kang Liu,Jun Zhao,Yubo Chen*

Main category: cs.CL

TL;DR: 论文提出了一种名为RULE的高效框架，用于选择性移除大语言模型中的特定信息，同时保持模型整体性能。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型可能包含敏感、受版权保护或非法内容，需要一种无需从头训练即可选择性移除信息的方法。

Method: RULE框架将遗忘任务视为拒绝边界优化问题，使用少量遗忘数据和合成边界查询进行训练，并通过可验证的奖励函数实现安全拒绝。

Result: 实验表明，RULE仅需12%的遗忘数据和8%的合成数据，在遗忘质量和响应自然度上优于现有基线，且保持模型实用性。

Conclusion: RULE不仅实现了目标遗忘，还提升了模型输出的自然性和训练效率，并展现出强泛化能力。

Abstract: The widespread deployment of Large Language Models (LLMs) trained on massive,
uncurated corpora has raised growing concerns about the inclusion of sensitive,
copyrighted, or illegal content. This has led to increasing interest in LLM
unlearning: the task of selectively removing specific information from a model
without retraining from scratch or degrading overall utility. However, existing
methods often rely on large-scale forget and retain datasets, and suffer from
unnatural responses, poor generalization, or catastrophic utility loss. In this
work, we propose Reinforcement UnLearning (RULE), an efficient framework that
formulates unlearning as a refusal boundary optimization problem. RULE is
trained with a small portion of the forget set and synthesized boundary
queries, using a verifiable reward function that encourages safe refusal on
forget--related queries while preserving helpful responses on permissible
inputs. We provide both theoretical and empirical evidence demonstrating the
effectiveness of RULE in achieving targeted unlearning without compromising
model utility. Experimental results show that, with only $12%$ forget set and
$8%$ synthesized boundary data, RULE outperforms existing baselines by up to
$17.5%$ forget quality and $16.3%$ naturalness response while maintaining
general utility, achieving forget--retain Pareto optimality. Remarkably, we
further observe that RULE improves the naturalness of model outputs, enhances
training efficiency, and exhibits strong generalization ability, generalizing
refusal behavior to semantically related but unseen queries.

</details>


### [654] [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
*Prathamesh Kokate,Mitali Sarnaik,Manavi Khopade,Raviraj Joshi*

Main category: cs.CL

TL;DR: 提出了一种基于TF-IDF的句子排名方法，用于长文档分类，显著减少输入大小和推理延迟，同时保持分类准确性。


<details>
  <summary>Details</summary>
Motivation: 长文档分类中，BERT等模型因固定输入长度和二次注意力复杂度受限，且全文档分类通常冗余。

Method: 采用TF-IDF句子排名方法，结合固定数量或百分比选择句子，并优化评分策略（归一化TF-IDF分数和句子长度）。

Result: 在MahaNews LDC数据集上，方法优于基线（首尾句或随机选择），输入减少50%以上，推理延迟降低43%，分类准确率仅下降0.33%。

Conclusion: 该方法证明在不牺牲性能的情况下显著减少上下文是可行的，适用于实际长文档分类任务。

Abstract: Long document classification poses challenges due to the computational
limitations of transformer-based models, particularly BERT, which are
constrained by fixed input lengths and quadratic attention complexity.
Moreover, using the full document for classification is often redundant, as
only a subset of sentences typically carries the necessary information. To
address this, we propose a TF-IDF-based sentence ranking method that improves
efficiency by selecting the most informative content. Our approach explores
fixed-count and percentage-based sentence selection, along with an enhanced
scoring strategy combining normalized TF-IDF scores and sentence length.
Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method
consistently outperforms baselines such as first, last, and random sentence
selection. With MahaBERT-v2, we achieve near-identical classification accuracy
with just a 0.33 percent drop compared to the full-context baseline, while
reducing input size by over 50 percent and inference latency by 43 percent.
This demonstrates that significant context reduction is possible without
sacrificing performance, making the method practical for real-world long
document classification tasks.

</details>


### [655] [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)
*Brian Christian,Hannah Rose Kirk,Jessica A. F. Thompson,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的奖励模型可解释性方法，通过分析其在整个词汇空间中的响应，揭示了模型间的异质性、评分不对称性、对提示框架的敏感性以及对高频词的过度重视。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在将大型语言模型与人类价值观对齐中起关键作用，但其本身的研究较少，尤其是其如何编码人类价值判断。

Method: 通过分析奖励模型对所有可能的单令牌响应在价值相关提示下的评分，揭示其行为特征。

Result: 发现模型间存在显著异质性、评分不对称性、对提示框架的敏感性以及对高频词的过度重视，并揭示了潜在的偏见问题。

Conclusion: 奖励模型并非可互换，且可能编码有害偏见，这对其作为人类价值观代理的适用性提出了挑战。

Abstract: Reward modeling has emerged as a crucial component in aligning large language
models with human values. Significant attention has focused on using reward
models as a means for fine-tuning generative models. However, the reward models
themselves -- which directly encode human value judgments by turning
prompt-response pairs into scalar rewards -- remain relatively understudied. We
present a novel approach to reward model interpretability through exhaustive
analysis of their responses across their entire vocabulary space. By examining
how different reward models score every possible single-token response to
value-laden prompts, we uncover several striking findings: (i) substantial
heterogeneity between models trained on similar objectives, (ii) systematic
asymmetries in how models encode high- vs low-scoring tokens, (iii) significant
sensitivity to prompt framing that mirrors human cognitive biases, and (iv)
overvaluation of more frequent tokens. We demonstrate these effects across ten
recent open-source reward models of varying parameter counts and architectures.
Our results challenge assumptions about the interchangeability of reward
models, as well as their suitability as proxies of complex and
context-dependent human values. We find that these models can encode concerning
biases toward certain identity groups, which may emerge as unintended
consequences of harmlessness training -- distortions that risk propagating
through the downstream large language models now deployed to millions.

</details>


### [656] [KScope: A Framework for Characterizing the Knowledge Status of Language Models](https://arxiv.org/abs/2506.07458)
*Yuxin Xiao,Shan Chen,Jack Gallifant,Danielle Bitterman,Thomas Hartvigsen,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 论文提出了一种名为KScope的分层框架，用于评估大型语言模型（LLM）的知识状态，并将其分为五种类型。通过实验验证了上下文支持、特征分析和总结对知识更新的影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注LLM在知识冲突下的行为，但未能全面评估模型对问题的知识掌握程度。

Method: 提出了KScope框架，通过分层统计测试将LLM的知识状态分为五种类型，并在多个数据集和模型上验证其有效性。

Result: 实验表明，上下文支持缩小了知识差距，特定特征（如难度、相关性和熟悉度）驱动了知识更新，不同LLM在知识状态下的行为存在差异。

Conclusion: KScope框架能够系统评估LLM的知识状态，上下文总结和特征分析进一步提升了知识更新的效果。

Abstract: Characterizing a large language model's (LLM's) knowledge of a given question
is challenging. As a result, prior work has primarily examined LLM behavior
under knowledge conflicts, where the model's internal parametric memory
contradicts information in the external context. However, this does not fully
reflect how well the model knows the answer to the question. In this paper, we
first introduce a taxonomy of five knowledge statuses based on the consistency
and correctness of LLM knowledge modes. We then propose KScope, a hierarchical
framework of statistical tests that progressively refines hypotheses about
knowledge modes and characterizes LLM knowledge into one of these five
statuses. We apply KScope to nine LLMs across four datasets and systematically
establish: (1) Supporting context narrows knowledge gaps across models. (2)
Context features related to difficulty, relevance, and familiarity drive
successful knowledge updates. (3) LLMs exhibit similar feature preferences when
partially correct or conflicted, but diverge sharply when consistently wrong.
(4) Context summarization constrained by our feature analysis, together with
enhanced credibility, further improves update effectiveness and generalizes
across LLMs.

</details>


### [657] [LoRMA: Low-Rank Multiplicative Adaptation for LLMs](https://arxiv.org/abs/2506.07621)
*Harsh Bihany,Shubham Patel,Ashutosh Modi*

Main category: cs.CL

TL;DR: 论文提出了一种名为LoRMA的新方法，通过矩阵乘法变换替代传统的加法更新，解决了计算复杂性和秩瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在NLP领域表现出色，但完全微调计算成本高。现有方法如LoRA采用加法更新，限制了性能。

Method: 提出LoRMA，将加法更新转换为矩阵乘法变换，通过操作重排序和秩膨胀策略优化计算。

Result: 实验表明，LoRMA在多种评估指标上表现优异。

Conclusion: LoRMA为大型语言模型的高效微调提供了新思路，具有显著潜力。

Abstract: Large Language Models have shown remarkable capabilities in the NLP domain.
Their effectiveness can mainly be attributed to their ability to adapt to an
array of downstream tasks. However, generally, full fine-tuning is a
computationally expensive job. To mitigate this, many techniques have been
developed that prime efficiency, a prominent one being Low-Rank Adaptation
(LoRA). However, LoRA and its variants employ re-parametrized additive updates.
In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which
shifts the paradigm of additive updates to a richer space of matrix
multiplicative transformations. We tackle challenges such as computational
complexity and rank bottleneck of matrix multiplication by effectively
re-ordering operations and introducing rank inflation strategies. We conduct
extensive experiments to demonstrate the effectiveness of our approach in terms
of various evaluation metrics.

</details>


### [658] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)
*Prarabdh Shukla,Wei Yin Chong,Yash Patel,Brennan Schaffner,Danish Pruthi,Arjun Bhagoji*

Main category: cs.CL

TL;DR: 论文审计了Twitch的自动审核工具AutoMod，发现其在识别仇恨内容时存在显著漏洞，94%的仇恨内容未被标记，同时误删了89.5%的良性内容。


<details>
  <summary>Details</summary>
Motivation: 在线平台依赖自动化系统进行内容审核，但实时互动（如直播评论）对审核系统的延迟和有效性提出了更高要求。目前对这类系统的有效性了解有限。

Method: 通过创建测试账户，使用Twitch API发送10.7万条评论（来自4个数据集），评估AutoMod对仇恨内容（如性别歧视、种族歧视等）的标记准确性。

Result: AutoMod漏标了高达94%的仇恨内容，但对包含敏感词的良性内容误删率达89.5%。其依赖敏感词作为审核信号，缺乏上下文理解能力。

Conclusion: AutoMod在审核能力上存在重大缺陷，需改进上下文理解能力以更有效地区分仇恨内容与良性内容。

Abstract: To meet the demands of content moderation, online platforms have resorted to
automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users
commenting on live streams) on platforms like Twitch exert additional pressures
on the latency expected of such moderation systems. Despite their prevalence,
relatively little is known about the effectiveness of these systems. In this
paper, we conduct an audit of Twitch's automated moderation tool
($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful
content. For our audit, we create streaming accounts to act as siloed test
beds, and interface with the live chat using Twitch's APIs to send over
$107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s
accuracy in flagging blatantly hateful content containing misogyny, racism,
ableism and homophobia. Our experiments reveal that a large fraction of hateful
messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$.
Contextual addition of slurs to these messages results in $100\%$ removal,
revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We
also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$
blocks up to $89.5\%$ of benign examples that use sensitive words in
pedagogical or empowering contexts. Overall, our audit points to large gaps in
$\texttt{AutoMod}$'s capabilities and underscores the importance for such
systems to understand context effectively.

</details>


### [659] [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
*Jiaming Li,Haoran Ye,Yukun Chen,Xinyue Li,Lei Zhang,Hamid Alinejad-Rokny,Jimmy Chih-Hsien Peng,Min Yang*

Main category: cs.CL

TL;DR: 论文提出了一种名为FAST的新训练方法，专门针对指令模型优化稀疏自编码器（SAEs），显著提升了重构质量和特征可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器训练方法主要针对基础模型，应用于指令模型时重构质量和可解释性下降，因此需要一种专门针对指令模型的优化方法。

Method: 提出FAST方法，通过调整训练过程以匹配指令模型的数据分布和激活模式，从而优化稀疏自编码器的性能。

Result: 在Qwen2.5-7B-Instruct上，FAST的均方误差为0.6468，显著优于基线方法（5.1985和1.5096）；在Llama3.2-3B-Instruct上，高质量特征占比达21.1%，优于基线（7.0%和10.2%）。

Conclusion: FAST方法显著提升了稀疏自编码器在指令模型上的性能，同时发现对特殊令牌激活的干预可以改善输出质量，为模型行为精细控制提供了新思路。

Abstract: As large language models (LLMs) grow in scale and capability, understanding
their internal mechanisms becomes increasingly critical. Sparse autoencoders
(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the
extraction of human-interpretable features from LLMs. However, existing SAE
training methods are primarily designed for base models, resulting in reduced
reconstruction quality and interpretability when applied to instruct models. To
bridge this gap, we propose
$\underline{\textbf{F}}$inetuning-$\underline{\textbf{a}}$ligned
$\underline{\textbf{S}}$equential $\underline{\textbf{T}}$raining
($\textit{FAST}$), a novel training method specifically tailored for instruct
models. $\textit{FAST}$ aligns the training process with the data distribution
and activation patterns characteristic of instruct models, resulting in
substantial improvements in both reconstruction and feature interpretability.
On Qwen2.5-7B-Instruct, $\textit{FAST}$ achieves a mean squared error of 0.6468
in token reconstruction, significantly outperforming baseline methods with
errors of 5.1985 and 1.5096. In feature interpretability, $\textit{FAST}$
yields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,
$21.1\%$ scored in the top range, compared to $7.0\%$ and $10.2\%$ for
$\textit{BT(P)}$ and $\textit{BT(F)}$. Surprisingly, we discover that
intervening on the activations of special tokens via the SAEs leads to
improvements in output quality, suggesting new opportunities for fine-grained
control of model behavior. Code, data, and 240 trained SAEs are available at
https://github.com/Geaming2002/FAST.

</details>


### [660] [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795)
*Xiaotian Ye,Mengqi Zhang,Shu Wu*

Main category: cs.CL

TL;DR: LLM 遗忘技术旨在消除模型中的不良知识，但现有方法因形式依赖偏差（Form-Dependent Bias）在现实场景中效果有限。研究提出新基准 ORT 评估方法鲁棒性，并引入 ROCR 方法以提升遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 解决 LLM 遗忘技术因形式依赖偏差导致的泛化能力不足问题，以应对现实安全关键场景中的多样化需求。

Method: 提出 ORT 基准量化形式依赖偏差，并开发 ROCR 方法，通过重定向激活的危险概念实现无需训练的快速遗忘。

Result: 实验表明 ROCR 显著优于传统方法，能高效且自然地完成遗忘任务。

Conclusion: LLM 遗忘需形式无关，ROCR 为解决形式依赖偏差提供了有效路径。

Abstract: Large Language Model (LLM) unlearning aims to erase or suppress undesirable
knowledge within the model, offering promise for controlling harmful or private
information to prevent misuse. However, recent studies highlight its limited
efficacy in real-world scenarios, hindering practical adoption. In this study,
we identify a pervasive issue underlying many downstream failures: the
effectiveness of existing unlearning methods heavily depends on the form of
training samples and frequently fails to generalize to alternate expressions of
the same knowledge. We formally characterize this problem as Form-Dependent
Bias and systematically investigate its specific manifestation patterns across
various downstream tasks. To quantify its prevalence and support future
research, we introduce ORT, a novel benchmark designed to evaluate the
robustness of unlearning methods against variations in knowledge expression.
Results reveal that Form-Dependent Bias is both widespread and severe among
current techniques.
  We argue that LLM unlearning should be form-independent to address the
endless forms of downstream tasks encountered in real-world security-critical
scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),
a novel training-free method, as a promising solution path. ROCR performs
unlearning by targeting the invariants in downstream tasks, specifically the
activated dangerous concepts. It is capable of modifying model parameters
within seconds to redirect the model's perception of a specific unlearning
target concept to another harmless concept. Extensive experiments demonstrate
that ROCR significantly improves unlearning effectiveness compared to
traditional methods while generating highly natural outputs.

</details>


### [661] [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
*Iustin Sirbu,Robert-Adrian Popovici,Cornelia Caragea,Stefan Trausan-Matu,Traian Rebedea*

Main category: cs.CL

TL;DR: MultiMatch是一种结合协同训练和一致性正则化的半监督学习算法，通过三重重伪标签加权模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决半监督学习中伪标签选择和加权的问题，提升模型在数据不平衡情况下的鲁棒性。

Method: 结合协同训练和一致性正则化，设计三重重伪标签加权模块，统一了多种现有技术。

Result: 在5个NLP数据集的10个设置中，9个达到SOTA，且在数据不平衡情况下表现优异。

Conclusion: MultiMatch在性能和鲁棒性上优于现有方法，特别适用于数据不平衡的文本分类任务。

Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm
combining the paradigms of co-training and consistency regularization with
pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label
weighting module designed for three key purposes: selecting and filtering
pseudo-labels based on head agreement and model confidence, and weighting them
according to the perceived classification difficulty. This novel module
enhances and unifies three existing techniques -- heads agreement from
Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average
Pseudo-Margins from MarginMatch -- resulting in a holistic approach that
improves robustness and performance in SSL settings. Experimental results on
benchmark datasets highlight the superior performance of MultiMatch, achieving
state-of-the-art results on 9 out of 10 setups from 5 natural language
processing datasets and ranking first according to the Friedman test among 19
methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly
imbalanced settings, outperforming the second-best approach by 3.26% -- and
data imbalance is a key factor for many text classification tasks.

</details>


### [662] [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)
*Ke Wang,Yiming Qin,Nikolaos Dimitriadis,Alessandro Favero,Pascal Frossard*

Main category: cs.CL

TL;DR: MEMOIR是一种新型的可扩展框架，通过残差记忆模块注入知识，避免干扰和遗忘，适用于大规模语言模型的持续编辑。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在实时更新知识时面临的效率、可靠性和扩展性问题，避免重训练或遗忘已有信息。

Method: 通过样本依赖的掩码稀疏化输入激活，将每个编辑限制在记忆参数的特定子集，减少编辑间的干扰。推理时通过稀疏激活模式匹配相关编辑。

Result: 在问答、幻觉修正和分布外泛化任务中，MEMOIR在可靠性、泛化性和局部性指标上表现最优，支持数千次连续编辑且遗忘最小。

Conclusion: MEMOIR为语言模型的持续知识更新提供了一种高效、可靠且可扩展的解决方案。

Abstract: Language models deployed in real-world systems often require post-hoc updates
to incorporate new or corrected knowledge. However, editing such models
efficiently and reliably - without retraining or forgetting previous
information - remains a major challenge. Existing methods for lifelong model
editing either compromise generalization, interfere with past edits, or fail to
scale to long editing sequences. We propose MEMOIR, a novel scalable framework
that injects knowledge through a residual memory, i.e., a dedicated parameter
module, while preserving the core capabilities of the pre-trained model. By
sparsifying input activations through sample-dependent masks, MEMOIR confines
each edit to a distinct subset of the memory parameters, minimizing
interference among edits. At inference, it identifies relevant edits by
comparing the sparse activation patterns of new queries to those stored during
editing. This enables generalization to rephrased queries by activating only
the relevant knowledge while suppressing unnecessary memory activation for
unrelated prompts. Experiments on question answering, hallucination correction,
and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral
demonstrate that MEMOIR achieves state-of-the-art performance across
reliability, generalization, and locality metrics, scaling to thousands of
sequential edits with minimal forgetting.

</details>


### [663] [Language Models over Canonical Byte-Pair Encodings](https://arxiv.org/abs/2506.07956)
*Tim Vieira,Tianyu Liu,Clemente Pasti,Yahya Emara,Brian DuSell,Benjamin LeBrun,Mario Giulianelli,Juan Luis Gastaldi,Timothy J. O'Donnell,Ryan Cotterell*

Main category: cs.CL

TL;DR: 现代语言模型通过确定性分词器（如字节对编码）将字符字符串的概率分布表示为更短的标记字符串分布，但存在非规范标记编码的问题。本文提出两种方法确保仅规范标记字符串获得正概率，并证明修正此问题能提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型对字符字符串的非规范标记编码分配了非零概率，这些编码虽能解码为有效字符串但实际不会出现在训练数据中，导致概率分配错误且浪费资源。

Method: 提出两种方法：(1) 通过条件推断在测试时确保规范标记；(2) 通过模型参数化在训练时保证规范输出。

Result: 实验表明，修正非规范标记问题能提升多个模型和语料库的似然性能。

Conclusion: 确保标记字符串的规范性可避免概率分配错误，提升模型效率。

Abstract: Modern language models represent probability distributions over character
strings as distributions over (shorter) token strings derived via a
deterministic tokenizer, such as byte-pair encoding. While this approach is
highly effective at scaling up language models to large corpora, its current
incarnations have a concerning property: the model assigns nonzero probability
mass to an exponential number of $\it{noncanonical}$ token encodings of each
character string -- these are token strings that decode to valid character
strings but are impossible under the deterministic tokenizer (i.e., they will
never be seen in any training corpus, no matter how large). This misallocation
is both erroneous, as noncanonical strings never appear in training data, and
wasteful, diverting probability mass away from plausible outputs. These are
avoidable mistakes! In this work, we propose methods to enforce canonicality in
token-level language models, ensuring that only canonical token strings are
assigned positive probability. We present two approaches: (1) canonicality by
conditioning, leveraging test-time inference strategies without additional
training, and (2) canonicality by construction, a model parameterization that
guarantees canonical outputs but requires training. We demonstrate that fixing
canonicality mistakes improves the likelihood of held-out data for several
models and corpora.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [664] [DART-Vetter: A Deep LeARning Tool for automatic triage of exoplanet candidates](https://arxiv.org/abs/2506.05556)
*Stefano Fiscale,Laura Inno,Alessandra Rotundi,Angelo Ciaramella,Alessio Ferone,Christian Magliano,Luca Cacciapuoti,Veselin Kostov,Elisa Quintana,Giovanni Covone,Maria Teresa Muscari Tomajoli,Vito Saggese,Luca Tonietti,Antonio Vanzanella,Vincenzo Della Corte*

Main category: astro-ph.EP

TL;DR: DART-Vetter是一种深度学习模型，用于区分行星候选信号和假阳性信号，具有简单紧凑的架构，并在TESS和Kepler数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 提高从不同凌星巡天数据中识别行星候选信号的鲁棒性。

Method: 使用卷积神经网络处理折叠后的光变曲线，简化架构并公开源代码。

Result: 在TESS和Kepler数据上召回率达91%，优于Exominer和Astronet-Triage。

Conclusion: DART-Vetter是一种高效且易于复制的工具，适用于自动化筛选或辅助人工验证。

Abstract: In the identification of new planetary candidates in transit surveys, the
employment of Deep Learning models proved to be essential to efficiently
analyse a continuously growing volume of photometric observations. To further
improve the robustness of these models, it is necessary to exploit the
complementarity of data collected from different transit surveys such as NASA's
Kepler, Transiting Exoplanet Survey Satellite (TESS), and, in the near future,
the ESA PLAnetary Transits and Oscillation of stars (PLATO) mission. In this
work, we present a Deep Learning model, named DART-Vetter, able to distinguish
planetary candidates (PC) from false positives signals (NPC) detected by any
potential transiting survey. DART-Vetter is a Convolutional Neural Network that
processes only the light curves folded on the period of the relative signal,
featuring a simpler and more compact architecture with respect to other
triaging and/or vetting models available in the literature. We trained and
tested DART-Vetter on several dataset of publicly available and homogeneously
labelled TESS and Kepler light curves in order to prove the effectiveness of
our model. Despite its simplicity, DART-Vetter achieves highly competitive
triaging performance, with a recall rate of 91% on an ensemble of TESS and
Kepler data, when compared to Exominer and Astronet-Triage. Its compact, open
source and easy to replicate architecture makes DART-Vetter a particularly
useful tool for automatizing triaging procedures or assisting human vetters,
showing a discrete generalization on TCEs with Multiple Event Statistic (MES) >
20 and orbital period < 50 days.

</details>


### [665] [DART-Vetter: A Deep LeARning Tool for automatic triage of exoplanet candidates](https://arxiv.org/abs/2506.05556)
*Stefano Fiscale,Laura Inno,Alessandra Rotundi,Angelo Ciaramella,Alessio Ferone,Christian Magliano,Luca Cacciapuoti,Veselin Kostov,Elisa Quintana,Giovanni Covone,Maria Teresa Muscari Tomajoli,Vito Saggese,Luca Tonietti,Antonio Vanzanella,Vincenzo Della Corte*

Main category: astro-ph.EP

TL;DR: DART-Vetter是一种深度学习模型，用于区分行星候选信号和假阳性信号，具有简单紧凑的架构，在TESS和Kepler数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 提高深度学习模型在行星候选识别中的鲁棒性，利用不同巡天数据的互补性。

Method: 使用卷积神经网络处理折叠光变曲线，简化架构，并在公开数据集上训练和测试。

Result: 在TESS和Kepler数据上召回率达91%，性能优于Exominer和Astronet-Triage。

Conclusion: DART-Vetter是一种高效且易于复制的工具，适用于自动化筛选或辅助人工验证。

Abstract: In the identification of new planetary candidates in transit surveys, the
employment of Deep Learning models proved to be essential to efficiently
analyse a continuously growing volume of photometric observations. To further
improve the robustness of these models, it is necessary to exploit the
complementarity of data collected from different transit surveys such as NASA's
Kepler, Transiting Exoplanet Survey Satellite (TESS), and, in the near future,
the ESA PLAnetary Transits and Oscillation of stars (PLATO) mission. In this
work, we present a Deep Learning model, named DART-Vetter, able to distinguish
planetary candidates (PC) from false positives signals (NPC) detected by any
potential transiting survey. DART-Vetter is a Convolutional Neural Network that
processes only the light curves folded on the period of the relative signal,
featuring a simpler and more compact architecture with respect to other
triaging and/or vetting models available in the literature. We trained and
tested DART-Vetter on several dataset of publicly available and homogeneously
labelled TESS and Kepler light curves in order to prove the effectiveness of
our model. Despite its simplicity, DART-Vetter achieves highly competitive
triaging performance, with a recall rate of 91% on an ensemble of TESS and
Kepler data, when compared to Exominer and Astronet-Triage. Its compact, open
source and easy to replicate architecture makes DART-Vetter a particularly
useful tool for automatizing triaging procedures or assisting human vetters,
showing a discrete generalization on TCEs with Multiple Event Statistic (MES) >
20 and orbital period < 50 days.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [666] [Enhancing Neural Autoregressive Distribution Estimators for Image Reconstruction](https://arxiv.org/abs/2506.05391)
*Ambrose Emmett-Iwaniw,Nathan Kirk*

Main category: eess.IV

TL;DR: 论文研究了通过观察图像像素子集预测未观测部分的方法，提出了一种改进的ConvNADE模型，并探讨了不同像素选择策略对重建质量的影响。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过少量像素预测图像未观测部分，提升自回归模型在图像数据分布学习中的效率和质量。

Method: 提出了一种改进的ConvNADE模型，适用于实值和彩色图像，并比较了随机像素块和低差异像素块对重建的影响。

Result: 实验表明，使用低差异序列选择的像素能降低测试损失并生成更真实的图像重建。

Conclusion: 低差异像素选择策略能有效提升图像重建质量，改进的ConvNADE模型在效率和性能上表现优异。

Abstract: Autoregressive models are often employed to learn distributions of image data
by decomposing the $D$-dimensional density function into a product of
one-dimensional conditional distributions. Each conditional depends on
preceding variables (pixels, in the case of image data), making the order in
which variables are processed fundamental to the model performance. In this
paper, we study the problem of observing a small subset of image pixels
(referred to as a pixel patch) to predict the unobserved parts of the image. As
our prediction mechanism, we propose a generalized and computationally
efficient version of the convolutional neural autoregressive distribution
estimator (ConvNADE) model adapted for real-valued and color images. Moreover,
we investigate the quality of image reconstruction when observing both random
pixel patches and low-discrepancy pixel patches inspired by quasi-Monte Carlo
theory. Experiments on benchmark datasets demonstrate that choosing the pixels
akin to a low-discrepancy sequence reduces test loss and produces more
realistic reconstructed images.

</details>


### [667] [Deep histological synthesis from mass spectrometry imaging for multimodal registration](https://arxiv.org/abs/2506.05441)
*Kimberley M. Bird,Xujiong Ye,Alan M. Race,James M. Brown*

Main category: eess.IV

TL;DR: 提出了一种基于pix2pix模型的方法，将质谱成像（MSI）合成为组织学图像，以实现单模态配准，初步结果显示合成图像质量较高。


<details>
  <summary>Details</summary>
Motivation: 组织学和MSI的图像形成过程和维度不同，导致两者配准困难，需要一种有效的解决方案。

Method: 使用pix2pix模型从MSI数据合成组织学图像，以实现单模态配准。

Result: 合成图像质量较好，互信息（MI）和结构相似性指数（SSIM）分别提高了+0.924和+0.419。

Conclusion: 该方法在组织学和MSI配准中表现出潜力，代码已开源。

Abstract: Registration of histological and mass spectrometry imaging (MSI) allows for
more precise identification of structural changes and chemical interactions in
tissue. With histology and MSI having entirely different image formation
processes and dimensionalities, registration of the two modalities remains an
ongoing challenge. This work proposes a solution that synthesises histological
images from MSI, using a pix2pix model, to effectively enable unimodal
registration. Preliminary results show promising synthetic histology images
with limited artifacts, achieving increases in mutual information (MI) and
structural similarity index measures (SSIM) of +0.924 and +0.419, respectively,
compared to a baseline U-Net model. Our source code is available on GitHub:
https://github.com/kimberley/MIUA2025.

</details>


### [668] [LinGuinE: Longitudinal Guidance Estimation for Volumetric Lung Tumour Segmentation](https://arxiv.org/abs/2506.06092)
*Nadine Garibli,Mayank Patwari,Bence Csiba,Yi Wei,Kostas Sidiropoulos*

Main category: eess.IV

TL;DR: LinGuinE是一种自动分割肺部肿瘤纵向CT扫描的方法，通过初始输入和点传播实现高效分割，显著提高了分割准确性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏自动化或半自动化的纵向肿瘤分割方法，而这对放疗、手术和化疗评估至关重要。

Method: LinGuinE通过刚性配准将初始肿瘤点传播到其他时间点，并使用点击有效性分类器筛选有效点以生成新分割。

Result: 在两个测试数据集上，LinGuinE的Dice分数提高了20%以上（p<0.05），适用于任意起始时间点。

Conclusion: LinGuinE在纵向肿瘤分割中表现出色，为临床实践提供了高效可靠的解决方案。

Abstract: Segmentation of lung gross tumour volumes is an important first step in
radiotherapy and surgical intervention, and is starting to play a role in
assessing chemotherapy response. Response to a drug is measured by tracking the
tumour volumes over a series of CT scans over a time period i.e. a longitudinal
study. However, there currently exist few solutions for automated or
semi-automated longitudinal tumour segmentation. This paper introduces
LinGuinE, an automated method to segment a longitudinal series of lung tumours.
A radiologist must provide an initial input, indicating the location of the
tumour in a CT scan at an arbitrary time point. LinGuinE samples points inside
this tumour and propagates them to another time point using rigid registration.
A click validity classifier selects points which still fall within the tumour;
these are used to automatically create a segmentation in the new time point. We
test LinGuinE on a dataset acquired from a phase 3 clinical trial for lung
tumours and the publicly available 4-D lung CBCT dataset. We find that LinGuinE
improves the Dice on both test sets by over 20% (p< 0.05) across 63
longitudinal studies. We show that any time point can be used as a starting
point, conduct ablation experiments, and find that our LinGuinE setup yields
the best results on both test datasets.

</details>


### [669] [Enhancing Neural Autoregressive Distribution Estimators for Image Reconstruction](https://arxiv.org/abs/2506.05391)
*Ambrose Emmett-Iwaniw,Nathan Kirk*

Main category: eess.IV

TL;DR: 论文研究了通过观察图像像素子集（像素块）预测未观察部分的方法，提出了一种改进的ConvNADE模型，并探讨了随机和低差异像素块对图像重建质量的影响。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过少量像素预测完整图像，提升自回归模型在图像数据分布学习中的效率和质量。

Method: 提出了一种适用于实值和彩色图像的改进版ConvNADE模型，并比较了随机像素块和低差异像素块对重建效果的影响。

Result: 实验表明，使用低差异序列选择的像素块能降低测试损失并生成更真实的图像。

Conclusion: 低差异像素块选择策略能显著提升图像重建质量，改进的ConvNADE模型在效率和性能上表现优异。

Abstract: Autoregressive models are often employed to learn distributions of image data
by decomposing the $D$-dimensional density function into a product of
one-dimensional conditional distributions. Each conditional depends on
preceding variables (pixels, in the case of image data), making the order in
which variables are processed fundamental to the model performance. In this
paper, we study the problem of observing a small subset of image pixels
(referred to as a pixel patch) to predict the unobserved parts of the image. As
our prediction mechanism, we propose a generalized and computationally
efficient version of the convolutional neural autoregressive distribution
estimator (ConvNADE) model adapted for real-valued and color images. Moreover,
we investigate the quality of image reconstruction when observing both random
pixel patches and low-discrepancy pixel patches inspired by quasi-Monte Carlo
theory. Experiments on benchmark datasets demonstrate that choosing the pixels
akin to a low-discrepancy sequence reduces test loss and produces more
realistic reconstructed images.

</details>


### [670] [Deep histological synthesis from mass spectrometry imaging for multimodal registration](https://arxiv.org/abs/2506.05441)
*Kimberley M. Bird,Xujiong Ye,Alan M. Race,James M. Brown*

Main category: eess.IV

TL;DR: 该论文提出了一种通过pix2pix模型从MSI合成组织学图像的方法，以实现单模态配准，初步结果显示合成图像质量较高，且性能优于基线U-Net模型。


<details>
  <summary>Details</summary>
Motivation: 组织学和MSI的图像形成过程和维度不同，导致两者配准困难，因此需要一种有效的解决方案。

Method: 使用pix2pix模型从MSI合成组织学图像，以实现单模态配准。

Result: 合成的组织学图像质量较高，互信息（MI）和结构相似性指数（SSIM）分别提高了+0.924和+0.419。

Conclusion: 该方法为组织学和MSI的配准提供了一种有效解决方案，且性能优于基线模型。

Abstract: Registration of histological and mass spectrometry imaging (MSI) allows for
more precise identification of structural changes and chemical interactions in
tissue. With histology and MSI having entirely different image formation
processes and dimensionalities, registration of the two modalities remains an
ongoing challenge. This work proposes a solution that synthesises histological
images from MSI, using a pix2pix model, to effectively enable unimodal
registration. Preliminary results show promising synthetic histology images
with limited artifacts, achieving increases in mutual information (MI) and
structural similarity index measures (SSIM) of +0.924 and +0.419, respectively,
compared to a baseline U-Net model. Our source code is available on GitHub:
https://github.com/kimberley/MIUA2025.

</details>


### [671] [LinGuinE: Longitudinal Guidance Estimation for Volumetric Lung Tumour Segmentation](https://arxiv.org/abs/2506.06092)
*Nadine Garibli,Mayank Patwari,Bence Csiba,Yi Wei,Kostas Sidiropoulos*

Main category: eess.IV

TL;DR: LinGuinE是一种自动化方法，用于分割肺部肿瘤的纵向CT扫描序列，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏自动化或半自动化的纵向肿瘤分割解决方案，而这是评估化疗反应的关键步骤。

Method: 通过刚性配准和点击有效性分类器，从初始肿瘤标记点传播到其他时间点，实现自动分割。

Result: 在两个测试数据集上，LinGuinE将Dice系数提高了20%以上（p<0.05）。

Conclusion: LinGuinE在纵向肿瘤分割中表现优异，且起始时间点灵活。

Abstract: Segmentation of lung gross tumour volumes is an important first step in
radiotherapy and surgical intervention, and is starting to play a role in
assessing chemotherapy response. Response to a drug is measured by tracking the
tumour volumes over a series of CT scans over a time period i.e. a longitudinal
study. However, there currently exist few solutions for automated or
semi-automated longitudinal tumour segmentation. This paper introduces
LinGuinE, an automated method to segment a longitudinal series of lung tumours.
A radiologist must provide an initial input, indicating the location of the
tumour in a CT scan at an arbitrary time point. LinGuinE samples points inside
this tumour and propagates them to another time point using rigid registration.
A click validity classifier selects points which still fall within the tumour;
these are used to automatically create a segmentation in the new time point. We
test LinGuinE on a dataset acquired from a phase 3 clinical trial for lung
tumours and the publicly available 4-D lung CBCT dataset. We find that LinGuinE
improves the Dice on both test sets by over 20% (p< 0.05) across 63
longitudinal studies. We show that any time point can be used as a starting
point, conduct ablation experiments, and find that our LinGuinE setup yields
the best results on both test datasets.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [672] [Applying Informer for Option Pricing: A Transformer-Based Approach](https://arxiv.org/abs/2506.05565)
*Feliks Bańka,Jarosław A. Chudziak*

Main category: cs.CE

TL;DR: 本文研究了Informer神经网络在期权定价中的应用，展示了其优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统期权定价模型（如Black-Scholes）受限于市场波动性，需要更准确且适应性强的方法。

Method: 采用Informer神经网络，利用其捕捉长期依赖关系和动态适应市场波动的能力。

Result: Informer在期权定价中表现优于传统方法，提高了预测准确性。

Conclusion: Informer为数据驱动的金融预测提供了更适应性强且稳健的框架。

Abstract: Accurate option pricing is essential for effective trading and risk
management in financial markets, yet it remains challenging due to market
volatility and the limitations of traditional models like Black-Scholes. In
this paper, we investigate the application of the Informer neural network for
option pricing, leveraging its ability to capture long-term dependencies and
dynamically adjust to market fluctuations. This research contributes to the
field of financial forecasting by introducing Informer's efficient architecture
to enhance prediction accuracy and provide a more adaptable and resilient
framework compared to existing methods. Our results demonstrate that Informer
outperforms traditional approaches in option pricing, advancing the
capabilities of data-driven financial forecasting in this domain.

</details>


### [673] [Applying Informer for Option Pricing: A Transformer-Based Approach](https://arxiv.org/abs/2506.05565)
*Feliks Bańka,Jarosław A. Chudziak*

Main category: cs.CE

TL;DR: 本文探讨了使用Informer神经网络进行期权定价，相比传统模型（如Black-Scholes），其能更好地捕捉长期依赖关系并适应市场波动，从而提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 由于市场波动和传统模型（如Black-Scholes）的局限性，准确的期权定价对交易和风险管理至关重要。

Method: 应用Informer神经网络，利用其捕捉长期依赖关系和动态适应市场波动的能力。

Result: Informer在期权定价中表现优于传统方法，提高了预测准确性。

Conclusion: Informer为数据驱动的金融预测提供了更适应性强且稳健的框架，推动了该领域的发展。

Abstract: Accurate option pricing is essential for effective trading and risk
management in financial markets, yet it remains challenging due to market
volatility and the limitations of traditional models like Black-Scholes. In
this paper, we investigate the application of the Informer neural network for
option pricing, leveraging its ability to capture long-term dependencies and
dynamically adjust to market fluctuations. This research contributes to the
field of financial forecasting by introducing Informer's efficient architecture
to enhance prediction accuracy and provide a more adaptable and resilient
framework compared to existing methods. Our results demonstrate that Informer
outperforms traditional approaches in option pricing, advancing the
capabilities of data-driven financial forecasting in this domain.

</details>


### [674] [Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market](https://arxiv.org/abs/2506.06356)
*Yimin Du*

Main category: cs.CE

TL;DR: 本文提出了一种结合深度学习与横截面股票预测的多日换手量化交易算法，用于中国A股市场，实现了高回报与低回撤的优异表现。


<details>
  <summary>Details</summary>
Motivation: 旨在通过深度学习和多模块集成，提升量化交易策略的资本效率和风险管理能力，适用于机构级部署。

Method: 结合五个模块：深度横截面预测网络选股、混合模型分析开盘信号、动态头寸规模调整、网格搜索优化的止盈止损机制，以及多粒度波动市场择时模型。

Result: 在2021-2024年回测中，年化收益15.2%，最大回撤低于5%，夏普比率1.87，策略具有高扩展性。

Conclusion: 该算法在多种市场环境下表现稳健，适合机构应用，同时保持高资本容量和风险调整后收益。

Abstract: This paper presents a sophisticated multi-day turnover quantitative trading
algorithm that integrates advanced deep learning techniques with comprehensive
cross-sectional stock prediction for the Chinese A-share market. Our framework
combines five interconnected modules: initial stock selection through deep
cross-sectional prediction networks, opening signal distribution analysis using
mixture models for arbitrage identification, market capitalization and
liquidity-based dynamic position sizing, grid-search optimized profit-taking
and stop-loss mechanisms, and multi-granularity volatility-based market timing
models. The algorithm employs a novel approach to balance capital efficiency
with risk management through adaptive holding periods and sophisticated
entry/exit timing. Trained on comprehensive A-share data from 2010-2020 and
rigorously backtested on 2021-2024 data, our method achieves remarkable
performance with 15.2\% annualized returns, maximum drawdown constrained below
5\%, and a Sharpe ratio of 1.87. The strategy demonstrates exceptional
scalability by maintaining 50-100 daily positions with a 9-day maximum holding
period, incorporating dynamic profit-taking and stop-loss mechanisms that
enhance capital turnover efficiency while preserving risk-adjusted returns. Our
approach exhibits robust performance across various market regimes while
maintaining high capital capacity suitable for institutional deployment.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [675] [Learning to Recover: Dynamic Reward Shaping with Wheel-Leg Coordination for Fallen Robots](https://arxiv.org/abs/2506.05516)
*Boyuan Deng,Luca Rossini,Jin Wang,Weijie Wang,Nikolaos Tsagarakis*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的框架，结合动态奖励调整和课程学习，用于轮腿机器人的自适应跌倒恢复，显著提高了恢复成功率和能效。


<details>
  <summary>Details</summary>
Motivation: 轮腿机器人在实际部署中需要具备自适应跌倒恢复能力，但传统方法因依赖预规划动作或简化动力学而难以生成鲁棒的恢复策略。

Method: 采用基于情节的动态奖励调整和课程学习，结合非对称演员-评论家架构和噪声注入观测，以加速训练并增强鲁棒性。

Result: 实验表明，该方法在两种四足平台上实现了高达99.1%和97.8%的恢复成功率，同时降低了关节扭矩消耗。

Conclusion: 该方法通过轮腿协同和能量转移机制，显著提升了恢复性能和能效，适用于不同平台且无需特定调优。

Abstract: Adaptive recovery from fall incidents are essential skills for the practical
deployment of wheeled-legged robots, which uniquely combine the agility of legs
with the speed of wheels for rapid recovery. However, traditional methods
relying on preplanned recovery motions, simplified dynamics or sparse rewards
often fail to produce robust recovery policies. This paper presents a
learning-based framework integrating Episode-based Dynamic Reward Shaping and
curriculum learning, which dynamically balances exploration of diverse recovery
maneuvers with precise posture refinement. An asymmetric actor-critic
architecture accelerates training by leveraging privileged information in
simulation, while noise-injected observations enhance robustness against
uncertainties. We further demonstrate that synergistic wheel-leg coordination
reduces joint torque consumption by 15.8% and 26.2% and improves stabilization
through energy transfer mechanisms. Extensive evaluations on two distinct
quadruped platforms achieve recovery success rates up to 99.1% and 97.8%
without platform-specific tuning. The supplementary material is available at
https://boyuandeng.github.io/L2R-WheelLegCoordination/

</details>


### [676] [BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning](https://arxiv.org/abs/2506.06072)
*Hongyi Zhou,Weiran Liao,Xi Huang,Yucheng Tang,Fabian Otto,Xiaogang Jia,Xinkai Jiang,Simon Hilber,Ge Li,Qian Wang,Ömer Erdinç Yağmurlu,Nils Blank,Moritz Reuss,Rudolf Lioutikov*

Main category: cs.RO

TL;DR: BEAST是一种基于B样条的动作序列标记器，无需单独训练，支持并行解码，生成平滑轨迹，并在多个任务中表现出高效和竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有动作标记器（如向量量化或字节对编码）需要单独训练且生成的标记长度不一致，BEAST旨在解决这些问题。

Method: 利用B样条编码动作序列为紧凑的离散或连续标记，支持三种模型架构（VAE、Transformer、Florence-2）进行验证。

Result: 在166个模拟任务和8个真实任务中，BEAST显著降低计算成本，生成平滑控制信号，并保持高任务成功率。

Conclusion: BEAST是一种高效、兼容性强的动作标记器，适用于连续控制任务，性能优于现有方法。

Abstract: We present the B-spline Encoded Action Sequence Tokenizer (BEAST), a novel
action tokenizer that encodes action sequences into compact discrete or
continuous tokens using B-splines. In contrast to existing action tokenizers
based on vector quantization or byte pair encoding, BEAST requires no separate
tokenizer training and consistently produces tokens of uniform length, enabling
fast action sequence generation via parallel decoding. Leveraging our B-spline
formulation, BEAST inherently ensures generating smooth trajectories without
discontinuities between adjacent segments. We extensively evaluate BEAST by
integrating it with three distinct model architectures: a Variational
Autoencoder (VAE) with continuous tokens, a decoder-only Transformer with
discrete tokens, and Florence-2, a pretrained Vision-Language Model with an
encoder-decoder architecture, demonstrating BEAST's compatibility and
scalability with large pretrained models. We evaluate BEAST across three
established benchmarks consisting of 166 simulated tasks and on three distinct
robot settings with a total of 8 real-world tasks. Experimental results
demonstrate that BEAST (i) significantly reduces both training and inference
computational costs, and (ii) consistently generates smooth, high-frequency
control signals suitable for continuous control tasks while (iii) reliably
achieves competitive task success rates compared to state-of-the-art methods.

</details>


### [677] [On-board Mission Replanning for Adaptive Cooperative Multi-Robot Systems](https://arxiv.org/abs/2506.06094)
*Elim Kwan,Rehman Qureshi,Liam Fletcher,Colin Laganier,Victoria Nockles,Richard Walters*

Main category: cs.RO

TL;DR: 论文提出了一种基于强化学习的快速重规划算法，用于解决多智能体协同任务中的动态环境适应问题，性能接近最优解且计算速度快。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体在远程、动态和危险环境中快速重规划的需求，克服现有方法在重规划、协作、任务时长建模和实际部署方面的不足。

Method: 提出了一种新的多旅行商问题变体，并开发了基于图注意力网络和注意力模型的编码器/解码器模型。

Result: 在树莓派上运行时，性能接近最优解（90%情况下误差在10%以内），且速度快85-370倍。

Conclusion: 该研究为自主多智能体系统的鲁棒性提供了新思路。

Abstract: Cooperative autonomous robotic systems have significant potential for
executing complex multi-task missions across space, air, ground, and maritime
domains. But they commonly operate in remote, dynamic and hazardous
environments, requiring rapid in-mission adaptation without reliance on fragile
or slow communication links to centralised compute. Fast, on-board replanning
algorithms are therefore needed to enhance resilience. Reinforcement Learning
shows strong promise for efficiently solving mission planning tasks when
formulated as Travelling Salesperson Problems (TSPs), but existing methods: 1)
are unsuitable for replanning, where agents do not start at a single location;
2) do not allow cooperation between agents; 3) are unable to model tasks with
variable durations; or 4) lack practical considerations for on-board
deployment. Here we define the Cooperative Mission Replanning Problem as a
novel variant of multiple TSP with adaptations to overcome these issues, and
develop a new encoder/decoder-based model using Graph Attention Networks and
Attention Models to solve it effectively and efficiently. Using a simple
example of cooperative drones, we show our replanner consistently (90% of the
time) maintains performance within 10% of the state-of-the-art LKH3 heuristic
solver, whilst running 85-370 times faster on a Raspberry Pi. This work paves
the way for increased resilience in autonomous multi-agent systems.

</details>


### [678] [BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly](https://arxiv.org/abs/2506.06221)
*Yan Shen,Ruihai Wu,Yubin Ke,Xinyuan Song,Zeyi Li,Xiaoqi Li,Hongwei Fan,Haoran Lu,Hao dong*

Main category: cs.RO

TL;DR: 论文提出了一种基于点级功能感知的双臂协作几何装配方法，解决了碎片几何多样性带来的评估模糊性问题，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 几何装配（如修复破碎的碗）是机器人技术中的重要挑战，需要识别几何线索以实现抓取、装配和双臂协作操作。

Method: 利用点级功能感知的几何泛化能力，学习支持双臂协作的长时程动作序列，并引入具有几何多样性和全局可重复性的真实世界基准。

Result: 实验表明，该方法在功能感知和模仿学习方法中均表现出优越性。

Conclusion: 提出的方法在几何装配任务中有效解决了评估模糊性问题，提升了双臂协作的装配能力。

Abstract: Shape assembly, the process of combining parts into a complete whole, is a
crucial robotic skill with broad real-world applications. Among various
assembly tasks, geometric assembly--where broken parts are reassembled into
their original form (e.g., reconstructing a shattered bowl)--is particularly
challenging. This requires the robot to recognize geometric cues for grasping,
assembly, and subsequent bimanual collaborative manipulation on varied
fragments. In this paper, we exploit the geometric generalization of
point-level affordance, learning affordance aware of bimanual collaboration in
geometric assembly with long-horizon action sequences. To address the
evaluation ambiguity caused by geometry diversity of broken parts, we introduce
a real-world benchmark featuring geometric variety and global reproducibility.
Extensive experiments demonstrate the superiority of our approach over both
previous affordance-based and imitation-based methods. Project page:
https://sites.google.com/view/biassembly/.

</details>


### [679] [Learning to Recover: Dynamic Reward Shaping with Wheel-Leg Coordination for Fallen Robots](https://arxiv.org/abs/2506.05516)
*Boyuan Deng,Luca Rossini,Jin Wang,Weijie Wang,Nikolaos Tsagarakis*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的框架，结合动态奖励调整和课程学习，用于轮腿机器人跌倒后的自适应恢复，显著提升了恢复成功率和能量效率。


<details>
  <summary>Details</summary>
Motivation: 轮腿机器人结合了腿的敏捷性和轮的速度，但传统方法在恢复策略上表现不佳，需要更鲁棒的学习方法。

Method: 采用动态奖励调整和课程学习，结合非对称actor-critic架构和噪声注入观察，优化恢复策略。

Result: 实验显示，该方法在两种四足平台上实现了高达99.1%和97.8%的恢复成功率，并减少了关节扭矩消耗。

Conclusion: 该框架显著提升了轮腿机器人的恢复能力和能量效率，无需平台特定调整。

Abstract: Adaptive recovery from fall incidents are essential skills for the practical
deployment of wheeled-legged robots, which uniquely combine the agility of legs
with the speed of wheels for rapid recovery. However, traditional methods
relying on preplanned recovery motions, simplified dynamics or sparse rewards
often fail to produce robust recovery policies. This paper presents a
learning-based framework integrating Episode-based Dynamic Reward Shaping and
curriculum learning, which dynamically balances exploration of diverse recovery
maneuvers with precise posture refinement. An asymmetric actor-critic
architecture accelerates training by leveraging privileged information in
simulation, while noise-injected observations enhance robustness against
uncertainties. We further demonstrate that synergistic wheel-leg coordination
reduces joint torque consumption by 15.8% and 26.2% and improves stabilization
through energy transfer mechanisms. Extensive evaluations on two distinct
quadruped platforms achieve recovery success rates up to 99.1% and 97.8%
without platform-specific tuning. The supplementary material is available at
https://boyuandeng.github.io/L2R-WheelLegCoordination/

</details>


### [680] [BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning](https://arxiv.org/abs/2506.06072)
*Hongyi Zhou,Weiran Liao,Xi Huang,Yucheng Tang,Fabian Otto,Xiaogang Jia,Xinkai Jiang,Simon Hilber,Ge Li,Qian Wang,Ömer Erdinç Yağmurlu,Nils Blank,Moritz Reuss,Rudolf Lioutikov*

Main category: cs.RO

TL;DR: BEAST是一种基于B样条的动作序列标记器，无需单独训练标记器，支持并行解码，生成平滑轨迹，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有动作标记器（如向量量化或字节对编码）需要单独训练且标记长度不一致，BEAST旨在解决这些问题。

Method: 利用B样条编码动作序列为紧凑的离散或连续标记，支持并行解码，确保轨迹平滑。

Result: 在166个模拟任务和8个真实任务中，BEAST显著降低计算成本，生成平滑控制信号，并保持高任务成功率。

Conclusion: BEAST是一种高效、兼容性强且性能优越的动作标记器，适用于连续控制任务。

Abstract: We present the B-spline Encoded Action Sequence Tokenizer (BEAST), a novel
action tokenizer that encodes action sequences into compact discrete or
continuous tokens using B-splines. In contrast to existing action tokenizers
based on vector quantization or byte pair encoding, BEAST requires no separate
tokenizer training and consistently produces tokens of uniform length, enabling
fast action sequence generation via parallel decoding. Leveraging our B-spline
formulation, BEAST inherently ensures generating smooth trajectories without
discontinuities between adjacent segments. We extensively evaluate BEAST by
integrating it with three distinct model architectures: a Variational
Autoencoder (VAE) with continuous tokens, a decoder-only Transformer with
discrete tokens, and Florence-2, a pretrained Vision-Language Model with an
encoder-decoder architecture, demonstrating BEAST's compatibility and
scalability with large pretrained models. We evaluate BEAST across three
established benchmarks consisting of 166 simulated tasks and on three distinct
robot settings with a total of 8 real-world tasks. Experimental results
demonstrate that BEAST (i) significantly reduces both training and inference
computational costs, and (ii) consistently generates smooth, high-frequency
control signals suitable for continuous control tasks while (iii) reliably
achieves competitive task success rates compared to state-of-the-art methods.

</details>


### [681] [On-board Mission Replanning for Adaptive Cooperative Multi-Robot Systems](https://arxiv.org/abs/2506.06094)
*Elim Kwan,Rehman Qureshi,Liam Fletcher,Colin Laganier,Victoria Nockles,Richard Walters*

Main category: cs.RO

TL;DR: 论文提出了一种基于强化学习的快速重规划算法，用于解决多智能体协同任务中的动态重规划问题，并在性能和速度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂环境中执行任务时，需要快速适应动态变化，但现有方法在重规划、协作、任务时长建模和实际部署方面存在不足。

Method: 提出了一种新的多旅行商问题变体，并开发了基于图注意力网络和注意力模型的编码器/解码器模型。

Result: 在无人机协同任务中，该算法90%的情况下性能接近最优解，且在树莓派上运行速度快85-370倍。

Conclusion: 该研究为自主多智能体系统的鲁棒性提供了新思路。

Abstract: Cooperative autonomous robotic systems have significant potential for
executing complex multi-task missions across space, air, ground, and maritime
domains. But they commonly operate in remote, dynamic and hazardous
environments, requiring rapid in-mission adaptation without reliance on fragile
or slow communication links to centralised compute. Fast, on-board replanning
algorithms are therefore needed to enhance resilience. Reinforcement Learning
shows strong promise for efficiently solving mission planning tasks when
formulated as Travelling Salesperson Problems (TSPs), but existing methods: 1)
are unsuitable for replanning, where agents do not start at a single location;
2) do not allow cooperation between agents; 3) are unable to model tasks with
variable durations; or 4) lack practical considerations for on-board
deployment. Here we define the Cooperative Mission Replanning Problem as a
novel variant of multiple TSP with adaptations to overcome these issues, and
develop a new encoder/decoder-based model using Graph Attention Networks and
Attention Models to solve it effectively and efficiently. Using a simple
example of cooperative drones, we show our replanner consistently (90% of the
time) maintains performance within 10% of the state-of-the-art LKH3 heuristic
solver, whilst running 85-370 times faster on a Raspberry Pi. This work paves
the way for increased resilience in autonomous multi-agent systems.

</details>


### [682] [BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly](https://arxiv.org/abs/2506.06221)
*Yan Shen,Ruihai Wu,Yubin Ke,Xinyuan Song,Zeyi Li,Xiaoqi Li,Hongwei Fan,Haoran Lu,Hao dong*

Main category: cs.RO

TL;DR: 论文提出了一种基于点级功能感知的双臂协作几何装配方法，解决了碎片几何多样性带来的评估模糊问题，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 几何装配（如修复破碎的碗）是机器人技术中的关键挑战，需要识别几何线索并实现双臂协作操作。

Method: 利用点级功能感知的几何泛化能力，学习双臂协作的功能感知，并引入具有几何多样性和全局可重复性的真实基准。

Result: 实验表明，该方法在几何装配任务中优于现有的功能感知和模仿学习方法。

Conclusion: 该方法为几何装配任务提供了一种有效的解决方案，并通过基准验证了其性能。

Abstract: Shape assembly, the process of combining parts into a complete whole, is a
crucial robotic skill with broad real-world applications. Among various
assembly tasks, geometric assembly--where broken parts are reassembled into
their original form (e.g., reconstructing a shattered bowl)--is particularly
challenging. This requires the robot to recognize geometric cues for grasping,
assembly, and subsequent bimanual collaborative manipulation on varied
fragments. In this paper, we exploit the geometric generalization of
point-level affordance, learning affordance aware of bimanual collaboration in
geometric assembly with long-horizon action sequences. To address the
evaluation ambiguity caused by geometry diversity of broken parts, we introduce
a real-world benchmark featuring geometric variety and global reproducibility.
Extensive experiments demonstrate the superiority of our approach over both
previous affordance-based and imitation-based methods. Project page:
https://sites.google.com/view/biassembly/.

</details>


### [683] [Tactile MNIST: Benchmarking Active Tactile Perception](https://arxiv.org/abs/2506.06361)
*Tim Schneider,Guillaume Duret,Cristiana de Farias,Roberto Calandra,Liming Chen,Jan Peters*

Main category: cs.RO

TL;DR: 论文提出了Tactile MNIST Benchmark Suite，一个用于主动触觉感知任务的开源基准测试套件，填补了触觉感知和主动感知领域缺乏标准化基准的空白。


<details>
  <summary>Details</summary>
Motivation: 触觉感知能增强机器人操作的灵活性，但由于其局部性，难以单独用于需要全局场景理解的任务。主动感知技术可以弥补这一缺陷，但缺乏标准化基准阻碍了研究进展。

Method: 引入Tactile MNIST Benchmark Suite，提供多样化的模拟场景和数据集，包括合成3D MNIST数字模型和真实触觉样本，并训练CycleGAN用于逼真触觉模拟渲染。

Result: 提供了13,500个合成3D MNIST数字模型和153,600个真实触觉样本的数据集，支持定位、分类和体积估计等任务。

Conclusion: 通过标准化协议和可复现的评估框架，该基准套件促进了触觉感知和主动感知领域的系统性进展。

Abstract: Tactile perception has the potential to significantly enhance dexterous
robotic manipulation by providing rich local information that can complement or
substitute for other sensory modalities such as vision. However, because
tactile sensing is inherently local, it is not well-suited for tasks that
require broad spatial awareness or global scene understanding on its own. A
human-inspired strategy to address this issue is to consider active perception
techniques instead. That is, to actively guide sensors toward regions with more
informative or significant features and integrate such information over time in
order to understand a scene or complete a task. Both active perception and
different methods for tactile sensing have received significant attention
recently. Yet, despite advancements, both fields lack standardized benchmarks.
To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an
open-source, Gymnasium-compatible benchmark specifically designed for active
tactile perception tasks, including localization, classification, and volume
estimation. Our benchmark suite offers diverse simulation scenarios, from
simple toy environments all the way to complex tactile perception tasks using
vision-based tactile sensors. Furthermore, we also offer a comprehensive
dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600
real-world tactile samples collected from 600 3D printed digits. Using this
dataset, we train a CycleGAN for realistic tactile simulation rendering. By
providing standardized protocols and reproducible evaluation frameworks, our
benchmark suite facilitates systematic progress in the fields of tactile
sensing and active perception.

</details>


### [684] [Multimodal Spatial Language Maps for Robot Navigation and Manipulation](https://arxiv.org/abs/2506.06862)
*Chenguang Huang,Oier Mees,Andy Zeng,Wolfram Burgard*

Main category: cs.RO

TL;DR: 论文提出了一种多模态空间语言地图（VLMaps和AVLMaps），通过融合预训练多模态特征与3D环境重建，实现自然语言命令到空间目标的翻译，并支持多模态目标导航。


<details>
  <summary>Details</summary>
Motivation: 现有方法在环境映射、空间精度和多模态信息利用方面存在不足，需要一种更全面的空间地图表示方法。

Method: 提出VLMaps和AVLMaps，结合预训练多模态特征与3D重建，支持自然语言命令翻译和多模态目标导航。

Result: 实验表明，该方法在仿真和真实环境中实现了零样本空间和多模态目标导航，并在模糊场景中召回率提升50%。

Conclusion: 多模态空间语言地图为机器人导航和交互提供了更灵活和精确的支持。

Abstract: Grounding language to a navigating agent's observations can leverage
pretrained multimodal foundation models to match perceptions to object or event
descriptions. However, previous approaches remain disconnected from environment
mapping, lack the spatial precision of geometric maps, or neglect additional
modality information beyond vision. To address this, we propose multimodal
spatial language maps as a spatial map representation that fuses pretrained
multimodal features with a 3D reconstruction of the environment. We build these
maps autonomously using standard exploration. We present two instances of our
maps, which are visual-language maps (VLMaps) and their extension to
audio-visual-language maps (AVLMaps) obtained by adding audio information. When
combined with large language models (LLMs), VLMaps can (i) translate natural
language commands into open-vocabulary spatial goals (e.g., "in between the
sofa and TV") directly localized in the map, and (ii) be shared across
different robot embodiments to generate tailored obstacle maps on demand.
Building upon the capabilities above, AVLMaps extend VLMaps by introducing a
unified 3D spatial representation integrating audio, visual, and language cues
through the fusion of features from pretrained multimodal foundation models.
This enables robots to ground multimodal goal queries (e.g., text, images, or
audio snippets) to spatial locations for navigation. Additionally, the
incorporation of diverse sensory inputs significantly enhances goal
disambiguation in ambiguous environments. Experiments in simulation and
real-world settings demonstrate that our multimodal spatial language maps
enable zero-shot spatial and multimodal goal navigation and improve recall by
50% in ambiguous scenarios. These capabilities extend to mobile robots and
tabletop manipulators, supporting navigation and interaction guided by visual,
audio, and spatial cues.

</details>


### [685] [Machine Learning-Based Self-Localization Using Internal Sensors for Automating Bulldozers](https://arxiv.org/abs/2506.07271)
*Hikaru Sawafuji,Ryota Ozaki,Takuto Motomura,Toyohisa Matsuda,Masanori Tojima,Kento Uchida,Shinichi Shirakawa*

Main category: cs.RO

TL;DR: 论文提出了一种基于机器学习的推土机自定位方法，以解决RTK-GNSS信号丢失的问题。该方法结合局部速度估计和扩展卡尔曼滤波，实验表明其能有效减少位置误差。


<details>
  <summary>Details</summary>
Motivation: RTK-GNSS信号在某些采矿条件下会丢失，因此需要不依赖RTK-GNSS的自定位方法。

Method: 通过机器学习模型从内部传感器估计局部速度，并将其输入扩展卡尔曼滤波器进行全局定位。

Result: 实验表明，该方法在滑动等情况下比基于运动学的方法更能抑制位置误差累积，且推土机专用传感器有助于提高定位精度。

Conclusion: 基于机器学习的自定位方法在推土机应用中具有潜力，尤其在RTK-GNSS信号不可靠时。

Abstract: Self-localization is an important technology for automating bulldozers.
Conventional bulldozer self-localization systems rely on RTK-GNSS (Real Time
Kinematic-Global Navigation Satellite Systems). However, RTK-GNSS signals are
sometimes lost in certain mining conditions. Therefore, self-localization
methods that do not depend on RTK-GNSS are required. In this paper, we propose
a machine learning-based self-localization method for bulldozers. The proposed
method consists of two steps: estimating local velocities using a machine
learning model from internal sensors, and incorporating these estimates into an
Extended Kalman Filter (EKF) for global localization. We also created a novel
dataset for bulldozer odometry and conducted experiments across various driving
scenarios, including slalom, excavation, and driving on slopes. The result
demonstrated that the proposed self-localization method suppressed the
accumulation of position errors compared to kinematics-based methods,
especially when slip occurred. Furthermore, this study showed that
bulldozer-specific sensors, such as blade position sensors and hydraulic
pressure sensors, contributed to improving self-localization accuracy.

</details>


### [686] [Real-Time Execution of Action Chunking Flow Policies](https://arxiv.org/abs/2506.07339)
*Kevin Black,Manuel Y. Galliker,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出了一种实时分块（RTC）算法，用于解决AI系统在高频控制任务中的延迟问题，实现平滑异步执行动作分块策略。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在实时性能上的需求日益增长，但现有通用模型的延迟问题限制了其在高频控制任务中的应用。

Method: RTC算法无需重新训练，适用于任何基于扩散或流的视觉语言动作模型，通过在执行当前动作分块时生成下一个分块，确保动作平滑过渡。

Result: 在Kinetix模拟器和真实世界双手机器人任务中，RTC显著提高了任务吞吐量和成功率，尤其在存在延迟的情况下表现优异。

Conclusion: RTC是一种高效、鲁棒的解决方案，能够显著提升AI系统在实时任务中的性能。

Abstract: Modern AI systems, especially those interacting with the physical world,
increasingly require real-time performance. However, the high latency of
state-of-the-art generalist models, including recent vision-language action
models (VLAs), poses a significant challenge. While action chunking has enabled
temporal consistency in high-frequency control tasks, it does not fully address
the latency problem, leading to pauses or out-of-distribution jerky movements
at chunk boundaries. This paper presents a novel inference-time algorithm that
enables smooth asynchronous execution of action chunking policies. Our method,
real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out
of the box with no re-training. It generates the next action chunk while
executing the current one, "freezing" actions guaranteed to execute and
"inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly
dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging
real-world bimanual manipulation tasks. Results demonstrate that RTC is fast,
performant, and uniquely robust to inference delay, significantly improving
task throughput and enabling high success rates in precise tasks
$\unicode{x2013}$ such as lighting a match $\unicode{x2013}$ even in the
presence of significant latency. See
https://pi.website/research/real_time_chunking for videos.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [687] [MLOps with Microservices: A Case Study on the Maritime Domain](https://arxiv.org/abs/2506.06202)
*Renato Cordeiro Ferreira,Rowanne Trapmann,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 本文介绍了构建海洋监测系统Ocean Guard的挑战与经验，重点是其微服务架构和基于合同的设计方法在MLOps中的应用。


<details>
  <summary>Details</summary>
Motivation: 为海事领域的异常检测开发一个机器学习系统（MLES），并通过微服务架构和合同设计实现团队协作。

Method: 采用微服务架构和基于合同的设计（代码、模型和数据合同）来协调多团队并行开发。

Result: 成功构建了Ocean Guard系统，展示了合同设计在MLOps中的有效性。

Conclusion: 该案例为软件工程师、机器学习工程师和数据科学家提供了可借鉴的系统设计方法。

Abstract: This case study describes challenges and lessons learned on building Ocean
Guard: a Machine Learning-Enabled System (MLES) for anomaly detection in the
maritime domain. First, the paper presents the system's specification, and
architecture. Ocean Guard was designed with a microservices' architecture to
enable multiple teams to work on the project in parallel. Then, the paper
discusses how the developers adapted contract-based design to MLOps for
achieving that goal. As a MLES, Ocean Guard employs code, model, and data
contracts to establish guidelines between its services. This case study hopes
to inspire software engineers, machine learning engineers, and data scientists
to leverage similar approaches for their systems.

</details>


### [688] [MLOps with Microservices: A Case Study on the Maritime Domain](https://arxiv.org/abs/2506.06202)
*Renato Cordeiro Ferreira,Rowanne Trapmann,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: Ocean Guard是一个基于机器学习的异常检测系统，采用微服务架构和契约设计，旨在为海事领域提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决海事领域异常检测的挑战，并通过微服务和契约设计实现多团队并行开发。

Method: 采用微服务架构和契约设计（代码、模型和数据契约）来构建系统。

Result: 成功构建了Ocean Guard系统，展示了契约设计在MLOps中的适用性。

Conclusion: 该案例为工程师和科学家提供了构建类似系统的灵感和方法。

Abstract: This case study describes challenges and lessons learned on building Ocean
Guard: a Machine Learning-Enabled System (MLES) for anomaly detection in the
maritime domain. First, the paper presents the system's specification, and
architecture. Ocean Guard was designed with a microservices' architecture to
enable multiple teams to work on the project in parallel. Then, the paper
discusses how the developers adapted contract-based design to MLOps for
achieving that goal. As a MLES, Ocean Guard employs code, model, and data
contracts to establish guidelines between its services. This case study hopes
to inspire software engineers, machine learning engineers, and data scientists
to leverage similar approaches for their systems.

</details>


### [689] [Is Your Training Pipeline Production-Ready? A Case Study in the Healthcare Domain](https://arxiv.org/abs/2506.06946)
*Daniel Lawand,Lucas Quaresma,Roberto Bolgheroni,Alfredo Goldman,Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 论文探讨了将机器学习训练管道部署到生产环境中的挑战，以SPIRA项目为例，比较了三种架构版本，从混乱到模块化再到微服务，以提高可维护性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过改进架构设计原则和模式，提升机器学习训练管道的软件质量属性（如可维护性、健壮性和扩展性）。

Method: 通过SPIRA项目，比较了三种架构版本（Big Ball of Mud、Modular Monolith、Microservices），分析其设计原则和模式的应用。

Result: 展示了架构演化的过程，证明了采用模块化和微服务架构能显著提升系统的可维护性和扩展性。

Conclusion: 为ML工程师和数据科学家提供了将ML训练管道生产化和采用MLOps实践的实用见解。

Abstract: Deploying a Machine Learning (ML) training pipeline into production requires
robust software engineering practices. This differs significantly from
experimental workflows. This experience report investigates this challenge in
SPIRA, a project whose goal is to create an ML-Enabled System (MLES) to
pre-diagnose insufficiency respiratory via speech analysis. The first version
of SPIRA's training pipeline lacked critical software quality attributes. This
paper presents an overview of the MLES, then compares three versions of the
architecture of the Continuous Training subsystem, which evolved from a Big
Ball of Mud, to a Modular Monolith, towards Microservices. By adopting
different design principles and patterns to enhance its maintainability,
robustness, and extensibility. In this way, the paper seeks to offer insights
for both ML Engineers tasked to productionize ML training pipelines and Data
Scientists seeking to adopt MLOps practices.

</details>


### [690] [Towards a Small Language Model Lifecycle Framework](https://arxiv.org/abs/2506.07695)
*Parsa Miraghaei,Sergio Moreschini,Antti Kolehmainen,David Hästbacka*

Main category: cs.SE

TL;DR: 该研究提出了一个全面的小型语言模型（SLM）生命周期框架，通过整合学术和实践资源，填补了现有研究的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 由于对高效且可部署语言模型的需求增长，但现有研究缺乏统一的生命周期视角，因此需要建立一个综合框架。

Method: 通过对36项工作的全面调查，分析并分类了与生命周期相关的技术。

Result: 提出了一个模块化的生命周期模型，包括主要、可选和跨领域组件，支持方法重用、协同适应和生命周期意识。

Conclusion: 该框架为SLM的开发和维护提供了连贯的基础，连接理论与实践，并指导未来研究和工具开发。

Abstract: Background: The growing demand for efficient and deployable language models
has led to increased interest in Small Language Models (SLMs). However,
existing research remains fragmented, lacking a unified lifecycle perspective.
  Objective: This study aims to define a comprehensive lifecycle framework for
SLMs by synthesizing insights from academic literature and practitioner
sources.
  Method: We conducted a comprehensive survey of 36 works, analyzing and
categorizing lifecycle-relevant techniques.
  Results: We propose a modular lifecycle model structured into main, optional,
and cross-cutting components. The model captures key interconnections across
stages, supporting method reuse, co-adaptation, and lifecycle-awareness.
  Conclusion: Our framework provides a coherent foundation for developing and
maintaining SLMs, bridging theory and practice, and guiding future research and
tool development.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [691] [Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning](https://arxiv.org/abs/2506.06306)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: eess.SP

TL;DR: 研究旨在利用多模态传感器数据开发机器学习方法，早期预测社区居住痴呆患者的躁动行为，引入新的上下文特征并评估多种模型，最终实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 躁动是痴呆患者常见行为，及时预测可减少护理负担并改善生活质量。

Method: 使用TIHM数据集，引入新特征，评估多种机器学习模型（包括二分类和异常检测）。

Result: 最佳模型AUC-ROC达0.9720，AUC-PR为0.4320。

Conclusion: 该方法支持高效、可解释的躁动预测，有助于主动护理和居家养老。

Abstract: Agitation is one of the most common responsive behaviors in people living
with dementia, particularly among those residing in community settings without
continuous clinical supervision. Timely prediction of agitation can enable
early intervention, reduce caregiver burden, and improve the quality of life
for both patients and caregivers. This study aimed to develop and benchmark
machine learning approaches for the early prediction of agitation in
community-dwelling older adults with dementia using multimodal sensor data. A
new set of agitation-related contextual features derived from activity data was
introduced and employed for agitation prediction. A wide range of machine
learning and deep learning models was evaluated across multiple problem
formulations, including binary classification for single-timestamp tabular
sensor data and multi-timestamp sequential sensor data, as well as anomaly
detection for single-timestamp tabular sensor data. The study utilized the
Technology Integrated Health Management (TIHM) dataset, the largest publicly
available dataset for remote monitoring of people living with dementia,
comprising 2,803 days of in-home activity, physiology, and sleep data. The most
effective setting involved binary classification of sensor data using the
current 6-hour timestamp to predict agitation at the subsequent timestamp.
Incorporating additional information, such as time of day and agitation
history, further improved model performance, with the highest AUC-ROC of 0.9720
and AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work
presents the first comprehensive benchmarking of state-of-the-art techniques
for agitation prediction in community-based dementia care using
privacy-preserving sensor data. The approach enables accurate, explainable, and
efficient agitation prediction, supporting proactive dementia care and aging in
place.

</details>


### [692] [Leveraging Novel Ensemble Learning Techniques and Landsat Multispectral Data for Estimating Olive Yields in Tunisia](https://arxiv.org/abs/2506.06309)
*Mohamed Kefi,Tien Dat Pham,Thin Nguyen,Mark G. Tjoelker,Viola Devasirvatham,Kenichi Kashiwagi*

Main category: eess.SP

TL;DR: 开发了一个基于遥感和机器学习的自动化橄榄产量估算框架，结合多光谱数据和实地调查，取得了高精度的预测结果。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致橄榄产量波动大，需要一种准确且可扩展的估算方法。

Method: 利用Landsat-8和Landsat-9的多光谱数据、植被指数和数字高程模型，结合实地调查数据，通过AutoGluon构建自动化集成学习框架。

Result: Landsat-8和Landsat-9的预测性能均较强，R2分别为0.8635和0.8378，RMSE分别为1.17和1.32吨/公顷。

Conclusion: 该方法具有可扩展性、成本效益和准确性，适用于全球多样化的农业区域。

Abstract: Olive production is an important tree crop in Mediterranean climates.
However, olive yield varies significantly due to climate change. Accurately
estimating yield using remote sensing and machine learning remains a complex
challenge. In this study, we developed a streamlined pipeline for olive yield
estimation in the Kairouan and Sousse governorates of Tunisia. We extracted
features from multispectral reflectance bands, vegetation indices derived from
Landsat-8 OLI and Landsat-9 OLI-2 satellite imagery, along with digital
elevation model data. These spatial features were combined with ground-based
field survey data to form a structured tabular dataset. We then developed an
automated ensemble learning framework, implemented using AutoGluon to train and
evaluate multiple machine learning models, select optimal combinations through
stacking, and generate robust yield predictions using five-fold
cross-validation. The results demonstrate strong predictive performance from
both sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per
ha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha. This
study highlights a scalable, cost-effective, and accurate method for olive
yield estimation, with potential applicability across diverse agricultural
regions globally.

</details>


### [693] [Enhancing Contrastive Learning-based Electrocardiogram Pretrained Model with Patient Memory Queue](https://arxiv.org/abs/2506.06310)
*Xiaoyu Sun,Yang Yang,Xunde Dong*

Main category: eess.SP

TL;DR: 提出了一种基于患者记忆队列（PMQ）的对比学习ECG预训练模型，通过增强患者一致性提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于标记数据有限，如何利用未标记数据构建鲁棒的ECG预训练模型是关键。现有方法未能充分利用患者一致性。

Method: 引入患者记忆队列（PMQ）以解决批次内患者样本不足问题，并采用两种额外数据增强方法。

Result: 在三个公共数据集上的实验表明，该方法性能优于现有对比学习方法，且在标记数据有限时更具鲁棒性。

Conclusion: PMQ方法有效提升了ECG预训练模型的性能，尤其在数据稀缺场景下表现优异。

Abstract: In the field of automatic Electrocardiogram (ECG) diagnosis, due to the
relatively limited amount of labeled data, how to build a robust ECG pretrained
model based on unlabeled data is a key area of focus for researchers. Recent
advancements in contrastive learning-based ECG pretrained models highlight the
potential of exploiting the additional patient-level self-supervisory signals
inherent in ECG. They are referred to as patient contrastive learning. Its
rationale is that multiple physical recordings from the same patient may share
commonalities, termed patient consistency, so redefining positive and negative
pairs in contrastive learning as intrapatient and inter-patient samples
provides more shared context to learn an effective representation. However,
these methods still fail to efficiently exploit patient consistency due to the
insufficient amount of intra-inter patient samples existing in a batch. Hence,
we propose a contrastive learning-based ECG pretrained model enhanced by the
Patient Memory Queue (PMQ), which incorporates a large patient memory queue to
mitigate model degeneration that can arise from insufficient intra-inter
patient samples. In order to further enhance the performance of the pretrained
model, we introduce two extra data augmentation methods to provide more
perspectives of positive and negative pairs for pretraining. Extensive
experiments were conducted on three public datasets with three different data
ratios. The experimental results show that the comprehensive performance of our
method outperforms previous contrastive learning methods and exhibits greater
robustness in scenarios with limited labeled data. The code is available at
https://github.com/3hiuwoo/PMQ.

</details>


### [694] [A Novel Shape-Aware Topological Representation for GPR Data with DNN Integration](https://arxiv.org/abs/2506.06311)
*Meiyan Kang,Shizuo Kaji,Sang-Yun Lee,Taegon Kim,Hee-Hwan Ryu,Suyoung Choi*

Main category: eess.SP

TL;DR: 该论文提出了一种结合拓扑数据分析和YOLOv5深度学习网络的新框架，用于增强地下管线的检测能力，并通过Sim2Real策略解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 传统的地面穿透雷达（GPR）解释方法在噪声敏感性和结构感知方面存在局限性，影响了地下管线检测的准确性。

Method: 通过拓扑数据分析（TDA）从B-scan GPR图像中提取形状感知的拓扑特征，并与YOLOv5结合，同时采用Sim2Real策略生成合成数据以弥补真实数据的不足。

Result: 实验结果显示，该方法在平均精度（mAP）上有显著提升，验证了其鲁棒性和有效性。

Conclusion: 该研究展示了TDA增强学习在实时地下物体检测中的潜力，适用于城市规划、安全检查和基础设施管理等领域。

Abstract: Ground Penetrating Radar (GPR) is a widely used Non-Destructive Testing (NDT)
technique for subsurface exploration, particularly in infrastructure inspection
and maintenance. However, conventional interpretation methods are often limited
by noise sensitivity and a lack of structural awareness. This study presents a
novel framework that enhances the detection of underground utilities,
especially pipelines, by integrating shape-aware topological features derived
from B-scan GPR images using Topological Data Analysis (TDA), with the spatial
detection capabilities of the YOLOv5 deep neural network (DNN). We propose a
novel shape-aware topological representation that amplifies structural features
in the input data, thereby improving the model's responsiveness to the
geometrical features of buried objects. To address the scarcity of annotated
real-world data, we employ a Sim2Real strategy that generates diverse and
realistic synthetic datasets, effectively bridging the gap between simulated
and real-world domains. Experimental results demonstrate significant
improvements in mean Average Precision (mAP), validating the robustness and
efficacy of our approach. This approach underscores the potential of
TDA-enhanced learning in achieving reliable, real-time subsurface object
detection, with broad applications in urban planning, safety inspection, and
infrastructure management.

</details>


### [695] [An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation](https://arxiv.org/abs/2506.06315)
*Masoud Rahimi,Reza Karbasi,Abdol-Hossein Vahabie*

Main category: eess.SP

TL;DR: 介绍了一个开源Python框架，用于生成合成ECG图像数据集，支持深度学习任务如ECG数字化、导联区域和名称检测以及波形分割。


<details>
  <summary>Details</summary>
Motivation: 推动ECG分析中的深度学习任务，提供多样化的数据集以支持研究。

Method: 利用PTB-XL信号数据集生成四种开放数据集，包括图像与信号配对、YOLO格式标注、单导联分割掩码等。

Result: 发布了开源框架和四种数据集，支持多种ECG分析任务。

Conclusion: 该框架和数据集为ECG分析研究提供了重要资源，促进深度学习应用的发展。

Abstract: We introduce an open-source Python framework for generating synthetic ECG
image datasets to advance critical deep learning-based tasks in ECG analysis,
including ECG digitization, lead region and lead name detection, and
pixel-level waveform segmentation. Using the PTB-XL signal dataset, our
proposed framework produces four open-access datasets: (1) ECG images in
various lead configurations paired with time-series signals for ECG
digitization, (2) ECG images annotated with YOLO-format bounding boxes for
detection of lead region and lead name, (3)-(4) cropped single-lead images with
segmentation masks compatible with U-Net-based models in normal and overlapping
versions. In the overlapping case, waveforms from neighboring leads are
superimposed onto the target lead image, while the segmentation masks remain
clean. The open-source Python framework and datasets are publicly available at
https://github.com/rezakarbasi/ecg-image-and-signal-dataset and
https://doi.org/10.5281/zenodo.15484519, respectively.

</details>


### [696] [Composite Reward Design in PPO-Driven Adaptive Filtering](https://arxiv.org/abs/2506.06323)
*Abdullah Burkan Bereketoglu*

Main category: eess.SP

TL;DR: 提出了一种基于PPO的自适应滤波框架，用于动态非平稳环境中的去噪，优于传统滤波器。


<details>
  <summary>Details</summary>
Motivation: 传统滤波器（如LMS、RLS等）在动态非平稳环境中表现受限，需要复杂调参或固定模型假设。

Method: 使用PPO算法，通过复合奖励函数（SNR提升、MSE降低和残差平滑）指导自适应滤波。

Result: 在合成信号上实验表明，PPO滤波器泛化能力强，实时性能优越，超越传统滤波器。

Conclusion: 证明了策略梯度强化学习在鲁棒、低延迟自适应信号滤波中的可行性。

Abstract: Model-free and reinforcement learning-based adaptive filtering methods are
gaining traction for denoising in dynamic, non-stationary environments such as
wireless signal channels. Traditional filters like LMS, RLS, Wiener, and Kalman
are limited by assumptions of stationary or requiring complex fine-tuning or
exact noise statistics or fixed models. This letter proposes an adaptive
filtering framework using Proximal Policy Optimization (PPO), guided by a
composite reward that balances SNR improvement, MSE reduction, and residual
smoothness. Experiments on synthetic signals with various noise types show that
our PPO agent generalizes beyond its training distribution, achieving real-time
performance and outperforming classical filters. This work demonstrates the
viability of policy-gradient reinforcement learning for robust, low-latency
adaptive signal filtering.

</details>


### [697] [Uncertainty-Aware Multi-view Arrhythmia Classification from ECG](https://arxiv.org/abs/2506.06342)
*Mohd Ashhad,Sana Rahmani,Mohammed Fayiz,Ali Etemad,Javad Hashemi*

Main category: eess.SP

TL;DR: 提出一种深度神经网络架构，用于ECG心律失常的多视角不确定性感知分类，通过融合1D和2D视图减少噪声干扰，提升分类性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: ECG数据中的噪声和伪影可能导致多视角信息冲突，需要一种方法融合不同视图并引入不确定性以提高分类准确性和鲁棒性。

Method: 包含三个模块：(1) 时间序列模块学习ECG形态特征；(2) 图像空间模块学习时空特征；(3) 不确定性感知融合模块融合多视角信息。

Result: 在两个真实数据集上验证，分类性能优于现有方法，对噪声和伪影表现更鲁棒。

Conclusion: 该方法通过多视角融合和不确定性建模，显著提升了ECG心律失常分类的性能和鲁棒性。

Abstract: We propose a deep neural architecture that performs uncertainty-aware
multi-view classification of arrhythmia from ECG. Our method learns two
different views (1D and 2D) of single-lead ECG to capture different types of
information. We use a fusion technique to reduce the conflict between the
different views caused by noise and artifacts in ECG data, thus incorporating
uncertainty to obtain stronger final predictions. Our framework contains the
following three modules (1) a time-series module to learn the morphological
features from ECG; (2) an image-space learning module to learn the
spatiotemporal features; and (3) the uncertainty-aware fusion module to fuse
the information from the two different views. Experimental results on two
real-world datasets demonstrate that our framework not only improves the
performance on arrhythmia classification compared to the state-of-the-art but
also shows better robustness to noise and artifacts present in ECG.

</details>


### [698] [A Reinforcement Learning Approach for RIS-aided Fair Communications](https://arxiv.org/abs/2506.06344)
*Alex Pierron,Michel Barbeau,Luca De Cicco,Jose Rubio-Hernan,Joaquin Garcia-Alfaro*

Main category: eess.SP

TL;DR: 本文提出了一种结合可重构智能表面（RIS）和强化学习（RL）的新方法，旨在实现高效且公平的多用户通信系统。


<details>
  <summary>Details</summary>
Motivation: 解决现有RIS-RL系统中公平性问题，确保所有用户设备（UE）都能获得足够的信号强度，避免因功率不足导致的服务剥夺。

Method: 提出了一种新颖的RIS-RL系统优化方法，通过实验和仿真验证其性能。

Result: 实验结果表明，该方法在保证网络性能和能源效率的同时，实现了通信公平性。

Conclusion: 该方法为多用户RIS-RL系统提供了一种高效且公平的解决方案，并开源了代码和数据集以促进进一步研究。

Abstract: Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements
that can dynamically alter electromagnetic wave properties to enhance
beamforming and leading to improvements in areas with low coverage properties.
They have the potential to be combined with Reinforcement Learning (RL)
techniques to achieve network performance and energy efficiency via
optimization techniques. In addition to performance and energy improvements, it
is also crucial to consider the concept of fair communications. RISs must
ensure that User Equipment (UE) units receive their signals with adequate
strength, without other UE being deprived of service due to insufficient power.
In this paper, we address such a problem. We explore the fairness properties of
previous work and propose a novel method that aims at obtaining an efficient
and fair duplex RIS-RL system for multiple legitimate UE units. We report and
discuss our experimental work and simulation results. We also release our code
and datasets to foster further research in the topic.

</details>


### [699] [LD-RPMNet: Near-Sensor Diagnosis for Railway Point Machines](https://arxiv.org/abs/2506.06346)
*Wei Li,Xiaochun Wu,Xiaoxi Hu,Yuxuan Zhang,Sebastian Bader,Yuhan Huang*

Main category: eess.SP

TL;DR: 提出了一种轻量级模型LD-RPMNet，结合Transformer和CNN，优化铁路应用的近传感器故障诊断。


<details>
  <summary>Details</summary>
Motivation: 近传感器诊断在工业中日益普及，但需要高效的计算模型。

Method: LD-RPMNet结合多尺度深度可分离卷积（MDSC）模块和广播自注意力（BSA）机制，优化特征提取和计算效率。

Result: 实验显示模型参数和计算复杂度降低50%，诊断准确率提升近3%，达到98.86%。

Conclusion: 证明了近传感器故障诊断在铁路转辙机中的可行性。

Abstract: Near-sensor diagnosis has become increasingly prevalent in industry. This
study proposes a lightweight model named LD-RPMNet that integrates Transformers
and Convolutional Neural Networks, leveraging both local and global feature
extraction to optimize computational efficiency for a practical railway
application. The LD-RPMNet introduces a Multi-scale Depthwise Separable
Convolution (MDSC) module, which decomposes cross-channel convolutions into
pointwise and depthwise convolutions while employing multi-scale kernels to
enhance feature extraction. Meanwhile, a Broadcast Self-Attention (BSA)
mechanism is incorporated to simplify complex matrix multiplications and
improve computational efficiency. Experimental results based on collected sound
signals during the operation of railway point machines demonstrate that the
optimized model reduces parameter count and computational complexity by 50%
while improving diagnostic accuracy by nearly 3%, ultimately achieving an
accuracy of 98.86%. This demonstrates the possibility of near-sensor fault
diagnosis applications in railway point machines.

</details>


### [700] [Multi-Platform Methane Plume Detection via Model and Domain Adaptation](https://arxiv.org/abs/2506.06348)
*Vassiliki Mancoridis,Brian Bue,Jake H. Lee,Andrew K. Thorpe,Daniel Cusworth,Alana Ayasse,Philip G. Brodrick,Riley Duren*

Main category: eess.SP

TL;DR: 本文提出了一种结合模型和数据驱动的机器学习方法，通过迁移学习和CycleGAN技术，优化了跨平台的甲烷羽流检测，显著提升了空间观测数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 甲烷对全球变暖影响显著，但现有甲烷羽流检测方法在跨平台数据对齐方面存在不足，亟需解决以提高检测效率。

Method: 利用AVIRIS-NG的机载观测数据，通过迁移学习优化空间观测模型，并使用CycleGAN进行数据分布对齐。

Result: 结合CycleGAN和迁移学习的方法显著提升了空间观测数据的甲烷羽流检测效果，优于独立的空间模型。

Conclusion: 该方法不仅适用于甲烷羽流检测，还可推广至其他遥感数据的跨平台对齐任务。

Abstract: Prioritizing methane for near-term climate action is crucial due to its
significant impact on global warming. Previous work used columnwise matched
filter products from the airborne AVIRIS-NG imaging spectrometer to detect
methane plume sources; convolutional neural networks (CNNs) discerned
anthropogenic methane plumes from false positive enhancements. However, as an
increasing number of remote sensing platforms are used for methane plume
detection, there is a growing need to address cross-platform alignment. In this
work, we describe model- and data-driven machine learning approaches that
leverage airborne observations to improve spaceborne methane plume detection,
reconciling the distributional shifts inherent with performing the same task
across platforms. We develop a spaceborne methane plume classifier using data
from the EMIT imaging spectroscopy mission. We refine classifiers trained on
airborne imagery from AVIRIS-NG campaigns using transfer learning,
outperforming the standalone spaceborne model. Finally, we use CycleGAN, an
unsupervised image-to-image translation technique, to align the data
distributions between airborne and spaceborne contexts. Translating spaceborne
EMIT data to the airborne AVIRIS-NG domain using CycleGAN and applying airborne
classifiers directly yields the best plume detection results. This methodology
is useful not only for data simulation, but also for direct data alignment.
Though demonstrated on the task of methane plume detection, our work more
broadly demonstrates a data-driven approach to align related products obtained
from distinct remote sensing instruments.

</details>


### [701] [Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning](https://arxiv.org/abs/2506.06349)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: eess.SP

TL;DR: 该研究通过传统机器学习和深度学习方法对ECG信号进行分类，发现基于手工特征的LightGBM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决ECG信号中心跳分类的问题，比较不同方法的性能。

Method: 采用两种方法：传统机器学习（手工特征提取）和深度学习（ECG信号图像化后分类）。

Result: LightGBM模型表现最优（准确率99%，F1分数0.94），优于基于图像的CNN方法。

Conclusion: 手工特征能更好捕捉ECG信号的时态和形态变化，未来可结合多导联信号提升分类效果。

Abstract: This study addresses the classification of heartbeats from ECG signals
through two distinct approaches: traditional machine learning utilizing
hand-crafted features and deep learning via transformed images of ECG beats.
The dataset underwent preprocessing steps, including downsampling, filtering,
and normalization, to ensure consistency and relevance for subsequent analysis.
In the first approach, features such as heart rate variability (HRV), mean,
variance, and RR intervals were extracted to train various classifiers,
including SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and
LightGBM. The second approach involved transforming ECG signals into images
using Gramian Angular Field (GAF), Markov Transition Field (MTF), and
Recurrence Plots (RP), with these images subsequently classified using CNN
architectures like VGG and Inception.
  Experimental results demonstrate that the LightGBM model achieved the highest
performance, with an accuracy of 99% and an F1 score of 0.94, outperforming the
image-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost
yielded significantly lower scores, indicating limited suitability for this
task. The findings underscore the superior ability of hand-crafted features to
capture temporal and morphological variations in ECG signals compared to
image-based representations of individual beats. Future investigations may
benefit from incorporating multi-lead ECG signals and temporal dependencies
across successive beats to enhance classification accuracy further.

</details>


### [702] [Deep learning methods for modeling infrasound transmission loss in the middle atmosphere](https://arxiv.org/abs/2506.06351)
*Alexis Le Pichon,Alice Janela Cameijo,Samir Aknine,Youcef Sklab,Souhila Arib,Quentin Brissaud,Sven Peter Naesholm*

Main category: eess.SP

TL;DR: 该研究提出了一种优化的卷积神经网络，用于在全球范围内预测次声波传输损失（TLs），解决了之前方法在高频和不利风条件下的局限性。


<details>
  <summary>Details</summary>
Motivation: 准确建模次声波传输损失对评估国际监测系统性能至关重要，但现有方法计算成本高且在某些条件下效果不佳。

Method: 开发了一种优化的卷积神经网络，基于全球模拟的温度和风场数据，预测4000公里范围内的TLs。

Result: 优化后的模型在整个频段（0.1-3.2 Hz）内平均误差为8.6 dB，适用于多种实际大气场景。

Conclusion: 该方法显著提升了TLs预测的准确性和计算效率，为全球监测应用提供了实用工具。

Abstract: Accurate modeling of infrasound transmission losses (TLs) is essential to
assess the performance of the global International Monitoring System infrasound
network. Among existing propagation modeling tools, parabolic equation (PE)
method enables TLs to be finely modeled, but its computational cost does not
allow exploration of a large parameter space for operational monitoring
applications. To reduce computation times, Brissaud et al. 2023 explored the
potential of convolutional neural networks trained on a large set of regionally
simulated wavefields (< 1000 km from the source) to predict TLs with negligible
computation times compared to PE simulations. However, this method struggles in
unfavorable initial wind conditions, especially at high frequencies, and causal
issues with winds at large distances from the source affecting ground TLs close
to the source. In this study, we have developed an optimized convolutional
network designed to minimize prediction errors while predicting TLs from
globally simulated combined temperature and wind fields spanning over
propagation ranges of 4000 km. Our approach enhances the previously proposed
one by implementing key optimizations that improve the overall architecture
performance. The implemented model predicts TLs with an average error of 8.6 dB
in the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric
scenarios.

</details>


### [703] [Large Language Models for EEG: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2506.06353)
*Naseem Babu,Jimson Mathew,A. P. Vinod*

Main category: eess.SP

TL;DR: 本文综述了大型语言模型（LLMs）与脑电图（EEG）研究的结合，系统梳理了其在神经解码、脑机接口和情感计算中的新方向。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在EEG分析中的应用潜力，推动自然语言处理与神经信号分析的结合。

Method: 通过系统综述和分类，将文献分为四个领域：LLM启发的EEG表示学习、EEG到语言解码、跨模态生成及临床应用。

Result: LLMs通过微调、少样本和零样本学习，使EEG模型能完成复杂任务，如自然语言生成和诊断辅助。

Conclusion: 本文为未来研究提供了结构化资源，旨在通过语言模型桥接自然语言处理与神经信号分析。

Abstract: The growing convergence between Large Language Models (LLMs) and
electroencephalography (EEG) research is enabling new directions in neural
decoding, brain-computer interfaces (BCIs), and affective computing. This
survey offers a systematic review and structured taxonomy of recent
advancements that utilize LLMs for EEG-based analysis and applications. We
organize the literature into four domains: (1) LLM-inspired foundation models
for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal
generation including image and 3D object synthesis, and (4) clinical
applications and dataset management tools. The survey highlights how
transformer-based architectures adapted through fine-tuning, few-shot, and
zero-shot learning have enabled EEG-based models to perform complex tasks such
as natural language generation, semantic interpretation, and diagnostic
assistance. By offering a structured overview of modeling strategies, system
designs, and application areas, this work serves as a foundational resource for
future work to bridge natural language processing and neural signal analysis
through language models.

</details>


### [704] [Towards Generalizable Drowsiness Monitoring with Physiological Sensors: A Preliminary Study](https://arxiv.org/abs/2506.06360)
*Jiyao Wang,Suzan Ayas,Jiahao Zhang,Xiao Wen,Dengbo He,Birsen Donmez*

Main category: eess.SP

TL;DR: 论文分析了基于生理信号的睡意检测方法，发现不同睡意诱因会导致不同的生理反应，客观评估比主观评估更敏感。


<details>
  <summary>Details</summary>
Motivation: 睡意检测对驾驶安全至关重要，生理信号方法比摄像头方法更保护隐私，但现有数据集中生理指标与睡意标签的关联存在冲突。

Method: 通过分析四个数据集中的ECG、EDA和RESP信号，建立二元逻辑回归模型，识别与睡意相关的生理指标。

Result: 发现心率稳定性增加、呼吸幅度降低和基础EDA减少与睡意增强显著相关。

Conclusion: 研究结果提升了睡意检测的理解，并为未来通用监测设计提供了依据。

Abstract: Accurately detecting drowsiness is vital to driving safety. Among all
measures, physiological-signal-based drowsiness monitoring can be more
privacy-preserving than a camera-based approach. However, conflicts exist
regarding how physiological metrics are associated with different drowsiness
labels across datasets. Thus, we analyzed key features from electrocardiograms
(ECG), electrodermal activity (EDA), and respiratory (RESP) signals across four
datasets, where different drowsiness inducers (such as fatigue and low arousal)
and assessment methods (subjective vs. objective) were used. Binary logistic
regression models were built to identify the physiological metrics that are
associated with drowsiness. Findings indicate that distinct different
drowsiness inducers can lead to different physiological responses, and
objective assessments were more sensitive than subjective ones in detecting
drowsiness. Further, the increased heart rate stability, reduced respiratory
amplitude, and decreased tonic EDA are robustly associated with increased
drowsiness. The results enhance understanding of drowsiness detection and can
inform future generalizable monitoring designs.

</details>


### [705] [Transformer-Based Decomposition of Electrodermal Activity for Real-World Mental Health Applications](https://arxiv.org/abs/2506.06378)
*Charalampos Tsirmpas,Stasinos Konstantopoulos,Dimitris Andrikopoulos,Konstantina Kyriakouli,Panagiotis Fatouros*

Main category: eess.SP

TL;DR: 该研究比较了知识驱动、统计和深度学习方法在EDA信号分解中的表现，提出了一种新型Transformer模型（Feel Transformer），在真实数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 分解EDA信号为相位和张力成分对提取情感和生理标志物至关重要，但现有方法在真实数据中表现不足。

Method: 提出Feel Transformer模型，基于Autoformer架构，利用池化和趋势去除机制实现无监督分解。

Result: Feel Transformer在特征保真度和抗噪性上优于Ledalab、cvxEDA等方法。

Conclusion: 该模型在实时生物信号分析和心理健康干预中具有潜力。

Abstract: Decomposing Electrodermal Activity (EDA) into phasic (short-term,
stimulus-linked responses) and tonic (longer-term baseline) components is
essential for extracting meaningful emotional and physiological biomarkers.
This study presents a comparative analysis of knowledge-driven, statistical,
and deep learning-based methods for EDA signal decomposition, with a focus on
in-the-wild data collected from wearable devices. In particular, the authors
introduce the Feel Transformer, a novel Transformer-based model adapted from
the Autoformer architecture, designed to separate phasic and tonic components
without explicit supervision. The model leverages pooling and trend-removal
mechanisms to enforce physiologically meaningful decompositions. Comparative
experiments against methods such as Ledalab, cvxEDA, and conventional
detrending show that the Feel Transformer achieves a balance between feature
fidelity (SCR frequency, amplitude, and tonic slope) and robustness to noisy,
real-world data. The model demonstrates potential for real-time biosignal
analysis and future applications in stress prediction, digital mental health
interventions, and physiological forecasting.

</details>


### [706] [Model-based Neural Data Augmentation for sub-wavelength Radio Localization](https://arxiv.org/abs/2506.06387)
*Baptiste Chatelier,Vincent Corlay,Musa Furkan Keskin,Matthieu Crussière,Henk Wymeersch,Luc Le Magoarou*

Main category: eess.SP

TL;DR: 论文提出了一种基于模型神经网络的指纹定位方法，显著提升了复杂无线环境中的定位精度，同时降低了内存需求。


<details>
  <summary>Details</summary>
Motivation: 传统信号处理技术在复杂无线环境（尤其是非视距传播场景）中定位精度下降，而现有机器学习方法计算复杂度高。

Method: 使用模型神经网络学习位置到信道的映射，作为生成神经信道模型，增强指纹比较字典并减少内存需求。

Result: 在非视距环境中实现亚波长级定位精度，定位精度提升数个数量级，内存需求降低一个数量级。

Conclusion: 该方法在提升定位精度的同时显著减少内存需求，适用于复杂无线环境。

Abstract: The increasing deployment of large antenna arrays at base stations has
significantly improved the spatial resolution and localization accuracy of
radio-localization methods. However, traditional signal processing techniques
struggle in complex radio environments, particularly in scenarios dominated by
non line of sight (NLoS) propagation paths, resulting in degraded localization
accuracy. Recent developments in machine learning have facilitated the
development of machine learning-assisted localization techniques, enhancing
localization accuracy in complex radio environments. However, these methods
often involve substantial computational complexity during both the training and
inference phases. This work extends the well-established fingerprinting-based
localization framework by simultaneously reducing its memory requirements and
improving its accuracy. Specifically, a model-based neural network is used to
learn the location-to-channel mapping, and then serves as a generative neural
channel model. This generative model augments the fingerprinting comparison
dictionary while reducing the memory requirements. The proposed method
outperforms fingerprinting baselines by achieving sub-wavelength localization
accuracy, even in NLoS environments. Remarkably, it offers an improvement by
several orders of magnitude in localization accuracy, while simultaneously
reducing memory requirements by an order of magnitude compared to classical
fingerprinting methods.

</details>


### [707] [IQFM A Wireless Foundational Model for I/Q Streams in AI-Native 6G](https://arxiv.org/abs/2506.06718)
*Omar Mashaal,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: IQFM是首个基于原始I/Q信号的无线通信基础模型，支持多种任务，无需复杂预处理或手工特征。通过对比自监督学习框架，其轻量级编码器在少量标记样本下表现优异，并展示了在多任务学习中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索基于原始I/Q信号的无线通信基础模型，填补现有研究空白，并支持多样化任务。

Method: 提出IQFM模型，采用任务感知增强策略和对比自监督学习框架，预训练轻量级编码器。

Result: 在调制分类和AoA分类中分别达到99.67%和65.45%的准确率，优于监督基线，并在新任务中表现良好。

Conclusion: IQFM展示了原始I/Q信号基础模型在6G系统中高效、可重用编码器的潜力。

Abstract: Foundational models have shown remarkable potential in natural language
processing and computer vision, yet remain in their infancy in wireless
communications. While a few efforts have explored image-based modalities such
as channel state information (CSI) and frequency spectrograms, foundational
models that operate directly on raw IQ data remain largely unexplored. This
paper presents, IQFM, the first I/Q signal foundational model for wireless
communications. IQFM supporting diverse tasks: modulation classification,
angle-of-arrival (AoA), beam prediction, and RF fingerprinting, without heavy
preprocessing or handcrafted features. We also introduce a task-aware
augmentation strategy that categorizes transformations into core augmentations,
such as cyclic time shifting, and task-specific augmentations. This strategy
forms the basis for structured, task-dependent representation learning within a
contrastive self-supervised learning (SSL) framework. Using this strategy, the
lightweight encoder, pre-trained via SSL on over-the-air multi-antenna IQ data,
achieves up to 99.67% and 65.45% accuracy on modulation and AoA classification,
respectively, using only one labeled sample per class, outperforming supervised
baselines by up to 7x and 145x. The model also generalizes to
out-of-distribution tasks; when adapted to new tasks using only 500 samples per
class and minimal parameter updates via LoRA, the same frozen encoder achieves
94.15% on beam prediction (vs. 89.53% supervised), 50.00% on RML2016a
modulation classification (vs. 49.30%), and 96.05% on RF fingerprinting (vs.
96.64%). These results demonstrate the potential of raw IQ-based foundational
models as efficient, reusable encoders for multi-task learning in AI-native 6G
systems.

</details>


### [708] [Conditional Denoising Diffusion for ISAC Enhanced Channel Estimation in Cell-Free 6G](https://arxiv.org/abs/2506.06942)
*Mohammad Farzanullah,Han Zhang,Akram Bin Sediq,Ali Afana,Melike Erol-Kantarci*

Main category: eess.SP

TL;DR: 提出了一种基于条件去噪扩散模型（CDDM）和多模态变换器（MMT）的新框架，用于提升无蜂窝集成感知与通信（ISAC）系统中的信道估计性能。


<details>
  <summary>Details</summary>
Motivation: 解决无蜂窝ISAC系统中信道估计面临的导频污染和噪声信道估计等挑战，提升通信可靠性。

Method: 结合CDDM和MMT，利用感知信息作为输入，通过MMT捕捉感知与位置数据的模态间关系，CDDM迭代去噪和优化信道估计。

Result: 相比LS和MMSE估计器，NMSE分别提升8 dB和9 dB；相比传统去噪扩散模型（TDDM），NMSE提升27.8%，且在低信噪比和导频污染下表现稳健。

Conclusion: 该框架显著提升了信道估计性能，尤其在感知目标附近用户中表现优异，验证了感知与通信信道相关性的重要性。

Abstract: Cell-free Integrated Sensing and Communication (ISAC) aims to revolutionize
6th Generation (6G) networks. By combining distributed access points with ISAC
capabilities, it boosts spectral efficiency, situational awareness, and
communication reliability. Channel estimation is a critical step in cell-free
ISAC systems to ensure reliable communication, but its performance is usually
limited by challenges such as pilot contamination and noisy channel estimates.
This paper presents a novel framework leveraging sensing information as a key
input within a Conditional Denoising Diffusion Model (CDDM). In this framework,
we integrate CDDM with a Multimodal Transformer (MMT) to enhance channel
estimation in ISAC-enabled cell-free systems. The MMT encoder effectively
captures inter-modal relationships between sensing and location data, enabling
the CDDM to iteratively denoise and refine channel estimates. Simulation
results demonstrate that the proposed approach achieves significant performance
gains. As compared with Least Squares (LS) and Minimum Mean Squared Error
(MMSE) estimators, the proposed model achieves normalized mean squared error
(NMSE) improvements of 8 dB and 9 dB, respectively. Moreover, we achieve a
27.8% NMSE improvement compared to the traditional denoising diffusion model
(TDDM), which does not incorporate sensing channel information. Additionally,
the model exhibits higher robustness against pilot contamination and maintains
high accuracy under challenging conditions, such as low signal-to-noise ratios
(SNRs). According to the simulation results, the model performs well for users
near sensing targets by leveraging the correlation between sensing and
communication channels.

</details>


### [709] [Diffusion Models-Aided Uplink Channel Estimation for RIS-Assisted Systems](https://arxiv.org/abs/2506.07770)
*Yang Wang,Yin Xu,Cixiao Zhang,Zhiyong Chen,Xiaowu Ou,Mingzeng Dai,Meixia Tao,Wenjun Zhang*

Main category: eess.SP

TL;DR: 提出了一种基于扩散模型（DM）的可重构智能表面（RIS）辅助系统信道估计方法，通过确定性采样策略和轻量级网络设计，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在信道估计中存在随机性问题，且计算复杂度高，需要一种更高效且准确的方法。

Method: 将信道估计问题转化为去噪过程，采用确定性采样策略和步长对齐机制，并设计轻量级网络以减少参数。

Result: 在多种信噪比（SNR）下性能优于基线，NMSE提升达13.5 dB（SNR=0 dB），轻量级网络参数仅为原U-Net的6.59%。

Conclusion: 该方法在性能和实用性上均有显著提升，适用于RIS辅助系统的信道估计。

Abstract: This letter proposes a channel estimation method for reconfigurable
intelligent surface (RIS)-assisted systems through a novel diffusion model (DM)
framework. We reformulate the channel estimation problem as a denoising
process, which aligns with the reverse process of the DM. To overcome the
inherent randomness in the reverse process of conventional DM approaches, we
adopt a deterministic sampling strategy with a step alignment mechanism that
ensures the accuracy of channel estimation while adapting to different
signal-to-noise ratio (SNR). Furthermore, to reduce the number of parameters of
the U-Net, we meticulously design a lightweight network that achieves
comparable performance, thereby enhancing the practicality of our proposed
method. Extensive simulations demonstrate superior performance over a wide
range of SNRs compared to baselines. For instance, the proposed method achieves
performance improvements of up to 13.5 dB in normalized mean square error
(NMSE) at SNR = 0 dB. Notably, the proposed lightweight network exhibits almost
no performance loss compared to the original U-Net, while requiring only 6.59\%
of its parameters.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [710] [Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization](https://arxiv.org/abs/2506.06305)
*Noémie Bergues,Arthur Carré,Paul Join-Lambert,Brice Hoffmann,Arnaud Blondel,Hamza Tajmouati*

Main category: q-bio.BM

TL;DR: 提出了一种基于模板的两阶段方法，用于预测小分子在蛋白质结合位点的3D构象，优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 解决药物设计中预测小分子在蛋白质结合位点3D构象的挑战，尤其是当存在模板时。

Method: 两阶段方法：1) 基于流匹配的分子对齐生成初始3D坐标；2) 可微位姿优化，考虑形状、药效团相似性、内能及蛋白质结合口袋。

Result: 在新基准测试中表现优于标准对接工具和开放对齐方法，尤其在模板相似性低或配体灵活性高的情况下。

Conclusion: 该方法为药物设计中的3D构象预测提供了有效解决方案，尤其在模板引导下表现优异。

Abstract: Predicting the 3D conformation of small molecules within protein binding
sites is a key challenge in drug design. When a crystallized reference ligand
(template) is available, it provides geometric priors that can guide 3D pose
prediction. We present a two-stage method for ligand conformation generation
guided by such templates. In the first stage, we introduce a molecular
alignment approach based on flow-matching to generate 3D coordinates for the
ligand, using the template structure as a reference. In the second stage, a
differentiable pose optimization procedure refines this conformation based on
shape and pharmacophore similarities, internal energy, and, optionally, the
protein binding pocket. We evaluate our approach on a new benchmark of ligand
pairs co-crystallized with the same target and show that it outperforms
standard docking tools and open-access alignment methods, especially in cases
involving low similarity to the template or high ligand flexibility.

</details>


### [711] [Graph Neural Networks in Modern AI-aided Drug Discovery](https://arxiv.org/abs/2506.06915)
*Odin Zhang,Haitao Lin,Xujun Zhang,Xiaorui Wang,Zhenxing Wu,Qing Ye,Weibo Zhao,Jike Wang,Kejun Ying,Yu Kang,Chang-yu Hsieh,Tingjun Hou*

Main category: q-bio.BM

TL;DR: 该综述全面介绍了图神经网络（GNNs）在药物发现中的方法基础和应用，包括分子性质预测、虚拟筛选等任务，并探讨了最新方法进展与实际挑战。


<details>
  <summary>Details</summary>
Motivation: GNNs因其对分子图结构的直接操作能力，成为药物发现中强大的工具，本文旨在总结其方法与应用。

Method: 综述了GNNs的方法论基础，包括几何GNNs、可解释模型等，并探讨其与其他深度学习方法的结合。

Result: GNNs在药物发现中表现出色，但实际应用中仍面临挑战和方法瓶颈。

Conclusion: 未来需进一步优化GNNs在药物发现中的实际应用，解决现有挑战。

Abstract: Graph neural networks (GNNs), as topology/structure-aware models within deep
learning, have emerged as powerful tools for AI-aided drug discovery (AIDD). By
directly operating on molecular graphs, GNNs offer an intuitive and expressive
framework for learning the complex topological and geometric features of
drug-like molecules, cementing their role in modern molecular modeling. This
review provides a comprehensive overview of the methodological foundations and
representative applications of GNNs in drug discovery, spanning tasks such as
molecular property prediction, virtual screening, molecular generation,
biomedical knowledge graph construction, and synthesis planning. Particular
attention is given to recent methodological advances, including geometric GNNs,
interpretable models, uncertainty quantification, scalable graph architectures,
and graph generative frameworks. We also discuss how these models integrate
with modern deep learning approaches, such as self-supervised learning,
multi-task learning, meta-learning and pre-training. Throughout this review, we
highlight the practical challenges and methodological bottlenecks encountered
when applying GNNs to real-world drug discovery pipelines, and conclude with a
discussion on future directions.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [712] [Learning Safe Strategies for Value Maximizing Buyers in Uniform Price Auctions](https://arxiv.org/abs/2406.03674)
*Negin Golrezaei,Sourav Sahoo*

Main category: cs.DS

TL;DR: 研究重复统一价格多单位拍卖中的投标问题，提出一种安全投标策略，确保每轮满足RoI约束，并开发多项式时间算法学习最优策略。


<details>
  <summary>Details</summary>
Motivation: 解决投标者在多轮拍卖中如何最大化累积价值同时满足每轮RoI约束的问题。

Method: 引入$m$-uniform投标格式和安全投标策略，开发多项式时间算法学习最优策略。

Result: 算法在在线设置中实现亚线性遗憾，实验显示安全策略表现优于理论界限。

Conclusion: 安全策略在实践中表现优异，适合实际应用。

Abstract: We study the bidding problem in repeated uniform price multi-unit auctions
from the perspective of a single value-maximizing buyer who aims to maximize
their cumulative value over $T$ rounds while adhering to return-on-investment
(RoI) constraints in each round. Buyers adopt $m$-uniform bidding format, where
they submit $m$ bid-quantity pairs $(b_i, q_i)$ to demand $q_i$ units at bid
$b_i$. We introduce safe bidding strategies as those that satisfy RoI
constraints in every auction, regardless of competing bids. We show that these
strategies depend only on the valuation curve of the bidder, and the bidder can
focus on a finite subset of this class without loss of generality. While the
number of strategies in this subset is exponential in $m$, we develop a
polynomial-time algorithm to learn the optimal safe strategy that achieves
sublinear regret in the online setting, where regret is measured against a
clairvoyant benchmark that knows the competing bids a priori and selects a
fixed hindsight optimal safe strategy. We then evaluate the performance of safe
strategies against a clairvoyant that selects the optimal strategy from a
richer class of strategies in the online setting. In this scenario, we compute
the richness ratio, $\alpha\in(0, 1]$ for the class of strategies chosen by the
clairvoyant and show that our algorithm, designed to learn safe strategies,
achieves $\alpha$-approximate sublinear regret against these stronger
benchmarks. Experiments on semi-synthetic data from real-world auctions show
that safe strategies substantially outperform the derived theoretical bounds,
making them quite appealing in practice.

</details>


### [713] [Learning-Augmented Hierarchical Clustering](https://arxiv.org/abs/2506.05495)
*Vladimir Braverman,Jon C. Ergun,Chen Wang,Samson Zhou*

Main category: cs.DS

TL;DR: 论文提出了一种利用分裂预言机辅助的层次聚类算法，克服了传统方法的近似困难，并在多项式时间和近线性时间内实现了对Dasgupta和Moseley-Wang目标的近似优化。


<details>
  <summary>Details</summary>
Motivation: 传统层次聚类目标存在近似算法的硬性限制，研究旨在通过引入分裂预言机辅助，突破这些限制。

Method: 利用分裂预言机提供的辅助信息，设计多项式时间和近线性时间算法，分别优化Dasgupta和Moseley-Wang目标。

Result: 算法在多项式时间内实现了Dasgupta目标的O(1)近似，在近线性时间内实现了Moseley-Wang目标的(1-o(1))近似。

Conclusion: 分裂预言机能够帮助算法超越传统层次聚类方法的限制，并在子线性设置下扩展应用。

Abstract: Hierarchical clustering (HC) is an important data analysis technique in which
the goal is to recursively partition a dataset into a tree-like structure while
grouping together similar data points at each level of granularity.
Unfortunately, for many of the proposed HC objectives, there exist strong
barriers to approximation algorithms with the hardness of approximation. Thus,
we consider the problem of hierarchical clustering given auxiliary information
from natural oracles. Specifically, we focus on a *splitting oracle* which,
when provided with a triplet of vertices $(u,v,w)$, answers (possibly
erroneously) the pairs of vertices whose lowest common ancestor includes all
three vertices in an optimal tree, i.e., identifying which vertex ``splits
away'' from the others. Using such an oracle, we obtain the following results:
  - A polynomial-time algorithm that outputs a hierarchical clustering tree
with $O(1)$-approximation to the Dasgupta objective (Dasgupta [STOC'16]).
  - A near-linear time algorithm that outputs a hierarchical clustering tree
with $(1-o(1))$-approximation to the Moseley-Wang objective (Moseley and Wang
[NeurIPS'17]).
  Under the plausible Small Set Expansion Hypothesis, no polynomial-time
algorithm can achieve any constant approximation for Dasgupta's objective or
$(1-C)$-approximation for the Moseley-Wang objective for some constant $C>0$.
As such, our results demonstrate that the splitting oracle enables algorithms
to outperform standard HC approaches and overcome hardness constraints.
Furthermore, our approaches extend to sublinear settings, in which we show new
streaming and PRAM algorithms for HC with improved guarantees.

</details>


### [714] [Learning Safe Strategies for Value Maximizing Buyers in Uniform Price Auctions](https://arxiv.org/abs/2406.03674)
*Negin Golrezaei,Sourav Sahoo*

Main category: cs.DS

TL;DR: 研究重复统一价格多单位拍卖中的投标问题，提出安全投标策略，满足每轮RoI约束，并开发多项式时间算法学习最优策略。


<details>
  <summary>Details</summary>
Motivation: 解决在重复拍卖中，买家如何在满足每轮RoI约束下最大化累积价值的问题。

Method: 引入m-uniform投标格式和安全投标策略，开发多项式时间算法学习最优策略。

Result: 算法在在线设置中实现亚线性遗憾，实验显示安全策略优于理论界限。

Conclusion: 安全策略在实践中表现优异，具有实际应用价值。

Abstract: We study the bidding problem in repeated uniform price multi-unit auctions
from the perspective of a single value-maximizing buyer who aims to maximize
their cumulative value over $T$ rounds while adhering to return-on-investment
(RoI) constraints in each round. Buyers adopt $m$-uniform bidding format, where
they submit $m$ bid-quantity pairs $(b_i, q_i)$ to demand $q_i$ units at bid
$b_i$. We introduce safe bidding strategies as those that satisfy RoI
constraints in every auction, regardless of competing bids. We show that these
strategies depend only on the valuation curve of the bidder, and the bidder can
focus on a finite subset of this class without loss of generality. While the
number of strategies in this subset is exponential in $m$, we develop a
polynomial-time algorithm to learn the optimal safe strategy that achieves
sublinear regret in the online setting, where regret is measured against a
clairvoyant benchmark that knows the competing bids a priori and selects a
fixed hindsight optimal safe strategy. We then evaluate the performance of safe
strategies against a clairvoyant that selects the optimal strategy from a
richer class of strategies in the online setting. In this scenario, we compute
the richness ratio, $\alpha\in(0, 1]$ for the class of strategies chosen by the
clairvoyant and show that our algorithm, designed to learn safe strategies,
achieves $\alpha$-approximate sublinear regret against these stronger
benchmarks. Experiments on semi-synthetic data from real-world auctions show
that safe strategies substantially outperform the derived theoretical bounds,
making them quite appealing in practice.

</details>


### [715] [Learning-Augmented Hierarchical Clustering](https://arxiv.org/abs/2506.05495)
*Vladimir Braverman,Jon C. Ergun,Chen Wang,Samson Zhou*

Main category: cs.DS

TL;DR: 论文提出了一种利用分裂预言机辅助的分层聚类方法，克服了传统HC算法的近似困难，并提供了多项式时间和近线性时间的近似算法。


<details>
  <summary>Details</summary>
Motivation: 传统分层聚类（HC）目标存在近似算法的困难，因此研究如何利用辅助信息（如分裂预言机）来克服这些限制。

Method: 使用分裂预言机，提供顶点三元组的信息，设计多项式时间和近线性时间的近似算法。

Result: 实现了对Dasgupta目标的O(1)近似和对Moseley-Wang目标的(1-o(1))近似，并扩展至子线性设置。

Conclusion: 分裂预言机能够帮助算法超越传统HC方法的限制，克服近似困难，并在子线性设置中提供新的算法保证。

Abstract: Hierarchical clustering (HC) is an important data analysis technique in which
the goal is to recursively partition a dataset into a tree-like structure while
grouping together similar data points at each level of granularity.
Unfortunately, for many of the proposed HC objectives, there exist strong
barriers to approximation algorithms with the hardness of approximation. Thus,
we consider the problem of hierarchical clustering given auxiliary information
from natural oracles. Specifically, we focus on a *splitting oracle* which,
when provided with a triplet of vertices $(u,v,w)$, answers (possibly
erroneously) the pairs of vertices whose lowest common ancestor includes all
three vertices in an optimal tree, i.e., identifying which vertex ``splits
away'' from the others. Using such an oracle, we obtain the following results:
  - A polynomial-time algorithm that outputs a hierarchical clustering tree
with $O(1)$-approximation to the Dasgupta objective (Dasgupta [STOC'16]).
  - A near-linear time algorithm that outputs a hierarchical clustering tree
with $(1-o(1))$-approximation to the Moseley-Wang objective (Moseley and Wang
[NeurIPS'17]).
  Under the plausible Small Set Expansion Hypothesis, no polynomial-time
algorithm can achieve any constant approximation for Dasgupta's objective or
$(1-C)$-approximation for the Moseley-Wang objective for some constant $C>0$.
As such, our results demonstrate that the splitting oracle enables algorithms
to outperform standard HC approaches and overcome hardness constraints.
Furthermore, our approaches extend to sublinear settings, in which we show new
streaming and PRAM algorithms for HC with improved guarantees.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [716] [Adaptive stable distribution and Hurst exponent by method of moments moving estimator for nonstationary time series](https://arxiv.org/abs/2506.05354)
*Jarek Duda*

Main category: stat.ME

TL;DR: 论文提出了一种基于移动估计器的非参数方法，用于适应非平稳时间序列，通过指数加权局部对数似然优化参数估计，并应用于金融数据中的alpha-Stable分布。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如ARMA-ARCH）假设固定的依赖类型，可能引入偏差。论文旨在通过更灵活的方法适应非平稳时间序列的动态变化。

Method: 使用移动估计器，通过指数加权局部对数似然优化参数估计，并利用EMA更新绝对中心矩等参数。特别关注alpha-Stable分布的应用。

Result: 方法成功应用于DJIA金融数据，动态估计了中心、尺度参数和α参数，用于评估市场稳定性和极端事件概率。

Conclusion: 移动估计器提供了一种灵活且无偏的方法，适用于非平稳时间序列分析，尤其在金融数据中表现优异。

Abstract: Nonstationarity of real-life time series requires model adaptation. In
classical approaches like ARMA-ARCH there is assumed some arbitrarily chosen
dependence type. To avoid their bias, we will focus on novel more agnostic
approach: moving estimator, which estimates parameters separately for every
time $t$: optimizing $F_t=\sum_{\tau<t} (1-\eta)^{t-\tau} \ln(\rho_\theta
(x_\tau))$ local log-likelihood with exponentially weakening weights of the old
values. In practice such moving estimates can be found by EMA (exponential
moving average) of some parameters, like $m_p=E[|x-\mu|^p]$ absolute central
moments, updated by $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$. We
will focus here on its applications for alpha-Stable distribution, which also
influences Hurst exponent, hence can be used for its adaptive estimation. Its
application will be shown on financial data as DJIA time series - beside
standard estimation of evolution of center $\mu$ and scale parameter $\sigma$,
there is also estimated evolution of $\alpha$ parameter allowing to
continuously evaluate market stability - tails having $\rho(x) \sim
1/|x|^{\alpha+1}$ behavior, controlling probability of potentially dangerous
extreme events.

</details>


### [717] [Adaptive stable distribution and Hurst exponent by method of moments moving estimator for nonstationary time series](https://arxiv.org/abs/2506.05354)
*Jarek Duda*

Main category: stat.ME

TL;DR: 论文提出了一种基于移动估计器的非参数方法，用于适应非平稳时间序列，并通过EMA更新参数，应用于alpha-Stable分布和金融数据（如DJIA）。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如ARMA-ARCH）假设固定的依赖类型，可能引入偏差。本文旨在通过更灵活的方法适应非平稳时间序列。

Method: 使用移动估计器，通过局部对数似然优化和EMA更新参数（如绝对中心矩），重点关注alpha-Stable分布的应用。

Result: 方法成功应用于金融数据（DJIA），动态估计中心、尺度参数和α参数，评估市场稳定性。

Conclusion: 移动估计器提供了一种灵活且无偏的方法，适用于非平稳时间序列分析，尤其在金融领域具有实际价值。

Abstract: Nonstationarity of real-life time series requires model adaptation. In
classical approaches like ARMA-ARCH there is assumed some arbitrarily chosen
dependence type. To avoid their bias, we will focus on novel more agnostic
approach: moving estimator, which estimates parameters separately for every
time $t$: optimizing $F_t=\sum_{\tau<t} (1-\eta)^{t-\tau} \ln(\rho_\theta
(x_\tau))$ local log-likelihood with exponentially weakening weights of the old
values. In practice such moving estimates can be found by EMA (exponential
moving average) of some parameters, like $m_p=E[|x-\mu|^p]$ absolute central
moments, updated by $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$. We
will focus here on its applications for alpha-Stable distribution, which also
influences Hurst exponent, hence can be used for its adaptive estimation. Its
application will be shown on financial data as DJIA time series - beside
standard estimation of evolution of center $\mu$ and scale parameter $\sigma$,
there is also estimated evolution of $\alpha$ parameter allowing to
continuously evaluate market stability - tails having $\rho(x) \sim
1/|x|^{\alpha+1}$ behavior, controlling probability of potentially dangerous
extreme events.

</details>


### [718] [Conditional Local Independence Testing with Application to Dynamic Causal Discovery](https://arxiv.org/abs/2506.07844)
*Mingzhou Liu,Xinwei Sun,Yizhou Wang*

Main category: stat.ME

TL;DR: 将条件局部独立性检验理论扩展到Ito过程，适用于动态系统中的因果发现。


<details>
  <summary>Details</summary>
Motivation: 扩展现有理论以适用于更广泛的动态系统，提升因果发现的适用性。

Method: 基于Christgau等人的理论，将其扩展到Ito过程。

Result: 理论扩展成功，适用于动态系统的因果发现。

Conclusion: 扩展后的理论为动态系统中的因果分析提供了新工具。

Abstract: In this note, we extend the conditional local independence testing theory
developed in Christgau et al. (2024) to Ito processes. The result can be
applied to causal discovery in dynamic systems.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [719] [Mapping correlations and coherence: adjacency-based approach to data visualization and regularity discovery](https://arxiv.org/abs/2506.05758)
*Guang-Xing Li*

Main category: physics.comp-ph

TL;DR: 提出一种基于斯托克斯参数的算法，用于生成表示相关性的类型和程度的地图，适用于复杂数据中的规律发现。


<details>
  <summary>Details</summary>
Motivation: 随着数据复杂性增加，系统性揭示规律成为挑战，传统相关性方法在复杂模式中可能失效。

Method: 利用斯托克斯参数考虑相关性向量的双重对称性，生成空间分辨率的相关性地图。

Result: 方法能够区分不同子区域的相关性类型，适用于物理系统和气候地图等。

Conclusion: 该方法为规律发现提供了新的高效数据表示方式，有望推动新计算方法的发展。

Abstract: The development of science has been transforming man's view towards nature
for centuries. Observing structures and patterns in an effective approach to
discover regularities from data is a key step toward theory-building. With
increasingly complex data being obtained, revealing regularities systematically
has become a challenge. Correlation is a most commonly-used and effective
approach to describe regularities in data, yet for complex patterns, spatial
inhomogeneity and complexity can often undermine the correlations. We present
an algorithm to derive maps representing the type and degree of correlations,
by taking the two-fold symmetry of the correlation vector into full account
using the Stokes parameter. The method allows for a spatially resolved view of
the nature and strength of correlations between physical quantities. In the
correlation view, a region can often be separated into different subregions
with different types of correlations. Subregions correspond to physical regimes
for physical systems, or climate zones for climate maps. The simplicity of the
method makes it widely applicable to a variety of data, where the
correlation-based approach makes the map particularly useful in revealing
regularities in physical systems and alike. As a new and efficient approach to
represent data, the method should facilitate the development of new
computational approaches to regularity discovery.

</details>


### [720] [Mapping correlations and coherence: adjacency-based approach to data visualization and regularity discovery](https://arxiv.org/abs/2506.05758)
*Guang-Xing Li*

Main category: physics.comp-ph

TL;DR: 本文提出了一种基于斯托克斯参数的算法，用于生成表示相关性的类型和程度的地图，解决了复杂数据中空间不均匀性和复杂性对相关性分析的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着数据复杂性的增加，系统性揭示数据中的规律性变得更具挑战性，尤其是空间不均匀性和复杂性会削弱相关性分析的效果。

Method: 利用斯托克斯参数充分考虑了相关性向量的双重对称性，提出了一种算法，生成空间分辨的相关性地图。

Result: 该方法能够将区域划分为具有不同相关性类型的子区域，适用于物理系统和气候地图等多种数据。

Conclusion: 这种简单且高效的方法为规律性发现提供了新的计算工具，有望推动相关领域的发展。

Abstract: The development of science has been transforming man's view towards nature
for centuries. Observing structures and patterns in an effective approach to
discover regularities from data is a key step toward theory-building. With
increasingly complex data being obtained, revealing regularities systematically
has become a challenge. Correlation is a most commonly-used and effective
approach to describe regularities in data, yet for complex patterns, spatial
inhomogeneity and complexity can often undermine the correlations. We present
an algorithm to derive maps representing the type and degree of correlations,
by taking the two-fold symmetry of the correlation vector into full account
using the Stokes parameter. The method allows for a spatially resolved view of
the nature and strength of correlations between physical quantities. In the
correlation view, a region can often be separated into different subregions
with different types of correlations. Subregions correspond to physical regimes
for physical systems, or climate zones for climate maps. The simplicity of the
method makes it widely applicable to a variety of data, where the
correlation-based approach makes the map particularly useful in revealing
regularities in physical systems and alike. As a new and efficient approach to
represent data, the method should facilitate the development of new
computational approaches to regularity discovery.

</details>


### [721] [Scientific machine learning in Hydrology: a unified perspective](https://arxiv.org/abs/2506.06308)
*Adoubi Vincent De Paul Adombi*

Main category: physics.comp-ph

TL;DR: 该论文综述了科学机器学习（SciML）在水文学中的应用，提出了一个统一的方法框架，以解决当前方法碎片化的问题，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前水文学中SciML方法缺乏统一框架，导致方法碎片化，难以评估创新性和进展潜力。

Method: 通过分类整理物理知识驱动的机器学习方法（如物理信息机器学习、物理引导机器学习等），提出统一框架。

Result: 构建了一个促进概念清晰和支持累积进展的SciML方法框架。

Conclusion: 论文为水文学中的SciML研究提供了系统性指导，并强调了未来研究的局限性及机会。

Abstract: Scientific machine learning (SciML) provides a structured approach to
integrating physical knowledge into data-driven modeling, offering significant
potential for advancing hydrological research. In recent years, multiple
methodological families have emerged, including physics-informed machine
learning, physics-guided machine learning, hybrid physics-machine learning, and
data-driven physics discovery. Within each of these families, a proliferation
of heterogeneous approaches has developed independently, often without
conceptual coordination. This fragmentation complicates the assessment of
methodological novelty and makes it difficult to identify where meaningful
advances can still be made in the absence of a unified conceptual framework.
This review, the first focused overview of SciML in hydrology, addresses these
limitations by proposing a unified methodological framework for each SciML
family, bringing together representative contributions into a coherent
structure that fosters conceptual clarity and supports cumulative progress in
hydrological modeling. Finally, we highlight the limitations and future
opportunities of each unified family to guide systematic research in hydrology,
where these methods remain underutilized.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [722] [Regional, Lattice and Logical Representations of Neural Networks](https://arxiv.org/abs/2506.05834)
*Sandro Preto,Marcelo Finger*

Main category: cs.LO

TL;DR: 该论文提出了一种将ReLU激活的前馈神经网络转换为分段线性函数区域表示的方法，并研究了其复杂性和适用性。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络可解释性的一种可能路径，即通过分段线性函数近似表示网络行为。

Method: 提出一种算法，将隐藏层使用ReLU激活、输出层使用截断恒等激活的前馈神经网络转换为区域表示。

Result: 通过实验研究了不同规模神经网络的区域表示复杂性，并验证了其满足特定性质的程度。

Conclusion: 区域表示为神经网络的可解释性提供了潜在路径，且其满足特定性质的能力为后续逻辑和格表示奠定了基础。

Abstract: A possible path to the interpretability of neural networks is to
(approximately) represent them in the regional format of piecewise linear
functions, where regions of inputs are associated to linear functions computing
the network outputs. We present an algorithm for the translation of feedforward
neural networks with ReLU activation functions in hidden layers and truncated
identity activation functions in the output layer. We also empirically
investigate the complexity of regional representations outputted by our method
for neural networks with varying sizes. Lattice and logical representations of
neural networks are straightforward from regional representations as long as
they satisfy a specific property. So we empirically investigate to what extent
the translations by our algorithm satisfy such property.

</details>


### [723] [Regional, Lattice and Logical Representations of Neural Networks](https://arxiv.org/abs/2506.05834)
*Sandro Preto,Marcelo Finger*

Main category: cs.LO

TL;DR: 论文提出了一种将ReLU激活的神经网络转换为分段线性函数区域表示的方法，并研究了其复杂性和适用性。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络可解释性的一种可能路径，通过将其表示为分段线性函数的区域形式。

Method: 提出一种算法，将具有ReLU隐藏层和截断恒等输出层的神经网络转换为区域表示。

Result: 通过实验研究了不同规模神经网络区域表示的复杂性，并验证了其满足特定性质的程度。

Conclusion: 区域表示为神经网络的可解释性提供了潜在路径，且算法生成的表示在特定条件下可直接转换为格和逻辑表示。

Abstract: A possible path to the interpretability of neural networks is to
(approximately) represent them in the regional format of piecewise linear
functions, where regions of inputs are associated to linear functions computing
the network outputs. We present an algorithm for the translation of feedforward
neural networks with ReLU activation functions in hidden layers and truncated
identity activation functions in the output layer. We also empirically
investigate the complexity of regional representations outputted by our method
for neural networks with varying sizes. Lattice and logical representations of
neural networks are straightforward from regional representations as long as
they satisfy a specific property. So we empirically investigate to what extent
the translations by our algorithm satisfy such property.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [724] [El0ps: An Exact L0-regularized Problems Solver](https://arxiv.org/abs/2506.06373)
*Théo Guyard,Cédric Herzet,Clément Elvira*

Main category: cs.MS

TL;DR: El0ps是一个Python工具箱，用于处理L0正则化问题，提供灵活框架、高性能求解器和内置机器学习流程。


<details>
  <summary>Details</summary>
Motivation: 为机器学习和信号处理等领域提供更灵活、高效的L0正则化问题解决方案。

Method: 通过自定义问题实例框架和专用求解器实现高性能，并提供内置机器学习流程。

Result: El0ps在L0正则化问题中表现出色，为实际应用提供新视角。

Conclusion: El0ps是一个全面的工具，推动了L0正则化问题在实际应用中的集成。

Abstract: This paper presents El0ps, a Python toolbox providing several utilities to
handle L0-regularized problems related to applications in machine learning,
statistics, and signal processing, among other fields. In contrast to existing
toolboxes, El0ps allows users to define custom instances of these problems
through a flexible framework, provides a dedicated solver achieving
state-of-the-art performance, and offers several built-in machine learning
pipelines. Our aim with El0ps is to provide a comprehensive tool which opens
new perspectives for the integration of L0-regularized problems in practical
applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [725] [Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage](https://arxiv.org/abs/2506.06472)
*Ziqi Yuan,Haoyang Zhang,Yirui Eric Zhou,Apoorve Mohan,I-Hsin Chung,Seetharami Seelam,Jian Huang*

Main category: cs.DC

TL;DR: TERAIO是一个基于SSD的GPU内存扩展框架，通过智能张量卸载/预取优化LLM训练性能。


<details>
  <summary>Details</summary>
Motivation: 观察到LLM训练中活跃张量仅占GPU内存的1.7%，非活跃张量占用大量内存且长时间未使用，为SSD卸载/预取提供了机会。

Method: 通过前几次迭代分析张量生命周期，生成优化卸载/预取计划，并集成到PyTorch中，利用GPUDirect存储直接迁移张量。

Result: 相比现有技术（如ZeRO-Offload和ZeRO-Infinity），TERAIO平均提升LLM训练性能1.47倍，达到理想性能的80.7%。

Conclusion: TERAIO通过高效利用SSD扩展GPU内存，显著提升LLM训练效率。

Abstract: We present the design and implementation of a new lifetime-aware tensor
offloading framework for GPU memory expansion using low-cost PCIe-based
solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for
large language model (LLM) training with multiple GPUs and multiple SSDs. Its
design is driven by our observation that the active tensors take only a small
fraction (1.7% on average) of allocated GPU memory in each LLM training
iteration, the inactive tensors are usually large and will not be used for a
long period of time, creating ample opportunities for offloading/prefetching
tensors to/from slow SSDs without stalling the GPU training process. TERAIO
accurately estimates the lifetime (active period of time in GPU memory) of each
tensor with the profiling of the first few iterations in the training process.
With the tensor lifetime analysis, TERAIO will generate an optimized tensor
offloading/prefetching plan and integrate it into the compiled LLM program via
PyTorch. TERAIO has a runtime tensor migration engine to execute the
offloading/prefetching plan via GPUDirect storage, which allows direct tensor
migration between GPUs and SSDs for alleviating the CPU bottleneck and
maximizing the SSD bandwidth utilization. In comparison with state-of-the-art
studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves
the training performance of various LLMs by 1.47x on average, and achieves
80.7% of the ideal performance assuming unlimited GPU memory.

</details>


### [726] [pFedSOP : Accelerating Training Of Personalized Federated Learning Using Second-Order Optimization](https://arxiv.org/abs/2506.07159)
*Mrinmay Sen,Chalavadi Krishna Mohan*

Main category: cs.DC

TL;DR: pFedSOP利用二阶优化加速个性化联邦学习，减少通信轮次并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习在数据异构性下模型泛化能力不足的问题，同时避免现有PFL方法因一阶优化导致的训练缓慢和额外计算负担。

Method: 通过Gompertz函数计算个性化梯度更新，利用正则化Fisher信息矩阵近似Hessian矩阵，实现二阶优化。

Result: 在异构图像分类数据集上，pFedSOP优于现有FL和PFL算法。

Conclusion: pFedSOP通过二阶优化有效加速训练并减少通信轮次，提升了PFL的性能。

Abstract: Personalized Federated Learning (PFL) enables clients to collaboratively
train personalized models tailored to their individual objectives, addressing
the challenge of model generalization in traditional Federated Learning (FL)
due to high data heterogeneity. However, existing PFL methods often require
increased communication rounds to achieve the desired performance, primarily
due to slow training caused by the use of first-order optimization, which has
linear convergence. Additionally, many of these methods increase local
computation because of the additional data fed into the model during the search
for personalized local models. One promising solution to this slow training is
second-order optimization, known for its quadratic convergence. However,
employing it in PFL is challenging due to the Hessian matrix and its inverse.
In this paper, we propose pFedSOP, which efficiently utilizes second-order
optimization in PFL to accelerate the training of personalized models and
enhance performance with fewer communication rounds. Our approach first
computes a personalized local gradient update using the Gompertz function-based
normalized angle between local and global gradient updates, incorporating
client-specific global information. We then use a regularized Fisher
Information Matrix (FIM), computed from this personalized gradient update, as
an approximation of the Hessian to update the personalized models. This
FIM-based second-order optimization speeds up training with fewer communication
rounds by tackling the challenges with exact Hessian and avoids additional data
being fed into the model during the search for personalized local models.
Extensive experiments on heterogeneously partitioned image classification
datasets with partial client participation demonstrate that pFedSOP outperforms
state-of-the-art FL and PFL algorithms.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [727] [Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds](https://arxiv.org/abs/2506.07614)
*Rishikesh Srinivasan,Dheeraj Nagaraj*

Main category: math.PR

TL;DR: 论文研究了使用泊松中点离散化方法从强对数凹分布中采样的收敛性，相比欧拉-马鲁亚马离散化，在目标精度上实现了三次加速。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效地从强对数凹分布中采样，特别是在高维空间中，以提升采样效率和精度。

Method: 采用泊松中点离散化方法（随机中点方法的变体）应用于过阻尼/欠阻尼朗之万动力学，并分析其在2-Wasserstein距离下的收敛性。

Result: 在目标精度上实现了三次加速，且在欠阻尼朗之万动力学中，复杂度远低于文献中建立的L2强误差收敛的下界。

Conclusion: 泊松中点离散化方法在采样效率和精度上优于现有方法，特别是在欠阻尼朗之万动力学中表现突出。

Abstract: We study the problem of sampling from strongly log-concave distributions over
$\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the
randomized midpoint method) for overdamped/underdamped Langevin dynamics. We
prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic
speedup in dependence on the target accuracy ($\epsilon$) over the
Euler-Maruyama discretization, surpassing existing bounds for randomized
midpoint methods. Notably, in the case of underdamped Langevin dynamics, we
demonstrate the complexity of $W_2$ convergence is much smaller than the
complexity lower bounds for convergence in $L^2$ strong error established in
the literature.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [728] [Emulating compact binary population synthesis simulations with robust uncertainty quantification and model comparison: Bayesian normalizing flows](https://arxiv.org/abs/2506.05657)
*Anarya Ray*

Main category: astro-ph.HE

TL;DR: 论文提出了一种基于贝叶斯归一化流的方法，用于量化并边缘化模拟紧凑双星并合（CBCs）参数分布中的不确定性，以支持引力波观测数据的分析。


<details>
  <summary>Details</summary>
Motivation: 现实模拟成本高昂，且稀疏训练集会导致流预测的不确定性。本文旨在解决这些问题，以更准确地约束天体物理初始条件和形成通道。

Method: 通过贝叶斯神经网络构建条件密度估计器（贝叶斯归一化流），利用精确似然函数对后验分布进行采样，量化并边缘化流的不确定性。

Result: 在共同包层演化形成的双黑洞群体模拟中，验证了方法的准确性、校准性和数据放大效果。

Conclusion: 该方法适用于引力波目录的模拟推断，并可推广到其他引力波天文学的模拟方法中。

Abstract: Population synthesis simulations of compact binary coalescences~(CBCs) play a
crucial role in extracting astrophysical insights from an ensemble of
gravitational wave~(GW) observations. However, realistic simulations are costly
to implement for a dense grid of initial conditions. Normalizing flows can
emulate the distribution functions of a simulated population of binary
parameters and thereby enable empirical constraints on the astrophysical
initial conditions and branching fractions of various formation channels given
data from a catalog of GW observations. They can also be used for data
amplification in sparse regions of the CBC parameter space to guide the
development of phenomenological population models for rarely synthesizable
systems with components in theorized mass gaps, without having to simulate a
prohibitively large number of binaries. But flow predictions are wrought with
uncertainties, especially for sparse training sets. In this work I develop a
method for quantifying and marginalizing uncertainties in the emulators by
introducing the Bayesian Normalizing flow, a conditional density estimator
constructed from Bayesian neural networks. Using the exact likelihood function
associated with density estimators I sample the posterior distribution of flow
parameters with suitably chosen priors to quantify and marginalize over flow
uncertainties. I demonstrate the accuracy, calibration, and data-amplification
impacts of the estimated uncertainties for simulations of binary black hole
populations formed through common envelope evolution. I outline applications of
the methodology in simulation-based inference from growing GW catalogs and
sketch other uses for general simulation-based approaches in GW astronomy.

</details>


### [729] [Emulating compact binary population synthesis simulations with robust uncertainty quantification and model comparison: Bayesian normalizing flows](https://arxiv.org/abs/2506.05657)
*Anarya Ray*

Main category: astro-ph.HE

TL;DR: 论文提出了一种基于贝叶斯归一化流的方法，用于量化并边缘化模拟紧凑双星合并（CBCs）分布函数中的不确定性，以支持引力波（GW）观测数据的分析。


<details>
  <summary>Details</summary>
Motivation: 由于紧凑双星合并的模拟成本高昂，且稀疏训练集会导致流预测的不确定性，需要一种方法来量化这些不确定性。

Method: 作者开发了贝叶斯归一化流，这是一种基于贝叶斯神经网络的密度估计器，通过采样流参数的后验分布来量化不确定性。

Result: 该方法在模拟黑洞双星群体时表现出高精度和良好的校准性，并能有效放大稀疏参数空间的数据。

Conclusion: 该方法适用于引力波观测数据的模拟推断，并有望推广到其他引力波天文学研究中。

Abstract: Population synthesis simulations of compact binary coalescences~(CBCs) play a
crucial role in extracting astrophysical insights from an ensemble of
gravitational wave~(GW) observations. However, realistic simulations are costly
to implement for a dense grid of initial conditions. Normalizing flows can
emulate the distribution functions of a simulated population of binary
parameters and thereby enable empirical constraints on the astrophysical
initial conditions and branching fractions of various formation channels given
data from a catalog of GW observations. They can also be used for data
amplification in sparse regions of the CBC parameter space to guide the
development of phenomenological population models for rarely synthesizable
systems with components in theorized mass gaps, without having to simulate a
prohibitively large number of binaries. But flow predictions are wrought with
uncertainties, especially for sparse training sets. In this work I develop a
method for quantifying and marginalizing uncertainties in the emulators by
introducing the Bayesian Normalizing flow, a conditional density estimator
constructed from Bayesian neural networks. Using the exact likelihood function
associated with density estimators I sample the posterior distribution of flow
parameters with suitably chosen priors to quantify and marginalize over flow
uncertainties. I demonstrate the accuracy, calibration, and data-amplification
impacts of the estimated uncertainties for simulations of binary black hole
populations formed through common envelope evolution. I outline applications of
the methodology in simulation-based inference from growing GW catalogs and
sketch other uses for general simulation-based approaches in GW astronomy.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [730] [Similarity Matching Networks: Hebbian Learning and Convergence Over Multiple Time Scales](https://arxiv.org/abs/2506.06134)
*Veronica Centorrino,Francesco Bullo,Giovanni Russo*

Main category: q-bio.NC

TL;DR: 论文提出了一种基于相似性匹配的连续时间神经网络，用于主成分子空间投影，通过多级优化框架证明了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 尽管相似性匹配成本函数在生物学上有明确解释且应用广泛，但其收敛性分析尚未完全解决。

Method: 构建了一个三阶段动态网络（神经、侧向突触、前馈突触），分别对应快、中、慢时间尺度，并采用Hebbian和反Hebbian学习规则。

Result: 通过多级优化框架，证明了快时间尺度的全局指数收敛、中时间尺度的正定矩阵空间内收敛，以及慢时间尺度的全局最小值收敛。

Conclusion: 数值实验支持了理论分析，验证了方法的有效性。

Abstract: A recent breakthrough in biologically-plausible normative frameworks for
dimensionality reduction is based upon the similarity matching cost function
and the low-rank matrix approximation problem. Despite clear biological
interpretation, successful application in several domains, and experimental
validation, a formal complete convergence analysis remains elusive. Building on
this framework, we consider and analyze a continuous-time neural network, the
\emph{similarity matching network}, for principal subspace projection. Derived
from a min-max-min objective, this biologically-plausible network consists of
three coupled dynamics evolving at different time scales: neural dynamics,
lateral synaptic dynamics, and feedforward synaptic dynamics at the fast,
intermediate, and slow time scales, respectively. The feedforward and lateral
synaptic dynamics consist of Hebbian and anti-Hebbian learning rules,
respectively. By leveraging a multilevel optimization framework, we prove
convergence of the dynamics in the offline setting. Specifically, at the first
level (fast time scale), we show strong convexity of the cost function and
global exponential convergence of the corresponding gradient-flow dynamics. At
the second level (intermediate time scale), we prove strong concavity of the
cost function and exponential convergence of the corresponding gradient-flow
dynamics within the space of positive definite matrices. At the third and final
level (slow time scale), we study a non-convex and non-smooth cost function,
provide explicit expressions for its global minima, and prove almost sure
convergence of the corresponding gradient-flow dynamics to the global minima.
These results rely on two empirically motivated conjectures that are supported
by thorough numerical experiments. Finally, we validate the effectiveness of
our approach via a numerical example.

</details>


### [731] [Similarity Matching Networks: Hebbian Learning and Convergence Over Multiple Time Scales](https://arxiv.org/abs/2506.06134)
*Veronica Centorrino,Francesco Bullo,Giovanni Russo*

Main category: q-bio.NC

TL;DR: 论文提出了一种基于相似性匹配的连续时间神经网络，用于主成分子空间投影，并通过多级优化框架证明了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 尽管相似性匹配成本函数在生物学解释和应用上取得了成功，但其完整的收敛分析尚未解决。本文旨在填补这一空白。

Method: 构建了一个包含神经动态、侧向突触动态和前馈突触动态的三层动态网络，分别对应快、中、慢三个时间尺度，并利用多级优化框架分析其收敛性。

Result: 在快时间尺度上证明了强凸性和全局指数收敛；在中时间尺度上证明了强凹性和指数收敛；在慢时间尺度上证明了几乎必然收敛到全局最小值。

Conclusion: 通过数值实验验证了方法的有效性，填补了相似性匹配框架收敛分析的空白。

Abstract: A recent breakthrough in biologically-plausible normative frameworks for
dimensionality reduction is based upon the similarity matching cost function
and the low-rank matrix approximation problem. Despite clear biological
interpretation, successful application in several domains, and experimental
validation, a formal complete convergence analysis remains elusive. Building on
this framework, we consider and analyze a continuous-time neural network, the
\emph{similarity matching network}, for principal subspace projection. Derived
from a min-max-min objective, this biologically-plausible network consists of
three coupled dynamics evolving at different time scales: neural dynamics,
lateral synaptic dynamics, and feedforward synaptic dynamics at the fast,
intermediate, and slow time scales, respectively. The feedforward and lateral
synaptic dynamics consist of Hebbian and anti-Hebbian learning rules,
respectively. By leveraging a multilevel optimization framework, we prove
convergence of the dynamics in the offline setting. Specifically, at the first
level (fast time scale), we show strong convexity of the cost function and
global exponential convergence of the corresponding gradient-flow dynamics. At
the second level (intermediate time scale), we prove strong concavity of the
cost function and exponential convergence of the corresponding gradient-flow
dynamics within the space of positive definite matrices. At the third and final
level (slow time scale), we study a non-convex and non-smooth cost function,
provide explicit expressions for its global minima, and prove almost sure
convergence of the corresponding gradient-flow dynamics to the global minima.
These results rely on two empirically motivated conjectures that are supported
by thorough numerical experiments. Finally, we validate the effectiveness of
our approach via a numerical example.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [732] [Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments](https://arxiv.org/abs/2506.07232)
*Xinran Li,Chenjia Bai,Zijian Li,Jiakun Zheng,Ting Xiao,Jun Zhang*

Main category: cs.MA

TL;DR: 论文提出了一种名为LIET的框架，通过个体学习和团队演化提升LLM在多智能体环境中的适应能力，实验证明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多智能体环境中的适应能力不足，需要一种方法提升其规划和协作能力。

Method: 提出LIET框架，结合个体学习（本地效用函数）和团队演化（共享合作知识列表），实现灵活适应。

Result: 在Communicative Watch-And-Help和ThreeD-World Multi-Agent Transport基准测试中，LIET表现优于现有方法。

Conclusion: LIET框架有效提升了LLM在多智能体环境中的适应性和协作能力。

Abstract: Large language models (LLMs) possess extensive knowledge bases and strong
reasoning capabilities, making them promising tools for complex, multi-agent
planning in embodied environments. However, despite LLMs' advanced abilities
and the sophisticated modular design of agentic methods, existing LLM-based
planning algorithms remain limited by weak adaptation capabilities to
multi-agent embodied scenarios. We address this limitation by introducing a
framework that enables LLM agents to learn and evolve both before and during
test time, equipping them with environment-relevant knowledge for better
planning and enhanced communication for improved cooperation. Inspired by
centralized training with decentralized execution in multi-agent reinforcement
learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)}
paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents
learn a local utility function from exploratory datasets to better comprehend
the embodied environment, which is then queried during test time to support
informed decision-making. At the team level, LLM agents collaboratively and
iteratively maintain and update a shared cooperation knowledge list based on
new experiences, using it to guide more effective communication. By combining
individual learning with team evolution, LIET enables comprehensive and
flexible adaptation for LLM agents. Our experiments on Communicative
Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate
that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing
baselines and exhibits strong cooperative planning abilities.

</details>


### [733] [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
*Guibin Zhang,Muxin Fu,Guancheng Wan,Miao Yu,Kun Wang,Shuicheng Yan*

Main category: cs.MA

TL;DR: G-Memory是一种为多智能体系统设计的分层记忆架构，解决了现有记忆机制过于简单和缺乏定制化的问题，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统的记忆机制过于简单，忽视了智能体间的协作轨迹，且缺乏跨任务和智能体特定的定制化记忆，限制了系统的自我进化能力。

Method: 提出G-Memory，一种受组织记忆理论启发的分层记忆系统，通过三层图结构（洞察图、查询图和交互图）管理智能体间的交互，并支持双向记忆检索。

Result: 在五个基准测试、三种LLM骨干和三种流行MAS框架中，G-Memory显著提升了任务成功率（最高20.89%）和知识问答准确率（最高10.12%）。

Conclusion: G-Memory通过分层记忆架构有效解决了多智能体系统的记忆限制，显著提升了系统性能，且无需修改原有框架。

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have
demonstrated cognitive and execution capabilities that far exceed those of
single LLM agents, yet their capacity for self-evolution remains hampered by
underdeveloped memory architectures. Upon close inspection, we are alarmed to
discover that prevailing MAS memory mechanisms (1) are overly simplistic,
completely disregarding the nuanced inter-agent collaboration trajectories, and
(2) lack cross-trial and agent-specific customization, in stark contrast to the
expressive memory developed for single agents. To bridge this gap, we introduce
G-Memory, a hierarchical, agentic memory system for MAS inspired by
organizational memory theory, which manages the lengthy MAS interaction via a
three-tier graph hierarchy: insight, query, and interaction graphs. Upon
receiving a new user query, G-Memory performs bi-directional memory traversal
to retrieve both $\textit{high-level, generalizable insights}$ that enable the
system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed
interaction trajectories}$ that compactly encode prior collaboration
experiences. Upon task execution, the entire hierarchy evolves by assimilating
new collaborative trajectories, nurturing the progressive evolution of agent
teams. Extensive experiments across five benchmarks, three LLM backbones, and
three popular MAS frameworks demonstrate that G-Memory improves success rates
in embodied action and accuracy in knowledge QA by up to $20.89\%$ and
$10.12\%$, respectively, without any modifications to the original frameworks.
Our codes are available at https://github.com/bingreeky/GMemory.

</details>


### [734] [MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models](https://arxiv.org/abs/2506.07400)
*Philip Liu,Sparsh Bansal,Jimmy Dinh,Aditya Pawar,Ramani Satishkumar,Shail Desai,Neeraj Gupta,Xin Wang,Shu Hu*

Main category: cs.MA

TL;DR: MedChat是一个多代理诊断框架，结合专业视觉模型和角色特定的LLM代理，通过协调代理提升青光眼检测的可靠性和报告效率。


<details>
  <summary>Details</summary>
Motivation: 解决通用LLM在医学影像中存在的幻觉、解释性不足和领域知识缺乏问题，同时模拟多学科医疗团队的复杂推理。

Method: 提出MedChat框架，结合专业视觉模型和多个角色特定的LLM代理，由协调代理统一管理。

Result: 提高了诊断可靠性，减少幻觉风险，支持交互式诊断报告生成。

Conclusion: MedChat为医学影像诊断提供了一种更可靠、高效的解决方案，适用于临床和教育场景。

Abstract: The integration of deep learning-based glaucoma detection with large language
models (LLMs) presents an automated strategy to mitigate ophthalmologist
shortages and improve clinical reporting efficiency. However, applying general
LLMs to medical imaging remains challenging due to hallucinations, limited
interpretability, and insufficient domain-specific medical knowledge, which can
potentially reduce clinical accuracy. Although recent approaches combining
imaging models with LLM reasoning have improved reporting, they typically rely
on a single generalist agent, restricting their capacity to emulate the diverse
and complex reasoning found in multidisciplinary medical teams. To address
these limitations, we propose MedChat, a multi-agent diagnostic framework and
platform that combines specialized vision models with multiple role-specific
LLM agents, all coordinated by a director agent. This design enhances
reliability, reduces hallucination risk, and enables interactive diagnostic
reporting through an interface tailored for clinical review and educational
use. Code available at https://github.com/Purdue-M2/MedChat.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [735] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: ChemGraph是一个基于AI的框架，用于自动化和简化计算化学与材料科学的工作流程，结合图神经网络和大语言模型，支持多种计算任务。


<details>
  <summary>Details</summary>
Motivation: 原子模拟在化学和材料科学中至关重要，但现有方法需要专业知识且操作复杂，ChemGraph旨在解决这一问题。

Method: ChemGraph利用图神经网络基础模型进行高效计算，并结合大语言模型实现自然语言交互和任务规划。

Result: 在13个基准任务中，小型LLM在简单任务中表现良好，复杂任务需要更大模型，多代理框架可提升小型模型性能。

Conclusion: ChemGraph通过自动化工作流程和多代理设计，显著提升了计算化学任务的效率和可访问性。

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [736] [On Inverse Problems, Parameter Estimation, and Domain Generalization](https://arxiv.org/abs/2506.06024)
*Deborah Pereg*

Main category: cs.IT

TL;DR: 本文分析了逆问题中的参数估计问题，提出了域偏移问题的重新表述，并揭示了当前域泛化方法的脆弱性（双义定理）。通过实验验证了理论发现，并比较了可逆与不可逆过程中的参数估计。


<details>
  <summary>Details</summary>
Motivation: 研究逆问题中的参数估计，解决域偏移问题，并探讨数据预处理对参数估计的影响。

Method: 重新表述域偏移问题，分析双义定理，并通过图像去模糊和医学图像去噪实验验证理论。比较可逆与不可逆过程的参数估计。

Result: 发现数据预处理（如基于生成模型的逆问题求解）未必能改善参数估计性能，与信息论数据处理不等式一致。

Conclusion: 本文为实践者提供了更深入的见解，有助于在安全敏感应用中制定更高效的系统策略。

Abstract: Signal restoration and inverse problems are key elements in most real-world
data science applications. In the past decades, with the emergence of machine
learning methods, inversion of measurements has become a popular step in almost
all physical applications, which is normally executed prior to downstream tasks
that often involve parameter estimation. In this work, we analyze the general
problem of parameter estimation in an inverse problem setting. First, we
address the domain-shift problem by re-formulating it in direct relation with
the discrete parameter estimation analysis. We analyze a significant
vulnerability in current attempts to enforce domain generalization, which we
dubbed the Double Meaning Theorem. Our theoretical findings are experimentally
illustrated for domain shift examples in image deblurring and speckle
suppression in medical imaging. We then proceed to a theoretical analysis of
parameter estimation given observed measurements before and after data
processing involving an inversion of the observations. We compare this setting
for invertible and non-invertible (degradation) processes. We distinguish
between continuous and discrete parameter estimation, corresponding with
regression and classification problems, respectively. Our theoretical findings
align with the well-known information-theoretic data processing inequality, and
to a certain degree question the common misconception that data-processing for
inversion, based on modern generative models that may often produce outstanding
perceptual quality, will necessarily improve the following parameter estimation
objective. It is our hope that this paper will provide practitioners with
deeper insights that may be leveraged in the future for the development of more
efficient and informed strategic system planning, critical in safety-sensitive
applications.

</details>


### [737] [On Inverse Problems, Parameter Estimation, and Domain Generalization](https://arxiv.org/abs/2506.06024)
*Deborah Pereg*

Main category: cs.IT

TL;DR: 论文分析了逆问题中的参数估计问题，提出了领域偏移的重新定义，揭示了当前领域泛化方法的漏洞（双义定理），并通过实验验证了理论发现。


<details>
  <summary>Details</summary>
Motivation: 研究逆问题中的参数估计，解决领域偏移问题，并探讨数据处理对参数估计的影响。

Method: 重新定义领域偏移问题，分析双义定理，并通过图像去模糊和医学成像中的斑点抑制实验验证理论。

Result: 理论发现与信息论数据处理不等式一致，质疑了基于现代生成模型的数据处理必然改善参数估计的常见误解。

Conclusion: 论文为实践者提供了更深入的见解，有助于在安全敏感应用中开发更高效的系统规划策略。

Abstract: Signal restoration and inverse problems are key elements in most real-world
data science applications. In the past decades, with the emergence of machine
learning methods, inversion of measurements has become a popular step in almost
all physical applications, which is normally executed prior to downstream tasks
that often involve parameter estimation. In this work, we analyze the general
problem of parameter estimation in an inverse problem setting. First, we
address the domain-shift problem by re-formulating it in direct relation with
the discrete parameter estimation analysis. We analyze a significant
vulnerability in current attempts to enforce domain generalization, which we
dubbed the Double Meaning Theorem. Our theoretical findings are experimentally
illustrated for domain shift examples in image deblurring and speckle
suppression in medical imaging. We then proceed to a theoretical analysis of
parameter estimation given observed measurements before and after data
processing involving an inversion of the observations. We compare this setting
for invertible and non-invertible (degradation) processes. We distinguish
between continuous and discrete parameter estimation, corresponding with
regression and classification problems, respectively. Our theoretical findings
align with the well-known information-theoretic data processing inequality, and
to a certain degree question the common misconception that data-processing for
inversion, based on modern generative models that may often produce outstanding
perceptual quality, will necessarily improve the following parameter estimation
objective. It is our hope that this paper will provide practitioners with
deeper insights that may be leveraged in the future for the development of more
efficient and informed strategic system planning, critical in safety-sensitive
applications.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [738] [Voice Impression Control in Zero-Shot TTS](https://arxiv.org/abs/2506.05688)
*Keinichi Fujita,Shota Horiguchi,Yusuke Ijima*

Main category: cs.SD

TL;DR: 提出了一种零样本TTS中通过低维向量控制语音印象的方法，利用大语言模型生成目标印象向量。


<details>
  <summary>Details</summary>
Motivation: 尽管零样本TTS在说话人保真度上表现优异，但如何通过调节副/非语言信息控制语音印象仍具挑战性。

Method: 使用低维向量表示语音印象对的强度（如暗-亮），并通过大语言模型从自然语言描述生成目标印象向量。

Result: 主客观评估均验证了该方法在印象控制上的有效性。

Conclusion: 该方法无需手动优化即可实现目标印象生成。

Abstract: Para-/non-linguistic information in speech is pivotal in shaping the
listeners' impression. Although zero-shot text-to-speech (TTS) has achieved
high speaker fidelity, modulating subtle para-/non-linguistic information to
control perceived voice characteristics, i.e., impressions, remains
challenging. We have therefore developed a voice impression control method in
zero-shot TTS that utilizes a low-dimensional vector to represent the
intensities of various voice impression pairs (e.g., dark-bright). The results
of both objective and subjective evaluations have demonstrated our method's
effectiveness in impression control. Furthermore, generating this vector via a
large language model enables target-impression generation from a natural
language description of the desired impression, thus eliminating the need for
manual optimization.

</details>


### [739] [Label-Context-Dependent Internal Language Model Estimation for CTC](https://arxiv.org/abs/2506.06096)
*Zijian Yang,Minh-Nghia Phan,Ralf Schlüter,Hermann Ney*

Main category: cs.SD

TL;DR: 论文研究了CTC模型中隐含的上下文依赖性，提出了基于知识蒸馏的上下文依赖ILM估计方法，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管CTC模型假设标签上下文独立，但由于现代强大编码器，它仍能隐式学习上下文依赖的内部语言模型（ILM）。本文旨在探究CTC中ILM的隐含上下文依赖性。

Method: 提出了基于知识蒸馏（KD）的上下文依赖ILM估计方法，并引入两种正则化方法。实验在Librispeech和TED-LIUM Release 2数据集上进行。

Result: 实验结果表明，上下文依赖的ILM在跨域评估中优于上下文独立的先验模型，且提出的标签级KD平滑方法表现最佳，词错误率相对降低了13%以上。

Conclusion: CTC确实学习到了上下文依赖的ILM，且提出的方法在性能上显著优于其他ILM估计方法。

Abstract: Although connectionist temporal classification (CTC) has the label context
independence assumption, it can still implicitly learn a context-dependent
internal language model (ILM) due to modern powerful encoders. In this work, we
investigate the implicit context dependency modeled in the ILM of CTC. To this
end, we propose novel context-dependent ILM estimation methods for CTC based on
knowledge distillation (KD) with theoretical justifications. Furthermore, we
introduce two regularization methods for KD. We conduct experiments on
Librispeech and TED-LIUM Release 2 datasets for in-domain and cross-domain
evaluation, respectively. Experimental results show that context-dependent ILMs
outperform the context-independent priors in cross-domain evaluation,
indicating that CTC learns a context-dependent ILM. The proposed label-level KD
with smoothing method surpasses other ILM estimation approaches, with more than
13% relative improvement in word error rate compared to shallow fusion.

</details>


### [740] [Voice Impression Control in Zero-Shot TTS](https://arxiv.org/abs/2506.05688)
*Keinichi Fujita,Shota Horiguchi,Yusuke Ijima*

Main category: cs.SD

TL;DR: 论文提出了一种在零样本TTS中通过低维向量控制语音印象的方法，并利用大语言模型生成目标印象向量。


<details>
  <summary>Details</summary>
Motivation: 语音中的副语言/非语言信息对听者的印象至关重要，但现有零样本TTS在控制这些细微信息以塑造语音印象方面仍有挑战。

Method: 使用低维向量表示语音印象对的强度（如暗-亮），并通过大语言模型从自然语言描述生成目标印象向量。

Result: 主客观评估表明该方法在语音印象控制上有效。

Conclusion: 该方法无需手动优化即可通过自然语言描述生成目标语音印象，具有实用价值。

Abstract: Para-/non-linguistic information in speech is pivotal in shaping the
listeners' impression. Although zero-shot text-to-speech (TTS) has achieved
high speaker fidelity, modulating subtle para-/non-linguistic information to
control perceived voice characteristics, i.e., impressions, remains
challenging. We have therefore developed a voice impression control method in
zero-shot TTS that utilizes a low-dimensional vector to represent the
intensities of various voice impression pairs (e.g., dark-bright). The results
of both objective and subjective evaluations have demonstrated our method's
effectiveness in impression control. Furthermore, generating this vector via a
large language model enables target-impression generation from a natural
language description of the desired impression, thus eliminating the need for
manual optimization.

</details>


### [741] [Label-Context-Dependent Internal Language Model Estimation for CTC](https://arxiv.org/abs/2506.06096)
*Zijian Yang,Minh-Nghia Phan,Ralf Schlüter,Hermann Ney*

Main category: cs.SD

TL;DR: 论文研究了CTC（连接时序分类）中隐式上下文依赖的内部语言模型（ILM），提出了基于知识蒸馏（KD）的新方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管CTC假设标签上下文独立，但现代强大编码器仍能隐式学习上下文依赖的ILM。本文旨在探究CTC中ILM的隐式上下文依赖性。

Method: 提出了基于知识蒸馏（KD）的上下文依赖ILM估计方法，并引入两种正则化方法。实验在Librispeech和TED-LIUM Release 2数据集上进行。

Result: 实验结果表明，上下文依赖的ILM在跨域评估中优于上下文独立的先验模型，且提出的标签级KD平滑方法显著优于其他方法，词错误率相对降低13%以上。

Conclusion: CTC确实学习了上下文依赖的ILM，提出的KD方法在跨域任务中表现优异。

Abstract: Although connectionist temporal classification (CTC) has the label context
independence assumption, it can still implicitly learn a context-dependent
internal language model (ILM) due to modern powerful encoders. In this work, we
investigate the implicit context dependency modeled in the ILM of CTC. To this
end, we propose novel context-dependent ILM estimation methods for CTC based on
knowledge distillation (KD) with theoretical justifications. Furthermore, we
introduce two regularization methods for KD. We conduct experiments on
Librispeech and TED-LIUM Release 2 datasets for in-domain and cross-domain
evaluation, respectively. Experimental results show that context-dependent ILMs
outperform the context-independent priors in cross-domain evaluation,
indicating that CTC learns a context-dependent ILM. The proposed label-level KD
with smoothing method surpasses other ILM estimation approaches, with more than
13% relative improvement in word error rate compared to shallow fusion.

</details>


### [742] [Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching](https://arxiv.org/abs/2506.07199)
*Ben Hayes,Charalampos Saitis,György Fazekas*

Main category: cs.SD

TL;DR: 论文研究了音频合成器参数反演的固有对称性问题，提出了一种基于条件生成模型和置换等变连续归一化流的方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 音频合成器参数反演是一个病态问题，主要由于合成器的内在对称性（如置换不变性）。本文旨在解决这一问题。

Method: 通过条件生成模型和置换等变连续归一化流处理对称性问题，并提出一种自适应发现对称性的策略。

Result: 在Surge XT合成器上的实验表明，该方法在音频重建指标上优于回归和生成基线方法。

Conclusion: 通过建模参数分布和自适应对称性发现，显著提升了音频合成器参数反演的性能。

Abstract: Many audio synthesizers can produce the same signal given different parameter
configurations, meaning the inversion from sound to parameters is an inherently
ill-posed problem. We show that this is largely due to intrinsic symmetries of
the synthesizer, and focus in particular on permutation invariance. First, we
demonstrate on a synthetic task that regressing point estimates under
permutation symmetry degrades performance, even when using a
permutation-invariant loss function or symmetry-breaking heuristics. Then,
viewing equivalent solutions as modes of a probability distribution, we show
that a conditional generative model substantially improves performance.
Further, acknowledging the invariance of the implicit parameter distribution,
we find that performance is further improved by using a permutation equivariant
continuous normalizing flow. To accommodate intricate symmetries in real
synthesizers, we also propose a relaxed equivariance strategy that adaptively
discovers relevant symmetries from data. Applying our method to Surge XT, a
full-featured open source synthesizer used in real world audio production, we
find our method outperforms regression and generative baselines across audio
reconstruction metrics.

</details>


### [743] [Towards Generalized Source Tracing for Codec-Based Deepfake Speech](https://arxiv.org/abs/2506.07294)
*Xuanjun Chen,I-Ming Lin,Lin Zhang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: 论文提出SASTNet模型，结合语义和声学特征编码，显著提升CodecFake语音的源追踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经音频编解码的语音生成模型（CoSG）的源追踪方法性能不佳，且如何在模拟数据上训练模型并保持对真实数据的泛化能力尚待解决。

Method: 提出SASTNet模型，联合使用Whisper进行语义特征编码和Wav2vec2与AudioMAE进行声学特征编码。

Result: SASTNet在CodecFake+数据集的CoSG测试集上达到最优性能。

Conclusion: SASTNet通过结合语义和声学特征，有效解决了源追踪的泛化问题。

Abstract: Recent attempts at source tracing for codec-based deepfake speech
(CodecFake), generated by neural audio codec-based speech generation (CoSG)
models, have exhibited suboptimal performance. However, how to train source
tracing models using simulated CoSG data while maintaining strong performance
on real CoSG-generated audio remains an open challenge. In this paper, we show
that models trained solely on codec-resynthesized data tend to overfit to
non-speech regions and struggle to generalize to unseen content. To mitigate
these challenges, we introduce the Semantic-Acoustic Source Tracing Network
(SASTNet), which jointly leverages Whisper for semantic feature encoding and
Wav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet
achieves state-of-the-art performance on the CoSG test set of the CodecFake+
dataset, demonstrating its effectiveness for reliable source tracing.

</details>


### [744] [Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream Multi-Modal Learning Framework](https://arxiv.org/abs/2506.07358)
*Kuiyuan Zhang,Wenjie Pei,Rushi Lan,Yifang Guo,Zhongyun Hua*

Main category: cs.SD

TL;DR: 提出了一种轻量级单流多模态学习框架，用于音视频深度伪造检测，通过协作学习块和多模态分类模块，显著减少了参数数量并提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用两个独立的子模型分别学习音视频特征，未能充分利用两者间的相关性，且模型冗余，不适用于资源受限环境。

Method: 设计了一个单流多模态学习框架，引入协作音视频学习块实现多模态特征的连续融合，并提出多模态分类模块增强分类器对模态内容的依赖性。

Result: 在DF-TIMIT、FakeAVCeleb和DFDC数据集上，该方法仅需0.48M参数，性能优于现有音视频联合检测方法，且对未见过的深度伪造类型也有效。

Conclusion: 该方法通过轻量级设计和高效的多模态特征融合，显著提升了音视频深度伪造检测的效率和性能。

Abstract: Deepfakes are AI-synthesized multimedia data that may be abused for spreading
misinformation. Deepfake generation involves both visual and audio
manipulation. To detect audio-visual deepfakes, previous studies commonly
employ two relatively independent sub-models to learn audio and visual
features, respectively, and fuse them subsequently for deepfake detection.
However, this may underutilize the inherent correlations between audio and
visual features. Moreover, utilizing two isolated feature learning sub-models
can result in redundant neural layers, making the overall model inefficient and
impractical for resource-constrained environments.
  In this work, we design a lightweight network for audio-visual deepfake
detection via a single-stream multi-modal learning framework. Specifically, we
introduce a collaborative audio-visual learning block to efficiently integrate
multi-modal information while learning the visual and audio features. By
iteratively employing this block, our single-stream network achieves a
continuous fusion of multi-modal features across its layers. Thus, our network
efficiently captures visual and audio features without the need for excessive
block stacking, resulting in a lightweight network design. Furthermore, we
propose a multi-modal classification module that can boost the dependence of
the visual and audio classifiers on modality content. It also enhances the
whole resistance of the video classifier against the mismatches between audio
and visual modalities. We conduct experiments on the DF-TIMIT, FakeAVCeleb, and
DFDC benchmark datasets. Compared to state-of-the-art audio-visual joint
detection methods, our method is significantly lightweight with only 0.48M
parameters, yet it achieves superiority in both uni-modal and multi-modal
deepfakes, as well as in unseen types of deepfakes.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [745] [Pegasus: A Universal Framework for Scalable Deep Learning Inference on the Dataplane](https://arxiv.org/abs/2506.05779)
*Yinchao Zhang,Su Yao,Yong Feng,Kang Chen,Tong Li,Zhuotao Liu,Yi Zhao,Lexuan Zhang,Xiangyu Gao,Feng Xiong,Qi Li,Ke Xu*

Main category: cs.NI

TL;DR: Pegasus提出了一种新的数据平面抽象方法，通过将深度学习操作转化为三种数据平面原语（Partition、Map、SumReduce）和Primitive Fusion技术，解决了现有MAT抽象与DL推理不匹配的问题，显著提升了准确性、规模和通用性。


<details>
  <summary>Details</summary>
Motivation: 当前数据平面上的MAT抽象与DL推理不匹配，导致准确性下降、规模受限和缺乏通用性。

Method: Pegasus将DL操作转化为三种数据平面原语（Partition、Map、SumReduce），并采用Primitive Fusion技术合并计算，同时使用全精度权重和定点激活来提高准确性。

Result: 在P4交换机上的实现表明，Pegasus能有效支持多种DL模型（如MLP、RNN、CNN、AutoEncoder），平均准确性提升达22.8%，模型规模和输入规模分别提升248倍和212倍。

Conclusion: Pegasus通过创新的数据平面抽象和优化技术，显著提升了DL模型在数据平面上的性能和通用性。

Abstract: The paradigm of Intelligent DataPlane (IDP) embeds deep learning (DL) models
on the network dataplane to enable intelligent traffic analysis at line-speed.
However, the current use of the match-action table (MAT) abstraction on the
dataplane is misaligned with DL inference, leading to several key limitations,
including accuracy degradation, limited scale, and lack of generality. This
paper proposes Pegasus to address these limitations. Pegasus translates DL
operations into three dataplane-oriented primitives to achieve generality:
Partition, Map, and SumReduce. Specifically, Partition "divides"
high-dimensional features into multiple low-dimensional vectors, making them
more suitable for the dataplane; Map "conquers" computations on the
low-dimensional vectors in parallel with the technique of fuzzy matching, while
SumReduce "combines" the computation results. Additionally, Pegasus employs
Primitive Fusion to merge computations, improving scalability. Finally, Pegasus
adopts full precision weights with fixed-point activations to improve accuracy.
Our implementation on a P4 switch demonstrates that Pegasus can effectively
support various types of DL models, including Multi-Layer Perceptron (MLP),
Recurrent Neural Network (RNN), Convolutional Neural Network (CNN), and
AutoEncoder models on the dataplane. Meanwhile, Pegasus outperforms
state-of-the-art approaches with an average accuracy improvement of up to
22.8%, along with up to 248x larger model size and 212x larger input scale.

</details>


### [746] [Pegasus: A Universal Framework for Scalable Deep Learning Inference on the Dataplane](https://arxiv.org/abs/2506.05779)
*Yinchao Zhang,Su Yao,Yong Feng,Kang Chen,Tong Li,Zhuotao Liu,Yi Zhao,Lexuan Zhang,Xiangyu Gao,Feng Xiong,Qi Li,Ke Xu*

Main category: cs.NI

TL;DR: Pegasus提出了一种将深度学习操作转化为数据平面原语的方法，解决了现有数据平面在深度学习推理中的局限性，显著提升了准确性、规模和通用性。


<details>
  <summary>Details</summary>
Motivation: 当前数据平面的匹配-动作表抽象与深度学习推理不匹配，导致准确性下降、规模受限和缺乏通用性。

Method: Pegasus将深度学习操作转化为三个数据平面原语：Partition、Map和SumReduce，并采用Primitive Fusion和全精度权重提升性能。

Result: 在P4交换机上的实现表明，Pegasus支持多种DL模型，准确性提升22.8%，模型规模和输入规模分别提升248倍和212倍。

Conclusion: Pegasus通过数据平面原语和优化技术，显著提升了深度学习在数据平面上的性能和适用性。

Abstract: The paradigm of Intelligent DataPlane (IDP) embeds deep learning (DL) models
on the network dataplane to enable intelligent traffic analysis at line-speed.
However, the current use of the match-action table (MAT) abstraction on the
dataplane is misaligned with DL inference, leading to several key limitations,
including accuracy degradation, limited scale, and lack of generality. This
paper proposes Pegasus to address these limitations. Pegasus translates DL
operations into three dataplane-oriented primitives to achieve generality:
Partition, Map, and SumReduce. Specifically, Partition "divides"
high-dimensional features into multiple low-dimensional vectors, making them
more suitable for the dataplane; Map "conquers" computations on the
low-dimensional vectors in parallel with the technique of fuzzy matching, while
SumReduce "combines" the computation results. Additionally, Pegasus employs
Primitive Fusion to merge computations, improving scalability. Finally, Pegasus
adopts full precision weights with fixed-point activations to improve accuracy.
Our implementation on a P4 switch demonstrates that Pegasus can effectively
support various types of DL models, including Multi-Layer Perceptron (MLP),
Recurrent Neural Network (RNN), Convolutional Neural Network (CNN), and
AutoEncoder models on the dataplane. Meanwhile, Pegasus outperforms
state-of-the-art approaches with an average accuracy improvement of up to
22.8%, along with up to 248x larger model size and 212x larger input scale.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [747] [Improving choice model specification using reinforcement learning](https://arxiv.org/abs/2506.06410)
*Gabriel Nova,Sander van Cranenburgh,Stephane Hess*

Main category: econ.GN

TL;DR: 论文提出了一种基于深度强化学习的框架，用于动态优化离散选择模型的规格，解决了传统方法的静态性和主观性问题。


<details>
  <summary>Details</summary>
Motivation: 传统离散选择模型规格的试错过程依赖专家经验、耗时且主观，现有元启发式方法无法动态调整搜索策略或转移知识。

Method: 采用深度强化学习框架，通过代理模型估计和基于拟合优度与简约性的奖励机制，动态调整模型规格。

Result: 实验表明，该框架能动态适应不同数据生成过程，具有鲁棒性和潜在的可转移性，无需先验领域知识。

Conclusion: 该框架为离散选择模型规格提供了一种自动化、动态化的解决方案，显著提升了效率和适应性。

Abstract: Discrete choice modelling is a theory-driven modelling framework for
understanding and forecasting choice behaviour. To obtain behavioural insights,
modellers test several competing model specifications in their attempts to
discover the 'true' data generation process. This trial-and-error process
requires expertise, is time-consuming, and relies on subjective theoretical
assumptions. Although metaheuristics have been proposed to assist choice
modellers, they treat model specification as a classic optimisation problem,
relying on static strategies, applying predefined rules, and neglecting
outcomes from previous estimated models. As a result, current metaheuristics
struggle to prioritise promising search regions, adapt exploration dynamically,
and transfer knowledge to other modelling tasks. To address these limitations,
we introduce a deep reinforcement learning-based framework where an 'agent'
specifies models by estimating them and receiving rewards based on
goodness-of-fit and parsimony. Results demonstrate the agent dynamically adapts
its strategies to identify promising specifications across data generation
processes, showing robustness and potential transferability, without prior
domain knowledge.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [748] [Training-Free Query Optimization via LLM-Based Plan Similarity](https://arxiv.org/abs/2506.05853)
*Nikita Vasilenko,Alexander Demin,Vladimir Boorlakov*

Main category: cs.DB

TL;DR: LLM-PM利用预训练的执行计划嵌入指导SQL查询执行，无需额外训练，通过邻居投票推荐数据库提示集，实现21%的查询延迟降低。


<details>
  <summary>Details</summary>
Motivation: 探索预训练LLM嵌入在数据库查询优化中的潜力，减少模型训练需求。

Method: 提出LLM-PM框架，嵌入查询默认执行计划，通过k近邻和投票推荐提示集，辅以一致性检查和回退机制。

Result: 在JOB-CEB基准测试中，平均减少21%查询延迟。

Conclusion: LLM嵌入为查询优化提供了实用改进，开辟了无训练嵌入优化系统的新方向。

Abstract: Large language model (LLM) embeddings offer a promising new avenue for
database query optimization. In this paper, we explore how pre-trained
execution plan embeddings can guide SQL query execution without the need for
additional model training. We introduce LLM-PM (LLM-based Plan Mapping), a
framework that embeds the default execution plan of a query, finds its k
nearest neighbors among previously executed plans, and recommends database
hintsets based on neighborhood voting. A lightweight consistency check
validates the selected hint, while a fallback mechanism searches the full hint
space when needed. Evaluated on the JOB-CEB benchmark using OpenGauss, LLM-PM
achieves an average speed-up of 21% query latency reduction. This work
highlights the potential of LLM-powered embeddings to deliver practical
improvements in query performance and opens new directions for training-free,
embedding-based optimizer guidance systems.

</details>


### [749] [Training-Free Query Optimization via LLM-Based Plan Similarity](https://arxiv.org/abs/2506.05853)
*Nikita Vasilenko,Alexander Demin,Vladimir Boorlakov*

Main category: cs.DB

TL;DR: LLM-PM利用预训练的执行计划嵌入指导SQL查询执行，无需额外训练，通过邻居投票推荐数据库提示集，平均减少21%查询延迟。


<details>
  <summary>Details</summary>
Motivation: 探索预训练的大型语言模型嵌入如何优化数据库查询执行，避免额外模型训练的开销。

Method: 提出LLM-PM框架，嵌入查询的默认执行计划，通过k近邻算法找到相似计划并推荐提示集，辅以一致性检查和回退机制。

Result: 在JOB-CEB基准测试中，使用OpenGauss实现平均21%的查询延迟降低。

Conclusion: LLM嵌入为查询优化提供了实用改进潜力，开辟了无训练、基于嵌入的优化器指导新方向。

Abstract: Large language model (LLM) embeddings offer a promising new avenue for
database query optimization. In this paper, we explore how pre-trained
execution plan embeddings can guide SQL query execution without the need for
additional model training. We introduce LLM-PM (LLM-based Plan Mapping), a
framework that embeds the default execution plan of a query, finds its k
nearest neighbors among previously executed plans, and recommends database
hintsets based on neighborhood voting. A lightweight consistency check
validates the selected hint, while a fallback mechanism searches the full hint
space when needed. Evaluated on the JOB-CEB benchmark using OpenGauss, LLM-PM
achieves an average speed-up of 21% query latency reduction. This work
highlights the potential of LLM-powered embeddings to deliver practical
improvements in query performance and opens new directions for training-free,
embedding-based optimizer guidance systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [750] [Heterogeneous Sequel-Aware Graph Neural Networks for Sequential Learning](https://arxiv.org/abs/2506.05625)
*Anushka Tiwari,Haimonti Dutta,Shahrzad Khanizadeh*

Main category: cs.IR

TL;DR: 该论文研究了基于图神经网络的推荐系统，通过引入时间序列信息（sequel-aware）提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注用户和物品的高阶嵌入及物品间相关性，但时间序列信息对推荐的影响较少被探索。

Method: 提出异构时间序列感知图神经网络（HSAL-GNNs），结合时间序列和高阶用户嵌入，并与多种算法（如Transformer、图神经网络、自编码器）进行比较。

Result: 在合成和真实数据集上的实验表明，引入时间序列信息显著提升了推荐性能。

Conclusion: 时间序列信息对推荐系统有重要价值，HSAL-GNNs在性能上优于或媲美不考虑序列信息的图推荐系统。

Abstract: Graph-based recommendation systems use higher-order user and item embeddings
for next-item predictions. Dynamically adding collaborative signals from
neighbors helps to use similar users' preferences during learning. While
item-item correlations and their impact on recommendations have been studied,
the efficacy of temporal item sequences for recommendations is much less
explored. In this paper, we examine temporal item sequence (sequel-aware)
embeddings along with higher-order user embeddings and show that sequel-aware
Graph Neural Networks have better (or comparable) recommendation performance
than graph-based recommendation systems that do not consider sequel
information. Extensive empirical results comparing Heterogeneous Sequel-aware
Graph Neural Networks (HSAL-GNNs) to other algorithms for sequential learning
(such as transformers, graph neural networks, auto-encoders) are presented on
three synthetic and three real-world datasets. Our results indicate that the
incorporation of sequence information from items greatly enhances
recommendations.

</details>


### [751] [Heterogeneous Sequel-Aware Graph Neural Networks for Sequential Learning](https://arxiv.org/abs/2506.05625)
*Anushka Tiwari,Haimonti Dutta,Shahrzad Khanizadeh*

Main category: cs.IR

TL;DR: 论文研究了时序物品序列（sequel-aware）嵌入与高阶用户嵌入的结合，证明其推荐性能优于或等同于不考虑时序信息的图推荐系统。


<details>
  <summary>Details</summary>
Motivation: 探索时序物品序列对推荐系统的影响，填补了现有研究中时序信息利用不足的空白。

Method: 提出Heterogeneous Sequel-aware Graph Neural Networks (HSAL-GNNs)，结合时序物品序列和高阶用户嵌入，与其他序列学习算法（如Transformer、图神经网络、自编码器）进行比较。

Result: 在三个合成和三个真实数据集上的实验表明，引入物品序列信息显著提升了推荐性能。

Conclusion: 时序物品序列信息对推荐系统有重要价值，HSAL-GNNs在推荐任务中表现优异。

Abstract: Graph-based recommendation systems use higher-order user and item embeddings
for next-item predictions. Dynamically adding collaborative signals from
neighbors helps to use similar users' preferences during learning. While
item-item correlations and their impact on recommendations have been studied,
the efficacy of temporal item sequences for recommendations is much less
explored. In this paper, we examine temporal item sequence (sequel-aware)
embeddings along with higher-order user embeddings and show that sequel-aware
Graph Neural Networks have better (or comparable) recommendation performance
than graph-based recommendation systems that do not consider sequel
information. Extensive empirical results comparing Heterogeneous Sequel-aware
Graph Neural Networks (HSAL-GNNs) to other algorithms for sequential learning
(such as transformers, graph neural networks, auto-encoders) are presented on
three synthetic and three real-world datasets. Our results indicate that the
incorporation of sequence information from items greatly enhances
recommendations.

</details>


### [752] [Preference-based learning for news headline recommendation](https://arxiv.org/abs/2506.06334)
*Alexandre Bouras,Audrey Durand,Richard Khoury*

Main category: cs.IR

TL;DR: 研究通过偏好学习优化新闻标题推荐，发现噪声环境下显式探索可能不必要。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过偏好学习优化新闻标题推荐，并研究翻译对用户参与度预测的影响。

Method: 使用真实用户互动数据，在上下文老虎机设置下训练标题推荐代理。

Result: 结果表明，在噪声环境下，显式探索可能不必要，可采用更简单高效的策略。

Conclusion: 研究为实践中简化但高效的推荐策略提供了可能性。

Abstract: This study explores strategies for optimizing news headline recommendations
through preference-based learning. Using real-world data of user interactions
with French-language online news posts, we learn a headline recommender agent
under a contextual bandit setting. This allows us to explore the impact of
translation on engagement predictions, as well as the benefits of different
interactive strategies on user engagement during data collection. Our results
show that explicit exploration may not be required in the presence of noisy
contexts, opening the door to simpler but efficient strategies in practice.

</details>


### [753] [Infinity Search: Approximate Vector Search with Projections on q-Metric Spaces](https://arxiv.org/abs/2506.06557)
*Antonio Pariente,Ignacio Hounie,Santiago Segarra,Alejandro Ribeiro*

Main category: cs.IR

TL;DR: 论文提出了一种利用q-度量空间的投影方法，通过强化三角不等式减少精确搜索的比较次数，使经典度量树算法在高维数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有向量搜索算法忽略了向量嵌入的度量结构，未充分利用其潜在特性。

Method: 提出一种投影方法，将任意相似性度量的向量数据集嵌入q-度量空间，并学习近似投影以高效转换查询点。

Result: 实验表明，该方法使经典度量树算法在高维文本和图像向量嵌入中达到与先进搜索方法竞争的性能。

Conclusion: 通过利用q-度量空间的特性，经典算法在高维数据中也能实现高效搜索。

Abstract: Despite the ubiquity of vector search applications, prevailing search
algorithms overlook the metric structure of vector embeddings, treating it as a
constraint rather than exploiting its underlying properties. In this paper, we
demonstrate that in $q$-metric spaces, metric trees can leverage a stronger
version of the triangle inequality to reduce comparisons for exact search.
Notably, as $q$ approaches infinity, the search complexity becomes logarithmic.
Therefore, we propose a novel projection method that embeds vector datasets
with arbitrary dissimilarity measures into $q$-metric spaces while preserving
the nearest neighbor. We propose to learn an approximation of this projection
to efficiently transform query points to a space where euclidean distances
satisfy the desired properties. Our experimental results with text and image
vector embeddings show that learning $q$-metric approximations enables classic
metric tree algorithms -- which typically underperform with high-dimensional
data -- to achieve competitive performance against state-of-the-art search
methods.

</details>


### [754] [Correcting for Position Bias in Learning to Rank: A Control Function Approach](https://arxiv.org/abs/2506.06989)
*Md Aminul Islam,Kathryn Vasilaky,Elena Zheleva*

Main category: cs.IR

TL;DR: 提出了一种基于控制函数的两阶段方法，用于纠正隐式反馈数据中的位置偏差，无需点击或倾向模型知识，且适用于非线性排名模型。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈数据（如用户点击）存在位置偏差，直接用于学习排名系统会导致性能不佳。

Method: 采用两阶段过程，第一阶段利用排名过程的残差外生变量纠正第二阶段点击方程中的位置偏差。

Result: 实验结果表明，该方法在纠正位置偏差方面优于现有方法。

Conclusion: 该方法通用性强，可应用于任何先进排名算法，且无需无偏验证数据即可优化模型。

Abstract: Implicit feedback data, such as user clicks, is commonly used in
learning-to-rank (LTR) systems because it is easy to collect and it often
reflects user preferences. However, this data is prone to various biases, and
training an LTR system directly on biased data can result in suboptimal ranking
performance. One of the most prominent and well-studied biases in implicit
feedback data is position bias, which occurs because users are more likely to
interact with higher-ranked documents regardless of their true relevance. In
this paper, we propose a novel control function-based method that accounts for
position bias in a two-stage process. The first stage uses exogenous variation
from the residuals of the ranking process to correct for position bias in the
second stage click equation. Unlike previous position bias correction methods,
our method does not require knowledge of the click or propensity model and
allows for nonlinearity in the underlying ranking model. Moreover, our method
is general and allows for debiasing any state-of-the-art ranking algorithm by
plugging it into the second stage. We also introduce a technique to debias
validation clicks for hyperparameter tuning to select the optimal model in the
absence of unbiased validation data. Experimental results demonstrate that our
method outperforms state-of-the-art approaches in correcting for position bias.

</details>


### [755] [RADAR: Recall Augmentation through Deferred Asynchronous Retrieval](https://arxiv.org/abs/2506.07261)
*Amit Jaspal,Qian Dang,Ajantha Ramineni*

Main category: cs.IR

TL;DR: RADAR框架通过异步离线计算预排名更大候选集，显著提升召回率，并在在线测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模推荐系统中初始检索阶段难以从海量内容中精准筛选高吸引力候选的问题。

Method: 引入RADAR框架，利用离线计算预排名更大候选集，并在在线推理中直接使用这些高质量候选。

Result: 离线实验显示召回率提升2倍，在线A/B测试验证了0.8%的参与度提升。

Conclusion: RADAR是一种在严格在线服务约束下提升推荐质量的有效方法。

Abstract: Modern large-scale recommender systems employ multi-stage ranking funnel
(Retrieval, Pre-ranking, Ranking) to balance engagement and computational
constraints (latency, CPU). However, the initial retrieval stage, often relying
on efficient but less precise methods like K-Nearest Neighbors (KNN), struggles
to effectively surface the most engaging items from billion-scale catalogs,
particularly distinguishing highly relevant and engaging candidates from merely
relevant ones. We introduce Recall Augmentation through Deferred Asynchronous
Retrieval (RADAR), a novel framework that leverages asynchronous, offline
computation to pre-rank a significantly larger candidate set for users using
the full complexity ranking model. These top-ranked items are stored and
utilized as a high-quality retrieval source during online inference, bypassing
online retrieval and pre-ranking stages for these candidates. We demonstrate
through offline experiments that RADAR significantly boosts recall (2X
Recall@200 vs DNN retrieval baseline) by effectively combining a larger
retrieved candidate set with a more powerful ranking model. Online A/B tests
confirm a +0.8% lift in topline engagement metrics, validating RADAR as a
practical and effective method to improve recommendation quality under strict
online serving constraints.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [756] [Sentiment Analysis in Learning Management Systems Understanding Student Feedback at Scale](https://arxiv.org/abs/2506.05490)
*Mohammed Almutairi*

Main category: cs.HC

TL;DR: 论文探讨了在新冠疫情后教育转向在线平台时，如何通过情感分析弥补师生非语言沟通的缺失，提升教育体验。


<details>
  <summary>Details</summary>
Motivation: 疫情导致教育转向在线平台，非语言沟通缺失，依赖语言反馈降低了教育效果。

Method: 研究结合数据准备、特征选择和深度神经网络（含词嵌入、LSTM和注意力机制），并与逻辑回归基线模型对比。

Result: 模型旨在通过情感分析理解学生反馈的情感背景，弥补师生沟通差距。

Conclusion: 研究为在线教育提供了改进师生沟通的新方法，提升教育质量。

Abstract: During the wake of the Covid-19 pandemic, the educational paradigm has
experienced a major change from in person learning traditional to online
platforms. The change of learning convention has impacted the teacher-student
especially in non-verbal communication. The absent of non-verbal communication
has led to a reliance on verbal feedback which diminished the efficacy of the
educational experience. This paper explores the integration of sentiment
analysis into learning management systems (LMS) to bridge the student-teacher's
gap by offering an alternative approach to interpreting student feedback beyond
its verbal context. The research involves data preparation, feature selection,
and the development of a deep neural network model encompassing word embedding,
LSTM, and attention mechanisms. This model is compared against a logistic
regression baseline to evaluate its efficacy in understanding student feedback.
The study aims to bridge the communication gap between instructors and students
in online learning environments, offering insights into the emotional context
of student feedback and ultimately improving the quality of online education.

</details>


### [757] [A Novel, Human-in-the-Loop Computational Grounded Theory Framework for Big Social Data](https://arxiv.org/abs/2506.06083)
*Lama Alqazlan,Zheng Fang,Michael Castelle,Rob Procter*

Main category: cs.HC

TL;DR: 论文提出了一种结合人类参与的计算扎根理论（CGT）框架，用于大规模定性数据分析，解决了传统方法难以扩展的问题，并通过Reddit数据集验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大数据时代下，传统定性分析方法难以应对大规模数据集，且机器学习工具的可信度问题亟待解决。

Method: 提出了一种‘人类参与循环’的计算扎根理论框架，结合ML和NLP工具，同时保持研究者对分析过程的控制。

Result: 在Reddit数据集上测试了该框架，成功分析了零工经济中导师的经验。

Conclusion: CGT框架既保留了扎根理论的严谨性，又解决了大规模定性数据分析的扩展性问题，增强了结果的可信度。

Abstract: The availability of big data has significantly influenced the possibilities
and methodological choices for conducting large-scale behavioural and social
science research. In the context of qualitative data analysis, a major
challenge is that conventional methods require intensive manual labour and are
often impractical to apply to large datasets. One effective way to address this
issue is by integrating emerging computational methods to overcome scalability
limitations. However, a critical concern for researchers is the trustworthiness
of results when Machine Learning (ML) and Natural Language Processing (NLP)
tools are used to analyse such data. We argue that confidence in the
credibility and robustness of results depends on adopting a 'human-in-the-loop'
methodology that is able to provide researchers with control over the
analytical process, while retaining the benefits of using ML and NLP. With this
in mind, we propose a novel methodological framework for Computational Grounded
Theory (CGT) that supports the analysis of large qualitative datasets, while
maintaining the rigour of established Grounded Theory (GT) methodologies. To
illustrate the framework's value, we present the results of testing it on a
dataset collected from Reddit in a study aimed at understanding tutors'
experiences in the gig economy.

</details>


### [758] [Sentiment Analysis in Learning Management Systems Understanding Student Feedback at Scale](https://arxiv.org/abs/2506.05490)
*Mohammed Almutairi*

Main category: cs.HC

TL;DR: 论文探讨了在新冠疫情期间，教育模式从传统面对面转向在线平台后，非语言沟通缺失导致教育效果下降的问题，提出通过情感分析技术结合深度学习模型来改善学生反馈的理解。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情导致教育模式转向在线平台，非语言沟通的缺失影响了师生互动效果，亟需一种新方法来弥补这一沟通鸿沟。

Method: 研究包括数据准备、特征选择，并开发了一个结合词嵌入、LSTM和注意力机制的深度神经网络模型，与逻辑回归基线进行比较。

Result: 模型旨在更准确地理解学生反馈的情感背景，为在线教育提供更有效的师生沟通工具。

Conclusion: 通过情感分析技术，研究为在线教育中的师生沟通提供了新思路，有望提升教育质量。

Abstract: During the wake of the Covid-19 pandemic, the educational paradigm has
experienced a major change from in person learning traditional to online
platforms. The change of learning convention has impacted the teacher-student
especially in non-verbal communication. The absent of non-verbal communication
has led to a reliance on verbal feedback which diminished the efficacy of the
educational experience. This paper explores the integration of sentiment
analysis into learning management systems (LMS) to bridge the student-teacher's
gap by offering an alternative approach to interpreting student feedback beyond
its verbal context. The research involves data preparation, feature selection,
and the development of a deep neural network model encompassing word embedding,
LSTM, and attention mechanisms. This model is compared against a logistic
regression baseline to evaluate its efficacy in understanding student feedback.
The study aims to bridge the communication gap between instructors and students
in online learning environments, offering insights into the emotional context
of student feedback and ultimately improving the quality of online education.

</details>


### [759] [A Novel, Human-in-the-Loop Computational Grounded Theory Framework for Big Social Data](https://arxiv.org/abs/2506.06083)
*Lama Alqazlan,Zheng Fang,Michael Castelle,Rob Procter*

Main category: cs.HC

TL;DR: 论文提出了一种结合人类监督与计算方法的框架（CGT），用于分析大规模定性数据，同时保持传统扎根理论的严谨性。


<details>
  <summary>Details</summary>
Motivation: 大数据时代下，传统定性分析方法难以应对大规模数据集，且计算方法的可信度问题亟待解决。

Method: 提出‘人在循环’（human-in-the-loop）方法，结合ML和NLP工具，开发了计算扎根理论（CGT）框架。

Result: 在Reddit数据集上测试了该框架，成功分析了零工经济中导师的经验。

Conclusion: CGT框架能够有效解决大规模定性数据分析的可扩展性和可信度问题。

Abstract: The availability of big data has significantly influenced the possibilities
and methodological choices for conducting large-scale behavioural and social
science research. In the context of qualitative data analysis, a major
challenge is that conventional methods require intensive manual labour and are
often impractical to apply to large datasets. One effective way to address this
issue is by integrating emerging computational methods to overcome scalability
limitations. However, a critical concern for researchers is the trustworthiness
of results when Machine Learning (ML) and Natural Language Processing (NLP)
tools are used to analyse such data. We argue that confidence in the
credibility and robustness of results depends on adopting a 'human-in-the-loop'
methodology that is able to provide researchers with control over the
analytical process, while retaining the benefits of using ML and NLP. With this
in mind, we propose a novel methodological framework for Computational Grounded
Theory (CGT) that supports the analysis of large qualitative datasets, while
maintaining the rigour of established Grounded Theory (GT) methodologies. To
illustrate the framework's value, we present the results of testing it on a
dataset collected from Reddit in a study aimed at understanding tutors'
experiences in the gig economy.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [760] [Partially-Supervised Neural Network Model For Quadratic Multiparametric Programming](https://arxiv.org/abs/2506.05567)
*Fuat Can Beylunioglu,Mehrdad Pirnia,P. Robert Duimering*

Main category: math.OC

TL;DR: 本文提出了一种部分监督神经网络（PSNN）架构，用于解决多参数二次优化问题（mp-QP），通过直接利用优化问题的数学结构，显著减少了训练数据需求并提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络在解决mp-QP时无法提供最优且可行的预测，即使在大数据集上训练。因此，需要一种更高效的方法。

Method: 提出PSNN架构，直接从优化问题的数学性质中推导大部分模型权重，减少对训练数据的依赖。

Result: PSNN在解精度和预测速度上优于传统商业求解器和通用深度神经网络，且对极端测试数据表现良好。

Conclusion: PSNN能够快速生成最优解，适用于大规模模拟和长期规划，尤其在能源管理问题中表现出色。

Abstract: Neural Networks (NN) with ReLU activation functions are used to model
multiparametric quadratic optimization problems (mp-QP) in diverse engineering
applications. Researchers have suggested leveraging the piecewise affine
property of deep NN models to solve mp-QP with linear constraints, which also
exhibit piecewise affine behaviour. However, traditional deep NN applications
to mp-QP fall short of providing optimal and feasible predictions, even when
trained on large datasets. This study proposes a partially-supervised NN (PSNN)
architecture that directly represents the mathematical structure of the global
solution function. In contrast to generic NN training approaches, the proposed
PSNN method derives a large proportion of model weights directly from the
mathematical properties of the optimization problem, producing more accurate
solutions despite significantly smaller training data sets. Many energy
management problems are formulated as QP, so we apply the proposed approach to
energy systems (specifically DC optimal power flow) to demonstrate proof of
concept. Model performance in terms of solution accuracy and speed of
predictions was compared against a commercial solver and a generic Deep NN
model based on classical training. Results show KKT sufficient conditions for
PSNN consistently outperform generic NN architectures with classical training
using far less data, including when tested on extreme, out-of-training
distribution test data. Given its speed advantages over traditional solvers,
the PSNN model can quickly produce optimal and feasible solutions within a
second for millions of input parameters sampled from a distribution of
stochastic demands and renewable generator dispatches, which can be used for
simulations and long term planning.

</details>


### [761] [Policy Optimization for Continuous-time Linear-Quadratic Graphon Mean Field Games](https://arxiv.org/abs/2506.05894)
*Philipp Plank,Yufei Zhang*

Main category: math.OC

TL;DR: 提出了一种针对连续时间有限视野线性二次图平均场游戏（GMFGs）的策略优化框架，通过双层优化算法实现纳什均衡的全局收敛。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习在大规模动态游戏中的可扩展性问题，同时捕捉玩家间的异质性。

Method: 设计了一种高效的策略参数化方法，将每个玩家的策略表示为私有状态的仿射函数，并开发了双层优化算法，交替进行策略梯度更新和分布更新。

Result: 证明了策略梯度步骤对最佳响应策略的线性收敛性，并建立了算法对纳什均衡的全局收敛性。

Conclusion: 数值实验验证了算法在不同图结构、噪声水平和动作频率下的收敛性和鲁棒性。

Abstract: Multi-agent reinforcement learning, despite its popularity and empirical
success, faces significant scalability challenges in large-population dynamic
games. Graphon mean field games (GMFGs) offer a principled framework for
approximating such games while capturing heterogeneity among players. In this
paper, we propose and analyze a policy optimization framework for
continuous-time, finite-horizon linear-quadratic GMFGs. Exploiting the
structural properties of GMFGs, we design an efficient policy parameterization
in which each player's policy is represented as an affine function of their
private state, with a shared slope function and player-specific intercepts. We
develop a bilevel optimization algorithm that alternates between policy
gradient updates for best-response computation under a fixed population
distribution, and distribution updates using the resulting policies. We prove
linear convergence of the policy gradient steps to best-response policies and
establish global convergence of the overall algorithm to the Nash equilibrium.
The analysis relies on novel landscape characterizations over
infinite-dimensional policy spaces. Numerical experiments demonstrate the
convergence and robustness of the proposed algorithm under varying graphon
structures, noise levels, and action frequencies.

</details>


### [762] [Convergence of linear programming hierarchies for Gibbs states of spin systems](https://arxiv.org/abs/2506.06125)
*Hamza Fawzi,Omar Fawzi*

Main category: math.OC

TL;DR: 论文研究了在自旋系统的Gibbs分布下计算局部函数期望值的两种线性规划层次方法，分别基于局部自旋翻转等式和马尔可夫链，证明了在特定条件下的快速收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决在自旋系统中计算局部函数期望值的问题，提供一种无需先验收敛分析的严格上下界方法。

Method: 提出两种线性规划层次方法：一是基于局部自旋翻转等式，二是基于马尔可夫链。证明了在空间混合或马尔可夫链快速混合条件下的收敛性。

Result: 两种方法在特定条件下能快速收敛，且能提供严格的上下界，计算复杂度为拟多项式。

Conclusion: 该方法相比蒙特卡洛方法具有优势，适用于任何系统且无需先验收敛分析。

Abstract: We consider the problem of computing expectation values of local functions
under the Gibbs distribution of a spin system. In particular, we study two
families of linear programming hierarchies for this problem. The first
hierarchy imposes local spin flip equalities and has been considered in the
bootstrap literature in high energy physics. For this hierarchy, we prove fast
convergence under a spatial mixing (decay of correlations) condition. This
condition is satisfied for example above the critical temperature for Ising
models on a $d$-dimensional grid. The second hierarchy is based on a Markov
chain having the Gibbs state as a fixed point and has been studied in the
optimization literature and more recently in the bootstrap literature. For this
hierarchy, we prove fast convergence provided the Markov chain mixes rapidly.
Both hierarchies lead to an $\varepsilon$-approximation for local expectation
values using a linear program of size quasi-polynomial in $n/\varepsilon$,
where $n$ is the total number of sites, provided the interactions can be
embedded in a $d$-dimensional grid with constant $d$. Compared to standard
Monte Carlo methods, an advantage of this approach is that it always (i.e., for
any system) outputs rigorous upper and lower bounds on the expectation value of
interest, without needing an a priori analysis of the convergence speed.

</details>


### [763] [Partially-Supervised Neural Network Model For Quadratic Multiparametric Programming](https://arxiv.org/abs/2506.05567)
*Fuat Can Beylunioglu,Mehrdad Pirnia,P. Robert Duimering*

Main category: math.OC

TL;DR: 提出了一种部分监督神经网络（PSNN）架构，直接表示全局解函数的数学结构，相比传统深度神经网络（NN）在解决多参数二次优化问题（mp-QP）时更准确且需要更少训练数据。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络在解决mp-QP问题时无法提供最优且可行的预测，即使在大数据集上训练。

Method: PSNN直接从优化问题的数学性质中推导大部分模型权重，而非依赖传统训练方法。

Result: PSNN在解精度和预测速度上优于商业求解器和传统深度神经网络，且所需训练数据更少。

Conclusion: PSNN在能源管理（如直流最优潮流）中表现出色，能快速生成最优解，适用于仿真和长期规划。

Abstract: Neural Networks (NN) with ReLU activation functions are used to model
multiparametric quadratic optimization problems (mp-QP) in diverse engineering
applications. Researchers have suggested leveraging the piecewise affine
property of deep NN models to solve mp-QP with linear constraints, which also
exhibit piecewise affine behaviour. However, traditional deep NN applications
to mp-QP fall short of providing optimal and feasible predictions, even when
trained on large datasets. This study proposes a partially-supervised NN (PSNN)
architecture that directly represents the mathematical structure of the global
solution function. In contrast to generic NN training approaches, the proposed
PSNN method derives a large proportion of model weights directly from the
mathematical properties of the optimization problem, producing more accurate
solutions despite significantly smaller training data sets. Many energy
management problems are formulated as QP, so we apply the proposed approach to
energy systems (specifically DC optimal power flow) to demonstrate proof of
concept. Model performance in terms of solution accuracy and speed of
predictions was compared against a commercial solver and a generic Deep NN
model based on classical training. Results show KKT sufficient conditions for
PSNN consistently outperform generic NN architectures with classical training
using far less data, including when tested on extreme, out-of-training
distribution test data. Given its speed advantages over traditional solvers,
the PSNN model can quickly produce optimal and feasible solutions within a
second for millions of input parameters sampled from a distribution of
stochastic demands and renewable generator dispatches, which can be used for
simulations and long term planning.

</details>


### [764] [Policy Optimization for Continuous-time Linear-Quadratic Graphon Mean Field Games](https://arxiv.org/abs/2506.05894)
*Philipp Plank,Yufei Zhang*

Main category: math.OC

TL;DR: 论文提出了一种针对连续时间、有限时域线性二次图平均场博弈的策略优化框架，通过双层优化算法实现纳什均衡的全局收敛。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在大规模动态博弈中面临可扩展性挑战，图平均场博弈提供了一种捕捉玩家异质性的近似框架。

Method: 设计了一种高效的策略参数化方法，每个玩家的策略表示为私有状态的仿射函数，并开发了双层优化算法，交替进行策略梯度更新和分布更新。

Result: 证明了策略梯度步骤线性收敛到最佳响应策略，整体算法全局收敛到纳什均衡。

Conclusion: 数值实验验证了算法在不同图结构、噪声水平和动作频率下的收敛性和鲁棒性。

Abstract: Multi-agent reinforcement learning, despite its popularity and empirical
success, faces significant scalability challenges in large-population dynamic
games. Graphon mean field games (GMFGs) offer a principled framework for
approximating such games while capturing heterogeneity among players. In this
paper, we propose and analyze a policy optimization framework for
continuous-time, finite-horizon linear-quadratic GMFGs. Exploiting the
structural properties of GMFGs, we design an efficient policy parameterization
in which each player's policy is represented as an affine function of their
private state, with a shared slope function and player-specific intercepts. We
develop a bilevel optimization algorithm that alternates between policy
gradient updates for best-response computation under a fixed population
distribution, and distribution updates using the resulting policies. We prove
linear convergence of the policy gradient steps to best-response policies and
establish global convergence of the overall algorithm to the Nash equilibrium.
The analysis relies on novel landscape characterizations over
infinite-dimensional policy spaces. Numerical experiments demonstrate the
convergence and robustness of the proposed algorithm under varying graphon
structures, noise levels, and action frequencies.

</details>


### [765] [Convergence of linear programming hierarchies for Gibbs states of spin systems](https://arxiv.org/abs/2506.06125)
*Hamza Fawzi,Omar Fawzi*

Main category: math.OC

TL;DR: 论文研究了计算自旋系统Gibbs分布下局部函数期望值的问题，提出了两种线性规划层次结构，并证明了在特定条件下的快速收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决在自旋系统中计算局部函数期望值的效率问题，特别是在高维网格中。

Method: 提出了两种线性规划层次结构：第一种基于局部自旋翻转等式，第二种基于马尔可夫链。

Result: 证明了在空间混合条件或马尔可夫链快速混合条件下，两种层次结构能快速收敛，并提供严格的上下界。

Conclusion: 该方法在无需预先分析收敛速度的情况下，能高效且严格地近似局部期望值。

Abstract: We consider the problem of computing expectation values of local functions
under the Gibbs distribution of a spin system. In particular, we study two
families of linear programming hierarchies for this problem. The first
hierarchy imposes local spin flip equalities and has been considered in the
bootstrap literature in high energy physics. For this hierarchy, we prove fast
convergence under a spatial mixing (decay of correlations) condition. This
condition is satisfied for example above the critical temperature for Ising
models on a $d$-dimensional grid. The second hierarchy is based on a Markov
chain having the Gibbs state as a fixed point and has been studied in the
optimization literature and more recently in the bootstrap literature. For this
hierarchy, we prove fast convergence provided the Markov chain mixes rapidly.
Both hierarchies lead to an $\varepsilon$-approximation for local expectation
values using a linear program of size quasi-polynomial in $n/\varepsilon$,
where $n$ is the total number of sites, provided the interactions can be
embedded in a $d$-dimensional grid with constant $d$. Compared to standard
Monte Carlo methods, an advantage of this approach is that it always (i.e., for
any system) outputs rigorous upper and lower bounds on the expectation value of
interest, without needing an a priori analysis of the convergence speed.

</details>


### [766] [Decentralized Optimization on Compact Submanifolds by Quantized Riemannian Gradient Tracking](https://arxiv.org/abs/2506.07351)
*Jun Chen,Lina Liu,Tianyi Zhu,Yong Liu,Guang Dai,Yunliang Jiang,Ivor W. Tsang*

Main category: math.OC

TL;DR: 论文提出了一种量化黎曼梯度跟踪算法（Q-RGT），用于解决紧致子流形上的去中心化优化问题，通过量化梯度减少通信瓶颈，同时保持与非量化方法相当的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 去中心化优化在紧致子流形上的效率常受通信瓶颈限制，量化梯度可缓解此问题。

Method: 提出Q-RGT算法，利用量化梯度更新局部变量，避免精确黎曼投影算子的约束。

Result: Q-RGT在量化情况下达到与非量化方法相同的收敛速度（O(1/K)），并明确量化水平与去中心化共识的下界关系。

Conclusion: Q-RGT在减少通信和计算开销的同时，性能与非量化方法相当，是首个在量化情况下实现高效收敛的算法。

Abstract: This paper considers the problem of decentralized optimization on compact
submanifolds, where a finite sum of smooth (possibly non-convex) local
functions is minimized by $n$ agents forming an undirected and connected graph.
However, the efficiency of distributed optimization is often hindered by
communication bottlenecks. To mitigate this, we propose the Quantized
Riemannian Gradient Tracking (Q-RGT) algorithm, where agents update their local
variables using quantized gradients. The introduction of quantization noise
allows our algorithm to bypass the constraints of the accurate Riemannian
projection operator (such as retraction), further improving iterative
efficiency. To the best of our knowledge, this is the first algorithm to
achieve an $\mathcal{O}(1/K)$ convergence rate in the presence of quantization,
matching the convergence rate of methods without quantization. Additionally, we
explicitly derive lower bounds on decentralized consensus associated with a
function of quantization levels. Numerical experiments demonstrate that Q-RGT
performs comparably to non-quantized methods while reducing communication
bottlenecks and computational overhead.

</details>


### [767] [Discrete and Continuous Difference of Submodular Minimization](https://arxiv.org/abs/2506.07952)
*George Orfanides,Tim Hoheisel,Marwa El Halabi*

Main category: math.OC

TL;DR: 论文研究了连续和离散域上两个子模函数差（DS）的最小化问题，扩展了之前仅针对集合函数的研究。提出了一种新的DC算法变体，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 子模函数在众多应用中广泛存在，但之前的研究主要集中在集合函数上，缺乏对连续和离散域的系统研究。

Method: 提出了一种新的DC算法变体（DCA），并将其应用于离散域和连续域（通过离散化）的DS最小化问题。

Result: 实验证明，该方法在整数压缩感知和整数最小二乘问题上优于基线方法。

Conclusion: 该研究扩展了DS最小化的适用范围，并提出了一种有效的算法，为实际应用提供了理论支持。

Abstract: Submodular functions, defined on continuous or discrete domains, arise in
numerous applications. We study the minimization of the difference of two
submodular (DS) functions, over both domains, extending prior work restricted
to set functions. We show that all functions on discrete domains and all smooth
functions on continuous domains are DS. For discrete domains, we observe that
DS minimization is equivalent to minimizing the difference of two convex (DC)
functions, as in the set function case. We propose a novel variant of the DC
Algorithm (DCA) and apply it to the resulting DC Program, obtaining comparable
theoretical guarantees as in the set function case. The algorithm can be
applied to continuous domains via discretization. Experiments demonstrate that
our method outperforms baselines in integer compressive sensing and integer
least squares.

</details>
